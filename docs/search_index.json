[
["index.html", "Encyclopedia of Quantitative Methods in R, vol. 5: Multilevel Models Welcome Blocked Notes Code and Output The Authors", " Encyclopedia of Quantitative Methods in R, vol. 5: Multilevel Models Sarah Schwartz &amp; Tyson Barrett Last updated: 2018-11-15 Welcome Backgroup and links to other volumes of this encyclopedia may be found at the Encyclopedia’s Home Website. Blocked Notes Thoughout all the eBooks in this encyclopedia, several small secitons will be blocked out in the following ways: These blocks denote an area UNDER CONSTRUCTION, so check back often. This massive undertaking started during the summer of 2018 and is far from complete. The outline of seven volumes is given above despite any one being complete. Feedback is welcome via either author’s email. These blocks denote something EXTREMELY IMPORTANT. Do NOT skip these notes as they will be used very sparingly. These blocks denote something to DOWNLOAD. This may include software installations, example datasets, or notebook code files. These blocks denote something INTERESTING. These point out information we found of interest or added value. These blocks denote LINKS to other websites. This may include instructional video clips, articles, or blog posts. We are all about NOT re-creating the wheel. If somebody else has described or illustrated a topic well, we celebrate it! Code and Output This is how \\(R\\) code is shown: 1 + 1 This is what the output of the \\(R\\) code above will look: ## [1] 2 The Authors Dr. Sarah Schwartz Dr. Tyson Barrett www.SarahSchwartzStats.com www.TysonBarrett.com Sarah.Schwartz@usu.edu Tyson.Barrett@usu.edu Statistical Consulting Studio Data Science and Discover Unit Why choose R ? Check it out: an article from Fall 2016… No more excuses: R is better than SPSS for psychology undergrads, and students agree FYI This entire encyclopedia is written in \\(R Markdown\\), using \\(R Studio\\) as the text editor and the bookdown package to turn a collection of markdown documents into a coherent whole. The book’s source code is hosted on GitHub. If you notice typos or other issues, feel free to email either of the authors. This work is licensed under the Attribution-NonCommercial-NoDerivatives 4.0 International License. "],
["formula-warehouse.html", "1 Formula Warehouse 1.1 Data Notation 1.2 Single-level Regression Analysis 1.3 Multi-level Regression Analysis 1.4 Intraclass Correlation (ICC) 1.5 Proporion of Variance Explianed 1.6 Using \\(\\LaTeX\\) for Equation Typesetting", " 1 Formula Warehouse This is the home for notation and formulas used thorugh this eBook. Most important equations will be located here. 1.1 Data Notation Sample Sizes: \\(n_j\\) = number of pupils in class \\(j\\) \\(N\\) = number of classes Indicators: \\(i \\in (1, 2, \\dots, n_j)\\) = index for pupil number \\(j \\in (1, 2, \\dots, N)\\) = index for class number Level Type of Variable Symbol pupil \\(i\\) in class \\(j\\) 1 Outcome (Dependent) \\(Y\\) \\(Y_{ij}\\) 1 Predictor (Independent) \\(X_1\\) \\(X_{1ij}\\) 1 Predictor (Independent) \\(X_2\\) \\(X_{2ij}\\) 2 Predictor (Independent) \\(Z\\) \\(Z_j\\) 1.2 Single-level Regression Analysis 1.2.1 The Only Equation Since we are don’t have or are ignoring clustering, there is only one level. Single-Level Regression Equation \\[ \\overbrace{Y_{ij}}^{Outcome} = \\underbrace{\\beta_{0}}_{\\text{Fixed}\\atop\\text{intercept}} + \\underbrace{\\beta_{1}}_{\\text{Fixed}\\atop\\text{slope } X_1} \\overbrace{X_{1ij}}^{\\text{Predictor 1}} + \\underbrace{\\beta_{2}}_{\\text{Fixed}\\atop\\text{slope } X_2} \\overbrace{X_{2ij}}^{\\text{Predictor 2}} + \\underbrace{e_{ij}}_{\\text{Random}\\atop\\text{residuals}} \\tag{Hox 2.1} \\] 1.2.2 Parameters Type Parameter of Interest Estimates This Fixed Intercept \\(\\beta_{0}\\) Fixed Slope or main effect of \\(X_1\\) \\(\\beta_{1}\\) Fixed Slope or main effect of \\(X_2\\) \\(\\beta_{2}\\) Random Residual Variance \\(var[e_{ij}]\\) \\(\\sigma^2_{e}\\) 1.2.3 Assumptions to Check The \\(e_{ij}\\)’s follow a normal distribution with a mean of \\(0\\) The \\(e_{ij}\\)’s have a constant variance (homoscedasticity) 1.3 Multi-level Regression Analysis Continue taking into account fixed slopes for two Level 1 variables, \\(X_1\\) and \\(X_2\\). 1.3.1 Level 1 Regression Equation* \\[ \\overbrace{Y_{ij}}^{\\text{Level 1}\\atop\\text{Outcome}} = \\underbrace{\\beta_{0j}}_{\\text{Level 2}\\atop\\text{intercepts}} + \\underbrace{\\beta_{1j}}_{\\text{Level 2}\\atop\\text{slopes}} \\overbrace{X_{1ij}}^{\\text{Level 1}\\atop\\text{Predictor 1}} + \\underbrace{\\beta_{2j}}_{\\text{Level 2}\\atop\\text{slopes}} \\overbrace{X_{2ij}}^{\\text{Level 1}\\atop\\text{Predictor 2}} + \\underbrace{e_{ij}}_{\\text{Random}\\atop\\text{residuals}} \\tag{Hox 2.1} \\] Now we take clustering into account and include random intercepts (\\(\\beta_{0j}\\)) and slopes (\\(\\beta_{1j}, \\beta_{2j}\\)), as well as including a single Level 2 variable, \\(Z\\) that interacts with both Level 1 variables. 1.3.2 Level 2 Regression Equations 1.3.2.1 Random Intercepts: \\[ \\overbrace{\\beta_{0j}}^{\\text{Level 2}\\atop\\text{intercepts}} = \\underbrace{\\gamma_{00}}_{\\text{Fixed}\\atop\\text{intercept}} + \\underbrace{\\gamma_{01}}_{\\text{Fixed}\\atop\\text{slope } Z} \\overbrace{Z_{j}}^{\\text{Level 2}\\atop\\text{Predictor 3}} + \\underbrace{u_{0j}}_{\\text{Intercept}\\atop\\text{residual}} \\tag{Hox 2.3} \\] 1.3.2.2 Random Slopes For the first predictor, \\(X_1\\): \\[ \\overbrace{\\beta_{1j}}^{\\text{Level 2}\\atop\\text{slopes}} = \\underbrace{\\gamma_{10}}_{\\text{Fixed}\\atop\\text{Slope } X_1} + \\underbrace{\\gamma_{11}}_{\\text{Fixed}\\atop X_1 \\times Z} \\overbrace{Z_{j}}^{\\text{Level 2}\\atop\\text{Predictor 3}} + \\underbrace{u_{1j}}_{\\text{Slope } X_1\\atop\\text{residual}} \\tag{Hox 2.4a} \\] For the second predictor, \\(X_2\\): \\[ \\overbrace{\\beta_{2j}}^{\\text{Level 2}\\atop\\text{slopes}} = \\underbrace{\\gamma_{20}}_{\\text{Fixed}\\atop\\text{Slope } X_2} + \\underbrace{\\gamma_{21}}_{\\text{Fixed}\\atop X_2 \\times Z} \\overbrace{Z_{j}}^{\\text{Level 2}\\atop\\text{Predictor 3}} + \\underbrace{u_{2j}}_{\\text{Slope } X_2\\atop\\text{residual}} \\tag{Hox 2.4a} \\] 1.3.2.3 Merging the Equations Starting with Level 1 equation (2.1) and allow the \\(\\beta\\)’s to be varry for each class and plug in the level 2 equations (2.3 and 2.4) into the level 1 equation (2.1) to make the combined equation. \\[ Y_{ij} = \\overbrace{(\\gamma_{00} + \\gamma_{01} Z_{j} + u_{0j})}^{\\beta_{0j}} + \\overbrace{(\\gamma_{10} + \\gamma_{11} Z_{j} + u_{1j})}^{\\beta_{1j}} X_{1ij} + \\overbrace{(\\gamma_{20} + \\gamma_{21} Z_{j} + u_{2j})}^{\\beta_{2j}} X_{2ij} + e_{ij} \\] Use the distributive property of multiplication to get rid of the parentheses. \\[ Y_{ij} = \\overbrace{\\gamma_{00} + \\gamma_{01} Z_{j} + u_{0j}}^{\\beta_{0j}} + \\overbrace{\\gamma_{10} X_{1ij} + \\gamma_{11} Z_{j} X_{1ij} + u_{1j} X_{1ij}}^{\\beta_{1j} \\times X_{1ij}} + \\overbrace{\\gamma_{20} X_{2ij} + \\gamma_{21} Z_{j} X_{2ij} + u_{2j} X_{2ij}}^{\\beta_{2j} \\times X_{2ij}} + e_{ij} \\] 1.3.3 Combinded, Multilevel Regression Equation Collect ‘like-terms’ (i.e. get the \\(\\gamma\\)’s together and the \\(u\\)’s together) Combinded, Multilevel Regression Equation - Generic \\[ Y_{ij} = \\overbrace{\\gamma_{00} + \\gamma_{10} X_{1ij} + \\gamma_{20} X_{2ij} + \\gamma_{01} Z_{j} + \\gamma_{11} Z_{j} X_{1ij} + \\gamma_{21} Z_{j} X_{2ij}}^{\\text{Fixed part}\\atop\\text{Deterministic}} + \\\\ \\underbrace{u_{0j} + u_{1j} X_{1ij} + u_{2j} X_{2ij} + e_{ij} }_{\\text{Random part}\\atop\\text{Stochastic}} \\tag{Hox 2.5} \\] 1.3.4 Parameters Type Parameter of Interest Estimates This Fixed Intercept \\(\\gamma_{00}\\) Fixed Main Effect of \\(X_1\\) \\(\\gamma_{10}\\) Fixed Main Effect of \\(X_2\\) \\(\\gamma_{20}\\) Fixed Main Effect of \\(Z\\) \\(\\gamma_{01}\\) Fixed Cross-Level interaction between \\(X_1\\) and \\(Z\\) \\(\\gamma_{11}\\) Fixed Cross-Level interaction between \\(X_2\\) and \\(Z\\) \\(\\gamma_{21}\\) Random Variance in random intercepts, \\(var[u_{0j}]\\) \\(\\sigma^2_{u0}\\) Random Variance in random slope of \\(X_1\\), \\(var[u_{1j}]\\) \\(\\sigma^2_{u1}\\) Random Variance in random slope of \\(X_2\\), \\(var[u_{2j}]\\) \\(\\sigma^2_{u2}\\) Random Covariance between random intercepts and random slope of \\(X_1\\), \\(cov[u_{0j}, u_{1j}]\\) \\(\\sigma^2_{u01}\\) Random Covariance between random intercepts and random slope of \\(X_2\\), \\(cov[u_{0j}, u_{2j}]\\) \\(\\sigma^2_{u02}\\) Random Covariance between random slopes of \\(X_1\\) and \\(X_2\\), \\(cov[u_{1j}, u_{2j}]\\) \\(\\sigma^2_{u12}\\) Random Residual Variance \\(var[e_{ij}]\\) \\(\\sigma^2_{e}\\) The \\(u_{1j}\\) and \\(u_{2j}\\) terms allow for heteroscedasticity by fitting different error terms for different values of \\(X_1\\) and \\(X_2\\). The HOV assumption is that AFTER accounting for this, the remaining residuals are HOV. 1.4 Intraclass Correlation (ICC) 1.4.1 Two Level Models Combined, Multilevel Model Equation - Null Model, 2 levels \\[ \\overbrace{Y_{ij}}^{Outcome} = \\underbrace{\\gamma_{00}}_{\\text{Fixed}\\atop\\text{intercept}} + \\underbrace{u_{0j}}_{\\text{Random}\\atop\\text{intercepts}} + \\underbrace{e_{ij}}_{\\text{Random}\\atop\\text{residuals}} \\tag{Hox 2.8} \\] Although the Null model above does not explain any variance in the dependent variable, since there are no independent variables, it does decompose (i.e. divide up) the variance into two pieces. We can compute the amount of total variance in the outcome that is attribute to the clustering of Level 1 untis (micro-units) into clusters of Level 2 units (macro-units) verses the total variance. Intraclass Correlation (ICC) Formula, 2 level model \\[ \\overbrace{\\rho}^{\\text{ICC}} = \\frac{\\overbrace{\\sigma^2_{u0}}^{\\text{Random Intercept}\\atop\\text{Variance}}} {\\underbrace{\\sigma^2_{u0}+\\sigma^2_{e}}_{\\text{Total}\\atop\\text{Variance}}} \\tag{Hox 2.9} \\] 1.4.2 Three Level Models Indicators: \\(i\\) = index for units in the lowest level (Level 1) \\(j\\) = index for units in the middle level (Level 2) \\(k\\) = index for units in the highest level (Level 3) Combined, Multilevel Model Equation - Null Model, 3 levels \\[ \\overbrace{Y_{ijk}}^{Outcome} = \\underbrace{\\gamma_{000}}_{\\text{Fixed}\\atop\\text{intercept}} + \\underbrace{v_{0k }}_{\\text{Random Intercepts}\\atop\\text{Level 3}} + \\underbrace{u_{0jk}}_{\\text{Random Intercepts}\\atop\\text{Level 2}} + \\underbrace{e_{ijk}}_{\\text{Random}\\atop\\text{residuals}} \\tag{Hox 2.15} \\] If you are interested in teh decomposition of variance across all levels, use the Davis and Scott method: Intraclass Correlation (ICC) Formula, 3 level model - Davis and Scott Method \\[ \\overbrace{\\rho_{mid}}^{\\text{ICC}\\atop\\text{at level 2}} = \\frac{\\overbrace{\\sigma^2_{u0}}^{\\text{Random Intercept}\\atop\\text{Variance Level 2}}} {\\underbrace{\\sigma^2_{v0}+\\sigma^2_{u0}+\\sigma^2_{e}}_{\\text{Total}\\atop\\text{Variance}}} \\tag{Hox 2.16} \\] \\[ \\overbrace{\\rho_{top}}^{\\text{ICC}\\atop\\text{ at level 3}} = \\frac{\\overbrace{\\sigma^2_{u0}}^{\\text{Random Intercept}\\atop\\text{Variance Level 3}}} {\\underbrace{\\sigma^2_{v0}+\\sigma^2_{u0}+\\sigma^2_{e}}_{\\text{Total}\\atop\\text{Variance}}} \\tag{Hox 2.17} \\] If you would like to estimate the expected (population) correlation between two randomly chosen elements of the same group: Intraclass Correlation (ICC) Formula, 3 level model - Siddiqui Method \\[ \\overbrace{\\rho_{mid}}^{\\text{ICC}\\atop\\text{at level 2}} = \\frac{\\overbrace{\\sigma^2_{v0}+\\sigma^2_{u0}}^{\\text{Random variance}\\atop\\text{at levels 2 &amp;amp; 3}}} {\\underbrace{\\sigma^2_{v0}+\\sigma^2_{u0}+\\sigma^2_{e}}_{\\text{Total}\\atop\\text{Variance}}} \\tag{Hox 2.18} \\] \\[ \\overbrace{\\rho_{top}}^{\\text{ICC}\\atop\\text{ at level 3}} = \\frac{\\overbrace{\\sigma^2_{u0}}^{\\text{Random variance}\\atop\\text{at only level 3}}} {\\underbrace{\\sigma^2_{v0}+\\sigma^2_{u0}+\\sigma^2_{e}}_{\\text{Total}\\atop\\text{Variance}}} \\tag{Hox 2.19} \\] 1.5 Proporion of Variance Explianed See pages 61-63 of Hox, Moerbeek, and Van de Schoot (2017) http://journals.sagepub.com/doi/10.1177/1094428114541701 Analogous to multiple \\(R^2\\) - done seperately by level \\(BL\\) = Baseline model (Null) \\(MC\\) = Model to Compare to 1.5.1 Level 1 Variance Explained MODELS SHOULD NOT INCLUDE ANY RANOM EFFECTS, OTHER THAN RANDOM INTERCEPTS. Different approaches differ in values and meaning. 1.5.1.1 Snijders and Bosker Explained variance is a proportion of the total variance, because in principle first-level variables can explain all variation, including the variation at the second level. Correction removes the spurious increase in \\(R^2\\) when random slopes are added to a model Snijders and Bosker Formula - Level 1 Random Intercepts Models Only, address potential negative \\(R^2\\) issue \\[ R^2_1 = 1 - \\frac{\\sigma^2_{e-MC} + \\sigma^2_{u0-MC}} {\\sigma^2_{e-BL} + \\sigma^2_{u0-BL}} \\] 1.5.1.2 Raudenbush and Bryk Explained variance is a proportion of first-level variance only A good option when the multilevel sampling process is is close to two-stage simple random sampling Raudenbush and Bryk Approximate Formula - Level 1 approximate \\[ approx \\;R^2_1 = \\frac{\\sigma^2_{e-BL} - \\sigma^2_{e-MC}} {\\sigma^2_{e-BL} } \\tag{Hox 4.8} \\] 1.5.2 Level 2 Variance Explined 1.5.2.1 Snijders and Bosker Snijders and Bosker Formula Extended - Level 2 \\[ R^2_2 = 1 - \\frac{\\frac{\\sigma^2_{e-MC}}{B} + \\sigma^2_{u0-MC}} {\\frac{\\sigma^2_{e-BL}}{B} + \\sigma^2_{u0-BL}} \\] \\(B\\) is the average size of the Level 2 units (schools). Technically, you should use the harmonic mean, but unless the clusters differ greatly in size, it doesn’t make a huge difference. 1.5.2.2 Raudenbush and Bryk Raudenbush and Bryk Approximate Formula - Level 2 \\[ approx \\; R^2_s = \\frac{\\sigma^2_{u0-BL} - \\sigma^2_{u0-MC}} {\\sigma^2_{u0-BL} } \\tag{Hox 4.9} \\] 1.6 Using \\(\\LaTeX\\) for Equation Typesetting R markdown is a user friendly, simplified language that allows for more complex formating utilizing standard \\(\\LaTeX\\) code. A great resource for learning how to many common tasks in \\(\\LaTeX\\) is the Sharewebsite. Specific mathematical equation documentation may be found on the Mathematical Expressions subpage. There are also many websites that offer Point-n-click interfaces to build \\(\\LaTeX\\) equations, including: Host Math, Code Cogs, LaTeX 4 Technics, and Sci-Weavers "],
["intro-2-level-model-example-pupil-popularity.html", "2 Intro 2-level Model Example: Pupil Popularity 2.1 Background 2.2 Exploratory Data Analysis 2.3 Single-level Regression Analysis 2.4 Multi-level Regression Analysis", " 2 Intro 2-level Model Example: Pupil Popularity library(tidyverse) library(haven) # read in SPSS dataset library(furniture) # nice table1() descriptives library(stargazer) # display nice tables: summary &amp; regression library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(RColorBrewer) # nice color palettes for plots library(gridExtra) # place ggplots together as one plot library(psych) # contains some useful functions, like headTail library(car) # Companion to Applied Regression library(nlme) # non-linear mixed-effects models library(lme4) # Linear, generalized linear, &amp; nonlinear mixed models library(lmerTest) # Tests on lmer objects library(HLMdiag) # Diagnostic Tools for for nlme &amp; lmer4 library(sjstats) # ICC calculations library(optimx) # Different optimizers to solve mlm&#39;s 2.1 Background The text “Multilevel Analysis: Techniques and Applications, Third Edition” (Hox, Moerbeek, and Van de Schoot 2017) has a companion website which includes links to all the data files used throughout the book (housed on the book’s GitHub repository). The following example is used through out Hox, Moerbeek, and Van de Schoot (2017)’s chapater 2. From Appendix E: The popularity data in popular2.sav are simulated data for 2000 pupils in 100 schools. The purpose is to offer a very simple example for multilevel regression analysis. The main outcome variable is the pupil popularity, a popularity rating on a scale of 1-10 derived by a sociometric procedure. Typically, a sociometric procedure asks all pupils in a class to rate all the other pupils, and then assigns the average received popularity rating to each pupil. Because of the sociometric procedure, group effects as apparent from higher level variance components are rather strong. There is a second outcome variable: pupil popularity as rated by their teacher, on a scale from 1-10. The explanatory variables are pupil gender (boy=0, girl=1), pupil extraversion (10-point scale) and teacher experience in years. The popularity data have been generated to be a ‘nice’ well-behaved data set: the sample sizes at both levels are sufficient, the residuals have a normal distribution, and the multilevel effects are strong. data_raw &lt;- haven::read_sav(&quot;https://github.com/MultiLevelAnalysis/Datasets-third-edition-Multilevel-book/raw/master/chapter%202/popularity/SPSS/popular2.sav&quot;) %&gt;% haven::as_factor() # retain the labels from SPSS --&gt; factor tibble::glimpse(data_raw) Observations: 2,000 Variables: 15 $ pupil &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1... $ class &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1... $ extrav &lt;dbl&gt; 5, 7, 4, 3, 5, 4, 5, 4, 5, 5, 5, 5, 5, 5, 5, 6, 4, 4... $ sex &lt;fct&gt; girl, boy, girl, girl, girl, boy, boy, boy, boy, boy... $ texp &lt;dbl&gt; 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, ... $ popular &lt;dbl&gt; 6.3, 4.9, 5.3, 4.7, 6.0, 4.7, 5.9, 4.2, 5.2, 3.9, 5.... $ popteach &lt;dbl&gt; 6, 5, 6, 5, 6, 5, 5, 5, 5, 3, 5, 5, 5, 6, 5, 5, 2, 3... $ Zextrav &lt;dbl&gt; -0.1703149, 1.4140098, -0.9624772, -1.7546396, -0.17... $ Zsex &lt;dbl&gt; 0.9888125, -1.0108084, 0.9888125, 0.9888125, 0.98881... $ Ztexp &lt;dbl&gt; 1.48615283, 1.48615283, 1.48615283, 1.48615283, 1.48... $ Zpopular &lt;dbl&gt; 0.88501327, -0.12762911, 0.16169729, -0.27229230, 0.... $ Zpopteach &lt;dbl&gt; 0.66905609, -0.04308451, 0.66905609, -0.04308451, 0.... $ Cextrav &lt;dbl&gt; -0.215, 1.785, -1.215, -2.215, -0.215, -1.215, -0.21... $ Ctexp &lt;dbl&gt; 9.737, 9.737, 9.737, 9.737, 9.737, 9.737, 9.737, 9.7... $ Csex &lt;dbl&gt; 0.5, -0.5, 0.5, 0.5, 0.5, -0.5, -0.5, -0.5, -0.5, -0... 2.1.1 Unique Identifiers We will restrict ourselves to a few of the variables and create a distinct identifier variable for each student. data_pop &lt;- data_raw %&gt;% dplyr::mutate(id = paste(class, pupil, sep = &quot;_&quot;) %&gt;% # create a unique id for each student (char) factor()) %&gt;% # declare id is a factor dplyr::select(id, pupil:popteach) # reduce the variables included tibble::glimpse(data_pop) Observations: 2,000 Variables: 8 $ id &lt;fct&gt; 1_1, 1_2, 1_3, 1_4, 1_5, 1_6, 1_7, 1_8, 1_9, 1_10, 1_... $ pupil &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16... $ class &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... $ extrav &lt;dbl&gt; 5, 7, 4, 3, 5, 4, 5, 4, 5, 5, 5, 5, 5, 5, 5, 6, 4, 4,... $ sex &lt;fct&gt; girl, boy, girl, girl, girl, boy, boy, boy, boy, boy,... $ texp &lt;dbl&gt; 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 2... $ popular &lt;dbl&gt; 6.3, 4.9, 5.3, 4.7, 6.0, 4.7, 5.9, 4.2, 5.2, 3.9, 5.7... $ popteach &lt;dbl&gt; 6, 5, 6, 5, 6, 5, 5, 5, 5, 3, 5, 5, 5, 6, 5, 5, 2, 3,... 2.1.2 Structure and variables Its a good idea to visually inspect the first few lines in the datast to get a sense of how it is organized. data_pop %&gt;% psych::headTail(top = 25, bottom = 5) %&gt;% pander::pander() id pupil class extrav sex texp popular popteach 1_1 1 1 5 girl 24 6.3 6 1_2 2 1 7 boy 24 4.9 5 1_3 3 1 4 girl 24 5.3 6 1_4 4 1 3 girl 24 4.7 5 1_5 5 1 5 girl 24 6 6 1_6 6 1 4 boy 24 4.7 5 1_7 7 1 5 boy 24 5.9 5 1_8 8 1 4 boy 24 4.2 5 1_9 9 1 5 boy 24 5.2 5 1_10 10 1 5 boy 24 3.9 3 1_11 11 1 5 girl 24 5.7 5 1_12 12 1 5 girl 24 4.8 5 1_13 13 1 5 boy 24 5 5 1_14 14 1 5 girl 24 5.5 6 1_15 15 1 5 girl 24 6 5 1_16 16 1 6 girl 24 5.7 5 1_17 17 1 4 boy 24 3.2 2 1_18 18 1 4 boy 24 3.1 3 1_19 19 1 7 girl 24 6.6 7 1_20 20 1 4 boy 24 4.8 4 2_1 1 2 8 girl 14 6.4 6 2_2 2 2 4 boy 14 2.4 3 2_3 3 2 6 boy 14 3.7 4 2_4 4 2 5 girl 14 4.4 4 2_5 5 2 5 girl 14 4.3 4 NA … … … NA … … … 100_16 16 100 4 girl 7 4.3 5 100_17 17 100 4 boy 7 2.6 2 100_18 18 100 8 girl 7 6.7 7 100_19 19 100 5 boy 7 2.9 3 100_20 20 100 9 boy 7 5.3 5 Visual inspection reveals that most of the variables are measurements at level 1 and apply to specific pupils (extrav, sex, popular, and popteach), while the teacher’s years of experiene is a level 2 variable since it applies to the entire class. Notice how the texp variable is identical for all pupils in the same class. This is call Disaggregated data. 2.2 Exploratory Data Analysis 2.2.1 Summarize Descriptive Statistics 2.2.1.1 The stargazer package Most posters, journal articles, and reports start with a table of descriptive statistics. Since it tends to come first, this type of table is often refered to as Table 1. The stargazer() function can be used to create such a table, but only for the entire dataset (Hlavac 2018). I haven’t been able to find a way to get it to summarize subsamples and compare them in the standard format. # Knit to Website: type = &quot;html&quot; # Knit to PDF: type = &quot;latex&quot; # View on Screen: type = &quot;text&quot; data_pop %&gt;% dplyr::select(extrav, texp, popular) %&gt;% data.frame() %&gt;% stargazer::stargazer(title = &quot;Descriptive statistics, aggregate over entire sample&quot;, header = FALSE, type = &quot;html&quot;) Descriptive statistics, aggregate over entire sample Statistic N Mean St. Dev. Min Pctl(25) Pctl(75) Max extrav 2,000 5.215 1.262 1 4 6 10 texp 2,000 14.263 6.552 2 8 20 25 popular 2,000 5.076 1.383 0.000 4.100 6.000 9.500 2.2.1.2 The furniture package Tyson Barrett’s furniture package includes the extremely useful function table1() which simplifies the common task of creating a stratified, comparative table of descriptive statistics. Full documentation can be accessed by executing ?furniture::table1. # Knit to Website: output = &quot;html&quot; # Knit to PDF: output = &quot;latex2&quot; # View on Screen: output = &quot;&quot;text&quot;, or &quot;markdown&quot;, &quot;html&quot; data_pop %&gt;% furniture::table1(&quot;Pupil&#39;s Extraversion (10 pt)&quot; = extrav, &quot;Teacher&#39;s Experience (years)&quot; = texp, &quot;Popularity, Sociometric Score&quot; = popular, &quot;Popularity, Teacher Evaluated&quot; = popteach, splitby = ~ sex, # divide sample into columns by... test = TRUE, # test groups different? output = &quot;html&quot;, # output for latex caption = &quot;Compare genders on four main variables&quot;) # title Table 2.1: Compare genders on four main variables boy girl P-Value n = 989 n = 1011 Pupil’s Extraversion (10 pt) &lt;.001 5.1 (1.2) 5.3 (1.3) Teacher’s Experience (years) 0.001 13.8 (6.3) 14.7 (6.8) Popularity, Sociometric Score &lt;.001 4.3 (1.1) 5.9 (1.1) Popularity, Teacher Evaluated &lt;.001 4.3 (1.2) 5.8 (1.2) 2.2.2 Visualizations of Raw Data 2.2.2.1 Ignore Clustering 2.2.2.1.1 Scatterplots For a first look, its useful to plot all the data points on a single scatterplot as displayed in Figure 2.1. Due to ganularity in the rating scale, many points end up being plotted on top of each other (overplotted), so its a good idea to use geom_count() rather than geom_point() so the size of the dot can convey the number of points at that location (Wickham et al. 2018). # Disaggregate: pupil (level 1) only, ignore level 2&#39;s existance # Extraversion treated: continuous measure data_pop %&gt;% ggplot() + aes(x = extrav, # x-axis variable y = popular) + # y-axis variable geom_count() + # POINTS w/ SIZE = COUNT geom_smooth(method = &quot;lm&quot;) + # linear regression line theme_bw() + # white background labs(x = &quot;Extraversion (10 pt scale)&quot;, # x-axis label y = &quot;Popularity, Sociometric Score&quot;, # y-axis label size = &quot;Count&quot;) + # legend key&#39;s title theme(legend.position = c(0.9, 0.2), # key at legend.background = element_rect(color = &quot;black&quot;)) + # key box scale_x_continuous(breaks = seq(from = 0, to = 10, by = 1)) + # x-ticks scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) # y-ticks Figure 2.1: Disaggregate: pupil level only with extraversion treated as an continuous measure. 2.2.2.1.2 Density Plots When the degree of overplotting as high as it is in Figure 2.1, it can be useful to represent the data with density contours as seen in Figure 2.2. I’ve chosen to leave the points displayed in this redition, but color them much lighter so that they are present, but do not detract from the pattern of association. # visualize all the data - another way data_pop %&gt;% ggplot() + aes(x = extrav, # x-axis variable y = popular) + # y-axis variable geom_count(color = &quot;gray&quot;) + # POINTS w/ SIZE = COUNT geom_density2d() + # DENSITY CURVES geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;) + # linear regression line theme_bw() + # white background labs(x = &quot;Extraversion (10 pt scale)&quot;, # x-axis label y = &quot;Popularity, Sociometric Score&quot;) + # y-axis label guides(size = FALSE) + # don&#39;t include a legend scale_x_continuous(breaks = seq(from = 0, to = 10, by = 1)) + # x-ticks scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) # y-ticks Figure 2.2: Disaggregate: pupil level only with extraversion treated as an continuous measure. The argument could be made that the extraversion score should be treated as an ordinal factor instead of as a truely continuous scale since the only valid values are the whole number 1 through 10 and there is no assurance that these category assignments represent a true ratio measurement scale. However, we must keep in mind that this was an observational study, ans as such, the number of pupils assignment each level of extraversion is not equal. # count the number of pupils in assigned each Extraversion value, 1:10 table &lt;- data_pop %&gt;% group_by(extrav) %&gt;% summarise(count = n_distinct(id), percent = 100 * count / 2000) # Knit to Website: type = &quot;html&quot; # Knit to PDF: type = &quot;latex&quot; # View on Screen: type = &quot;text&quot; table %&gt;% stargazer(summary = FALSE, rownames = FALSE, header = FALSE, type = &quot;html&quot;, title = &quot;Distribution of extraversion in pupils&quot;) Distribution of extraversion in pupils extrav count percent 1 3 0.15 2 13 0.65 3 119 5.95 4 423 21.15 5 688 34.4 6 478 23.9 7 194 9.7 8 58 2.9 9 18 0.9 10 6 0.3 2.2.2.1.3 Boxplots Figure 2.3 displays the same data as Figure 2.1, but uses boxplots for the distribution of scores at each level of extraversion. On one extreme, the lowest extraversion score possible was a value of “one”, but only 3 pupils or 0.15% of the 2000 pupils recieved this value. On the other extreme, the middle value of “five” was applied to 688 pupils or a wopping 34.4%. The option varwidth=TRUE in the geom_boxplot() function helps reflect such unbalanced sample sizes by allowing the width of the boxes to be proportional to the square-roots of the number of observations each box represents. # Disaggregate: pupil (level 1) only, ignore level 2&#39;s existance # Extraversion treated: ordinal factor ggplot(data_pop, # dataset&#39;s name aes(x = factor(extrav), # x-axis values - make factor! y = popular, # y-axis values fill = factor(extrav))) + # makes seperate boxes geom_boxplot(varwidth = TRUE) + # draw boxplots instead of points theme_bw() + # white background guides(fill = FALSE) + # don&#39;t include a legend scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) + # y-ticks labs(x = &quot;Extraversion (10 pt scale)&quot;, # x-axis label y = &quot;Popularity, Sociometric Score&quot;) + # y-axis label scale_fill_brewer(palette = &quot;Spectral&quot;, direction = 1) # select color Figure 2.3: Disaggregate: pupil level only with extraversion treated as an ordinal factor. The width of the boxes are proportional to the square-roots of the number of observations each box represents. 2.2.3 Consider Clustering 2.2.3.1 Scatterplots Up to this point, all investigation of this dataset has been only at the pupil level and any nesting or clustering within classes has been ignored. Plotting is a good was to start to get an idea of the class-to-class variability. # compare the first 9 classrooms becuase all of there are too many at once data_pop %&gt;% dplyr::filter(class &lt;= 9) %&gt;% # select ONLY NINE classes ggplot(aes(x = extrav, # x-axis values y = popular)) + # y-axis values geom_count() + # POINTS w/ SIZE = COUNT geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;) + # linear regression line theme_bw() + # white background labs(x = &quot;Extraversion (10 pt scale)&quot;, # x-axis label y = &quot;Popularity, Sociometric Score&quot;, # y-axis label size = &quot;Count&quot;) + # legend key&#39;s title guides(size = FALSE) + # don&#39;t include a legend scale_x_continuous(breaks = seq(from = 0, to = 10, by = 3)) + # x-ticks scale_y_continuous(breaks = seq(from = 0, to = 10, by = 3)) + # y-ticks facet_wrap(~ class, labeller = label_both) + theme(strip.background = element_rect(colour = NA, fill = NA)) Figure 2.4: Illustration of the degree of class level variability in the association between extraversion and popularity. Each panel represents a class and each point a pupil in that class. First nice classes shown. # select specific classes by number for illustration purposes data_pop %&gt;% dplyr::filter(class %in% c(15, 25, 33, 35, 51, 64, 76, 94, 100)) %&gt;% ggplot(aes(x = extrav, # x-axis values y = popular)) + # y-axis values geom_count() + # POINTS w/ SIZE = COUNT geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;) + # linear regression line theme_bw() + # white background labs(x = &quot;Extraversion (10 pt scale)&quot;, # x-axis label y = &quot;Popularity, Sociometric Score&quot;, # y-axis label size = &quot;Count&quot;) + # legend key&#39;s title guides(size = FALSE) + # don&#39;t include a legend scale_x_continuous(breaks = seq(from = 0, to = 10, by = 3)) + # x-ticks scale_y_continuous(breaks = seq(from = 0, to = 10, by = 3)) + # y-ticks facet_wrap(~ class) + theme(strip.background = element_blank(), strip.text = element_blank()) Figure 2.5: Illustration of the degree of class level variability in the association between extraversion and popularity. Each panel represents a class and each point a pupil in that class. A set of nine classes was chosen to show a sampling of variability. The facet labels are not shown as the identification number probably would not be advisable for a general publication. 2.2.3.2 Cluster-wise Regression # compare all 100 classrooms via linear model for each ggplot(data_pop, aes(x = extrav, # x-axis values y = popular, # y-axis values group = class)) + # GROUPs for LINES geom_smooth(method = &quot;lm&quot;, # linear regression line color = &quot;gray40&quot;, size = 0.4, se = FALSE) + theme_bw() + # white background labs(x = &quot;Extraversion (10 pt scale)&quot;, # x-axis label y = &quot;Popularity, Sociometric Score&quot;) + # y-axis label scale_x_continuous(breaks = seq(from = 0, to = 10, by = 2)) + # x-ticks scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) # y-ticks Figure 2.6: Spaghetti plot of seperate, independent linear models for each of the 100 classes. A helpful resource for choosing colors to use in plots: R color cheatsheet # compare all 100 classrooms via independent linear models data_pop %&gt;% dplyr::mutate(texp3 = cut(texp, breaks = c(0, 10, 18, 30)) %&gt;% factor(labels = c(&quot;&lt; 10 yrs&quot;, &quot;10 - 18 yrs&quot;, &quot;&gt; 18 yrs&quot;))) %&gt;% ggplot(aes(x = extrav, # x-axis values y = popular, # y-axis values group = class)) + # GROUPs for LINES geom_smooth(aes(color = sex), size = 0.3, method = &quot;lm&quot;, # linear regression line se = FALSE) + theme_bw() + # white background labs(x = &quot;Extraversion (10 pt scale)&quot;, # x-axis label y = &quot;Popularity, Sociometric Score&quot;) + # y-axis label guides(color = FALSE) + # don&#39;t include a legend scale_x_continuous(breaks = seq(from = 0, to = 10, by = 3)) + # x-ticks scale_y_continuous(breaks = seq(from = 0, to = 10, by = 3)) + # y-ticks scale_color_manual(values = c(&quot;dodgerblue&quot;, &quot;maroon1&quot;)) + facet_grid(texp3 ~ sex) Figure 2.7: Spaghetti plot of seperate, independent linear models for each of the 100 classes. Seperate panels are used to untangle the ‘hairball’ in the previous figure. The columns are seperated by the pupils’ gender and the rows by the teacher’s experince in years. 2.3 Single-level Regression Analysis 2.3.1 Null Model In a Null, intercept-only, or Empty model, no predictors are included. 2.3.1.1 Equations Single-Level Regression Equation - Null Model \\[ \\overbrace{POP_{ij}}^{Outcome} = \\underbrace{\\beta_{0}}_{\\text{Fixed}\\atop\\text{intercept}} + \\underbrace{e_{ij}}_{\\text{Random}\\atop\\text{residuals}} \\] 2.3.1.2 Parameters Type Parameter of Interest Estimates This Fixed Intercept \\(\\beta_{0}\\) Random Residual Variance \\(var[e_{ij}]\\) \\(\\sigma^2_{e}\\) 2.3.1.3 Fit the Model pop_lm_0 &lt;- lm(popular ~ 1, # The 1 represents the intercept data = data_pop) summary(pop_lm_0) Call: lm(formula = popular ~ 1, data = data_pop) Residuals: Min 1Q Median 3Q Max -5.0765 -0.9765 0.0235 0.9236 4.4235 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.07645 0.03091 164.2 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.383 on 1999 degrees of freedom \\(\\hat{\\beta_0}\\) = 5.08 is the grand mean 2.3.1.4 Model Fit Residual variance: sigma(pop_lm_0) # standard deviation of the residuals [1] 1.382522 sigma(pop_lm_0)^2 # variance of the residuals [1] 1.911366 \\(\\hat{\\sigma_e^2}\\) = 1.9114 is residual variance (RMSE is sigma = 1.3825) Variance Explained: summary(pop_lm_0)$r.squared [1] 0 \\(R^2\\) = 0 is the proportion of variance in popularity that is explained by the grand mean alone. Deviance: -2 * logLik(pop_lm_0) &#39;log Lik.&#39; 6970.39 (df=2) 2.3.1.5 Interpretation The grand average popularity of all pupils in all the classes is 5.08, and there is strong evidence that it is statistically significantly different than zero, \\(p&lt;.0001\\). The mean alone accounts for none of the variance in popularity. The residual variance is the same as the total variance in popularity, 1.9114. Just to make sure… mean(data_pop$popular) [1] 5.07645 var(data_pop$popular) [1] 1.911366 2.3.2 Add Predictors to the Model 2.3.2.1 Equations LEVEL 1: Student-specific predictors: \\(X_1 = GEN\\), pupils’s gender (girl vs. boy) \\(X_2 = EXT\\), pupil’s extraversion (scale: 1-10) Single-Level Regression Equation \\[ \\overbrace{POP_{ij}}^{Outcome} = \\underbrace{\\beta_{0}}_{\\text{Fixed}\\atop\\text{intercept}} + \\underbrace{\\beta_{1}}_{\\text{Fixed}\\atop\\text{slope}} \\overbrace{GEN_{ij}}^{\\text{Predictor 1}} + \\underbrace{\\beta_{2}}_{\\text{Fixed}\\atop\\text{slope}} \\overbrace{EXT_{ij}}^{\\text{Predictor 2}} + \\underbrace{e_{ij}}_{\\text{Random}\\atop\\text{residuals}} \\tag{Hox 2.1} \\] 2.3.2.2 Parameters Type Parameter of Interest Estimates This Fixed Intercept \\(\\beta_{0}\\) Fixed Slope or effect of sex \\(\\beta_{1}\\) Fixed Slope or effect of extrav \\(\\beta_{2}\\) Random Residual Variance \\(var[e_{ij}]\\) \\(\\sigma^2_{e}\\) 2.3.2.3 Fit the Model pop_lm_1 &lt;- lm(popular ~ sex + extrav, # implies: 1 + sex + extrav data = data_pop) summary(pop_lm_1) Call: lm(formula = popular ~ sex + extrav, data = data_pop) Residuals: Min 1Q Median 3Q Max -4.2527 -0.6652 -0.0454 0.7422 3.0473 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 2.78954 0.10355 26.94 &lt;2e-16 *** sexgirl 1.50508 0.04836 31.12 &lt;2e-16 *** extrav 0.29263 0.01916 15.28 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.077 on 1997 degrees of freedom Multiple R-squared: 0.3938, Adjusted R-squared: 0.3932 F-statistic: 648.6 on 2 and 1997 DF, p-value: &lt; 2.2e-16 \\(\\hat{\\beta_0}\\) = 2.79 is the extrapolated mean for boys with an extroversion score of 0. \\(\\hat{\\beta_1}\\) = 1.51 is the mean difference between girls and boys with the same extroversion score. \\(\\hat{\\beta_2}\\) = 0.29 is the mean difference for pupils of the same gender that differ in extroversion by one point. 2.3.2.4 Model Fit Residual variance: sigma(pop_lm_1) # standard deviation of the residuals [1] 1.076985 sigma(pop_lm_1)^2 # variance of the residuals [1] 1.159898 \\(\\hat{\\sigma_e^2}\\) = 1.1599 is residual variance (RMSE is sigma) Variance Explained: summary(pop_lm_1)$r.squared [1] 0.393765 Deviance: -2 * logLik(pop_lm_1) &#39;log Lik.&#39; 5969.415 (df=4) \\(R^2\\) = 0.394 is the proportion of variance in popularity that is explained by tha pupils gender and extroversion score. 2.3.2.5 Interpretation On average, girls were rated 1.51 points more popular than boys with the same extroversion score, \\(p&lt;.0001\\). One point higher extroversion scores were associated with 0.29 points higher popularity, within each gender, \\(p&lt;.0001\\). Together, these two factors account for 39.38% of the variance in populartiy. 2.3.3 Compare Fixed Effects 2.3.3.1 Compare Nested Models Create a table to compare the two nested models: # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(pop_lm_0, pop_lm_1), custom.model.names = c(&quot;Null Model&quot;, &quot;With Predictors&quot;), caption = &quot;Single Level Models&quot;, caption.above = TRUE, single.row = TRUE) Single Level Models Null Model With Predictors (Intercept) 5.08 (0.03)*** 2.79 (0.10)*** sexgirl 1.51 (0.05)*** extrav 0.29 (0.02)*** R2 0.00 0.39 Adj. R2 0.00 0.39 Num. obs. 2000 2000 RMSE 1.38 1.08 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 When comparing the fit of two single-level models fit via the lm() function, the anova() function runs an F-test where the test statistic is the difference in RSS. anova(pop_lm_0, pop_lm_1) Analysis of Variance Table Model 1: popular ~ 1 Model 2: popular ~ sex + extrav Res.Df RSS Df Sum of Sq F Pr(&gt;F) 1 1999 3820.8 2 1997 2316.3 2 1504.5 648.55 &lt; 2.2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Obviously the model with predictors fits better than the model with no predictors. 2.3.3.2 Terminology The following terminology applies to single-level models fit with ordinary least-squared estimation (the lm() function in \\(R\\)). Values are calculated below for the NULL model. Mean squared error (MSE) is the MEAN of the square of the residuals: mse &lt;- mean(residuals(pop_lm_0)^2) mse [1] 1.91041 Root mean squared error (RMSE) which is the SQUARE ROOT of MSE: rmse &lt;- sqrt(mse) rmse [1] 1.382176 Residual sum of squares (RSS) is the SUM of the squared residuals: rss &lt;- sum(residuals(pop_lm_0)^2) rss [1] 3820.821 Residual standard error (RSE) is the SQUARE ROOT of (RSS / degrees of freedom): rse &lt;- sqrt( sum(residuals(pop_lm_0)^2) / pop_lm_0$df.residual ) rse [1] 1.382522 The same calculation, may be simplified with the previously calculated RSS: sqrt(rss / pop_lm_0$df.residual) [1] 1.382522 When the ‘deviance()’ function is applied to a single-level model fit via ‘lm()’, the Residual sum of squares (RSS) is returned, not the deviance as defined as twice the negative log likelihood (-2LL). deviance(pop_lm_0) # returns the RSS, not deviance = -2LL [1] 3820.821 -2 * logLik(pop_lm_0) # this is how get deviance = -2LL &#39;log Lik.&#39; 6970.39 (df=2) 2.4 Multi-level Regression Analysis 2.4.1 Intercept-only or Null Model In a Null, intercept-only, or Empty model, no predictors are included. “The intercept-only model is useful as a null-model that serves as a benchmark with which other models are compared.” Hox, Moerbeek, and Van de Schoot (2017), page 13 2.4.1.1 Equations Level 1 Model Equation: \\[ \\overbrace{Y_{ij}}^{Outcome} = \\underbrace{\\beta_{0j}}_{\\text{Level 2}\\atop\\text{intercepts}} + \\underbrace{e_{ij}}_{\\text{Random}\\atop\\text{residuals}} \\tag{Hox 2.6} \\] Level 2 Model Equation: \\[ \\overbrace{\\beta_{0j}}^{\\text{Level 2}\\atop\\text{intercepts}} = \\underbrace{\\gamma_{00}}_{\\text{Fixed}\\atop\\text{intercept}} + \\underbrace{u_{0j}}_{\\text{Random}\\atop\\text{intercepts}} \\tag{Hox 2.7} \\] Substitute equation (2.7) into equation (2.6): Combined, Multilevel Model Equation - Null Model \\[ \\overbrace{Y_{ij}}^{Outcome} = \\underbrace{\\gamma_{00}}_{\\text{Fixed}\\atop\\text{intercept}} + \\underbrace{u_{0j}}_{\\text{Random}\\atop\\text{intercepts}} + \\underbrace{e_{ij}}_{\\text{Random}\\atop\\text{residuals}} \\tag{Hox 2.8} \\] 2.4.1.2 Parameters Type Parameter of Interest Estimates This Fixed Intercept \\(\\gamma_{00}\\) Random Variance in random intercepts, \\(var[u_{0j}]\\) \\(\\sigma^2_{u0}\\) Random Residual Variance \\(var[e_{ij}]\\) \\(\\sigma^2_{e}\\) Hox, Moerbeek, and Van de Schoot (2017) labeled the Null model for this dataset “\\(M_0\\)” in chapter 2: Combined, Multilevel Model Equation - Popularity, Random Intercepts Only! \\[ \\overbrace{POP_{ij}}^{Outcome} = \\underbrace{\\gamma_{00}}_{\\text{Fixed}\\atop\\text{intercept}} + \\underbrace{u_{0j}}_{\\text{Random}\\atop\\text{intercepts}} + \\underbrace{e_{ij}}_{\\text{Random}\\atop\\text{residuals}} \\tag{M0: intercept only} \\] 2.4.1.3 Fit the Model Fit the model to the data. pop_lmer_0_re &lt;- lme4::lmer(popular ~ 1 + (1|class), # include a fixed and random intercept data = data_pop, REML = TRUE) # fit via REML (the default) for ICC calculations summary(pop_lmer_0_re) Linear mixed model fit by REML [&#39;lmerMod&#39;] Formula: popular ~ 1 + (1 | class) Data: data_pop REML criterion at convergence: 6330.5 Scaled residuals: Min 1Q Median 3Q Max -3.5655 -0.6975 0.0020 0.6758 3.3175 Random effects: Groups Name Variance Std.Dev. class (Intercept) 0.7021 0.8379 Residual 1.2218 1.1053 Number of obs: 2000, groups: class, 100 Fixed effects: Estimate Std. Error t value (Intercept) 5.07786 0.08739 58.1 Estimation Methods Multilevel models may be fit by various methods. The most commonly used (and availabel in ‘lme4’) optimize various criterions: Maximum Likelihood (ML) -or- Restricted Maximum Likelihood (REML). Hox, Moerbeek, and Van de Schoot (2017) discusses these and other methods in chapter 3. At the end of chapter 2, the authors’ second note staes that the details of estimation methods are glossed over in the current example in an effort to simplfy the introductory. Here we follow these guidelines: Use ML for fitting: nested models that differ only by inclusion/exclusion of FIXED effects, to test parameter significance via a likelihood ratio test Use REML for fitting: the NULL model, on which to base ICC calculations nested models that differ only by inclusion/exclusion of RANDOM effects, to test parameter significance via a likelihood ratio test the FINAL model This often leads to refitting identical models via BOTH estimation methods. 2.4.1.4 Interpretation The grand average popularity of all students is 5.0779 and the class averages tend to vary by about 0.8333 points above or below that. 2.4.2 Intraclass Correlation (ICC) Although the Null model above does not explain any variance in the dependent variable (popularity), since there are no independent variables, it does decompose (i.e. divide up) the variance into two pieces. We can compute the amount of total variance in popularity that is attribute to the clustering of students in classes verses the residual variance. Intraclass Correlation (ICC) Formula \\[ \\overbrace{\\rho}^{\\text{ICC}} = \\frac{\\overbrace{\\sigma^2_{u0}}^{\\text{Random Intercept}\\atop\\text{Variance}}} {\\underbrace{\\sigma^2_{u0}+\\sigma^2_{e}}_{\\text{Total}\\atop\\text{Variance}}} \\tag{Hox 2.9} \\] The VarCorr() function in the lme4 package returns the standard deviations, not the variances (\\(var = SD^2\\)) for a model fit via the lme4::lmer() function. The summary() function reports both the variances and the stadard deviations. lme4::VarCorr(pop_lmer_0_re) # extract random compondent: varrainces and correlations Groups Name Std.Dev. class (Intercept) 0.83792 Residual 1.10535 sjstats::re_var(pop_lmer_0_re) Within-group-variance: 1.222 Between-group-variance: 0.702 (class) \\[ \\begin{align*} \\text{classes} \\rightarrow \\; &amp; \\sigma^2_{u0} = 0.83792^2 = 0.702\\\\ \\text{pupils within classes} \\rightarrow \\; &amp; \\sigma^2_{e} = 1.10535^2 = 1.222\\\\ \\end{align*} \\] 2.4.2.1 By Hand Calculate the ICC by hand: \\[ \\overbrace{\\rho}^{\\text{ICC}} = \\frac{\\overbrace{\\sigma^2_{u0}}^{\\text{Random Intercept}\\atop\\text{Variance}}} {\\underbrace{\\sigma^2_{u0}+\\sigma^2_{e}}_{\\text{Total}\\atop\\text{Variance}}} = \\frac{0.702} {0.702+1.222} = \\frac{0.702} {1.924} = 0.3648649 \\] 0.702 / (0.702 + 1.222) [1] 0.3648649 2.4.2.2 The sjstats package Calculate the ICC with the icc() fucntion in the sjstats package: sjstats::icc(pop_lmer_0_re) Linear mixed model Family : gaussian (identity) Formula: popular ~ 1 + (1 | class) ICC (class): 0.3649 2.4.2.3 Interpretation WOW! 36.5% of the variance of the popularity scores is at the group level, which is very high for social science data. The ICC should be based on a Null (intercept only) model fit via REML (restricted maximum likelihood) estimation. This is the default for the ‘lme4::lmer()’ function. In chapter 2, Hox, Moerbeek, and Van de Schoot (2017) presents the numbers based on fitting the model via ML (maximum likelihood) estimation and thus does not match the presentation above exactly (not just rounding error). This is because: (1) estimation methods (REML &amp; ML) are not discussed until chapter 3 and (2) due to the Null model also being used for model fit comparisons in Table 2.1 on the top of page 14. Here we will fit the empty model twice, above by ML and below by REML 2.4.3 Add Predictors to the Model Hox, Moerbeek, and Van de Schoot (2017) labeled this as “\\(M_1\\)” in chapter 2 for their Table 2.1 (page 14), but adjusted it for Tables 2.2 (page 15) and 2.3 (page 17). LEVEL 1: Student-specific predictors: \\(X_1 = GEN\\), pupils’s gender (girl vs. boy) \\(X_2 = EXT\\), pupil’s extraversion (scale: 1-10) LEVEL 2: Class-specific Predictors: \\(Z = YRS\\), teacher’s experience (range of 2-25 years) 2.4.3.1 Equations Level 1 Model Equation: Include main effects for sex and extrav \\[ \\overbrace{POP_{ij}}^{Outcome} = \\underbrace{\\beta_{0j}}_{\\text{Level 2}\\atop\\text{intercept}} + \\underbrace{\\beta_{1j}}_{\\text{Level 2}\\atop\\text{slopes}} \\overbrace{GEN_{ij}}^{\\text{Level 1}\\atop\\text{Predictor 1}} + \\underbrace{\\beta_{2j}}_{\\text{Level 2}\\atop\\text{slopes}} \\overbrace{EXT_{ij}}^{\\text{Level 1}\\atop\\text{Predictor 2}} + \\underbrace{e_{ij}}_{\\text{Random}\\atop\\text{residuals}} \\] Level 2 Model Equations: Include a random intercepts and random slopes for both for sex and extrav, but NO cross level interactions for now. We will assume this is due to some theoretical reasoning to be our starting point after the fitting of the null model. Random Intercepts: \\[ \\overbrace{\\beta_{0j}}^{\\text{Level 2}\\atop\\text{intercepts}} = \\underbrace{\\gamma_{00}}_{\\text{Fixed}\\atop\\text{intercept}} + \\underbrace{\\gamma_{01}}_{\\text{Fixed}\\atop\\text{slope } Z} \\overbrace{YRS_{j}}^{\\text{Level 2}\\atop\\text{Predictor 3}} + \\underbrace{u_{0j}}_{\\text{Intercept}\\atop\\text{residual}} \\] Random Slopes, for the first predictor, sex: \\[ \\overbrace{\\beta_{1j}}^{\\text{Level 2}\\atop\\text{slopes}} = \\underbrace{\\gamma_{10}}_{\\text{Fixed}\\atop\\text{Slope } X_1} + \\underbrace{u_{1j}}_{\\text{Slope } X_1\\atop\\text{residual}} \\] Random Slopes, for the second predictor, extrav: \\[ \\overbrace{\\beta_{2j}}^{\\text{Level 2}\\atop\\text{slopes}} = \\underbrace{\\gamma_{20}}_{\\text{Fixed}\\atop\\text{Slope } X_2} + \\underbrace{u_{2j}}_{\\text{Slope } X_2\\atop\\text{residual}} \\] Substitute the level 2 equations into the level 1 equation: Combined, Multilevel Model Equation - Popularity, Include Predictors (no cross-level interactions) \\[ \\overbrace{POP_{ij}}^{Outcome} = \\overbrace{\\gamma_{00} + \\gamma_{10} GEN_{ij} + \\gamma_{20} EXT_{ij} + \\gamma_{01} YRS_{j}}^{\\text{Fixed part}\\atop\\text{Deterministic}} + \\\\ \\underbrace{u_{0j} + u_{1j} GEN_{ij} + u_{2j} EXT_{ij} + e_{ij} }_{\\text{Random part}\\atop\\text{Stochastic}} \\tag{M1} \\] 2.4.3.2 Parameters Type Parameter of Interest Estimates This Fixed Intercept \\(\\gamma_{00}\\) Fixed Main Effect of sex \\(\\gamma_{10}\\) Fixed Main Effect of extrav \\(\\gamma_{20}\\) Fixed Main Effect of texp \\(\\gamma_{01}\\) Random Variance in random intercepts, \\(var[u_{0j}]\\) \\(\\sigma^2_{u0}\\) Random Variance in random slope of sex, \\(var[u_{1j}]\\) \\(\\sigma^2_{u1}\\) Random Variance in random slope of extrav, \\(var[u_{2j}]\\) \\(\\sigma^2_{u2}\\) Random Covariance between random intercepts and random slope of sex, \\(cov[u_{0j}, u_{1j}]\\) \\(\\sigma^2_{u01}\\) Random Covariance between random intercepts and random slope of extrav, \\(cov[u_{0j}, u_{2j}]\\) \\(\\sigma^2_{u02}\\) Random Covariance between random slopes of sex and extrav, \\(cov[u_{1j}, u_{2j}]\\) \\(\\sigma^2_{u12}\\) Random Residual Variance \\(var[e_{ij}]\\) \\(\\sigma^2_{e}\\) Troubleshooting ‘lme4’ Linear Mixed-Effects Models website. This website attempts to summarize some of the common problems with fitting lmer models and how to troubleshoot them. This is a helpful post on Stack Exchange regarding using differen t optimizers to get the ‘lme4::lmer()’ function to converge. Note: Convergence issues MAY signify problems in the model specification. 2.4.3.3 Fit the Model pop_lmer_0_ml &lt;- lme4::lmer(popular ~ 1 + (1|class), data = data_pop, REML = FALSE) # refit via ML to compare the model below to pop_lmer_1_ml &lt;- lme4::lmer(popular ~ sex + extrav + texp + (sex + extrav|class), data = data_pop, REML = FALSE, control = lmerControl(optimizer =&quot;Nelder_Mead&quot;)) #helps converge summary(pop_lmer_1_ml) Linear mixed model fit by maximum likelihood [&#39;lmerMod&#39;] Formula: popular ~ sex + extrav + texp + (sex + extrav | class) Data: data_pop Control: lmerControl(optimizer = &quot;Nelder_Mead&quot;) AIC BIC logLik deviance df.resid 4833.3 4894.9 -2405.6 4811.3 1989 Scaled residuals: Min 1Q Median 3Q Max -3.1686 -0.6550 -0.0227 0.6728 2.9571 Random effects: Groups Name Variance Std.Dev. Corr class (Intercept) 1.319429 1.14866 sexgirl 0.002389 0.04888 -0.40 extrav 0.034115 0.18470 -0.88 -0.09 Residual 0.551144 0.74239 Number of obs: 2000, groups: class, 100 Fixed effects: Estimate Std. Error t value (Intercept) 0.760097 0.195930 3.879 sexgirl 1.251022 0.036921 33.884 extrav 0.452877 0.024511 18.477 texp 0.089417 0.008533 10.480 Correlation of Fixed Effects: (Intr) sexgrl extrav sexgirl -0.063 extrav -0.720 -0.066 texp -0.683 -0.040 0.090 2.4.3.4 Interpretation After accounting for the heiarchical nesting of students in classes, girls were rated 1.25 points more popular on average, than boys with the same extroversion score. One point higher extroversion scores were associated with 0.45 points higher popularity, within each gender. Reproduce Table 2.1 on the top of page 14 (Hox, Moerbeek, and Van de Schoot 2017) # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(pop_lm_0, pop_lmer_0_ml, pop_lmer_1_ml), custom.model.names = c(&quot;Single-level&quot;, &quot;M0: int only&quot;, &quot;M1: w pred&quot;), caption = &quot;Hox Table 2.1 on the top of page 14&quot;, caption.above = TRUE, single.row = TRUE) Hox Table 2.1 on the top of page 14 Single-level M0: int only M1: w pred (Intercept) 5.08 (0.03)*** 5.08 (0.09)*** 0.76 (0.20)*** sexgirl 1.25 (0.04)*** extrav 0.45 (0.02)*** texp 0.09 (0.01)*** R2 0.00 Adj. R2 0.00 Num. obs. 2000 2000 2000 RMSE 1.38 AIC 6333.47 4833.29 BIC 6350.27 4894.90 Log Likelihood -3163.73 -2405.64 Num. groups: class 100 100 Var: class (Intercept) 0.69 1.32 Var: Residual 1.22 0.55 Var: class sexgirl 0.00 Var: class extrav 0.03 Cov: class (Intercept) sexgirl -0.02 Cov: class (Intercept) extrav -0.19 Cov: class sexgirl extrav -0.00 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 The regression tables from the texreg package include estimates of the covariances between random components. “These covarianes are rarely interpreted (for an exception see Chapter 5 and Chapter 16 where growth models are discussed), and for that reason they are often not included in the reported tables. However, as Table 2.2 demonstrates, they can be quite large adn significant, so as a rule they are always included in the model.” Hox, Moerbeek, and Van de Schoot (2017), Chapter 2, pages 15-16 Comparing Model Fit Residual Variance in the Residuals In single-level regression, the Root Mean Squared Error (RMSE) is usually reported. It is the standard deviation of the residuals and is called “Residual standard error” in the R output of summary() function applied to an model fit via lm. In multi-level regression, residual variance is reported as \\(\\sigma_e^2\\). \\[ {\\text{RMSE}}^2 = MSE = \\sigma_e^2 \\] Deviance In single-level regression, the model is fit in such a way as to make the sum of the squared residuals as small as possible. Deviance is the sum of the squared residuals. In multi-level regression, the model is fit via a method called ‘Maximum Likelihood’. \\[ \\text{Deviance} = -2LL = -2 \\times log(likelihood) \\] 2.4.4 Testing Random Components In Hox’s table 2.1 (page 14) we see that the MLM with predictors (\\(M_0\\)) includes a random compondnt with virtually no variance. This is likely why the model didn’t easily converge (a different optimizer was employed). It makes sence to remove the random slope component for gender and refit the model. While we are at it, we will also fit a third model dropping the second random slope component for extraversion. 2.4.4.1 Fit Nested Models Since we are going to compare models that are nested on random effects (identical except for inclusing/exclusing of random components, we will specify the REML = TRUE option. pop_lmer_1_re &lt;- lme4::lmer(popular ~ sex + extrav + texp + (sex + extrav|class), data = data_pop, REML = TRUE, control = lmerControl(optimizer =&quot;Nelder_Mead&quot;)) #helps converge pop_lmer_1a_re &lt;- lme4::lmer(popular ~ sex + extrav + texp + (extrav|class), data = data_pop, REML = TRUE) pop_lmer_1b_re &lt;- lme4::lmer(popular ~ sex + extrav + texp + (1 |class), data = data_pop, REML = TRUE) Create a table to compare the three nested models: The middle column below reproduces Hox’s Table 2.2 found on the bottom of page 15 (Hox, Moerbeek, and Van de Schoot 2017), except the values differ slightly becuase here the model was fit via REML where as in the text, Hox used ML. # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(pop_lmer_1_re, pop_lmer_1a_re, pop_lmer_1b_re), custom.model.names = c(&quot;M1&quot;, &quot;M1a&quot;, &quot;M1b&quot;), caption = &quot;Assessing Significance of Random Slopes&quot;, caption.above = TRUE, single.row = TRUE) Assessing Significance of Random Slopes M1 M1a M1b (Intercept) 0.76 (0.20)*** 0.74 (0.20)*** 0.81 (0.17)*** sexgirl 1.25 (0.04)*** 1.25 (0.04)*** 1.25 (0.04)*** extrav 0.45 (0.02)*** 0.45 (0.02)*** 0.45 (0.02)*** texp 0.09 (0.01)*** 0.09 (0.01)*** 0.09 (0.01)*** AIC 4855.26 4850.77 4897.02 BIC 4916.87 4895.58 4930.63 Log Likelihood -2416.63 -2417.38 -2442.51 Num. obs. 2000 2000 2000 Num. groups: class 100 100 100 Var: class (Intercept) 1.34 1.30 0.30 Var: class sexgirl 0.00 Var: class extrav 0.03 0.03 Cov: class (Intercept) sexgirl -0.02 Cov: class (Intercept) extrav -0.19 -0.19 Cov: class sexgirl extrav -0.00 Var: Residual 0.55 0.55 0.59 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 2.4.4.2 Compare Fit Likelihood Ratio Test (LRT) of Nested MLM Models When comparing the fit of two multi-level models fit via the lme::lmer() function, the anova() function runs an Chi-squared test where the test statistic is the difference in -2LL (deviances). Likelihood Ratio Test (LRT) for Random Effects When using the ‘anova()’ function to conduct a LRT for RANDOM effects, make sure: the nested models have identical FIXED effects never test models that differ in fixed and random effects at the same time the models were fit with ‘REML = TRUE’ this results in the best variance/covariance component estimation add the ‘refit = FALSE’ option to the ‘anova()’ call without this \\(R\\) re-runs the models with ‘REML = FALSE’ for you Investigate dropping the random slope component for sex These two models are identical, except for the inclusing/exclusion of the random specification of the level 1 sex predictor. Note, both models were fit with REML. Although we are dropping only ONE variance component, we are also dropping TWO covariances (sex paired with both the random intercept and random slope for extrav). This results in a \\(\\chi^2\\) test with THREE degrees of freedom. anova(pop_lmer_1_re, pop_lmer_1a_re, refit = FALSE) # don&#39;t let it refit the models via LM Data: data_pop Models: pop_lmer_1a_re: popular ~ sex + extrav + texp + (extrav | class) pop_lmer_1_re: popular ~ sex + extrav + texp + (sex + extrav | class) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) pop_lmer_1a_re 8 4850.8 4895.6 -2417.4 4834.8 pop_lmer_1_re 11 4855.3 4916.9 -2416.6 4833.3 1.5133 3 0.6792 The NON-significance likelihood ratio test (LRT: \\(\\chi^2(3) = 1.51\\), \\(p = .679\\)) conveys that the more complex model does NOT fit the data better. Thus the more SIMPLE model does just as good of a job. This is evidence for the EXCLUSION of sex as a random component. Investigate dropping the random slope component for extrav These two models are identical, except for the inclusing/exclusion of the random specification of the level 1 extrav predictor. Note, both models were fit with REML. Although we are dropping only ONE variance component, we are also dropping ONE covariances (extrav paired with the random intercept). This results in a \\(\\chi^2\\) test with TWO degrees of freedom. anova(pop_lmer_1a_re, pop_lmer_1b_re, refit = FALSE) # don&#39;t let it refit the models via LM Data: data_pop Models: pop_lmer_1b_re: popular ~ sex + extrav + texp + (1 | class) pop_lmer_1a_re: popular ~ sex + extrav + texp + (extrav | class) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) pop_lmer_1b_re 6 4897.0 4930.6 -2442.5 4885.0 pop_lmer_1a_re 8 4850.8 4895.6 -2417.4 4834.8 50.256 2 1.222e-11 pop_lmer_1b_re pop_lmer_1a_re *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The significance likelihood ratio test (LRT: \\(\\chi^2(2) = 50.26\\), \\(p &lt; .0001\\)) conveys that the more complex model DOES fit the data better. Thus the more COMPLEX model does just as good of a job. This is evidence for the INCLUSION of extrav as a random component. 2.4.5 Testing Cross-Level Interactions We have already seen formulas of this form for a NULL or emply models, as well as for intercept implied models of main effects: intercept only Y ~ 1 intercept implied Y ~ A = Y ~ 1 + A Y ~ A + B = Y ~ 1 + A + B Including Interactions in Formulas If we wish to include an interaction between the two predictors, we signify this with a colon (:) between the two predictor names. A shortcut may also be employed to signify the including of the main effects and the interaction at the same time by placing an astric (*) between the two variable names. Both of the following specify the outcome is being predicted by an intercept (implied), the main effects for 2 predictors, and the interaction between the two predictors Y ~ A + B + A:B Y ~ A*B Examples 2-way: A*B = A + B + A:B 3-way: A*B*C = A + B + C + A:B + A:C + B:C + A:B:C 4-way: A*B*C*D = A + B + C + D + A:B + A:C + A:D + B:C + B:D + A:B:C + A:B:D+ A:C:D + B:C:D + A:B:C:D 2.4.5.1 Fit Nested Models “Given the significant variance of the regression coefficient of pupil extraversion across the classes, it is attractive to attempt to predict its variation using class-level variables. We have one class-level variable: teacher experience.” Hox, Moerbeek, and Van de Schoot (2017), Chapter 2, page 16 Now that we wish to compare nested that will differ only in terms of the inclusing/exclusion of a FIXED effect, the estimation method should be standard maximum likelihood (REML = FALSE). pop_lmer_1a_ml &lt;- lme4::lmer(popular ~ sex + extrav + texp + (extrav|class), # main effects only data = data_pop, REML = FALSE) pop_lmer_2_ml &lt;- lme4::lmer(popular ~ sex + extrav*texp + (extrav|class), # include cross-level interaction data = data_pop, REML = FALSE) pop_lmer_3_ml &lt;- lme4::lmer(popular ~ extrav*texp + sex*texp + sex*extrav + (extrav|class), data = data_pop, REML = FALSE) pop_lmer_4_ml &lt;- lme4::lmer(popular ~ extrav*texp*sex + (extrav|class), data = data_pop, REML = FALSE) Create a table to compare the two nested models: # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(pop_lmer_1a_ml, pop_lmer_2_ml), custom.model.names = c(&quot;M1a: Main Effects&quot;, &quot;M2: With Interaction&quot;), caption = &quot;Hox Table 2.3 on page 17&quot;, caption.above = TRUE, single.row = TRUE) Hox Table 2.3 on page 17 M1a: Main Effects M2: With Interaction (Intercept) 0.74 (0.20)*** -1.21 (0.27)*** sexgirl 1.25 (0.04)*** 1.24 (0.04)*** extrav 0.45 (0.02)*** 0.80 (0.04)*** texp 0.09 (0.01)*** 0.23 (0.02)*** extrav:texp -0.02 (0.00)*** AIC 4828.81 4765.62 BIC 4873.61 4816.03 Log Likelihood -2406.40 -2373.81 Num. obs. 2000 2000 Num. groups: class 100 100 Var: class (Intercept) 1.28 0.45 Var: class extrav 0.03 0.00 Cov: class (Intercept) extrav -0.18 -0.03 Var: Residual 0.55 0.55 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 Investigate further interactions, not shown in by Hox, Moerbeek, and Van de Schoot (2017). # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(pop_lmer_1a_ml, pop_lmer_2_ml, pop_lmer_3_ml, pop_lmer_4_ml), custom.model.names = c(&quot;M1a: Main Effects&quot;, &quot;M2: With Interaction&quot;, &quot;Add 2-way Inter&quot;, &quot;Add 3-way Interaction&quot;), caption = &quot;Hox Table 2.3 on page 17&quot;, caption.above = TRUE, single.row = TRUE) Hox Table 2.3 on page 17 M1a: Main Effects M2: With Interaction Add 2-way Inter Add 3-way Interaction (Intercept) 0.74 (0.20)*** -1.21 (0.27)*** -1.09 (0.28)*** -0.94 (0.33)** sexgirl 1.25 (0.04)*** 1.24 (0.04)*** 0.96 (0.21)*** 0.66 (0.38) extrav 0.45 (0.02)*** 0.80 (0.04)*** 0.78 (0.04)*** 0.75 (0.05)*** texp 0.09 (0.01)*** 0.23 (0.02)*** 0.23 (0.02)*** 0.22 (0.02)*** extrav:texp -0.02 (0.00)*** -0.02 (0.00)*** -0.02 (0.00)*** texp:sexgirl 0.00 (0.01) 0.02 (0.02) extrav:sexgirl 0.05 (0.03) 0.10 (0.06) extrav:texp:sexgirl -0.00 (0.00) AIC 4828.81 4765.62 4767.17 4768.26 BIC 4873.61 4816.03 4828.78 4835.47 Log Likelihood -2406.40 -2373.81 -2372.58 -2372.13 Num. obs. 2000 2000 2000 2000 Num. groups: class 100 100 100 100 Var: class (Intercept) 1.28 0.45 0.49 0.49 Var: class extrav 0.03 0.00 0.01 0.01 Cov: class (Intercept) extrav -0.18 -0.03 -0.03 -0.03 Var: Residual 0.55 0.55 0.55 0.55 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 2.4.5.2 Compare Fit Since these two models only differe by the inclusing/exclusing of a FIXED effect, they both employed ML estimation. Thus we do not need worry about the anova() function refitting the models prior to conduction the LRT. anova(pop_lmer_1a_ml, pop_lmer_2_ml) Data: data_pop Models: pop_lmer_1a_ml: popular ~ sex + extrav + texp + (extrav | class) pop_lmer_2_ml: popular ~ sex + extrav * texp + (extrav | class) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) pop_lmer_1a_ml 8 4828.8 4873.6 -2406.4 4812.8 pop_lmer_2_ml 9 4765.6 4816.0 -2373.8 4747.6 65.183 1 6.827e-16 pop_lmer_1a_ml pop_lmer_2_ml *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The significance likelihood ratio test (LRT: \\(\\chi^2(1) = 65.18\\), \\(p &lt; .0001\\)) conveys that the more complex model DOES fit the data better. Thus the more COMPLEX model does just as good of a job. This is evidence for the INCLUSION of cross-level interaction between extrav and texp as a fixed component. anova(pop_lmer_2_ml, pop_lmer_3_ml) Data: data_pop Models: pop_lmer_2_ml: popular ~ sex + extrav * texp + (extrav | class) pop_lmer_3_ml: popular ~ extrav * texp + sex * texp + sex * extrav + (extrav | pop_lmer_3_ml: class) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) pop_lmer_2_ml 9 4765.6 4816.0 -2373.8 4747.6 pop_lmer_3_ml 11 4767.2 4828.8 -2372.6 4745.2 2.4552 2 0.293 The significance likelihood ratio test (LRT: \\(\\chi^2(2) = 2.46\\), \\(p=.293\\)) conveys that the more complex model does NOT fit the data better. Thus the more SIMPLE model does just as good of a job. This is evidence for the EXCLUSION of the additional 2-way interactions as a fixed components. anova(pop_lmer_2_ml, pop_lmer_4_ml) Data: data_pop Models: pop_lmer_2_ml: popular ~ sex + extrav * texp + (extrav | class) pop_lmer_4_ml: popular ~ extrav * texp * sex + (extrav | class) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) pop_lmer_2_ml 9 4765.6 4816.0 -2373.8 4747.6 pop_lmer_4_ml 12 4768.3 4835.5 -2372.1 4744.3 3.3636 3 0.3389 The significance likelihood ratio test (LRT: \\(\\chi^2(3) = 3.36\\), \\(p=.339\\)) conveys that the more complex model does NOT fit the data better. Thus the more SIMPLE model does just as good of a job. This is evidence for the EXCLUSION of the additional 3-way interactions as a fixed components. 2.4.6 Final Model 2.4.6.1 Refit with REML pop_lmer_2_re &lt;- lme4::lmer(popular ~ sex + extrav*texp + (extrav|class), data = data_pop, REML = TRUE) # re-fit the final model via REML 2.4.6.2 Parameters # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(pop_lmer_2_ml), custom.model.names = c(&quot;Final Model&quot;), caption = &quot;MLM for Popularity&quot;, caption.above = TRUE, single.row = TRUE) MLM for Popularity Final Model (Intercept) -1.21 (0.27)*** sexgirl 1.24 (0.04)*** extrav 0.80 (0.04)*** texp 0.23 (0.02)*** extrav:texp -0.02 (0.00)*** AIC 4765.62 BIC 4816.03 Log Likelihood -2373.81 Num. obs. 2000 Num. groups: class 100 Var: class (Intercept) 0.45 Var: class extrav 0.00 Cov: class (Intercept) extrav -0.03 Var: Residual 0.55 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 2.4.6.3 Visualization Get Estimated Marginal Means - default ‘nice’ predictor values: Focal predictors: All combinations of… * sex categorical, both levels * extrav continuous 1-10, default: 1, 3, 6, 8, 10 * texp continuous, default: 2.0, 7.8, 14.0, 19.0, 25.0 Always followed by: * fit estimated marginal mean * se standard error for the marginal mean * lower lower end of the 95% confidence interval around the estimated marginal mean * upper upper end of the 95% confidence interval around the estimated marginal mean effects::Effect(focal.predictors = c(&quot;sex&quot;, &quot;extrav&quot;, &quot;texp&quot;), mod = pop_lmer_2_re) %&gt;% data.frame() %&gt;% head(n = 12) sex extrav texp fit se lower upper 1 boy 1 2.0 -0.003093328 0.2113423 -0.4175680 0.4113813 2 girl 1 2.0 1.237604358 0.2138286 0.8182537 1.6569551 3 boy 3 2.0 1.505151652 0.1580324 1.1952259 1.8150774 4 girl 3 2.0 2.745849338 0.1602424 2.4315894 3.0601092 5 boy 6 2.0 3.767519123 0.1201265 3.5319325 4.0031058 6 girl 6 2.0 5.008216809 0.1208436 4.7712238 5.2452098 7 boy 8 2.0 5.275764103 0.1416448 4.9979769 5.5535513 8 girl 8 2.0 6.516461789 0.1410047 6.2399299 6.7929937 9 boy 10 2.0 6.784009084 0.1892770 6.4128078 7.1552104 10 girl 10 2.0 8.024706769 0.1878594 7.6562857 8.3931279 11 boy 1 7.8 1.165428795 0.1405330 0.8898220 1.4410356 12 girl 1 7.8 2.406126481 0.1432180 2.1252539 2.6869991 Pick ‘nicer’ illustrative values for texp effects::Effect(focal.predictors = c(&quot;sex&quot;, &quot;extrav&quot;, &quot;texp&quot;), mod = pop_lmer_2_re, xlevels = list(texp = c(5, 15, 25))) %&gt;% data.frame() %&gt;% head(n = 12) sex extrav texp fit se lower upper 1 boy 1 5 0.6013147 0.17311707 0.2618055 0.9408239 2 girl 1 5 1.8420124 0.17570978 1.4974184 2.1866063 3 boy 3 5 1.9611908 0.12950817 1.7072054 2.2151762 4 girl 3 5 3.2018885 0.13175720 2.9434923 3.4602846 5 boy 6 5 4.0010050 0.09857046 3.8076931 4.1943168 6 girl 6 5 5.2417027 0.09913884 5.0472761 5.4361292 7 boy 8 5 5.3608811 0.11621095 5.1329735 5.5887886 8 girl 8 5 6.6015788 0.11532799 6.3754029 6.8277547 9 boy 10 5 6.7207572 0.15520385 6.4163786 7.0251358 10 girl 10 5 7.9614549 0.15351541 7.6603876 8.2625222 11 boy 1 15 2.6160080 0.09471181 2.4302636 2.8017524 12 girl 1 15 3.8567057 0.09677671 3.6669116 4.0464997 Basic, default plot Other than selecting three illustrative values for the teacher extraversion rating, most everything is left to default. effects::Effect(focal.predictors = c(&quot;sex&quot;, &quot;extrav&quot;, &quot;texp&quot;), mod = pop_lmer_2_re, xlevels = list(texp = c(5, 15, 25))) %&gt;% data.frame() %&gt;% dplyr::mutate(texp = factor(texp)) %&gt;% ggplot() + aes(x = extrav, y = fit, fill = texp, linetype = texp) + geom_ribbon(aes(ymin = lower, ymax = upper), alpha = .3) + geom_line(aes(color = texp)) + facet_grid(.~ sex) More Clean Plot There are many ways to clean up a plot, including labeling the axes. effects::Effect(focal.predictors = c(&quot;sex&quot;, &quot;extrav&quot;, &quot;texp&quot;), mod = pop_lmer_2_re, xlevels = list(texp = c(5, 15, 25))) %&gt;% data.frame() %&gt;% dplyr::mutate(texp = factor(texp)) %&gt;% dplyr::mutate(sex = sex %&gt;% forcats::fct_recode(&quot;Amoung Boys&quot; = &quot;boy&quot;, &quot;Among Girls&quot; = &quot;girl&quot;)) %&gt;% ggplot() + aes(x = extrav, y = fit, fill = texp, linetype = texp) + geom_ribbon(aes(ymin = lower, ymax = upper), alpha = .3) + geom_line(aes(color = texp)) + theme_bw() + facet_grid(.~ sex) + labs(x = &quot;Pupil&#39;s Extroversion, Rated by Teacher&quot;, y = &quot;Estimated Marginal Mean\\nPupil Popularity, Mean Rating of Classroom Peers&quot;, color = &quot;Teacher&#39;s Experience, Years&quot;, linetype = &quot;Teacher&#39;s Experience, Years&quot;, fill = &quot;Teacher&#39;s Experience, Years&quot;) + theme(legend.position = &quot;bottom&quot;) + scale_x_continuous(breaks = seq(from = 0, to = 10, by = 2)) Publishable Plot Since gender only exhibited a main effect and is not involved in any interactions, if would be a better use of space to not muddy the water with seperate panels. The Effect() function will estimate the marginal means using the reference category for categorical variables and the mean for continuous variables. effects::Effect(focal.predictors = c(&quot;extrav&quot;, &quot;texp&quot;), # choose not to investigate sex (the reference category will be used) mod = pop_lmer_2_re, xlevels = list(texp = c(5, 15, 25))) %&gt;% data.frame() %&gt;% dplyr::mutate(texp = factor(texp) %&gt;% forcats::fct_rev()) %&gt;% ggplot() + aes(x = extrav, y = fit, linetype = texp) + geom_ribbon(aes(ymin = lower, ymax = upper), fill = &quot;black&quot;, alpha = .3) + geom_line() + theme_bw() + labs(x = &quot;Pupil&#39;s Extroversion, Rated by Teacher&quot;, y = &quot;Estimated Marginal Mean\\nPupil Popularity, Mean Rating of Classroom Peers&quot;, color = &quot;Teacher&#39;s\\nExperience,\\nYears&quot;, linetype = &quot;Teacher&#39;s\\nExperience,\\nYears&quot;, alpha = &quot;Teacher&#39;s\\nExperience,\\nYears&quot;) + theme(legend.key.width = unit(2, &quot;cm&quot;), legend.background = element_rect(color = &quot;Black&quot;), legend.position = c(1, 0), legend.justification = c(1, 0)) + scale_linetype_manual(values = c(&quot;solid&quot;, &quot;dashed&quot;, &quot;dotted&quot;)) + scale_x_continuous(breaks = seq(from = 0, to = 10, by = 2)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 1)) 2.4.6.4 Interpretation After accounting for class-to-class variation and the effect of gender, a positive association was found between teacher rated extroversion and peer rated popularity. This relationship was more marked for less experienced teachers. "],
["intro-3-level-model-example-nurses-stress-intervention.html", "3 Intro 3-Level Model Example: Nurse’s Stress Intervention 3.1 Background 3.2 Exploratory Data Analysis 3.3 MLM: Null Model 3.4 Estimate the ICC 3.5 MLM: Add Fixed Effects 3.6 MLM: Add Random Slope 3.7 MLM: Add Cross-Level Interaction 3.8 Final Model 3.9 Interpretation 3.10 Reproduction of Table 2.5", " 3 Intro 3-Level Model Example: Nurse’s Stress Intervention library(tidyverse) library(haven) # read in SPSS dataset library(furniture) # nice table1() descriptives library(stargazer) # display nice tables: summary &amp; regression library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(psych) # contains some useful functions, like headTail library(car) # Companion to Applied Regression library(lme4) # Linear, generalized linear, &amp; nonlinear mixed models library(lmerTest) # Tests on lmer objects library(sjstats) # ICC calculations library(effects) # Effects for regression models library(optimx) # Different optimizers to solve mlm&#39;s 3.1 Background The text “Multilevel Analysis: Techniques and Applications, Third Edition” (Hox, Moerbeek, and Van de Schoot 2017) has a companion website which includes links to all the data files used throughout the book (housed on the book’s GitHub repository). The following example is used through out Hox, Moerbeek, and Van de Schoot (2017)’s chapater 2. From Appendix E: The nurses.sav file contains three-level simulated data from a hypothetical study on stress in hospitals. The data are from nurses working in wards nested within hospitals. It is a cluster-randomized experiment. In each of 25 hospitals, four wards are selected and randomly assigned to an experimental and a control condition. In the experimental condition, a training program is offered to all nurses to cope with job-related stress. After the program is completed, a sample of about 10 nurses from each ward is given a test that measures job-related stress. Additional variables (covariates) are: nurse age (years), nurse experience (years), nurse gender (0=male, 1 = female), type of ward (0=general care, 1=special care), and hospital size (0=small, 1 = medium, 2=large). The data have been generated to illustrate three-level analysis with a random slope for the effect of the intervention. Here the data is read in and the SPSS variables with labels are converted to \\(R\\) factors. data_raw &lt;- haven::read_sav(&quot;https://github.com/MultiLevelAnalysis/Datasets-third-edition-Multilevel-book/raw/master/chapter%202/Nurses/SPSS/Nurses.sav&quot;) %&gt;% haven::as_factor() # retain the labels from SPSS --&gt; factor 3.1.1 Unique Identifiers All standardized (starts with “Z”) and mean centered (starts with “C”) variables will be remove so that their creation may be shown later. A new indicator varible for nurses with be created by combining the hospital, ward, and nurse indicators. Having a unique, distinct identifier variable for each of the units on lower (Level 1 and 2) levels is helpful for multilevel anlayses. data_nurse &lt;- data_raw %&gt;% dplyr::mutate(genderF = factor(gender, labels = c(&quot;Male&quot;, &quot;Female&quot;))) %&gt;% # apply factor labels dplyr::mutate(id = paste(hospital, ward, nurse, sep = &quot;_&quot;) %&gt;% # cunique id for each student factor()) %&gt;% # declare id is a factor dplyr::mutate_at(vars(hospital, ward, wardid, nurse), factor) %&gt;% # declare to be factors dplyr::mutate(age = age %&gt;% as.character %&gt;% as.numeric) %&gt;% # declare to be numeric dplyr::select(id, wardid, nurse, ward, hospital, age, gender, genderF, experien, wardtype, hospsize, expcon, stress) # reduce variables included tibble::glimpse(data_nurse) Observations: 1,000 Variables: 13 $ id &lt;fct&gt; 1_1_1, 1_1_2, 1_1_3, 1_1_4, 1_1_5, 1_1_6, 1_1_7, 1_1_... $ wardid &lt;fct&gt; 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 1... $ nurse &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16... $ ward &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2,... $ hospital &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... $ age &lt;dbl&gt; 36, 45, 32, 57, 46, 60, 23, 32, 60, 45, 57, 47, 32, 4... $ gender &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,... $ genderF &lt;fct&gt; Male, Male, Male, Female, Female, Female, Female, Fem... $ experien &lt;dbl&gt; 11, 20, 7, 25, 22, 22, 13, 13, 17, 21, 24, 24, 14, 13... $ wardtype &lt;fct&gt; general care, general care, general care, general car... $ hospsize &lt;fct&gt; large, large, large, large, large, large, large, larg... $ expcon &lt;fct&gt; experiment, experiment, experiment, experiment, exper... $ stress &lt;dbl&gt; 7, 7, 7, 6, 6, 6, 6, 7, 7, 6, 6, 6, 6, 6, 6, 5, 5, 6,... 3.1.2 Centering Variables When variables are involved in an interaction, it may be advantageous to center the variables. Hox, Moerbeek, and Van de Schoot (2017) covers this in chapter 4. To center categorical variables: 1. Convert then to integers, starting with zero: \\(0, 1, \\dots\\) 2. Subtract the mean data_nurse %&gt;% dplyr::mutate(expconN = as.numeric(expcon) - 1) %&gt;% # Numeric Version of experimental condition dplyr::mutate(hospsizeN = as.numeric(hospsize) - 1) %&gt;% # Numeric Version of hospital size dplyr::select(expcon, expconN, hospsize, hospsizeN) %&gt;% summary() expcon expconN hospsize hospsizeN control :496 Min. :0.000 small :374 Min. :0.000 experiment:504 1st Qu.:0.000 medium:476 1st Qu.:0.000 Median :1.000 large :150 Median :1.000 Mean :0.504 Mean :0.776 3rd Qu.:1.000 3rd Qu.:1.000 Max. :1.000 Max. :2.000 data_nurse &lt;- data_nurse %&gt;% dplyr::mutate(expconN = as.numeric(expcon) - 1) %&gt;% # Numeric Version of experimental condition dplyr::mutate(hospsizeN = as.numeric(hospsize) - 1) %&gt;% # Numeric Version of hospital size dplyr::mutate(expconNG = expconN - 0.504) %&gt;% # Grand-Mean Centered version of experimental condition dplyr::mutate(hospsizeNG = hospsizeN - 0.776) # Grand-Mean Centered version of ehospital size data_nurse %&gt;% dplyr::select(expcon, expconNG) %&gt;% table() expconNG expcon -0.504 0.496 control 496 0 experiment 0 504 data_nurse %&gt;% dplyr::select(hospsize, hospsizeNG) %&gt;% table() hospsizeNG hospsize -0.776 0.224 1.224 small 374 0 0 medium 0 476 0 large 0 0 150 3.2 Exploratory Data Analysis 3.2.1 Summarize Descriptive Statistics 3.2.1.1 The stargazer package Most posters, journal articles, and reports start with a table of descriptive statistics. Since it tends to come first, this type of table is often refered to as Table 1. The stargazer() function can be used to create such a table, but only for the entire dataset (Hlavac 2018). I haven’t been able to find a way to get it to summarize subsamples and compare them in the standard format. # Knit to Website: type = &quot;html&quot; # Knit to PDF: type = &quot;latex&quot; # View on Screen: type = &quot;text&quot; data_nurse %&gt;% data.frame() %&gt;% stargazer::stargazer(title = &quot;Descriptive statistics, aggregate over entire sample&quot;, header = FALSE, type = &quot;html&quot;) Descriptive statistics, aggregate over entire sample Statistic N Mean St. Dev. Min Pctl(25) Pctl(75) Max age 1,000 43.005 12.042 23 33 53 64 gender 1,000 0.735 0.442 0 0 1 1 experien 1,000 17.057 6.042 1 13 21 38 stress 1,000 4.977 0.980 1 4 6 7 expconN 1,000 0.504 0.500 0 0 1 1 hospsizeN 1,000 0.776 0.689 0 0 1 2 expconNG 1,000 -0.000 0.500 -0.504 -0.504 0.496 0.496 hospsizeNG 1,000 -0.000 0.689 -0.776 -0.776 0.224 1.224 3.2.1.2 The furniture package Tyson Barrett’s furniture package includes the extremely useful function table1() which simplifies the common task of creating a stratified, comparative table of descriptive statistics. Full documentation can be accessed by executing ?furniture::table1. # Knit to Website: output = &quot;html&quot; # Knit to PDF: output = &quot;latex2&quot; # View on Screen: output = &quot;&quot;text&quot;, or &quot;markdown&quot;, &quot;html&quot; data_nurse %&gt;% furniture::table1(age, genderF, experien, wardtype, hospsize, hospsizeN, hospsizeNG, splitby = ~ expcon, # var to divide sample by test = TRUE, # test groups different? type = &quot;full&quot;, # give the test statistic output = &quot;html&quot;, # output for html caption = &quot;Compare Intervention groups on five main variables&quot;) # title Table 3.1: Compare Intervention groups on five main variables control experiment Test P-Value n = 496 n = 504 age T-Test: 0.82 0.411 43.3 (11.6) 42.7 (12.5) genderF Chi Square: 0.19 0.661 Male 135 (27.2%) 130 (25.8%) Female 361 (72.8%) 374 (74.2%) experien T-Test: 0.69 0.491 17.2 (5.8) 16.9 (6.3) wardtype Chi Square: 0 1 general care 247 (49.8%) 252 (50%) special care 249 (50.2%) 252 (50%) hospsize Chi Square: 0.01 0.993 small 185 (37.3%) 189 (37.5%) medium 237 (47.8%) 239 (47.4%) large 74 (14.9%) 76 (15.1%) hospsizeN T-Test: 0.01 0.992 0.8 (0.7) 0.8 (0.7) hospsizeNG T-Test: 0.01 0.992 0.0 (0.7) -0.0 (0.7) The t-test performed by the furniture::table1() function will always assume indepent groups and that HOV is not violated. This may or may not be appropriate. 3.3 MLM: Null Model In a Null, intercept-only, or Empty model, no predictors are included. 3.3.0.1 Fit the Model Fit the model to the data, with both ML and REML. nurse_lmer_0_re &lt;- lme4::lmer(stress ~ 1 + # Fixed Intercept for all nurses (1|hospital/ward), # Random Intercepts for wards within hospitals data = data_nurse, REML = TRUE) # fit via REML (the default) for ICC calculations nurse_lmer_0_ml &lt;- lme4::lmer(stress ~ 1 + # Fixed Intercept for all nurses (1|hospital/ward), # Random Intercepts for wards within hospitals data = data_nurse, REML = FALSE) # fit via ML for comparing FIXED effects inclusion # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(nurse_lmer_0_ml, nurse_lmer_0_re), custom.model.names = c(&quot;M0: Null, ML&quot;, &quot;M0: Null, REML&quot;), caption = &quot;NULL Model: different estimation methods&quot;, caption.above = TRUE, single.row = TRUE) NULL Model: different estimation methods M0: Null, ML M0: Null, REML (Intercept) 5.00 (0.11)*** 5.00 (0.11)*** AIC 1950.36 1952.95 BIC 1969.99 1972.58 Log Likelihood -971.18 -972.48 Num. obs. 1000 1000 Num. groups: ward:hospital 100 100 Num. groups: hospital 25 25 Var: ward:hospital (Intercept) 0.49 0.49 Var: hospital (Intercept) 0.16 0.17 Var: Residual 0.30 0.30 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 3.4 Estimate the ICC The ICC is calculated by dividing the between-group-variance (random intercept variance) by the total variance (i.e. sum of between-group-variance and within-group (residual) variance). lme4::VarCorr(nurse_lmer_0_re) Groups Name Std.Dev. ward:hospital (Intercept) 0.69916 hospital (Intercept) 0.41749 Residual 0.54887 lme4::VarCorr(nurse_lmer_0_re) %&gt;% print(comp = c(&quot;Variance&quot;, &quot;Std.Dev&quot;), digits = 3) Groups Name Variance Std.Dev. ward:hospital (Intercept) 0.489 0.699 hospital (Intercept) 0.174 0.417 Residual 0.301 0.549 vc &lt;- lme4::VarCorr(nurse_lmer_0_re) %&gt;% data.frame() pie(x = vc$vcov, labels = vc$grp) The sjstats package has a few really helpful funcitons: sjstats::re_var(nurse_lmer_0_re) Within-group-variance: 0.301 Between-group-variance: 0.489 (ward:hospital) Between-group-variance: 0.174 (hospital) \\[ \\begin{align*} \\text{hospitals} \\rightarrow \\; &amp; \\sigma^2_{v0} = 0.417^2 = 0.174\\\\ \\text{wards within hospitals} \\rightarrow \\; &amp; \\sigma^2_{u0} = 0.699^2 = 0.489\\\\ \\text{nurses within wards within hospitals} \\rightarrow \\; &amp; \\sigma^2_{e} = 0.549^2 = 0.301\\\\ \\end{align*} \\] Intraclass Correlation (ICC) Formula, 3 level model - Davis and Scott Method \\[ \\overbrace{\\rho_{mid}}^{\\text{ICC}\\atop\\text{at level 2}} = \\frac{\\overbrace{\\sigma^2_{u0}}^{\\text{Random Intercept}\\atop\\text{Variance Level 2}}} {\\underbrace{\\sigma^2_{v0}+\\sigma^2_{u0}+\\sigma^2_{e}}_{\\text{Total}\\atop\\text{Variance}}} \\tag{Hox 2.16} \\] \\[ \\overbrace{\\rho_{top}}^{\\text{ICC}\\atop\\text{ at level 3}} = \\frac{\\overbrace{\\sigma^2_{u0}}^{\\text{Random Intercept}\\atop\\text{Variance Level 3}}} {\\underbrace{\\sigma^2_{v0}+\\sigma^2_{u0}+\\sigma^2_{e}}_{\\text{Total}\\atop\\text{Variance}}} \\tag{Hox 2.17} \\] 0.489 / (0.174 + 0.489 + 0.301) # middle level (wards) [1] 0.5072614 0.174 / (0.174 + 0.489 + 0.301) # top level (hospitals) [1] 0.1804979 For more than two levels, the ‘sjstats::icc()’ function computes ICC’s by the Davis and Scott method. sjstats::icc(nurse_lmer_0_re) Linear mixed model Family : gaussian (identity) Formula: stress ~ 1 + (1 | hospital/ward) ICC (ward:hospital): 0.5069 ICC (hospital): 0.1807 The proportion of variange in nurse stress level is 0.51 at the ward level and 0.18 at the hospital level. To test if the three level model is justified statistically, compare the null models with and without the nesting of wards in hospitals. nurse_lmer_0_re_2level &lt;- lme4::lmer(stress ~ 1 + (1|wardid), # each hospital contains several wards data = data_nurse, REML = TRUE) # fit via REML (the default) for ICC calculations # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(nurse_lmer_0_re_2level, nurse_lmer_0_re), custom.model.names = c(&quot;2 levels&quot;, &quot;3 levels&quot;), caption = &quot;MLM: Two or Three Levels?&quot;, caption.above = TRUE, single.row = TRUE) MLM: Two or Three Levels? 2 levels 3 levels (Intercept) 5.00 (0.08)*** 5.00 (0.11)*** AIC 1958.43 1952.95 BIC 1973.15 1972.58 Log Likelihood -976.21 -972.48 Num. obs. 1000 1000 Num. groups: wardid 100 Var: wardid (Intercept) 0.66 Var: Residual 0.30 0.30 Num. groups: ward:hospital 100 Num. groups: hospital 25 Var: ward:hospital (Intercept) 0.49 Var: hospital (Intercept) 0.17 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 The deviance test or likelihood-ratio test shows that the inclusion of the nesting of wards within hospitals better explains the variance in nurse stress levels. anova(nurse_lmer_0_re, nurse_lmer_0_re_2level, refit = FALSE) Data: data_nurse Models: nurse_lmer_0_re_2level: stress ~ 1 + (1 | wardid) nurse_lmer_0_re: stress ~ 1 + (1 | hospital/ward) Df AIC BIC logLik deviance Chisq Chi Df nurse_lmer_0_re_2level 3 1958.4 1973.2 -976.21 1952.4 nurse_lmer_0_re 4 1953.0 1972.6 -972.48 1945.0 7.4738 1 Pr(&gt;Chisq) nurse_lmer_0_re_2level nurse_lmer_0_re 0.00626 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 3.5 MLM: Add Fixed Effects 3.5.1 Fit the Model Hox, Moerbeek, and Van de Schoot (2017), page 22: “In this example, the variable expcon is of main interest, and the other variables are covariates. Their funciton is to control for differences between the groups, which can occur even if randomization is used, especially with small samples, and to explain variance in the outcome variable stress. To the extent that these variables successfully explain the variance, the power of the test for the effect of expcon will be increased.” nurse_lmer_1_ml &lt;- lme4::lmer(stress ~ expcon + # experimental condition = CATEGORICAL FACTOR age + gender + experien + # level 1 covariates wardtype + # level 2 covariates hospsize + # level 3 covariates, hospital size = CATEGORICAL FACTOR (1|hospital/ward), # Random Intercepts for wards within hospitals data = data_nurse, REML = FALSE) # fit via ML for nested FIXED effects # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(nurse_lmer_0_ml, nurse_lmer_1_ml), custom.model.names = c(&quot;M0: null&quot;, &quot;M1: fixed pred&quot;), caption = &quot;Nested Models: Fixed effects via ML&quot;, caption.above = TRUE, single.row = TRUE) Nested Models: Fixed effects via ML M0: null M1: fixed pred (Intercept) 5.00 (0.11)*** 5.38 (0.18)*** expconexperiment -0.70 (0.12)*** age 0.02 (0.00)*** gender -0.45 (0.03)*** experien -0.06 (0.00)*** wardtypespecial care 0.05 (0.12) hospsizemedium 0.49 (0.19)** hospsizelarge 0.90 (0.26)*** AIC 1950.36 1626.32 BIC 1969.99 1680.30 Log Likelihood -971.18 -802.16 Num. obs. 1000 1000 Num. groups: ward:hospital 100 100 Num. groups: hospital 25 25 Var: ward:hospital (Intercept) 0.49 0.33 Var: hospital (Intercept) 0.16 0.10 Var: Residual 0.30 0.22 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 3.5.2 Assess Significance anova(nurse_lmer_0_ml, nurse_lmer_1_ml) Data: data_nurse Models: nurse_lmer_0_ml: stress ~ 1 + (1 | hospital/ward) nurse_lmer_1_ml: stress ~ expcon + age + gender + experien + wardtype + hospsize + nurse_lmer_1_ml: (1 | hospital/ward) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) nurse_lmer_0_ml 4 1950.4 1970.0 -971.18 1942.4 nurse_lmer_1_ml 11 1626.3 1680.3 -802.16 1604.3 338.04 7 &lt; 2.2e-16 nurse_lmer_0_ml nurse_lmer_1_ml *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 It is clear that the inclusing of these fixed, main effects improves the model’s fit, but it is questionable that the type of ward is significant (Wald test is non-significant). Rather than test it directly, we will leave it in the model. This is common practice to show that an expected variable is not significant. 3.5.3 Centering Variables Because we will will find that the experimental condition is moderated by hospital size (in other words there is a significant interaction between expcon and hospsize), Hox, Moerbeek, and Van de Schoot (2017) presents the models fit with centered values for these two variables. Let us see how this changes the model. (1) Experimental Condition Experimental conditon is a BINARY or two-level factor. When it is alternatively coded as a numeric, continuous variable taking the values of zero (\\(0\\)) for the reference category and one (\\(1\\)) for the other category, the model estimates are exactly the same, including the paramters for the variables and the intercept, AND the model fit statistics. When the numeric, continuous variable is further grand-mean centered by additionally subtraction the MEAN of the numberic version, the value of the intercept is the only estimate that changes. nurse_lmer_1a_ml &lt;- lme4::lmer(stress ~ expconN + # experimental condition = CONTINUOUS CODED 0/1 age + gender + experien + wardtype + hospsize + # hospital size = CATEGORICAL FACTOR (1|hospital/ward), data = data_nurse, REML = FALSE) nurse_lmer_1b_ml &lt;- lme4::lmer(stress ~ expconNG + # experimental condition = CONTINUOUS GRAND-MEAN CENTERED age + gender + experien + wardtype + hospsize + # hospital size = CATEGORICAL FACTOR (1|hospital/ward), data = data_nurse, REML = FALSE) # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(nurse_lmer_1_ml, nurse_lmer_1a_ml, nurse_lmer_1b_ml), custom.model.names = c(&quot;Factor&quot;, &quot;0 vs 1&quot;, &quot;Centered&quot;), caption = &quot;MLM: Model 1 - Expereimental Condiditon Coding (2-levels)&quot;, caption.above = TRUE, single.row = TRUE) MLM: Model 1 - Expereimental Condiditon Coding (2-levels) Factor 0 vs 1 Centered (Intercept) 5.38 (0.18)*** 5.38 (0.18)*** 5.03 (0.17)*** expconexperiment -0.70 (0.12)*** age 0.02 (0.00)*** 0.02 (0.00)*** 0.02 (0.00)*** gender -0.45 (0.03)*** -0.45 (0.03)*** -0.45 (0.03)*** experien -0.06 (0.00)*** -0.06 (0.00)*** -0.06 (0.00)*** wardtypespecial care 0.05 (0.12) 0.05 (0.12) 0.05 (0.12) hospsizemedium 0.49 (0.19)** 0.49 (0.19)** 0.49 (0.19)** hospsizelarge 0.90 (0.26)*** 0.90 (0.26)*** 0.90 (0.26)*** expconN -0.70 (0.12)*** expconNG -0.70 (0.12)*** AIC 1626.32 1626.32 1626.32 BIC 1680.30 1680.30 1680.30 Log Likelihood -802.16 -802.16 -802.16 Num. obs. 1000 1000 1000 Num. groups: ward:hospital 100 100 100 Num. groups: hospital 25 25 25 Var: ward:hospital (Intercept) 0.33 0.33 0.33 Var: hospital (Intercept) 0.10 0.10 0.10 Var: Residual 0.22 0.22 0.22 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 (2) Hospital Size Experimental conditon is a three-level factor. When it is alternatively coded as a numeric, continuous variables taking the values of whole numbers, starting with zero (\\(0, 1, 2, \\dots\\)), the model there will only be ONE parameter estimated instead of several (one less than the number of categories). This is becuase the levels are treated as being equally different from each other in terms of the outcome. This treats the effect of the categorical variable as if it is linear, which may or may not be appropriate. User beware! When the numeric, continuous variable is further grand-mean centered by additionally subtraction the MEAN of the numberic version, the value of the intercept is the only estimate that changes. nurse_lmer_1c_ml &lt;- lme4::lmer(stress ~ expconNG + # experimental condition = CONTINUOUS GRAND-MEAN CENTERED age + gender + experien + wardtype + hospsizeN + # hospital size = CONTINUOUS CODED 0/1 (1|hospital/ward), data = data_nurse, REML = FALSE) nurse_lmer_1d_ml &lt;- lme4::lmer(stress ~ expconNG + # experimental condition = CONTINUOUS GRAND-MEAN CENTERED age + gender + experien + wardtype + hospsizeNG + # hospital size = CONTINUOUS GRAND-MEAN CENTERED (1|hospital/ward), data = data_nurse, REML = FALSE) # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(nurse_lmer_1b_ml, nurse_lmer_1c_ml, nurse_lmer_1d_ml), custom.model.names = c(&quot;Factor&quot;, &quot;0 vs 1&quot;, &quot;Centered&quot;), caption = &quot;MLM: Model 1 - Hospital Coding (3-levels)&quot;, caption.above = TRUE, single.row = TRUE) MLM: Model 1 - Hospital Coding (3-levels) Factor 0 vs 1 Centered (Intercept) 5.03 (0.17)*** 5.04 (0.16)*** 5.40 (0.12)*** expconNG -0.70 (0.12)*** -0.70 (0.12)*** -0.70 (0.12)*** age 0.02 (0.00)*** 0.02 (0.00)*** 0.02 (0.00)*** gender -0.45 (0.03)*** -0.45 (0.03)*** -0.45 (0.03)*** experien -0.06 (0.00)*** -0.06 (0.00)*** -0.06 (0.00)*** wardtypespecial care 0.05 (0.12) 0.05 (0.12) 0.05 (0.12) hospsizemedium 0.49 (0.19)** hospsizelarge 0.90 (0.26)*** hospsizeN 0.46 (0.12)*** hospsizeNG 0.46 (0.12)*** AIC 1626.32 1624.36 1624.36 BIC 1680.30 1673.44 1673.44 Log Likelihood -802.16 -802.18 -802.18 Num. obs. 1000 1000 1000 Num. groups: ward:hospital 100 100 100 Num. groups: hospital 25 25 25 Var: ward:hospital (Intercept) 0.33 0.33 0.33 Var: hospital (Intercept) 0.10 0.10 0.10 Var: Residual 0.22 0.22 0.22 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 3.6 MLM: Add Random Slope Hox, Moerbeek, and Van de Schoot (2017), page 22: “Although logically we can test if explanatory variables at the first level have random coefficients (slopes) at the second or third level, and if explanatory variables at teh second level have random coefficients (slopes) at the third level, these possibilities are not pursued. We DO test a model with a random coefficient (slope) for expcon at the third level, where there turns out to be significant slope variation.” 3.6.1 Fit the Model nurse_lmer_1d_re &lt;- lme4::lmer(stress ~ expconNG + # experimental condition = CONTINUOUS GRAND-MEAN CENTERED age + gender + experien + # level 1 covariates wardtype + # level 2 covariate hospsizeNG + # level 3 covariate, hospital size = CONTINUOUS GRAND-MEAN CENTERED (1|hospital/ward), # Random Intercepts for wards within hospitals data = data_nurse, REML = TRUE) # fit via REML for nested Random Effects nurse_lmer_2_re &lt;- lme4::lmer(stress ~ expconNG + # experimental condition = CONTINUOUS GRAND-MEAN CENTERED age + gender + experien + # level 1 covariates wardtype + # level 2 covariate hospsizeNG + # level 3 covariate, hospital size = CONTINUOUS GRAND-MEAN CENTERED (1|hospital/ward) + # Random Intercepts for wards within hospitals (0 + expconNG|hospital), # RANDOM SLOPES for exp cond within hospital (does not vary witin a ward!) data = data_nurse, REML = TRUE) # fit via REML for nested Random Effects # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(nurse_lmer_1d_re, nurse_lmer_2_re), custom.model.names = c(&quot;M1: RI&quot;, &quot;M2: RIAS&quot;), caption = &quot;Nested Models: Random Slope via REML&quot;, caption.above = TRUE, single.row = TRUE) Nested Models: Random Slope via REML M1: RI M2: RIAS (Intercept) 5.40 (0.12)*** 5.39 (0.11)*** expconNG -0.70 (0.12)*** -0.70 (0.18)*** age 0.02 (0.00)*** 0.02 (0.00)*** gender -0.45 (0.03)*** -0.45 (0.03)*** experien -0.06 (0.00)*** -0.06 (0.00)*** wardtypespecial care 0.05 (0.12) 0.05 (0.07) hospsizeNG 0.46 (0.13)*** 0.46 (0.13)*** AIC 1659.89 1633.18 BIC 1708.97 1687.17 Log Likelihood -819.94 -805.59 Num. obs. 1000 1000 Num. groups: ward:hospital 100 100 Num. groups: hospital 25 25 Var: ward:hospital (Intercept) 0.34 Var: hospital (Intercept) 0.11 0.17 Var: Residual 0.22 0.22 Var: ward.hospital (Intercept) 0.11 Var: hospital.1 expconNG 0.69 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 3.6.2 Assess Significance anova(nurse_lmer_1d_re, nurse_lmer_2_re, refit = FALSE) Data: data_nurse Models: nurse_lmer_1d_re: stress ~ expconNG + age + gender + experien + wardtype + hospsizeNG + nurse_lmer_1d_re: (1 | hospital/ward) nurse_lmer_2_re: stress ~ expconNG + age + gender + experien + wardtype + hospsizeNG + nurse_lmer_2_re: (1 | hospital/ward) + (0 + expconNG | hospital) Df AIC BIC logLik deviance Chisq Chi Df nurse_lmer_1d_re 10 1659.9 1709.0 -819.94 1639.9 nurse_lmer_2_re 11 1633.2 1687.2 -805.59 1611.2 28.708 1 Pr(&gt;Chisq) nurse_lmer_1d_re nurse_lmer_2_re 8.417e-08 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The inclusion of a random slope effect for the experimental condition expcon significantly improves the models’s fit, thus is should be retained. 3.7 MLM: Add Cross-Level Interaction Hox, Moerbeek, and Van de Schoot (2017), page 22: “The varying slope can be predicted by adding a cross-level interaction between the variables expcon and hospsize. In view of this interaction, the variables expcon and hospsize have been centered on tehir overal means.” 3.7.1 Fit the Model nurse_lmer_2_ml &lt;- lme4::lmer(stress ~ expconNG + # experimental condition = CONTINUOUS GRAND-MEAN CENTERED age + gender + experien + # level 1 covariates wardtype + # level 2 covariate hospsizeNG + # level 3 covariate, hospital size = CONTINUOUS GRAND-MEAN CENTERED (1|hospital/ward) + # Random Intercepts for wards within hospitals (0 + expconNG|hospital), # RANDOM SLOPES for exp cond within hospital (does not vary witin a ward!) data = data_nurse, REML = FALSE) # fit via ML for nested FIXED Effects nurse_lmer_3_ml &lt;- lme4::lmer(stress ~ expconNG + # experimental condition = CONTINUOUS GRAND-MEAN CENTERED age + gender + experien + # level 1 covariates wardtype + # level 2 covariate hospsizeNG + # level 3 covariate, hospital size = CONTINUOUS GRAND-MEAN CENTERED expconNG*hospsizeNG + # CROSS-LEVEL interaction (1|hospital/ward) + # Random Intercepts for wards within hospitals (0 + expconNG|hospital), # RANDOM SLOPES for exp cond within hospital (does not vary witin a ward!) data = data_nurse, REML = FALSE) # fit via ML for nested FIXED Effects # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(nurse_lmer_2_ml, nurse_lmer_3_ml), custom.model.names = c(&quot;M2: RAIS&quot;, &quot;M3: Xlevel Int&quot;), caption = &quot;Nested Models: Fixed Cross-Level Interaction via ML&quot;, caption.above = TRUE, single.row = TRUE) Nested Models: Fixed Cross-Level Interaction via ML M2: RAIS M3: Xlevel Int (Intercept) 5.39 (0.11)*** 5.39 (0.11)*** expconNG -0.70 (0.18)*** -0.72 (0.11)*** age 0.02 (0.00)*** 0.02 (0.00)*** gender -0.46 (0.03)*** -0.46 (0.03)*** experien -0.06 (0.00)*** -0.06 (0.00)*** wardtypespecial care 0.05 (0.07) 0.05 (0.07) hospsizeNG 0.46 (0.12)*** 0.46 (0.12)*** expconNG:hospsizeNG 1.00 (0.16)*** AIC 1597.48 1576.07 BIC 1651.47 1634.96 Log Likelihood -787.74 -776.03 Num. obs. 1000 1000 Num. groups: ward:hospital 100 100 Num. groups: hospital 25 25 Var: ward.hospital (Intercept) 0.11 0.11 Var: hospital (Intercept) 0.15 0.15 Var: hospital.1 expconNG 0.66 0.18 Var: Residual 0.22 0.22 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 3.7.2 Assess Significance anova(nurse_lmer_2_ml, nurse_lmer_3_ml) Data: data_nurse Models: nurse_lmer_2_ml: stress ~ expconNG + age + gender + experien + wardtype + hospsizeNG + nurse_lmer_2_ml: (1 | hospital/ward) + (0 + expconNG | hospital) nurse_lmer_3_ml: stress ~ expconNG + age + gender + experien + wardtype + hospsizeNG + nurse_lmer_3_ml: expconNG * hospsizeNG + (1 | hospital/ward) + (0 + expconNG | nurse_lmer_3_ml: hospital) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) nurse_lmer_2_ml 11 1597.5 1651.5 -787.74 1575.5 nurse_lmer_3_ml 12 1576.1 1635.0 -776.03 1552.1 23.413 1 1.307e-06 nurse_lmer_2_ml nurse_lmer_3_ml *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 There is evidence that hospital size moderated the effect of the intervention. We will want to plot the estimated marginal means to interpret the meaning of this interaction. 3.8 Final Model 3.8.1 Fit the model The final model should be fit via REML. nurse_lmer_3_re &lt;- lme4::lmer(stress ~ expconNG + # experimental condition = CONTINUOUS GRAND-MEAN CENTERED age + gender + experien + # level 1 covariates wardtype + # level 2 covariate hospsizeNG + # level 3 covariate, hospital size = CONTINUOUS GRAND-MEAN CENTERED expconNG*hospsizeNG + # CROSS-LEVEL interaction (1|hospital/ward) + # Random Intercepts for wards within hospitals (0 + expconNG|hospital), # RANDOM SLOPES for exp cond within hospital (does not vary witin a ward!) data = data_nurse, REML = TRUE) # fit via REML for final model 3.8.2 Table of Estimated Parameters # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(nurse_lmer_3_re), custom.model.names = c(&quot;M3: Xlevel Int&quot;), caption = &quot;Final Model: with REML&quot;, caption.above = TRUE, single.row = TRUE) Final Model: with REML M3: Xlevel Int (Intercept) 5.39 (0.11)*** expconNG -0.72 (0.12)*** age 0.02 (0.00)*** gender -0.46 (0.03)*** experien -0.06 (0.00)*** wardtypespecial care 0.05 (0.07) hospsizeNG 0.46 (0.13)*** expconNG:hospsizeNG 1.00 (0.17)*** AIC 1614.47 BIC 1673.36 Log Likelihood -795.23 Num. obs. 1000 Num. groups: ward:hospital 100 Num. groups: hospital 25 Var: ward.hospital (Intercept) 0.11 Var: hospital (Intercept) 0.17 Var: hospital.1 expconNG 0.20 Var: Residual 0.22 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 ICC for unconditional and conditional model Usually, the ICC is calculated for the null model (“unconditional model”). However, according to Raudenbush and Bryk (2002) or Rabe-Hesketh and Skrondal (2012) it is also feasible to compute the ICC for full models with covariates (“conditional models”) and compare how much a level-2 variable explains the portion of variation in the grouping structure (random intercept). ICC for random-slope models Caution!!! For models with random slopes and random intercepts, the ICC would differ at each unit of the predictors. Hence, the ICC for these kind of models cannot be understood simply as proportion of variance (see Goldstein et al. 2010). For convenience reasons, as the ‘icc()’ function also extracts the different random effects variances, the ICC for random-slope-intercept-models is reported nonetheless, but it is usually not a meaningful summary of the proportion of variances. ICC for models with multiple or nested random effects Caution: By default, for three-level-models, depending on the nested structure of the model, or for models with multiple random effects, ‘icc()’ only reports the proportion of variance explained for each grouping level. Use ‘adjusted = TRUE’ to calculate the adjusted and conditional ICC. sjstats::icc(nurse_lmer_3_re) Linear mixed model Family : gaussian (identity) Formula: stress ~ expconNG + age + gender + experien + wardtype + hospsizeNG + expconNG * hospsizeNG + (1 | hospital/ward) + (0 + expconNG | hospital) ICC (ward.hospital): 0.1595 ICC (hospital): 0.2395 ICC (hospital.1): 0.2898 3.8.3 Visualization: Estimated Marginal Means Plot Although there are many variables in this model, only two are involved in any interaction(s). For this reason, we will choose to display the estimated marginal means across only experimental condition and hospital size. For this illustration, all other continuous predictors are taken to be at their mean and categorical predictors at their reference category. sjPlot::plot_model(nurse_lmer_3_re, type = &quot;pred&quot;, terms = c(&quot;hospsizeNG&quot;, &quot;expconNG&quot;)) sjPlot::plot_model(nurse_lmer_3_re, type = &quot;pred&quot;, terms = c(&quot;hospsizeNG [-0.776, 0.224, 1.224]&quot;, &quot;expconNG [-0.504, 0.496]&quot;)) + scale_x_continuous(breaks = c(-0.776, 0.224, 1.224), labels = c(&quot;small&quot;, &quot;medium&quot;, &quot;large&quot;)) + scale_color_manual(labels = c(&quot;Control&quot;, &quot;Intervention&quot;), values = c(&quot;red&quot;, &quot;blue&quot;)) + labs(title = &quot;Multilevel Modeling of Hospital Nurse Stress Intervention&quot;, subtitle = &quot;Ribbons display 95% Confidene Intervals&quot;, x = &quot;Hospital Size&quot;, y = &quot;Estimated Marginal Mean\\nNorse&#39;s Stress Score&quot;, color = &quot;Condition&quot;) + theme_bw() Get Estimated Marginal Means - default ‘nice’ predictor values: Focal predictors: All combinations of… expcon categorical, both levels control and experiment hospsize categorical, all three levelssmall , medium, large Always followed by: fit estimated marginal mean se standard error for the marginal mean lower lower end of the 95% confidence interval around the estimated marginal mean upper upper end of the 95% confidence interval around the estimated marginal mean effects::Effect(focal.predictors = c(&quot;expconNG&quot;, &quot;hospsizeNG&quot;), mod = nurse_lmer_3_re, xlevels = list(expconNG = c(-0.504, 0.496), hospsizeNG = c(-0.776, 0.224, 1.224))) %&gt;% data.frame() expconNG hospsizeNG fit se lower upper 1 -0.504 -0.776 5.382989 0.1631614 5.062808 5.703170 2 0.496 -0.776 3.885142 0.1626431 3.565978 4.204305 3 -0.504 0.224 5.341652 0.1112654 5.123309 5.559994 4 0.496 0.224 4.842621 0.1109172 4.624961 5.060280 5 -0.504 1.224 5.300315 0.2138891 4.880588 5.720042 6 0.496 1.224 5.800100 0.2131947 5.381735 6.218464 effects::Effect(focal.predictors = c(&quot;expconNG&quot;, &quot;hospsizeNG&quot;), mod = nurse_lmer_3_re, xlevels = list(expconNG = c(-0.504, 0.496), hospsizeNG = c(-0.776, 0.224, 1.224))) %&gt;% data.frame() %&gt;% dplyr::mutate(expcon = factor(expconNG + 0.504, labels = c(&quot;Control&quot;, &quot;Intervention&quot;))) %&gt;% dplyr::mutate(hopsize = factor(hospsizeNG + 0.776, labels = c(&quot;Small&quot;, &quot;Medium&quot;, &quot;Large&quot;))) expconNG hospsizeNG fit se lower upper expcon 1 -0.504 -0.776 5.382989 0.1631614 5.062808 5.703170 Control 2 0.496 -0.776 3.885142 0.1626431 3.565978 4.204305 Intervention 3 -0.504 0.224 5.341652 0.1112654 5.123309 5.559994 Control 4 0.496 0.224 4.842621 0.1109172 4.624961 5.060280 Intervention 5 -0.504 1.224 5.300315 0.2138891 4.880588 5.720042 Control 6 0.496 1.224 5.800100 0.2131947 5.381735 6.218464 Intervention hopsize 1 Small 2 Small 3 Medium 4 Medium 5 Large 6 Large effects::Effect(focal.predictors = c(&quot;expconNG&quot;, &quot;hospsizeNG&quot;), mod = nurse_lmer_3_re, xlevels = list(expconNG = c(-0.504, 0.496), hospsizeNG = c(-0.776, 0.224, 1.224))) %&gt;% data.frame() %&gt;% dplyr::mutate(expcon = factor(expconNG + 0.504, labels = c(&quot;Control&quot;, &quot;Intervention&quot;))) %&gt;% dplyr::mutate(hospsize = factor(hospsizeNG + 0.776, labels = c(&quot;Small&quot;, &quot;Medium&quot;, &quot;Large&quot;))) %&gt;% ggplot() + aes(x = hospsize, y = fit, group = expcon, shape = expcon, color = expcon) + geom_errorbar(aes(ymin = fit - se, # mean plus/minus one Std Error ymax = fit + se), width = .4, position = position_dodge(width = .2)) + geom_errorbar(aes(ymin = lower, # 95% CIs ymax = upper), width = .2, position = position_dodge(width = .2)) + geom_line(aes(linetype = expcon), position = position_dodge(width = .2)) + geom_point(size = 4, position = position_dodge(width = .2)) + theme_bw() + labs(x = &quot;Hospital Size&quot;, y = &quot;Estimated Marginal Mean, Stress&quot;, shape = &quot;Condition&quot;, color = &quot;Condition&quot;, linetype = &quot;Condition&quot;) + theme(legend.key.width = unit(2, &quot;cm&quot;), legend.background = element_rect(color = &quot;black&quot;), legend.position = c(1, 0), legend.justification = c(1, 0)) This plot illustrates the estimated marginal means among male (gender’s reference category) nurses at the overall mean age (43.01 years), with the mean level experience (17.06 years), since thoes variables were not included as focal.predictors in the effects::Effect() function. Different values for thoes predictors would yield the exact sample plot, shifted as a whole either up or down. 3.9 Interpretation There is evidence this intervention lowered stress among nurses working in small hospitals and to a smaller degree in medium sized hospitals. The intervention did not exhibit an effect in large hospitals. 3.9.1 Strength This analysis was able to incorporated all three levels of clustering while additionally controlling for many covariates, both categorical (nurse gender and ward type) and continuous (nurse age and experience in years). Also heterogeneity was accounted for in terms of the interventions’s effect at various hospitals. This would NOT be possible via any ANOVA type anlysis. 3.9.2 Weakness The approach presented by Hox, Moerbeek, and Van de Schoot (2017) and shown above involved mean-centering categorical variables. This would only be appropriate for a factor with more than two levels if its effect on the outcome was linear. Also, as the mean-centered variables are treated as continuous variables, post hoc tests are increasingly difficult. 3.10 Reproduction of Table 2.5 Hox, Moerbeek, and Van de Schoot (2017) presents a table on page 23 comparing various models. Note, that table includes models only fit via maximum likelihood, not REML. Also, the model \\(M_3\\): with cross-level interaction is slightly different for an unknown reason. # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(nurse_lmer_0_ml, nurse_lmer_1d_ml, nurse_lmer_2_ml, nurse_lmer_3_ml), custom.model.names = c(&quot;M0&quot;, &quot;M1&quot;, &quot;M2&quot;, &quot;M3&quot;), caption = &quot;Hox Table 2.5: Models for stress in hospitals and wards&quot;, caption.above = TRUE, single.row = TRUE) Hox Table 2.5: Models for stress in hospitals and wards M0 M1 M2 M3 (Intercept) 5.00 (0.11)*** 5.40 (0.12)*** 5.39 (0.11)*** 5.39 (0.11)*** expconNG -0.70 (0.12)*** -0.70 (0.18)*** -0.72 (0.11)*** age 0.02 (0.00)*** 0.02 (0.00)*** 0.02 (0.00)*** gender -0.45 (0.03)*** -0.46 (0.03)*** -0.46 (0.03)*** experien -0.06 (0.00)*** -0.06 (0.00)*** -0.06 (0.00)*** wardtypespecial care 0.05 (0.12) 0.05 (0.07) 0.05 (0.07) hospsizeNG 0.46 (0.12)*** 0.46 (0.12)*** 0.46 (0.12)*** expconNG:hospsizeNG 1.00 (0.16)*** AIC 1950.36 1624.36 1597.48 1576.07 BIC 1969.99 1673.44 1651.47 1634.96 Log Likelihood -971.18 -802.18 -787.74 -776.03 Num. obs. 1000 1000 1000 1000 Num. groups: ward:hospital 100 100 100 100 Num. groups: hospital 25 25 25 25 Var: ward:hospital (Intercept) 0.49 0.33 Var: hospital (Intercept) 0.16 0.10 0.15 0.15 Var: Residual 0.30 0.22 0.22 0.22 Var: ward.hospital (Intercept) 0.11 0.11 Var: hospital.1 expconNG 0.66 0.18 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 Deviance: c(deviance(nurse_lmer_0_ml), deviance(nurse_lmer_1d_ml), deviance(nurse_lmer_2_ml), deviance(nurse_lmer_3_ml)) %&gt;% round(1) [1] 1942.4 1604.4 1575.5 1552.1 "],
["intro-centering-and-scaling-example-reading-achievement-2-levels-only.html", "4 Intro Centering and Scaling Example: Reading Achievement (2-levels only) 4.1 Background 4.2 Exploratory Data Analysis 4.3 Single-Level Regression 4.4 MLM - Step 1: Null Model, only fixed and random intercepts 4.5 MLM - Step 2: Add Lower-level explanatory variables, fixed, ML 4.6 MLM - Step 3: Higher-level explanatory variables, fixed, ML 4.7 MLM - Step 4: Explanatory variables predict Slopes, random, REML 4.8 MLM - Step 5: Cross-Level interactions between explanatory variables - fixed, ML 4.9 Centering Predictors: Change Center 4.10 Rescaling Predictors: Change Units or Standardize 4.11 Final Model", " 4 Intro Centering and Scaling Example: Reading Achievement (2-levels only) library(tidyverse) # all things tidy library(haven) # read in SPSS dataset library(furniture) # nice table1() descriptives library(stargazer) # display nice tables: summary &amp; regression library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(psych) # contains some useful functions, like headTail library(sjstats) # ICC calculations library(effects) # Effects for regression models library(optimx) # Different optimizers to solve mlm&#39;s library(lme4) # non-linear mixed-effects models library(haven) # read in SPSS dataset 4.1 Background The following example was included in the text “Multilevel Modeling in R” by Finch, Bolin, and Kelley (2016). The datasets for this textbook may be downloaded from the website: http://www.mlminr.com/data-sets/. I was unable to find any documentation on this dataset in the book or online, so I contacted the authors. There were unable to provide much either, but based on visual inspection designated the class of factor to thoes vairables that seem to represent categorical quantities. The labels for gender and class size are relative to the frequencies in the journal article the authors did point me to (although the samples sizes do not match up). FOR THIS CHAPTER WE WILL IGNORE ALL LEVELS EXCEPT FOR STUDNETS BEING NESTED WITHIN SCHOOLS. Read the SPSS data in with the haven package . data_raw &lt;- haven::read_sav(&quot;http://www.mlminr.com/data-sets/Achieve.sav?attredirects=0&quot;) Declare all categorical variables to be factors and apply labels where meaningful. Student-specific * gender = Male or Female * age = Age, in months * gevocab = Vocabulary Score * geread = Reading Score Class-specific * classsize = category of class’s size School-specific * senroll = school enrollment * ses = school’s SES level data_achieve &lt;- data_raw %&gt;% dplyr::mutate_at(vars(id, region, corp, school, class), factor) %&gt;% dplyr::mutate(gender = gender %&gt;% factor(labels = c(&quot;Female&quot;, &quot;Male&quot;))) %&gt;% dplyr::mutate(classize = classize %&gt;% factor(labels = c(&quot;12-17&quot;, &quot;18-21&quot;, &quot;22-26&quot;, &quot;&gt;26&quot;))) %&gt;% dplyr::select(id, region, corp, school, class, # Identifiers gender, age, geread, gevocab, # Pupil-level vars classize, # Class-Level vars senroll, ses) # School-level vars 4.1.1 Sample Structure It is obvious that the sample is hiarchical in nature. The nesting starts with students (level 1) nested within class (level 2), which are further nested within school (level 3), corp (level 4), and finally region (level 5). For this chapter we will only focus on TWO levels: students (level 1) are the units on which the outcome is measured and schools (level 2) are the units in which they are nested. The number of regions = 9: num_regions &lt;- data_achieve %&gt;% dplyr::group_by(region) %&gt;% dplyr::tally() %&gt;% nrow() num_regions [1] 9 The number of corps = 60: num_corps &lt;- data_achieve %&gt;% dplyr::group_by(region, corp) %&gt;% dplyr::tally() %&gt;% nrow() num_corps [1] 60 The number of schools = 160 num_schools &lt;- data_achieve %&gt;% dplyr::group_by(region, corp, school) %&gt;% dplyr::tally() %&gt;% nrow() num_schools [1] 160 The number of classes = 568 num_classes &lt;- data_achieve %&gt;% dplyr::group_by(region, corp, school, class) %&gt;% dplyr::tally() %&gt;% nrow() num_classes [1] 568 The number of students = 10320 num_subjects &lt;- data_achieve %&gt;% nrow num_subjects [1] 10320 4.2 Exploratory Data Analysis 4.2.1 Summarize Descriptive Statistics 4.2.1.1 The stargazer package Most posters, journal articles, and reports start with a table of descriptive statistics. Since it tends to come first, this type of table is often refered to as Table 1. The stargazer() function can be used to create such a table, but only for the entire dataset. I haven’t been able to find a way to get it to summarize subsamples and compare them in the standard format. Also, it only summarises continuous, not categorical variables. # Knit to Website: type = &quot;html&quot; # Knit to PDF: type = &quot;latex&quot; # View on Screen: type = &quot;text&quot; data_achieve %&gt;% dplyr::select(classize, gender, geread, gevocab, age, ses, senroll) %&gt;% data.frame() %&gt;% stargazer::stargazer(header = FALSE, title = &quot;Summary of the numeric variables with `stargazer`&quot;, type = &quot;html&quot;) Summary of the numeric variables with stargazer Statistic N Mean St. Dev. Min Pctl(25) Pctl(75) Max geread 10,320 4.341 2.332 0.000 2.800 4.900 12.000 gevocab 10,320 4.494 2.368 0.000 2.900 5.200 11.200 age 10,320 107.529 5.060 82 104 111 135 ses 10,320 72.849 21.982 0.000 66.300 87.800 100.000 senroll 10,320 533.415 154.797 115 438 644 916 4.2.1.2 The furniture package Tyson Barrett’s furniture package includes the extremely useful function table1() which simplifies the common task of creating a stratified, comparative table of descriptive statistics. Full documentation can be accessed by executing ?furniture::table1. # Knit to Website: output = &quot;html&quot; # Knit to PDF: output = &quot;latex2&quot; # View on Screen: output = &quot;&quot;text&quot;, or &quot;markdown&quot;, &quot;html&quot; data_achieve %&gt;% furniture:: table1(&quot;Reading Score&quot; = geread, &quot;Vocabulary Score&quot; = gevocab, &quot;Age (in months)&quot; = age, &quot;School SES&quot; = ses, &quot;School&#39;s Enrollment&quot; = senroll, splitby = ~ gender, # var to divide sample by test = TRUE, # test groups different? caption = &quot;Summary of the numeric variables with `table1`&quot;, output = &quot;html&quot;) Table 4.1: Summary of the numeric variables with table1 Female Male P-Value n = 5143 n = 5177 Reading Score 0.218 4.4 (2.3) 4.3 (2.3) Vocabulary Score &lt;.001 4.6 (2.4) 4.4 (2.3) Age (in months) &lt;.001 107.1 (5.0) 107.9 (5.1) School SES 0.155 72.5 (22.3) 73.2 (21.7) School’s Enrollment 0.483 532.3 (154.5) 534.5 (155.1) 4.2.2 Visualization of Raw Data 4.2.2.1 Level One Plots: Disaggregate or ignore higher levels For a first look, its useful to plot all the data points on a single scatterplot as displayed in the previous plot. Due to the large sample size, many points end up being plotted on top of or very near each other (overplotted). When this is the case, it can be useful to use geom_binhex() rather than geom_point() so the color saturation of the hexigons convey the number of points at that location, as seen in Figure . Note: I had to manually install the package hexbin for the geom_hex() to run. data_achieve %&gt;% ggplot() + aes(x = gevocab, y = geread) + stat_binhex(colour = &quot;grey85&quot;, na.rm = TRUE) + # outlines scale_fill_gradientn(colors = c(&quot;grey80&quot;,&quot;navyblue&quot;), # fill color extremes name = &quot;Frequency&quot;, # legend title na.value = NA) + # color for count = 0 theme_bw() Figure 4.1: Raw Data: Density, Vocab vs. Reading 4.2.2.2 Multilevel plots: illustrate two nested levels Up to this point, all investigation of this dataset has been only at the pupil level and any nesting or clustering within schools has been ignored. Plotting is a good was to start to get an idea of the school-to-school variability. This figure displays four handpicked school to illustrate the degreen of school-to-school variability in the association between vocab and reading scores. data_achieve %&gt;% dplyr::filter(school %in% c(1321, 6181, 6197, 6823)) %&gt;% # choose school numbers ggplot(aes(x = gevocab, y = geread))+ geom_count() + # creates points, size by overplotted number geom_smooth(method = &quot;lm&quot;) + # linear model (OLS) facet_wrap(~ school) + # panels by school theme_bw() Figure 4.2: Raw Data: Independent Single-Level Regression within each school, a few illustrative cases Another way to explore the school-to-school variability is to plot the linear model fit independently to each of the schools. This next figure displays only the smooth lines without the standard error bands or the raw data in the form of points or hexagons. data_achieve %&gt;% ggplot(aes(x = gevocab, y = geread)) + geom_smooth(aes(group = school), method = &quot;lm&quot;, se = FALSE, # do NOT want the SE bands size = 0.3) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;red&quot;, # do NOT want the SE bands size = 2) + # make the lines thinner theme_bw() Figure 4.3: Raw Data: Independent Single-Level Regression within each school, all schools shown together Due to the high number of schools, the figure with all the school’s independent linear regression lines resembles a hairball and is hard to deduce much about individual schools. By using the facet_grid() layer, we can seperate the schools out so better see school-to-school variability. It also allows investigation of higher level predictors, such as the school’s SES (median split with ntile(var, 2)) and class size. data_achieve %&gt;% dplyr::mutate(ses2 = ntile(ses, 2) %&gt;% # median split factor(labels = c(&quot;SES: Lower Half&quot;, &quot;SES: Upper Half&quot;))) %&gt;% dplyr::mutate(senroll = ntile(senroll, 3) %&gt;% factor(labels = c(&quot;Enroll: Smallest Third&quot;, &quot;Enroll: Middle Third&quot;, &quot;Enroll: Largest Third&quot;))) %&gt;% ggplot(aes(x = gevocab, y = geread, group = school)) + # sepearates students into schools geom_smooth(method = &quot;lm&quot;, se = FALSE, size = 0.3, color = &quot;black&quot;, alpha = .2) + theme_bw() + facet_grid(senroll ~ ses2) # makes seperate panels (rows ~ columns) Figure 4.4: Raw Data: Independent Single-Level Regression within each school, sepearated by school size and school SES 4.3 Single-Level Regression 4.3.1 Fit Nested Models Ignoring the fact that students are nested or clustered within schools, is called dissagregating. This treats all students as independent units. # linear model - ignores school (for reference only) fit_read_lm_0 &lt;- lm(formula = geread ~ 1, # intercept only data = data_achieve) fit_read_lm_1 &lt;- lm(formula = geread ~ gevocab , # one predictor data = data_achieve) fit_read_lm_2 &lt;- lm(formula = geread ~ gevocab + age, # two predictors data = data_achieve) fit_read_lm_3 &lt;- lm(formula = geread ~ gevocab*age, # interation+main effects data = data_achieve) Now compare the models: # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(fit_read_lm_0, fit_read_lm_1, fit_read_lm_2, fit_read_lm_3), custom.model.names = c(&quot;Null&quot;, &quot;1 IV&quot;, &quot;2 IV&quot;, &quot;Interaction&quot;), caption = &quot;OLS: Investigate Fixed, Pupil-level Predictors&quot;, caption.above = TRUE, single.row = TRUE) OLS: Investigate Fixed, Pupil-level Predictors Null 1 IV 2 IV Interaction (Intercept) 4.34 (0.02)*** 1.96 (0.04)*** 3.19 (0.42)*** 5.28 (0.87)*** gevocab 0.53 (0.01)*** 0.53 (0.01)*** 0.01 (0.19) age -0.01 (0.00)** -0.03 (0.01)*** gevocab:age 0.00 (0.00)** R2 0.00 0.29 0.29 0.29 Adj. R2 0.00 0.29 0.29 0.29 Num. obs. 10320 10320 10320 10320 RMSE 2.33 1.97 1.97 1.96 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 Assess the significance of terms in the last ‘best’ model summary(fit_read_lm_3) Call: lm(formula = geread ~ gevocab * age, data = data_achieve) Residuals: Min 1Q Median 3Q Max -6.2069 -1.1250 -0.4362 0.6041 8.6476 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.282607 0.869769 6.074 1.3e-09 *** gevocab 0.009154 0.189113 0.048 0.961394 age -0.030814 0.008066 -3.820 0.000134 *** gevocab:age 0.004830 0.001759 2.746 0.006039 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.965 on 10316 degrees of freedom Multiple R-squared: 0.2902, Adjusted R-squared: 0.29 F-statistic: 1406 on 3 and 10316 DF, p-value: &lt; 2.2e-16 sjstats::r2(fit_read_lm_3) R-Squared for Generalized Linear Mixed Model R-squared: 0.290 adjusted R-squared: 0.290 anova(fit_read_lm_3) Analysis of Variance Table Response: geread Df Sum Sq Mean Sq F value Pr(&gt;F) gevocab 1 16224 16223.9 4202.2783 &lt; 2.2e-16 *** age 1 34 33.7 8.7356 0.003128 ** gevocab:age 1 29 29.1 7.5419 0.006039 ** Residuals 10316 39827 3.9 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 4.3.2 Visualize the Interaction effects::Effect(focal.predictors = c(&quot;gevocab&quot;, &quot;age&quot;), # chooses defaul values for mod = fit_read_lm_3) # continuous vars gevocab*age effect age gevocab 82 95 110 120 140 0 2.755830 2.355243 1.893028 1.584884 0.9685969 3 3.971571 3.759370 3.514523 3.351291 3.0248284 6 5.187313 5.163497 5.136018 5.117699 5.0810599 8 5.997807 6.099582 6.217015 6.295304 6.4518809 10 6.808301 7.035667 7.298012 7.472909 7.8227019 effects::Effect(focal.predictors = c(&quot;gevocab&quot;, &quot;age&quot;), # chooses defaul values for mod = fit_read_lm_3) %&gt;% # continuous vars data.frame() %&gt;% mutate(age = factor(age)) %&gt;% # must make a factor to seperate lines ggplot(aes(x = gevocab, y = fit, color = age)) + geom_point() + geom_line() Here is a better version of the plot. Age is in months, so we want multiples of 12 for good visualization summary(data_achieve$age)/12 # divide by 12 to change months to years Min. 1st Qu. Median Mean 3rd Qu. Max. 6.833 8.667 8.917 8.961 9.250 11.250 A good set set of illustrative ages could be: 7, 9, and 11: c(7, 9, 11) * 12 # times by 12 to change years to months [1] 84 108 132 effects::Effect(focal.predictors = c(&quot;gevocab&quot;, &quot;age&quot;), mod = fit_read_lm_3, xlevels = list(age = c(84, 108, 132))) %&gt;% # age is in months data.frame() %&gt;% mutate(age_yr = factor(age/12)) %&gt;% # it would be nice to plot age in years ggplot(aes(x = gevocab, y = fit, color = age_yr, linetype = age_yr)) + geom_line(size = 1.25) + theme_bw() + labs(title = &quot;Best Linear Model - Disaggregated Data (OLS)&quot;, x = &quot;Vocabulary Score&quot;, y = &quot;Reading Score&quot;, linetype = &quot;Age (yrs)&quot;, color = &quot;Age (yrs)&quot;) + theme(legend.position = c(0.85, 0.2), legend.key.width = unit(2, &quot;cm&quot;), legend.background = element_rect(color = &quot;black&quot;)) + scale_linetype_manual(values = c(&quot;solid&quot;, &quot;longdash&quot;, &quot;dotted&quot;)) + scale_x_continuous(breaks = seq(from = 0, to = 11, by = 2)) + scale_y_continuous(breaks = seq(from = 0, to = 11, by = 1)) 4.4 MLM - Step 1: Null Model, only fixed and random intercepts A so called Empty Model only includes random intercepts. No independent variables are involved, other the grouping or clustering variable that designates how level 1 units are nested within level 2 units. For a cross-sectional study design this would be the grouping variables, where as for longitudinal or repeated measures designs this would be the subject identifier. This nested structure variable should be set to have class factor. 4.4.1 Fit the Model fit_read_0ml &lt;- lme4::lmer(geread ~ 1 + (1|school), data = data_achieve, REML = FALSE) # fit via ML (not the default) fit_read_0re &lt;- lme4::lmer(geread ~ 1 + (1|school) , data = data_achieve, REML = TRUE) # fit = REML (the default) Compare the two models to OLS: # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(fit_read_lm_0, fit_read_0ml, fit_read_0re), custom.model.names = c(&quot;OLS&quot;, &quot;MLM-ML&quot;, &quot;MLM-REML&quot;), caption = &quot;MLM: NULL Model,two estimation methods&quot;, caption.above = TRUE, single.row = TRUE) MLM: NULL Model,two estimation methods OLS MLM-ML MLM-REML (Intercept) 4.34 (0.02)*** 4.31 (0.05)*** 4.31 (0.05)*** R2 0.00 Adj. R2 0.00 Num. obs. 10320 10320 10320 RMSE 2.33 AIC 46270.34 46274.31 BIC 46292.06 46296.03 Log Likelihood -23132.17 -23134.15 Num. groups: school 160 160 Var: school (Intercept) 0.39 0.39 Var: Residual 5.05 5.05 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 Notice that the estimate for the intercept is nearly the same in the linear regression and intercept only models, but the standard errors are quite different. When there is clustering in sample, the result of ignoring it is under estimation of the standard errors and over stating the significance of associations. This table was made with the screenreg() function in the self named package. I tend to prefer this display over stargazer(). 4.4.2 Estimate the ICC First, ask for the variance compenents: lme4::VarCorr(fit_read_0re) %&gt;% print(comp = c(&quot;Variance&quot;, &quot;Std.Dev&quot;), digits = 4) Groups Name Variance Std.Dev. school (Intercept) 0.3915 0.6257 Residual 5.0450 2.2461 sjstats::re_var(fit_read_0re) Within-group-variance: 5.045 Between-group-variance: 0.392 (school) \\[ \\begin{align*} \\text{schools} \\rightarrow \\; &amp; \\sigma^2_{u0} = 0.6257^2 = 0.392 \\\\ \\text{students within schools} \\rightarrow \\; &amp; \\sigma^2_{e} = 2.2461^2 = 5.045 \\\\ \\end{align*} \\] Intraclass Correlation (ICC) Formula \\[ \\overbrace{\\rho}^{\\text{ICC}} = \\frac{\\overbrace{\\sigma^2_{u0}}^{\\text{Random Intercept}\\atop\\text{Variance}}} {\\underbrace{\\sigma^2_{u0}+\\sigma^2_{e}}_{\\text{Total}\\atop\\text{Variance}}} \\tag{Hox 2.9} \\] Then you can manually caluclate the ICC. 0.392 / (0.392 + 5.045) [1] 0.07209858 Or you can use the icc() function in the sjstats package. sjstats::icc(fit_read_0re) Linear mixed model Family : gaussian (identity) Formula: geread ~ 1 + (1 | school) ICC (school): 0.0720 Note: On page 45 (Finch, Bolin, and Kelley 2016), the authors substituted standard deviations into the formula, rather than variances. The mistake is listed on their webpage errata (http://www.mlminr.com/errata) and is repeated through the text. 4.5 MLM - Step 2: Add Lower-level explanatory variables, fixed, ML Variance Component models (steps 2 and 3) - decompose the INTERCEPT variance into different variance compondents for each level. The regression intercepts are assumed to varry ACROSS the groups, while the slopes are assumed fixed (no random effects). Fixed effects selection should come prior to random effects. You should use Maximum Likelihood (ML) estimation when fitting these models. IF: only level 1 predictors and random intercepts are incorporated Then: MLM \\(\\approx\\) ANCOVA . 4.5.1 Add pupil’s vocab score as a fixed effects predictor fit_read_1ml &lt;- lme4::lmer(geread ~ gevocab + (1|school), data = data_achieve, REML = FALSE) # to compare fixed var sig fit_read_1re &lt;- lme4::lmer(geread ~ gevocab + (1|school), data = data_achieve, REML = TRUE) # for R-sq calcs # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(fit_read_0ml, fit_read_1ml), custom.model.names = c(&quot;Null&quot;, &quot;w Pred&quot;), caption = &quot;MLM: Investigate a Fixed Pupil-level Predictor&quot;, caption.above = TRUE, doctype = FALSE, digits = 4) MLM: Investigate a Fixed Pupil-level Predictor Null w Pred (Intercept) 4.3068*** 2.0231*** (0.0548) (0.0492) gevocab 0.5130*** (0.0084) AIC 46270.3388 43132.4318 BIC 46292.0643 43161.3991 Log Likelihood -23132.1694 -21562.2159 Num. obs. 10320 10320 Num. groups: school 160 160 Var: school (Intercept) 0.3885 0.0987 Var: Residual 5.0450 3.7661 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 4.5.1.1 Assess Significance of Effects Likelihood Ratio Test (LRT) Since models 0 and 1 are nested models, only differing by the the inclusion or exclusion of the fixed effects predictor gevocab, AND both models were fit via Maximum Likelihood, we can compare the model fit may be compared via the Likilihood-Ratio Test (LRT). The Likelihood Ratio value (L. Ratio) is found by subtracting the two model’s -2 * logLik or deviance values. Significance is judged by the Chi Squared distribution, using the difference in the number of parameters fit as the degrees of freedom. anova(fit_read_0ml, fit_read_1ml) Data: data_achieve Models: fit_read_0ml: geread ~ 1 + (1 | school) fit_read_1ml: geread ~ gevocab + (1 | school) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) fit_read_0ml 3 46270 46292 -23132 46264 fit_read_1ml 4 43132 43161 -21562 43124 3139.9 1 &lt; 2.2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 What does the model look like? effects::Effect(focal.predictors = c(&quot;gevocab&quot;), mod = fit_read_1ml) %&gt;% data.frame() %&gt;% ggplot(aes(x = gevocab, y = fit)) + geom_ribbon(aes(ymin = lower, ymax = upper), alpha = .3) + geom_line() + theme_bw() 4.5.1.2 Proportion of Variance Explained Extract the variance-covariance estimates: BL = BAseline: The Null Model (fit via REML) sjstats::re_var(fit_read_0re) Within-group-variance: 5.045 Between-group-variance: 0.392 (school) \\[ \\sigma^2_{u0-BL} = 0.392 \\\\ \\sigma^2_{e-BL} = 5.045 \\] MC = Model to Compare: Model with Predictor (fit via REML) sjstats::re_var(fit_read_1re) Within-group-variance: 3.766 Between-group-variance: 0.100 (school) \\[ \\sigma^2_{u0-MC} = 0.100 \\\\ \\sigma^2_{e-MC} = 3.766 \\] Level 1 \\(R^2\\) - Snijders and Bosker Found on page 47 (Finch, Bolin, and Kelley 2016), theproportion of variance in the outcome explained by predictor on level one is given by: Snijders and Bosker Formula - Level 1 \\[ R^2_1 = 1 - \\frac{\\sigma^2_{e-MC} + \\sigma^2_{u0-MC}} {\\sigma^2_{e-BL} + \\sigma^2_{u0-BL}} \\] Note: This formula also apprears in the Finch errata. The subscripts in the denominator of the fraction should be for model 0, not model 1. The formula is given correctly here. They did substitute in the correct values. Calculate the value by hand: 1 - (0.100 + 3.766)/(0.392 + 5.045) [1] 0.2889461 Or use the sjstats package to help out: 1 - sum(sjstats::re_var(fit_read_1re)) / sum(sjstats::re_var(fit_read_0re)) [1] 0.288838 This means nearly 30% of the variance in reading scores, above and beyond that accounted for by school membership (i.e. school makeup or school-to-school variation), is attributable to vocabulary scores. Level 1 \\(R^2\\) - Raudenbush and Bryk Hox, Moerbeek, and Van de Schoot (2017) presents this formula on page 58 of chapter 2 Raudenbush and Bryk Approximate Formula - Level 1 \\[ approx\\; R^2_1 = \\frac{\\sigma^2_{e-BL} - \\sigma^2_{e-MC}} {\\sigma^2_{e-BL} } \\tag{Hox 4.8} \\] Calculate the value by hand: (5.045 - 3.766) / 5.045 [1] 0.2535183 Although slightly different in value and meaning, this value also conveys that vcabulary scores are highly associated with reading scores. Level 2 \\(R^2\\) - Snijders and Bosker Formula Extended Snijders and Bosker Formula Extended - Level 2 \\[ R^2_2 = 1 - \\frac{\\frac{\\sigma^2_{e-MC}}{B} + \\sigma^2_{u0-MC}} {\\frac{\\sigma^2_{e-BL}}{B} + \\sigma^2_{u0-BL}} \\] \\(B\\) is the average size of the Level 2 units (schools). Technically, you should use the harmonic mean, but unless the clusters differ greatly in size, it doesn’t make a huge difference. Average sample cluster size num_subjects / num_schools [1] 64.5 Calculate by hand: 1 - ((3.766 / 64.5) + 0.100)/ ((5.045 / 64.5) + 0.391) [1] 0.6624428 This means that over two-thrids in school mean reading levelsmay be explained by their student’s vocabulary scores. Level 2 \\(R^2\\) - Raudenbush and Bryk Raudenbush and Bryk Approximate Formula - Level 2 \\[ R^2_1 = \\frac{\\sigma^2_{u0-BL} - \\sigma^2_{u0-MC}} {\\sigma^2_{u0-BL} } \\tag{Hox 4.9} \\] (0.392 - 0.100)/(0.392) [1] 0.744898 Remeber that these ‘variance accounted for’ estimations are not as straight forwards as we would like. 4.5.2 Investigate More Level 1 Predictors Part of investigating lower level explanatory variables, is checking for interactions between these variables. The interaction between fixed effects is also considered to be a fixed effect, so we need to employ Maximum Likelihood estimation to compare nested models. fit_read_2ml &lt;- lmer(geread ~ gevocab + age + (1 | school), # add main effect of age data = data_achieve, REML = FALSE) fit_read_3ml &lt;- lmer(geread ~ gevocab*age + (1 | school), # add interaction between vocab and age data = data_achieve, REML = FALSE) # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(fit_read_1ml, fit_read_2ml, fit_read_3ml), custom.model.names = c(&quot;Only Vocab&quot;, &quot;Both Main Effects&quot;, &quot;Interaction&quot;), caption = &quot;MLM: Investigate Other Fixed Pupil-level Predictors&quot;, caption.above = TRUE, doctype = FALSE, digits = 4) MLM: Investigate Other Fixed Pupil-level Predictors Only Vocab Both Main Effects Interaction (Intercept) 2.0231*** 3.0049*** 5.1874*** (0.0492) (0.4172) (0.8666) gevocab 0.5130*** 0.5121*** -0.0279 (0.0084) (0.0084) (0.1881) age -0.0091* -0.0294*** (0.0038) (0.0080) gevocab:age 0.0050** (0.0017) AIC 43132.4318 43128.8201 43122.5687 BIC 43161.3991 43165.0293 43166.0198 Log Likelihood -21562.2159 -21559.4100 -21555.2844 Num. obs. 10320 10320 10320 Num. groups: school 160 160 160 Var: school (Intercept) 0.0987 0.0973 0.0977 Var: Residual 3.7661 3.7646 3.7614 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 4.5.2.1 Assess Significance of Effects Likelihood Ratio Test (LRT) anova(fit_read_1ml, fit_read_2ml, fit_read_3ml) Data: data_achieve Models: fit_read_1ml: geread ~ gevocab + (1 | school) fit_read_2ml: geread ~ gevocab + age + (1 | school) fit_read_3ml: geread ~ gevocab * age + (1 | school) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) fit_read_1ml 4 43132 43161 -21562 43124 fit_read_2ml 5 43129 43165 -21559 43119 5.6117 1 0.017841 * fit_read_3ml 6 43123 43166 -21555 43111 8.2514 1 0.004072 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Not only is student’s age predictive of their reading level (I could have guessed that), but that age moderated the relationship between vocabulary and reading. 4.5.2.2 Visulaize the Interation Visulaizations are extremely helpful to interpred interactions. summary(data_achieve$age) Min. 1st Qu. Median Mean 3rd Qu. Max. 82.0 104.0 107.0 107.5 111.0 135.0 effects::Effect(focal.predictors = c(&quot;gevocab&quot;, &quot;age&quot;), # variables involved in the interaction mod = fit_read_3ml, xlevels = list(age = c(84, 108, 132))) %&gt;% # age is in months data.frame() %&gt;% mutate(age_yr = factor(age/12)) %&gt;% # it would be nice to plot age in years ggplot(aes(x = gevocab, y = fit, color = age_yr)) + geom_line() + theme_bw() There is a positive association between vocabulary and reading, but it is strongest for older childred. Among younger children, reading scores are more stable across vocabulary differences. 4.6 MLM - Step 3: Higher-level explanatory variables, fixed, ML School enrollment (senroll) applies to each school as a whole. When a variable is measured at a higher level, all units in the same group have the same value. In this case, all student in the same school have the same value for senroll. fit_read_4ml &lt;- lme4::lmer(geread ~ gevocab*age + senroll + (1 | school), data = data_achieve, REML = FALSE) # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(fit_read_0ml, fit_read_3ml, fit_read_4ml), custom.model.names = c(&quot;Null&quot;, &quot;Level 1 only&quot;, &quot;Level 2 Pred&quot;), caption = &quot;MLM: Investigate a Fixed School-Level Predictor&quot;, caption.above = TRUE, single.row = TRUE) MLM: Investigate a Fixed School-Level Predictor Null Level 1 only Level 2 Pred (Intercept) 4.31 (0.05)*** 5.19 (0.87)*** 5.24 (0.87)*** gevocab -0.03 (0.19) -0.03 (0.19) age -0.03 (0.01)*** -0.03 (0.01)*** gevocab:age 0.01 (0.00)** 0.01 (0.00)** senroll -0.00 (0.00) AIC 46270.34 43122.57 43124.31 BIC 46292.06 43166.02 43175.01 Log Likelihood -23132.17 -21555.28 -21555.16 Num. obs. 10320 10320 10320 Num. groups: school 160 160 160 Var: school (Intercept) 0.39 0.10 0.10 Var: Residual 5.05 3.76 3.76 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 4.6.1 Assess Significance of Effects Likelihood Ratio Test (LRT) anova(fit_read_0ml, fit_read_3ml, fit_read_4ml) Data: data_achieve Models: fit_read_0ml: geread ~ 1 + (1 | school) fit_read_3ml: geread ~ gevocab * age + (1 | school) fit_read_4ml: geread ~ gevocab * age + senroll + (1 | school) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) fit_read_0ml 3 46270 46292 -23132 46264 fit_read_3ml 6 43123 43166 -21555 43111 3153.7701 3 &lt;2e-16 fit_read_4ml 7 43124 43175 -21555 43110 0.2548 1 0.6137 fit_read_0ml fit_read_3ml *** fit_read_4ml --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 School enrollment (or size) does not seem be related to reading scores. 4.7 MLM - Step 4: Explanatory variables predict Slopes, random, REML Random Coefficient models - decompose the SLOPE variance BETWEEN groups. The fixed effect of the predictor captures the overall association it has with the outcome (intercept), while the random effect of the predictor captures the group-to-group variation in the association (slope). Note: A variable can be fit as BOTH a fixed and random effect. fit_read_3re &lt;- lme4::lmer(geread ~ gevocab*age + (1 | school), # refit the previous &#39;best&#39; model via REML data = data_achieve, REML = TRUE) #fit_read_5re &lt;- lmer(geread ~ gevocab + (gevocab | school), # data = achieve, # REML = TRUE) # failed to converge :( fit_read_5re &lt;- lme4::lmer(geread ~ gevocab*age + (gevocab | school), data = data_achieve, REML = TRUE, control = lmerControl(optimizer = &quot;optimx&quot;, # get it to converge calc.derivs = FALSE, optCtrl = list(method = &quot;nlminb&quot;, starttests = FALSE, kkt = FALSE))) # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(fit_read_3re, fit_read_5re), custom.model.names = c(&quot;Rand Int&quot;, &quot;Rand Int and Slopes&quot;), caption = &quot;MLM: Investigate Random Effects&quot;, caption.above = TRUE, single.row = TRUE) MLM: Investigate Random Effects Rand Int Rand Int and Slopes (Intercept) 5.19 (0.87)*** 5.61 (0.87)*** gevocab -0.03 (0.19) -0.14 (0.19) age -0.03 (0.01)*** -0.03 (0.01)*** gevocab:age 0.01 (0.00)** 0.01 (0.00)*** AIC 43155.49 43011.65 BIC 43198.95 43069.58 Log Likelihood -21571.75 -21497.82 Num. obs. 10320 10320 Num. groups: school 160 160 Var: school (Intercept) 0.10 0.28 Var: Residual 3.76 3.66 Var: school gevocab 0.02 Cov: school (Intercept) gevocab -0.06 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 4.7.0.1 Assess Significance of Effect Likelihood Ratio Test (LRT) for Random Effects You can use the Chi-squared LRT test based on deviances even though we fit our modesl with REML, since the models only differ in terms of including/exclusing of a random effects; they have same fixed effects. Just make sure to include the refit = FALSE option. anova(fit_read_3re, fit_read_5re, refit = FALSE) Data: data_achieve Models: fit_read_3re: geread ~ gevocab * age + (1 | school) fit_read_5re: geread ~ gevocab * age + (gevocab | school) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) fit_read_3re 6 43155 43199 -21572 43143 fit_read_5re 8 43012 43070 -21498 42996 147.84 2 &lt; 2.2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 There is evidence the effect child vocabulary has on reading varies across schools. 4.7.0.2 Visualize the Model What does the model look like? effects::Effect(focal.predictors = c(&quot;gevocab&quot;, &quot;age&quot;), mod = fit_read_5re, # just different model xlevels = list(age = c(84, 108, 132))) %&gt;% data.frame() %&gt;% dplyr::mutate(age_yr = factor(age/12)) %&gt;% ggplot(aes(x = gevocab, y = fit, color = age_yr)) + geom_line() + theme_bw() We are seeming much the same trends, but perhaps more separation between the lines. 4.8 MLM - Step 5: Cross-Level interactions between explanatory variables - fixed, ML Cross-level interacitons involve variables at different levels. Here we will investigate the school-level enrollment moderating vocabulary’s effect since we say that vocab’s effect differs across schools (step 4). Remember that an interaction beween fixed effects is also fixed. fit_read_5ml &lt;- lme4::lmer(geread ~ gevocab*age + (gevocab | school), data = data_achieve, REML = FALSE, control = lmerControl(optimizer = &quot;optimx&quot;, calc.derivs = FALSE, optCtrl = list(method = &quot;nlminb&quot;, starttests = FALSE, kkt = FALSE))) fit_read_6ml &lt;- lme4::lmer(geread ~ gevocab*age + senroll + (gevocab | school), data = data_achieve, REML = FALSE) fit_read_7ml &lt;- lme4::lmer(geread ~ gevocab*age + gevocab*senroll + (gevocab | school), data = data_achieve, REML = FALSE) fit_read_8ml &lt;- lme4::lmer(geread ~ gevocab*age*senroll + (gevocab | school), data = data_achieve, REML = FALSE) If you get thelmer() message: Some predictor variables are on very different scales: consider rescaling, you can trust your results, but you really should try re-scaling your variables. We are getting this message since gevoab is on mostly a single digit scale,0 to 11.2, and age (in months) ranges in the low thripe-digits, 82 through 135, while school enrollment is in the mid-hundreds, 112-916. When we compute the interactions we get much, much larger values. Having variables on such widely different ranges of values can cause estimation problems. data_achieve %&gt;% dplyr::mutate(gevocab_x_age = gevocab*age, gevocab_x_senroll = gevocab*senroll, gevocab_x_age_x_senroll = gevocab*senroll*age) %&gt;% dplyr::select(gevocab, age, senroll, gevocab_x_age, gevocab_x_senroll, gevocab_x_age_x_senroll) %&gt;% summary() gevocab age senroll gevocab_x_age Min. : 0.000 Min. : 82.0 Min. :115.0 Min. : 0.0 1st Qu.: 2.900 1st Qu.:104.0 1st Qu.:438.0 1st Qu.: 310.3 Median : 3.800 Median :107.0 Median :519.0 Median : 408.0 Mean : 4.494 Mean :107.5 Mean :533.4 Mean : 482.5 3rd Qu.: 5.200 3rd Qu.:111.0 3rd Qu.:644.0 3rd Qu.: 560.0 Max. :11.200 Max. :135.0 Max. :916.0 Max. :1355.2 gevocab_x_senroll gevocab_x_age_x_senroll Min. : 0 Min. : 0 1st Qu.: 1378 1st Qu.: 148054 Median : 1998 Median : 214379 Mean : 2401 Mean : 257821 3rd Qu.: 2970 3rd Qu.: 319752 Max. :10259 Max. :1149030 For now, let us look at the results. # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(fit_read_3ml, fit_read_6ml, fit_read_7ml, fit_read_8ml), custom.model.names = c(&quot;Level 1 only&quot;, &quot;Both Levels&quot;, &quot;Cross-Level&quot;, &quot;3-way&quot;), caption = &quot;MLM: Investigate a Fixed Cross-level Interaction&quot;, caption.above = TRUE, single.row = TRUE) MLM: Investigate a Fixed Cross-level Interaction Level 1 only Both Levels Cross-Level 3-way (Intercept) 5.19 (0.87)*** 5.60 (0.88)*** 5.52 (0.89)*** 5.50 (3.11) gevocab -0.03 (0.19) -0.14 (0.19) -0.11 (0.20) 0.53 (0.66) age -0.03 (0.01)*** -0.03 (0.01)*** -0.03 (0.01)*** -0.03 (0.03) gevocab:age 0.01 (0.00)** 0.01 (0.00)*** 0.01 (0.00)*** 0.00 (0.01) senroll 0.00 (0.00) 0.00 (0.00) 0.00 (0.01) gevocab:senroll -0.00 (0.00) -0.00 (0.00) age:senroll -0.00 (0.00) gevocab:age:senroll 0.00 (0.00) AIC 43122.57 42981.70 42983.44 42982.85 BIC 43166.02 43046.87 43055.86 43069.75 Log Likelihood -21555.28 -21481.85 -21481.72 -21479.43 Num. obs. 10320 10320 10320 10320 Num. groups: school 160 160 160 160 Var: school (Intercept) 0.10 0.27 0.27 0.27 Var: Residual 3.76 3.66 3.66 3.66 Var: school gevocab 0.02 0.02 0.02 Cov: school (Intercept) gevocab -0.06 -0.06 -0.06 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 There is no evidence school enrollment moderates either of age or vocabulary’s effects. 4.8.0.1 Assess Significance of Effects Likelihood Ratio Test (LRT) When you have a list of sequentially nested models, you can test them in order with one call to the anova() funtion. anova(fit_read_3ml, fit_read_6ml, fit_read_7ml, fit_read_8ml) Data: data_achieve Models: fit_read_3ml: geread ~ gevocab * age + (1 | school) fit_read_6ml: geread ~ gevocab * age + senroll + (gevocab | school) fit_read_7ml: geread ~ gevocab * age + gevocab * senroll + (gevocab | school) fit_read_8ml: geread ~ gevocab * age * senroll + (gevocab | school) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) fit_read_3ml 6 43123 43166 -21555 43111 fit_read_6ml 9 42982 43047 -21482 42964 146.8710 3 &lt;2e-16 *** fit_read_7ml 10 42983 43056 -21482 42963 0.2534 1 0.6147 fit_read_8ml 12 42983 43070 -21479 42959 4.5932 2 0.1006 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 4.9 Centering Predictors: Change Center Centering variables measured on the lowest level only involves subtacting the mean from every value. The spread or standard deviation is not changed. Although there are functions to automatically center and standardize variables, it is beneficial to manually create these variables, as it is more transparent and facilitates un-centering them later. 4.9.1 Compute the Grand Means # Knit to Website: output = &quot;html&quot; # Knit to PDF: output = &quot;latex2&quot; # View on Screen: output = &quot;&quot;text&quot;, or &quot;markdown&quot;, &quot;html&quot; data_achieve %&gt;% furniture::table1(gevocab, age, senroll, output = &quot;html&quot;, digits = 4) Mean/Count (SD/%) n = 10320 gevocab 4.4938 (2.3679) age 107.5290 (5.0596) senroll 533.4148 (154.7968) 4.9.1.1 Subtract the Grand Mean Subract the grand-mean from each observation’s vale to create grand-mean centered variables. data_achieve_center &lt;- data_achieve %&gt;% dplyr::mutate(gevocab_c = gevocab - 4.4938) %&gt;% dplyr::mutate(age_c = age - 107.5290) %&gt;% dplyr::mutate(senroll_c = senroll - 533.4148) 4.9.1.2 Compare the centered and uncentered measures View the first and last few observations: data_achieve_center %&gt;% dplyr::select(id, school, gevocab, gevocab_c, age, age_c, senroll, senroll_c) %&gt;% headTail() id school gevocab gevocab_c age age_c senroll senroll_c 1 1 767 3.1 -1.39 104 -3.53 463 -70.41 2 2 767 2.8 -1.69 106 -1.53 463 -70.41 3 3 767 1.7 -2.79 112 4.47 463 -70.41 4 4 767 2.1 -2.39 109 1.47 463 -70.41 5 &lt;NA&gt; &lt;NA&gt; ... ... ... ... ... ... 6 7893 9085 4.7 0.21 108 0.47 603 69.59 7 7894 9085 3.1 -1.39 108 0.47 603 69.59 8 7895 9085 3.8 -0.69 114 6.47 603 69.59 9 7896 9085 3 -1.49 106 -1.53 603 69.59 Compare the summary statistcs: # Knit to Website: output = &quot;html&quot; # Knit to PDF: output = &quot;latex2&quot; # View on Screen: output = &quot;&quot;text&quot;, or &quot;markdown&quot;, &quot;html&quot; data_achieve_center %&gt;% furniture::table1(gevocab, gevocab_c, age, age_c, senroll, senroll_c, output = &quot;html&quot;, digits = 4) Mean/Count (SD/%) n = 10320 gevocab 4.4938 (2.3679) gevocab_c 0.0000 (2.3679) age 107.5290 (5.0596) age_c -0.0000 (5.0596) senroll 533.4148 (154.7968) senroll_c 0.0000 (154.7968) 4.9.2 Use Centered Variables NOTE: The models with CENTERED variables are able to be fit with the default optimizer settings and do not return the error: “unable to evaluate scaled gradientModel failed to converge: degenerate Hessian with 1 negative eigenvalues” fit_read_9ml &lt;- lme4::lmer(geread ~ gevocab + age + (gevocab | school), data = data_achieve_center, REML = FALSE, control = lmerControl(optimizer = &quot;optimx&quot;, calc.derivs = FALSE, optCtrl = list(method = &quot;nlminb&quot;, starttests = FALSE, kkt = FALSE))) fit_read_10ml &lt;- lme4::lmer(geread ~ gevocab*age + (gevocab | school), data = data_achieve_center, REML = FALSE, control = lmerControl(optimizer = &quot;optimx&quot;, calc.derivs = FALSE, optCtrl = list(method = &quot;nlminb&quot;, starttests = FALSE, kkt = FALSE))) fit_read_9ml_c &lt;- lme4::lmer(geread ~ gevocab_c + age_c + (gevocab_c | school), data = data_achieve_center, REML = FALSE) fit_read_10ml_c &lt;- lme4::lmer(geread ~ gevocab_c*age_c + (gevocab_c | school), data = data_achieve_center, REML = FALSE) 4.9.2.1 Compare the Models # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(fit_read_9ml, fit_read_10ml, fit_read_9ml_c, fit_read_10ml_c), custom.model.names = c(&quot;Main Effects&quot;, &quot;Interaction&quot;, &quot;Main Effects&quot;, &quot;Interaction&quot;), groups = list(&quot;Raw Scale&quot; = 2:4, &quot;Mean Centered&quot; = 5:7), caption = &quot;MLM: Investigate Centering Variables Involved in an Interaction&quot;, caption.above = TRUE, single.row = TRUE) MLM: Investigate Centering Variables Involved in an Interaction Main Effects Interaction Main Effects Interaction (Intercept) 2.98 (0.41)*** 5.61 (0.87)*** 4.34 (0.03)*** 4.35 (0.03)*** Raw Scale gevocab 0.52 (0.01)*** -0.14 (0.19) age -0.01 (0.00)* -0.03 (0.01)*** gevocab:age 0.01 (0.00)*** Mean Centered gevocab_c 0.52 (0.01)*** 0.52 (0.01)*** age_c -0.01 (0.00)* -0.01 (0.00) gevocab_c:age_c 0.01 (0.00)*** AIC 42989.44 42979.71 42989.44 42979.71 BIC 43040.13 43037.65 43040.13 43037.65 Log Likelihood -21487.72 -21481.86 -21487.72 -21481.86 Num. obs. 10320 10320 10320 10320 Num. groups: school 160 160 160 160 Var: school (Intercept) 0.27 0.27 0.10 0.10 Var: school gevocab 0.02 0.02 Cov: school (Intercept) gevocab -0.06 -0.06 Var: Residual 3.66 3.66 3.66 3.66 Var: school gevocab_c 0.02 0.02 Cov: school (Intercept) gevocab_c 0.02 0.02 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 Notice that the interactions yield the exact same parameter estimates and significances, but the main effects (including the interactions) are different. Model fit statistics include \\(-2LL\\) are exactly the same, too. anova(fit_read_9ml, fit_read_10ml) Data: data_achieve_center Models: fit_read_9ml: geread ~ gevocab + age + (gevocab | school) fit_read_10ml: geread ~ gevocab * age + (gevocab | school) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) fit_read_9ml 7 42989 43040 -21488 42975 fit_read_10ml 8 42980 43038 -21482 42964 11.726 1 0.0006162 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(fit_read_9ml_c, fit_read_10ml_c) Data: data_achieve_center Models: fit_read_9ml_c: geread ~ gevocab_c + age_c + (gevocab_c | school) fit_read_10ml_c: geread ~ gevocab_c * age_c + (gevocab_c | school) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) fit_read_9ml_c 7 42989 43040 -21488 42975 fit_read_10ml_c 8 42980 43038 -21482 42964 11.726 1 0.0006162 fit_read_9ml_c fit_read_10ml_c *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 4.9.2.2 Visualize the Model What does the model look like? First plot the model fit to the centered variables with all defaut settings. effects::Effect(focal.predictors = c(&quot;gevocab_c&quot;, &quot;age_c&quot;), mod = fit_read_10ml_c) %&gt;% data.frame() %&gt;% dplyr::mutate(age_c = factor(age_c)) %&gt;% ggplot(aes(x = gevocab_c, y = fit, color = age_c)) + geom_line() + theme_bw() Notice that the vocab and ages are centered at zero. This makes it hard to read the plot, especially if we are unsure what the means of the variables were. Plot the model again, but this time UN-centering the variables by adding back the grand-mean we subtracted prior to fitting the model. effects::Effect(focal.predictors = c(&quot;gevocab_c&quot;, &quot;age_c&quot;), mod = fit_read_10ml_c, xlevels = list(age_c = c(84, 108, 132) - 107.5290)) %&gt;% # add back the mean of age data.frame() %&gt;% dplyr::mutate(age_yrs = factor((age_c + 107.5290)/12)) %&gt;% # add back the mean of age dplyr::mutate(gevocab = gevocab_c + 4.4938) %&gt;% # add back the mean of vocab ggplot(aes(x = gevocab, y = fit, color = age_yrs)) + geom_line() + theme_bw() Now the plot is back on to the origial variables. 4.10 Rescaling Predictors: Change Units or Standardize Where centering variables involved subtracting a set value, scalling a varaibles involves dividing by a set amount. When we both center to the mean and divide by the standard deviation, the new resulting varaible is said to be standardized (not to be confusing with normalizing, which is does not do). To retain meaningful units, you can multiply or divide all the measured values of a variable by a set amount, like a multiple of 10. This retains the meaning behind the units while still bringing them into line with other variables in the model and can avoid some convergence issues. 4.10.1 Scale Varaibles 4.10.1.1 Divide by a Meaningful Value data_achieve_center %&gt;% dplyr::select(senroll_c, ses) %&gt;% summary() senroll_c ses Min. :-418.41 Min. : 0.00 1st Qu.: -95.41 1st Qu.: 66.30 Median : -14.41 Median : 81.70 Mean : 0.00 Mean : 72.85 3rd Qu.: 110.59 3rd Qu.: 87.80 Max. : 382.59 Max. :100.00 For this situation, lets both center at the mean and scale (by 100) the school enrollemnt variable. For SES, lets only divide by ten. data_achieve_center_scale &lt;- data_achieve_center %&gt;% dplyr::mutate(senroll_ch = senroll_c / 100) %&gt;% # centered AND divided by one hundred dplyr::mutate(ses_t = ses / 10) # JUST divide by ten 4.10.1.2 Compare the scaled and unscaled measures View the first and last few observations: data_achieve_center_scale %&gt;% dplyr::select(id, school, age_c, gevocab_c, senroll, senroll_c, senroll_ch, ses, ses_t) %&gt;% headTail() id school age_c gevocab_c senroll senroll_c senroll_ch ses ses_t 1 1 767 -3.53 -1.39 463 -70.41 -0.7 80.4 8.04 2 2 767 -1.53 -1.69 463 -70.41 -0.7 80.4 8.04 3 3 767 4.47 -2.79 463 -70.41 -0.7 80.4 8.04 4 4 767 1.47 -2.39 463 -70.41 -0.7 80.4 8.04 5 &lt;NA&gt; &lt;NA&gt; ... ... ... ... ... ... ... 6 7893 9085 0.47 0.21 603 69.59 0.7 84.4 8.44 7 7894 9085 0.47 -1.39 603 69.59 0.7 84.4 8.44 8 7895 9085 6.47 -0.69 603 69.59 0.7 84.4 8.44 9 7896 9085 -1.53 -1.49 603 69.59 0.7 84.4 8.44 Compare the summary statistcs: # Knit to Website: output = &quot;html&quot; # Knit to PDF: output = &quot;latex2&quot; # View on Screen: output = &quot;&quot;text&quot;, or &quot;markdown&quot;, &quot;html&quot; data_achieve_center_scale %&gt;% furniture::table1(senroll, senroll_c, senroll_ch, ses, ses_t, output = &quot;html&quot;, digits = 4) Mean/Count (SD/%) n = 10320 senroll 533.4148 (154.7968) senroll_c 0.0000 (154.7968) senroll_ch 0.0000 (1.5480) ses 72.8490 (21.9817) ses_t 7.2849 (2.1982) 4.10.2 Use Scaled Variables Using the new versions of our variables, investigate is SES has an effect, either in 2-way or 3-way interactions with age and vocabulary. fit_read_10ml_s &lt;- lme4::lmer(geread ~ gevocab_c*age_c + # no SES (gevocab_c | school), data = data_achieve_center_scale, REML = FALSE) fit_read_11ml_s &lt;- lme4::lmer(geread ~ gevocab_c*age_c + gevocab_c*ses_t + # 2 2-way interactions (gevocab_c | school), data = data_achieve_center_scale, REML = FALSE) fit_read_12ml_s &lt;- lme4::lmer(geread ~ gevocab_c*age_c*ses_t + # 3-way interaction (gevocab_c | school), data = data_achieve_center_scale, REML = FALSE) # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(fit_read_10ml_s, fit_read_11ml_s, fit_read_12ml_s), custom.model.names = c(&quot;no SES&quot;, &quot;2-way&quot;, &quot;3-way&quot;), caption = &quot;MLM: Investigate More Complex Fixed Interactions&quot;, caption.above = TRUE, single.row = TRUE) MLM: Investigate More Complex Fixed Interactions no SES 2-way 3-way (Intercept) 4.35 (0.03)*** 3.94 (0.11)*** 3.94 (0.11)*** gevocab_c 0.52 (0.01)*** 0.67 (0.05)*** 0.67 (0.05)*** age_c -0.01 (0.00) -0.00 (0.00) -0.01 (0.01) gevocab_c:age_c 0.01 (0.00)*** 0.01 (0.00)** 0.00 (0.01) ses_t 0.06 (0.01)*** 0.06 (0.01)*** gevocab_c:ses_t -0.02 (0.01)** -0.02 (0.01)** age_c:ses_t 0.00 (0.00) gevocab_c:age_c:ses_t 0.00 (0.00) AIC 42979.71 42928.81 42932.05 BIC 43037.65 43001.23 43018.95 Log Likelihood -21481.86 -21454.41 -21454.02 Num. obs. 10320 10320 10320 Num. groups: school 160 160 160 Var: school (Intercept) 0.10 0.08 0.08 Var: school gevocab_c 0.02 0.02 0.02 Cov: school (Intercept) gevocab_c 0.02 0.03 0.03 Var: Residual 3.66 3.66 3.66 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 anova(fit_read_10ml_s, fit_read_11ml_s, fit_read_12ml_s) Data: data_achieve_center_scale Models: fit_read_10ml_s: geread ~ gevocab_c * age_c + (gevocab_c | school) fit_read_11ml_s: geread ~ gevocab_c * age_c + gevocab_c * ses_t + (gevocab_c | fit_read_11ml_s: school) fit_read_12ml_s: geread ~ gevocab_c * age_c * ses_t + (gevocab_c | school) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) fit_read_10ml_s 8 42980 43038 -21482 42964 fit_read_11ml_s 10 42929 43001 -21454 42909 54.8974 2 1.2e-12 fit_read_12ml_s 12 42932 43019 -21454 42908 0.7674 2 0.6813 fit_read_10ml_s fit_read_11ml_s *** fit_read_12ml_s --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 There is evidence that SES moderates the main effect of vocabualry, after accounting for the interaction between age and vocabulary. 4.11 Final Model Always refit the final model via REML. fit_read_11re_s &lt;- lme4::lmer(geread ~ gevocab_c*age_c + gevocab_c*ses_t + # 2 2-way interactions (gevocab_c | school), data = data_achieve_center_scale, REML = TRUE) 4.11.1 Table # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(fit_read_11re_s), caption = &quot;MLM: Final Model&quot;, caption.above = TRUE, single.row = TRUE) MLM: Final Model Model 1 (Intercept) 3.94 (0.11)*** gevocab_c 0.67 (0.05)*** age_c -0.00 (0.00) ses_t 0.06 (0.01)*** gevocab_c:age_c 0.01 (0.00)** gevocab_c:ses_t -0.02 (0.01)** AIC 42976.51 BIC 43048.93 Log Likelihood -21478.26 Num. obs. 10320 Num. groups: school 160 Var: school (Intercept) 0.08 Var: school gevocab_c 0.02 Cov: school (Intercept) gevocab_c 0.03 Var: Residual 3.66 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 4.11.1.1 Visualize the Model Recall the scales that the revised variables are now on: data_achieve_center_scale %&gt;% dplyr::select(gevocab_c, age_c, ses_t) %&gt;% summary() gevocab_c age_c ses_t Min. :-4.493800 Min. :-25.529000 Min. : 0.000 1st Qu.:-1.593800 1st Qu.: -3.529000 1st Qu.: 6.630 Median :-0.693800 Median : -0.529000 Median : 8.170 Mean : 0.000044 Mean : -0.000027 Mean : 7.285 3rd Qu.: 0.706200 3rd Qu.: 3.471000 3rd Qu.: 8.780 Max. : 6.706200 Max. : 27.471000 Max. :10.000 effects::Effect(focal.predictors = c(&quot;gevocab_c&quot;, &quot;age_c&quot;, &quot;ses_t&quot;), mod = fit_read_11re_s, xlevels = list(age_c = c(84, 108, 132) - 107.5290, ses_t = c(0, 5, 10))) %&gt;% data.frame() %&gt;% dplyr::mutate(age_yrs = factor((age_c + 107.5290)/12)) %&gt;% dplyr::mutate(gevocab = gevocab_c + 4.4938) %&gt;% dplyr::mutate(ses = factor(ses_t * 10)) %&gt;% ggplot(aes(x = gevocab, y = fit, color = age_yrs)) + geom_line() + theme_bw() + facet_grid(.~ ses, labeller = &quot;label_both&quot;) + labs(x = &quot;Vocabulary Score&quot;, y = &quot;Reading Score\\nEstimated Marginal Mean&quot;, color = &quot;Student Age&quot;) There is evidence that higher vocabulary scores correlate with higher reading scores. This relationship os strongest in low SES schools and among older students. This relationship is especially weaker in younger students attending high SES schools. See: Nakagawa and Schielzeth (2013) for more regarding model \\(R^2\\) From the sjstats::r2() documentation, for mixed models: marginal r-squared considers only the variance of the fixed effects conditional r-squared takes both the fixed and random effects into account sjstats::r2(fit_read_11re_s) R-Squared for Generalized Linear Mixed Model Family : gaussian (identity) Formula: geread ~ gevocab_c * age_c + gevocab_c * ses_t + (gevocab_c | school) Marginal R2: 0.289 Conditional R2: 0.322 Helpful links: http://maths-people.anu.edu.au/~johnm/r-book/xtras/mlm-ohp.pdf http://ase.tufts.edu/gsc/gradresources/guidetomixedmodelsinr/mixed%20model%20guide.html http://web.stanford.edu/class/psych252/section_2015/Section_week9.html https://www.r-bloggers.com/visualizing-generalized-linear-mixed-effects-models-with-ggplot-rstats-lme4/ https://www.r-bloggers.com/visualizing-generalized-linear-mixed-effects-models-part-2-rstats-lme4/ http://www.strengejacke.de/sjPlot/sjp.lmer/ "],
["centering-and-standardizing-explanitory-variables.html", "5 Centering and Standardizing Explanitory Variables 5.1 Background 5.2 Grand-Mean-Centering and Standardizing Variables 5.3 RI = ONLY Random Intercepts 5.4 RIAS = Random Intercepts AND Slopes", " 5 Centering and Standardizing Explanitory Variables library(tidyverse) # all things tidy library(broom) # converst stats objestcs to tidy tibbles library(haven) # read in SPSS dataset library(furniture) # nice table1() descriptives library(stargazer) # display nice tables: summary &amp; regression library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(psych) # contains some useful functions, like headTail library(sjstats) # ICC calculations library(lme4) # non-linear mixed-effects models library(haven) # read in SPSS dataset 5.1 Background The text “Multilevel Analysis: Techniques and Applications, Third Edition” (Hox, Moerbeek, and Van de Schoot 2017) has a companion website which includes links to all the data files used throughout the book (housed on the book’s GitHub repository). The following example is used through out Hox, Moerbeek, and Van de Schoot (2017)’s chapater 2. From Appendix E: The popularity data in popular2.sav are simulated data for 2000 pupils in 100 schools. The purpose is to offer a very simple example for multilevel regression analysis. The main outcome variable is the pupil popularity, a popularity rating on a scale of 1-10 derived by a sociometric procedure. Typically, a sociometric procedure asks all pupils in a class to rate all the other pupils, and then assigns the average received popularity rating to each pupil. Because of the sociometric procedure, group effects as apparent from higher level variance components are rather strong. There is a second outcome variable: pupil popularity as rated by their teacher, on a scale from 1-10. The explanatory variables are pupil gender (boy=0, girl=1), pupil extraversion (10-point scale) and teacher experience in years. The popularity data have been generated to be a ‘nice’ well-behaved data set: the sample sizes at both levels are sufficient, the residuals have a normal distribution, and the multilevel effects are strong. data_raw &lt;- haven::read_sav(&quot;https://github.com/MultiLevelAnalysis/Datasets-third-edition-Multilevel-book/raw/master/chapter%202/popularity/SPSS/popular2.sav&quot;) %&gt;% haven::as_factor() # retain the labels from SPSS --&gt; factor data_pop &lt;- data_raw %&gt;% dplyr::mutate(id = paste(class, pupil, sep = &quot;_&quot;) %&gt;% # create a unique id for each student (char) factor()) %&gt;% # declare id is a factor dplyr::select(id, pupil:popteach) # reduce the variables included tibble::glimpse(data_pop) Observations: 2,000 Variables: 8 $ id &lt;fct&gt; 1_1, 1_2, 1_3, 1_4, 1_5, 1_6, 1_7, 1_8, 1_9, 1_10, 1_... $ pupil &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16... $ class &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... $ extrav &lt;dbl&gt; 5, 7, 4, 3, 5, 4, 5, 4, 5, 5, 5, 5, 5, 5, 5, 6, 4, 4,... $ sex &lt;fct&gt; girl, boy, girl, girl, girl, boy, boy, boy, boy, boy,... $ texp &lt;dbl&gt; 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 2... $ popular &lt;dbl&gt; 6.3, 4.9, 5.3, 4.7, 6.0, 4.7, 5.9, 4.2, 5.2, 3.9, 5.7... $ popteach &lt;dbl&gt; 6, 5, 6, 5, 6, 5, 5, 5, 5, 3, 5, 5, 5, 6, 5, 5, 2, 3,... data_pop %&gt;% ggplot() + aes(x = extrav, y = popular, group = class) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;black&quot;, size = .2) + theme_bw() + geom_vline(xintercept = 0, color = &quot;red&quot;) + labs(title = &quot;OLS: Single Level Regression&quot;, subtitle = &quot;Thin black lines are OLS regression ran independently on each class&quot;, x = &quot;Student&#39;s Extroversion, as rated by their teacher&quot;, y = &quot;Student&#39;s Populartity, mean rating by their peers&quot;) + coord_cartesian(xlim = c(0, 10), ylim = c(0, 10)) + scale_x_continuous(breaks = seq(from = 0, to = 10, by = 2)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) 5.2 Grand-Mean-Centering and Standardizing Variables It is best to manually determine the variable’s mean (mean()) and standard deviation (sd()). mean(data_pop$extrav) [1] 5.215 sd(data_pop$extrav) [1] 1.262368 5.2.1 Grand-Mean-Centering \\[ VAR_G = VAR - mean(VAR) \\] 5.2.2 Standardizing \\[ VAR_Z = \\frac{VAR - mean(VAR)}{sd(VAR)} \\] data_pop &lt;- data_pop %&gt;% dplyr::mutate(extravG = extrav - 5.215) %&gt;% dplyr::mutate(extravZ = (extrav - 5.215) / 1.262368) data_pop %&gt;% dplyr::select(extrav, extravG, extravZ) %&gt;% data.frame() %&gt;% stargazer::stargazer(title = &quot;Descriptive statistics: Three versions of Extraversion&quot;, header = FALSE, type = &quot;html&quot;) Descriptive statistics: Three versions of Extraversion Statistic N Mean St. Dev. Min Pctl(25) Pctl(75) Max extrav 2,000 5.215 1.262 1 4 6 10 extravG 2,000 0.000 1.262 -4.215 -1.215 0.785 4.785 extravZ 2,000 0.000 1.000 -3.339 -0.962 0.622 3.790 5.3 RI = ONLY Random Intercepts 5.3.1 Fit MLM with all 3 versions of the predictor pop_lmer_1_raw &lt;- lme4::lmer(popular ~ extrav + (1|class), data = data_pop, REML = FALSE) pop_lmer_1_cen &lt;- lme4::lmer(popular ~ extravG + (1|class), data = data_pop, REML = FALSE) pop_lmer_1_std &lt;- lme4::lmer(popular ~ extravZ + (1|class), data = data_pop, REML = FALSE) # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(pop_lmer_1_raw, pop_lmer_1_cen, pop_lmer_1_std), custom.model.names = c(&quot;Raw&quot;, &quot;Centered&quot;, &quot;Standardized&quot;), caption = &quot;MLM - RI: Effect of Grand-Mean Centering and Standardizing&quot;, caption.above = TRUE, single.row = TRUE) MLM - RI: Effect of Grand-Mean Centering and Standardizing Raw Centered Standardized (Intercept) 2.54 (0.14)*** 5.08 (0.09)*** 5.08 (0.09)*** extrav 0.49 (0.02)*** extravG 0.49 (0.02)*** extravZ 0.61 (0.03)*** AIC 5831.78 5831.78 5831.78 BIC 5854.18 5854.18 5854.18 Log Likelihood -2911.89 -2911.89 -2911.89 Num. obs. 2000 2000 2000 Num. groups: class 100 100 100 Var: class (Intercept) 0.83 0.83 0.83 Var: Residual 0.93 0.93 0.93 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 ** MLM - Random Intercepts ONLY** Grand-Mean Centering a Predictor Different than when using the Raw Predictor: fixed intercept Same as when using the Raw Predictor: fixed estimates or slopes for all predictors (main effects and interactions) random estimates, i.e. variance and covariance components, includin the residual variance model fit statistics, including AIC, BIC, and the Log Loikelihood (-2LL or deviance) Standardize a Predictor Different than when using the Raw Predictor: fixed intercept (same as if using the grand-mean centered predictor) fixed estimate (slope) for that variable Stays the SAME: random estimates, i.e. variance and covariance components, includin the residual variance model fit statistics, including AIC, BIC, and the Log Loikelihood (-2LL or deviance) 5.3.2 Investigating a MLM-RI Model pop_lmer_1_raw %&gt;% broom::tidy() # A tibble: 4 x 5 term estimate std.error statistic group &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 (Intercept) 2.54 0.141 18.1 fixed 2 extrav 0.486 0.0201 24.1 fixed 3 sd_(Intercept).class 0.912 NA NA class 4 sd_Observation.Residual 0.964 NA NA Residual pop_lmer_1_raw %&gt;% broom::glance() # A tibble: 1 x 6 sigma logLik AIC BIC deviance df.residual &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; 1 0.964 -2912. 5832. 5854. 5824. 1996 pop_lmer_1_raw %&gt;% broom::augment() %&gt;% head() # onle line per observation (2000 students) popular extrav class .fitted .resid .cooksd .fixed .mu 1 6.3 5 1 5.138703 1.16129692 2.712828e-05 4.973704 5.138703 2 4.9 7 1 6.111103 -1.21110349 1.752270e-03 5.946104 6.111103 3 5.3 4 1 4.652503 0.64749713 8.856823e-05 4.487503 4.652503 4 4.7 3 1 4.166303 0.53369734 2.626814e-04 4.001303 4.166303 5 6.0 5 1 5.138703 0.86129692 1.492250e-05 4.973704 5.138703 6 4.7 4 1 4.652503 0.04749713 4.765813e-07 4.487503 4.652503 .offset .sqrtXwt .sqrtrwt .weights .wtres 1 0 1 1 1 1.16129692 2 0 1 1 1 -1.21110349 3 0 1 1 1 0.64749713 4 0 1 1 1 0.53369734 5 0 1 1 1 0.86129692 6 0 1 1 1 0.04749713 5.3.2.1 Fixed Effects: intercept and slope There is only ONE fixed intercept and ONE fixed slope. The fixef() function extracts the estimates of the fixed effects. fixef(pop_lmer_1_raw) (Intercept) extrav 2.5427027 0.4862002 fixef(pop_lmer_1_raw)[[&quot;(Intercept)&quot;]] [1] 2.542703 fixef(pop_lmer_1_raw)[[&quot;extrav&quot;]] [1] 0.4862002 data_pop %&gt;% ggplot() + aes(x = extrav, y = popular, group = class) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;black&quot;, size = .2) + geom_abline(intercept = fixef(pop_lmer_1_raw)[[&quot;(Intercept)&quot;]], slope = fixef(pop_lmer_1_raw)[[&quot;extrav&quot;]], color = &quot;hot pink&quot;, size = 2) + theme_bw() + geom_vline(xintercept = 0, color = &quot;red&quot;) + labs(title = &quot;MLM-RI: Extroversion = raw score&quot;, subtitle = &quot;Thin black lines are OLS regression ran independently on each class&quot;, x = &quot;Student&#39;s Extroversion, as rated by their teacher&quot;, y = &quot;Student&#39;s Populartity, mean rating by their peers&quot;) + coord_cartesian(xlim = c(0, 10), ylim = c(0, 10)) + scale_x_continuous(breaks = seq(from = 0, to = 10, by = 2)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) 5.3.2.2 Random Effects: intercepts There is a different random intercept for EACH CLASS. These tell how far each class’s average is off of the grand average. The ranef() function extracts the random effects from a fitted model object ranef(pop_lmer_1_raw) %&gt;% str() List of 1 $ class:&#39;data.frame&#39;: 100 obs. of 1 variable: ..$ (Intercept): num [1:100] 0.165 -0.7536 -0.3646 0.5405 -0.0994 ... - attr(*, &quot;class&quot;)= chr &quot;ranef.mer&quot; ranef(pop_lmer_1_raw)$class %&gt;% head() # onle line per group (100 classes) (Intercept) 1 0.16499938 2 -0.75362983 3 -0.36464658 4 0.54049206 5 -0.09943663 6 -0.60487822 ranef(pop_lmer_1_raw)$class %&gt;% dplyr::rename(Random_Intercepts = &quot;(Intercept)&quot;) %&gt;% ggplot(aes(Random_Intercepts)) + geom_histogram(binwidth = .25) 5.3.2.3 Predictions predict(pop_lmer_1_raw) %&gt;% str() Named num [1:2000] 5.14 6.11 4.65 4.17 5.14 ... - attr(*, &quot;names&quot;)= chr [1:2000] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... predict(pop_lmer_1_raw) %&gt;% head() # onle value per observation (2000 students) 1 2 3 4 5 6 5.138703 6.111103 4.652503 4.166303 5.138703 4.652503 data_pop %&gt;% dplyr::mutate(pred = predict(pop_lmer_1_raw)) %&gt;% ggplot(aes(x = extrav, y = pred, group = class)) + geom_line(size = .2) + geom_abline(intercept = fixef(pop_lmer_1_raw)[[&quot;(Intercept)&quot;]], slope = fixef(pop_lmer_1_raw)[[&quot;extrav&quot;]], color = &quot;hot pink&quot;, size = 2) + theme_bw() + geom_vline(xintercept = 0, color = &quot;red&quot;) + geom_vline(xintercept = 5.215, color = &quot;blue&quot;) + labs(title = &quot;MLM-RI: Extroversion = raw score&quot;, subtitle = &quot;Thin black lines are group-wise predictions, one per class&quot;, x = &quot;Student&#39;s Extroversion, as rated by their teacher&quot;, y = &quot;Predicted\\nStudent&#39;s Populartity, mean rating by their peers&quot;) + coord_cartesian(xlim = c(0, 10), ylim = c(0, 10)) + scale_x_continuous(breaks = seq(from = 0, to = 10, by = 2)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) 5.3.2.4 Combined Effects The coef() function computes the sum of the random and fixed effects coefficients for each explanatory variable for each level of each grouping factor. coef(pop_lmer_1_raw) %&gt;% str() List of 1 $ class:&#39;data.frame&#39;: 100 obs. of 2 variables: ..$ (Intercept): num [1:100] 2.71 1.79 2.18 3.08 2.44 ... ..$ extrav : num [1:100] 0.486 0.486 0.486 0.486 0.486 ... - attr(*, &quot;class&quot;)= chr &quot;coef.mer&quot; coef(pop_lmer_1_raw)$class %&gt;% head() # onle line per group (100 classes) (Intercept) extrav 1 2.707702 0.4862002 2 1.789073 0.4862002 3 2.178056 0.4862002 4 3.083195 0.4862002 5 2.443266 0.4862002 6 1.937824 0.4862002 data_pop %&gt;% dplyr::mutate(pred = predict(pop_lmer_1_raw)) %&gt;% ggplot() + aes(x = extrav, y = pred, group = class) + geom_rect(aes(xmin = 0 - 0.25, xmax = 0 + 0.25, ymin = fixef(pop_lmer_1_raw)[[&quot;(Intercept)&quot;]]- 2.5, ymax = fixef(pop_lmer_1_raw)[[&quot;(Intercept)&quot;]]+ 2.5), fill = &quot;yellow&quot;, alpha = 0.05) + geom_abline(data = coef(pop_lmer_1_raw)$class %&gt;% dplyr::rename(Intercept = &quot;(Intercept)&quot;), aes(intercept = Intercept, slope = extrav), color = &quot;gray&quot;, size = .1) + geom_line(size = .2) + geom_abline(intercept = fixef(pop_lmer_1_raw)[[&quot;(Intercept)&quot;]], slope = fixef(pop_lmer_1_raw)[[&quot;extrav&quot;]], color = &quot;hot pink&quot;, size = 2) + theme_bw() + geom_vline(xintercept = 0, color = &quot;red&quot;) + geom_vline(xintercept = 5.215, color = &quot;blue&quot;) + labs(title = &quot;MLM-RI: Extroversion = raw score&quot;, subtitle = &quot;Thin black lines are group-wise predictions, one per class EXTRAPOLATED OUT&quot;, x = &quot;Student&#39;s Extroversion, as rated by their teacher&quot;, y = &quot;Predicted\\nStudent&#39;s Populartity, mean rating by their peers&quot;) + coord_cartesian(xlim = c(0, 10), ylim = c(0, 10)) + scale_x_continuous(breaks = seq(from = 0, to = 10, by = 2)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) 5.3.3 Comapre the Centered Version data_pop %&gt;% dplyr::mutate(pred = predict(pop_lmer_1_cen)) %&gt;% ggplot() + aes(x = extravG, y = pred, group = class) + geom_rect(aes(xmin = -5.215 - 0.25, xmax = -5.215 + 0.25, ymin = fixef(pop_lmer_1_raw)[[&quot;(Intercept)&quot;]]- 2.5, ymax = fixef(pop_lmer_1_raw)[[&quot;(Intercept)&quot;]]+ 2.5), fill = &quot;yellow&quot;, alpha = 0.05) + geom_rect(aes(xmin = 0 - 0.25, xmax = 0 + 0.25, ymin = fixef(pop_lmer_1_cen)[[&quot;(Intercept)&quot;]]- 2.5, ymax = fixef(pop_lmer_1_cen)[[&quot;(Intercept)&quot;]]+ 2.5), fill = &quot;yellow&quot;, alpha = 0.05) + geom_abline(data = coef(pop_lmer_1_cen)$class %&gt;% dplyr::rename(Intercept = &quot;(Intercept)&quot;), aes(intercept = Intercept, slope = extravG), color = &quot;gray&quot;, size = .1) + geom_line(size = .2) + geom_abline(intercept = fixef(pop_lmer_1_cen)[[&quot;(Intercept)&quot;]], slope = fixef(pop_lmer_1_cen)[[&quot;extravG&quot;]], color = &quot;hot pink&quot;, size = 2) + theme_bw() + geom_vline(xintercept = -5.215, color = &quot;blue&quot;) + geom_vline(xintercept = 0, color = &quot;red&quot;) + labs(title = &quot;MLM-RI: Extroversion = grand-mean centered&quot;, subtitle = &quot;Thin black lines are group-wise predictions, one per class EXTRAPOLATED OUT&quot;, x = &quot;GRAND-MEAN CENTERED Student&#39;s Extroversion, as rated by their teacher&quot;, y = &quot;Predicted\\nStudent&#39;s Populartity, mean rating by their peers&quot;) + coord_cartesian(xlim = c(-5, 5), ylim = c(0, 10)) + scale_x_continuous(breaks = seq(from = -4, to = 4, by = 2)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) 5.3.4 Comapre the Standardized Version data_pop %&gt;% dplyr::mutate(pred = predict(pop_lmer_1_std)) %&gt;% ggplot() + aes(x = extravZ, y = pred, group = class) + geom_rect(aes(xmin = 0 - 0.25, xmax = 0 + 0.25, ymin = fixef(pop_lmer_1_cen)[[&quot;(Intercept)&quot;]]- 2.5, ymax = fixef(pop_lmer_1_cen)[[&quot;(Intercept)&quot;]]+ 2.5), fill = &quot;yellow&quot;, alpha = 0.05) + geom_abline(data = coef(pop_lmer_1_std)$class %&gt;% dplyr::rename(Intercept = &quot;(Intercept)&quot;), aes(intercept = Intercept, slope = extravZ), color = &quot;gray&quot;, size = .1) + geom_line(size = .2) + geom_abline(intercept = fixef(pop_lmer_1_std)[[&quot;(Intercept)&quot;]], slope = fixef(pop_lmer_1_std)[[&quot;extravZ&quot;]], color = &quot;hot pink&quot;, size = 2) + theme_bw() + geom_vline(xintercept = 0, color = &quot;red&quot;) + geom_vline(xintercept = -5.215, color = &quot;blue&quot;) + labs(title = &quot;MLM-RI: Extroversion = standardized&quot;, subtitle = &quot;Thin black lines are group-wise predictions, one per class EXTRAPOLATED OUT&quot;, x = &quot;STANDARDIZED Student&#39;s Extroversion, as rated by their teacher&quot;, y = &quot;Predicted\\nStudent&#39;s Populartity, mean rating by their peers&quot;) + coord_cartesian(xlim = c(-5, 5), ylim = c(0, 10)) + scale_x_continuous(breaks = seq(from = -4, to = 4, by = 2)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) 5.4 RIAS = Random Intercepts AND Slopes 5.4.1 Fit MLM with all 3 versions of the predictor pop_lmer_2_raw &lt;- lme4::lmer(popular ~ extrav + (extrav|class), data = data_pop, REML = FALSE) pop_lmer_2_cen &lt;- lme4::lmer(popular ~ extravG + (extravG|class), data = data_pop, REML = FALSE) pop_lmer_2_std &lt;- lme4::lmer(popular ~ extravZ + (extravZ|class), data = data_pop, REML = FALSE) # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(pop_lmer_2_raw, pop_lmer_2_cen, pop_lmer_2_std), custom.model.names = c(&quot;Raw&quot;, &quot;Centered&quot;, &quot;Standardized&quot;), caption = &quot;MLM - RIAS: Effect of Grand-Mean Centering and Standardizing&quot;, caption.above = TRUE, single.row = TRUE) MLM - RIAS: Effect of Grand-Mean Centering and Standardizing Raw Centered Standardized (Intercept) 2.46 (0.20)*** 5.03 (0.10)*** 5.03 (0.10)*** extrav 0.49 (0.03)*** extravG 0.49 (0.03)*** extravZ 0.62 (0.03)*** AIC 5782.69 5782.69 5782.69 BIC 5816.29 5816.29 5816.29 Log Likelihood -2885.34 -2885.34 -2885.34 Num. obs. 2000 2000 2000 Num. groups: class 100 100 100 Var: class (Intercept) 2.95 0.88 0.88 Var: class extrav 0.03 Cov: class (Intercept) extrav -0.26 Var: Residual 0.90 0.90 0.90 Var: class extravG 0.03 Cov: class (Intercept) extravG -0.13 Var: class extravZ 0.04 Cov: class (Intercept) extravZ -0.17 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 ** MLM - Random Intercepts AND Slopes** Grand-Mean Centering a Predictor Different than when using the Raw Predictor: fixed intercept random estimates, i.e. variance and covariance components, includin the residual variance Same as when using the Raw Predictor: fixed estimates or slopes for all predictors (main effects and interactions) model fit statistics, including AIC, BIC, and the Log Loikelihood (-2LL or deviance) Standardize a Predictor Different than when using the Raw Predictor: fixed intercept (same as if using the grand-mean centered predictor) fixed estimate (slope) for that variable random estimates, i.e. variance and covariance components, includin the residual variance Stays the SAME: model fit statistics, including AIC, BIC, and the Log Loikelihood (-2LL or deviance) 5.4.2 Investigating a MLM-RI Model pop_lmer_2_raw %&gt;% broom::tidy() # A tibble: 6 x 5 term estimate std.error statistic group &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 (Intercept) 2.46 0.202 12.2 fixed 2 extrav 0.493 0.0253 19.5 fixed 3 sd_(Intercept).class 1.72 NA NA class 4 sd_extrav.class 0.159 NA NA class 5 cor_(Intercept).extrav.class -0.969 NA NA class 6 sd_Observation.Residual 0.946 NA NA Residual pop_lmer_2_raw %&gt;% broom::glance() # A tibble: 1 x 6 sigma logLik AIC BIC deviance df.residual &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; 1 0.946 -2885. 5783. 5816. 5771. 1994 pop_lmer_2_raw %&gt;% broom::augment() %&gt;% head() # onle line per observation (2000 students) popular extrav class .fitted .resid .cooksd .fixed .mu 1 6.3 5 1 5.131082 1.16891841 3.872950e-05 4.925861 5.131082 2 4.9 7 1 6.062463 -1.16246304 1.546709e-03 5.911665 6.062463 3 5.3 4 1 4.665391 0.63460913 5.582607e-05 4.432959 4.665391 4 4.7 3 1 4.199700 0.50029985 1.873118e-04 3.940057 4.199700 5 6.0 5 1 5.131082 0.86891841 2.140087e-05 4.925861 5.131082 6 4.7 4 1 4.665391 0.03460913 1.660373e-07 4.432959 4.665391 .offset .sqrtXwt .sqrtrwt .weights .wtres 1 0 1 1 1 1.16891841 2 0 1 1 1 -1.16246304 3 0 1 1 1 0.63460913 4 0 1 1 1 0.50029985 5 0 1 1 1 0.86891841 6 0 1 1 1 0.03460913 5.4.2.1 Fixed Effects: intercept and slope There is only ONE fixed intercept and ONE fixed slope. The fixef() function extracts the estimates of the fixed effects. fixef(pop_lmer_2_raw) (Intercept) extrav 2.461351 0.492902 fixef(pop_lmer_2_raw)[[&quot;(Intercept)&quot;]] [1] 2.461351 fixef(pop_lmer_2_raw)[[&quot;extrav&quot;]] [1] 0.492902 data_pop %&gt;% ggplot() + aes(x = extrav, y = popular, group = class) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;black&quot;, size = .2) + geom_abline(intercept = fixef(pop_lmer_2_raw)[[&quot;(Intercept)&quot;]], slope = fixef(pop_lmer_2_raw)[[&quot;extrav&quot;]], color = &quot;hot pink&quot;, size = 2) + theme_bw() + geom_vline(xintercept = 0, color = &quot;red&quot;) + geom_vline(xintercept = 5.215, color = &quot;blue&quot;) + labs(title = &quot;MLM-RIAS: Extroversion = raw score&quot;, subtitle = &quot;Thin black lines are OLS regression ran independently on each class&quot;, x = &quot;Student&#39;s Extroversion, as rated by their teacher&quot;, y = &quot;Student&#39;s Populartity, mean rating by their peers&quot;) + coord_cartesian(xlim = c(0, 10), ylim = c(0, 10)) + scale_x_continuous(breaks = seq(from = 0, to = 10, by = 2)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) 5.4.2.2 Random Effects: intercepts There is a different random intercept AND random slope for EACH CLASS. These tell how far each class’s average is off of the grand average AND how far each class’s slope is off of the grand average sope. The ranef() function extracts the random effects from a fitted model object ranef(pop_lmer_2_raw) %&gt;% str() List of 1 $ class:&#39;data.frame&#39;: 100 obs. of 2 variables: ..$ (Intercept): num [1:100] 0.341 -1.18 -0.627 1.079 -0.191 ... ..$ extrav : num [1:100] -0.0272 0.0974 0.0565 -0.0988 0.0208 ... - attr(*, &quot;class&quot;)= chr &quot;ranef.mer&quot; ranef(pop_lmer_2_raw)$class %&gt;% head() # onle line per group (100 classes) (Intercept) extrav 1 0.3412771 -0.02721131 2 -1.1800352 0.09737305 3 -0.6268933 0.05654048 4 1.0791097 -0.09876733 5 -0.1910488 0.02083082 6 -0.9833296 0.08369927 ranef(pop_lmer_2_raw)$class %&gt;% dplyr::rename(Random_Intercepts = &quot;(Intercept)&quot;) %&gt;% ggplot(aes(Random_Intercepts)) + geom_histogram() ranef(pop_lmer_2_raw)$class %&gt;% ggplot(aes(extrav)) + geom_histogram() 5.4.2.3 Predictions predict(pop_lmer_2_raw) %&gt;% str() Named num [1:2000] 5.13 6.06 4.67 4.2 5.13 ... - attr(*, &quot;names&quot;)= chr [1:2000] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... predict(pop_lmer_2_raw) %&gt;% head() # onle value per observation (2000 students) 1 2 3 4 5 6 5.131082 6.062463 4.665391 4.199700 5.131082 4.665391 data_pop %&gt;% dplyr::mutate(pred = predict(pop_lmer_2_raw)) %&gt;% ggplot(aes(x = extrav, y = pred, group = class)) + geom_line(size = .2) + geom_abline(intercept = fixef(pop_lmer_2_raw)[[&quot;(Intercept)&quot;]], slope = fixef(pop_lmer_2_raw)[[&quot;extrav&quot;]], color = &quot;hot pink&quot;, size = 2) + theme_bw() + geom_vline(xintercept = 0, color = &quot;red&quot;) + geom_vline(xintercept = 5.215, color = &quot;blue&quot;) + labs(title = &quot;MLM-RIAS: Extroversion = raw score&quot;, subtitle = &quot;Thin black lines are group-wise predictions, one per class&quot;, x = &quot;Student&#39;s Extroversion, as rated by their teacher&quot;, y = &quot;Predicted\\nStudent&#39;s Populartity, mean rating by their peers&quot;) + coord_cartesian(xlim = c(0, 10), ylim = c(0, 10)) + scale_x_continuous(breaks = seq(from = 0, to = 10, by = 2)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) 5.4.2.4 Combined Effects The coef() function computes the sum of the random and fixed effects coefficients for each explanatory variable for each level of each grouping factor. coef(pop_lmer_1_raw) %&gt;% str() List of 1 $ class:&#39;data.frame&#39;: 100 obs. of 2 variables: ..$ (Intercept): num [1:100] 2.71 1.79 2.18 3.08 2.44 ... ..$ extrav : num [1:100] 0.486 0.486 0.486 0.486 0.486 ... - attr(*, &quot;class&quot;)= chr &quot;coef.mer&quot; coef(pop_lmer_1_raw)$class %&gt;% head() # onle line per group (100 classes) (Intercept) extrav 1 2.707702 0.4862002 2 1.789073 0.4862002 3 2.178056 0.4862002 4 3.083195 0.4862002 5 2.443266 0.4862002 6 1.937824 0.4862002 data_pop %&gt;% dplyr::mutate(pred = predict(pop_lmer_2_raw)) %&gt;% ggplot() + aes(x = extrav, y = pred, group = class) + geom_rect(aes(xmin = 0 - 0.25, xmax = 0 + 0.25, ymin = fixef(pop_lmer_2_raw)[[&quot;(Intercept)&quot;]]- 4.5, ymax = fixef(pop_lmer_2_raw)[[&quot;(Intercept)&quot;]]+ 4.5), fill = &quot;yellow&quot;, alpha = 0.05) + geom_abline(data = coef(pop_lmer_2_raw)$class %&gt;% dplyr::rename(Intercept = &quot;(Intercept)&quot;), aes(intercept = Intercept, slope = extrav), color = &quot;gray&quot;, size = .1) + geom_line(size = .2) + geom_abline(intercept = fixef(pop_lmer_2_raw)[[&quot;(Intercept)&quot;]], slope = fixef(pop_lmer_2_raw)[[&quot;extrav&quot;]], color = &quot;hot pink&quot;, size = 2) + theme_bw() + geom_vline(xintercept = 0, color = &quot;red&quot;) + geom_vline(xintercept = 5.215, color = &quot;blue&quot;) + labs(title = &quot;MLM-RIAS: Extroversion = raw score&quot;, subtitle = &quot;Thin black lines are group-wise predictions, one per class EXTRAPOLATED OUT&quot;, x = &quot;Student&#39;s Extroversion, as rated by their teacher&quot;, y = &quot;Predicted\\nStudent&#39;s Populartity, mean rating by their peers&quot;) + coord_cartesian(xlim = c(0, 10), ylim = c(0, 10)) + scale_x_continuous(breaks = seq(from = 0, to = 10, by = 2)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) 5.4.3 Comapre the Centered Version data_pop %&gt;% dplyr::mutate(pred = predict(pop_lmer_2_cen)) %&gt;% ggplot() + aes(x = extravG, y = pred, group = class) + geom_rect(aes(xmin = -5.215 - 0.25, xmax = -5.215 + 0.25, ymin = fixef(pop_lmer_2_raw)[[&quot;(Intercept)&quot;]]- 4.5, ymax = fixef(pop_lmer_2_raw)[[&quot;(Intercept)&quot;]]+ 4.5), fill = &quot;yellow&quot;, alpha = 0.05) + geom_rect(aes(xmin = 0 - 0.25, xmax = 0 + 0.25, ymin = fixef(pop_lmer_2_cen)[[&quot;(Intercept)&quot;]]- 2.5, ymax = fixef(pop_lmer_2_cen)[[&quot;(Intercept)&quot;]]+ 2.5), fill = &quot;yellow&quot;, alpha = 0.05) + geom_abline(data = coef(pop_lmer_2_cen)$class %&gt;% dplyr::rename(Intercept = &quot;(Intercept)&quot;), aes(intercept = Intercept, slope = extravG), color = &quot;gray&quot;, size = .1) + geom_line(size = .2) + geom_abline(intercept = fixef(pop_lmer_2_cen)[[&quot;(Intercept)&quot;]], slope = fixef(pop_lmer_2_cen)[[&quot;extravG&quot;]], color = &quot;hot pink&quot;, size = 2) + theme_bw() + geom_vline(xintercept = -5.215, color = &quot;blue&quot;) + geom_vline(xintercept = 0, color = &quot;red&quot;) + labs(title = &quot;MLM-RIAS: Extroversion = grand-mean centered&quot;, subtitle = &quot;Thin black lines are group-wise predictions, one per class EXTRAPOLATED OUT&quot;, x = &quot;GRAND-MEAN CENTERED Student&#39;s Extroversion, as rated by their teacher&quot;, y = &quot;Predicted\\nStudent&#39;s Populartity, mean rating by their peers&quot;) + coord_cartesian(xlim = c(-5, 5), ylim = c(0, 10)) + scale_x_continuous(breaks = seq(from = -4, to = 4, by = 2)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) 5.4.4 Comapre the Standardized Version data_pop %&gt;% dplyr::mutate(pred = predict(pop_lmer_2_std)) %&gt;% ggplot() + aes(x = extravZ, y = pred, group = class) + geom_rect(aes(xmin = 0 - 0.25, xmax = 0 + 0.25, ymin = fixef(pop_lmer_2_cen)[[&quot;(Intercept)&quot;]]- 2.5, ymax = fixef(pop_lmer_2_cen)[[&quot;(Intercept)&quot;]]+ 2.5), fill = &quot;yellow&quot;, alpha = 0.05) + geom_abline(data = coef(pop_lmer_2_std)$class %&gt;% dplyr::rename(Intercept = &quot;(Intercept)&quot;), aes(intercept = Intercept, slope = extravZ), color = &quot;gray&quot;, size = .1) + geom_line(size = .2) + geom_abline(intercept = fixef(pop_lmer_2_std)[[&quot;(Intercept)&quot;]], slope = fixef(pop_lmer_2_std)[[&quot;extravZ&quot;]], color = &quot;hot pink&quot;, size = 2) + theme_bw() + geom_vline(xintercept = 0, color = &quot;red&quot;) + geom_vline(xintercept = -5.215, color = &quot;blue&quot;) + labs(title = &quot;MLM-RIAS: Extroversion = standardized&quot;, subtitle = &quot;Thin black lines are group-wise predictions, one per class EXTRAPOLATED OUT&quot;, x = &quot;STANDARDIZED Student&#39;s Extroversion, as rated by their teacher&quot;, y = &quot;Predicted\\nStudent&#39;s Populartity, mean rating by their peers&quot;) + coord_cartesian(xlim = c(-5, 5), ylim = c(0, 10)) + scale_x_continuous(breaks = seq(from = -4, to = 4, by = 2)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) "],
["variance-explained.html", "6 Variance Explained 6.1 For Generalized MLMs", " 6 Variance Explained Nakagawa and Schielzeth (2013) https://ecologyforacrowdedplanet.wordpress.com/2013/08/27/r-squared-in-mixed-models-the-easy-way/ 6.1 For Generalized MLMs https://stats.stackexchange.com/questions/111150/calculating-r2-in-mixed-models-using-nakagawa-schielzeths-2013-r2glmm-me I am answering by pasting Douglas Bates’s reply in the R-Sig-ME mailing list, on 17 Dec 2014 on the question of how to calculate an \\(R^2\\) statistic for generalized linear mixed models, which I believe is required reading for anyone interested in such a thing. Bates is the original author of the lme4 package for \\(R\\) and co-author of nlme, as well as co-author of a well-known book on mixed models, and CV will benefit from having the text in an answer, rather than just a link to it. I must admit to getting a little twitchy when people speak of the “\\(R^2\\) for GLMMs”. \\(R^2\\) for a linear model is well-defined and has many desirable properties. For other models one can define different quantities that reflect some but not all of these properties. But this is not calculating an \\(R^2\\) in the sense of obtaining a number having all the properties that the \\(R^2\\) for linear models does. Usually there are several different ways that such a quantity could be defined. Especially for GLMs and GLMMs before you can define “proportion of response variance explained” you first need to define what you mean by “response variance”. The whole point of GLMs and GLMMs is that a simple sum of squares of deviations does not meaningfully reflect the variability in the response because the variance of an individual response depends on its mean. Confusion about what constitutes \\(R^2\\) or degrees of freedom of any of the other quantities associated with linear models as applied to other models comes from confusing the formula with the concept. Although formulas are derived from models the derivation often involves quite sophisticated mathematics. To avoid a potentially confusing derivation and just “cut to the chase” it is easier to present the formulas. But the formula is not the concept. Generalizing a formula is not equivalent to generalizing the concept. And those formulas are almost never used in practice, especially for generalized linear models, analysis of variance and random effects. I have a “meta-theorem” that the only quantity actually calculated according to the formulas given in introductory texts is the sample mean. It may seem that I am being a grumpy old man about this, and perhaps I am, but the danger is that people expect an “\\(R^2\\)-like” quantity to have all the properties of an \\(R^2\\) for linear models. It can’t. There is no way to generalize all the properties to a much more complicated model like a GLMM. I was once on the committee reviewing a thesis proposal for Ph.D. candidacy. The proposal was to examine I think 9 different formulas that could be considered ways of computing an \\(R^2\\) for a nonlinear regression model to decide which one was “best”. Of course, this would be done through a simulation study with only a couple of different models and only a few different sets of parameter values for each. My suggestion that this was an entirely meaningless exercise was not greeted warmly. "],
["sjplot-package.html", "7 sjPlot Package 7.1 Plotting Coefficients 7.2 Plotting Marginal Effects 7.3 Model Diagnostics", " 7 sjPlot Package Daniel Lüdecke is German researcher that has put together several GREAT packages, including sjPlot which we will detail here. Documentation can be found at: http://www.strengejacke.de/sjPlot/index.html library(tidyverse) # all things tidy library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(lme4) # Linear, generalized linear, &amp; nonlinear mixed modelsts models library(haven) # read in SPSS dataset library(sjPlot) # Quick predicitive and diagnostic plots Read the SPSS data in with the haven package and prepare it (see previous chapter). data_raw &lt;- haven::read_sav(&quot;http://www.mlminr.com/data-sets/Achieve.sav?attredirects=0&quot;) data_achieve_center_scale &lt;- data_raw %&gt;% dplyr::mutate_at(vars(id, region, corp, school, class), factor) %&gt;% dplyr::mutate(gender = gender %&gt;% factor(labels = c(&quot;Female&quot;, &quot;Male&quot;))) %&gt;% dplyr::mutate(classize = classize %&gt;% factor(labels = c(&quot;12-17&quot;, &quot;18-21&quot;, &quot;22-26&quot;, &quot;&gt;26&quot;))) %&gt;% dplyr::select(id, region, corp, school, class, # Identifiers gender, age, geread, gevocab, # Pupil-level vars classize, # Class-Level vars senroll, ses) %&gt;% # School-level vars dplyr::mutate(gevocab_c = gevocab - 4.4938) %&gt;% dplyr::mutate(age_c = age - 107.5290) %&gt;% dplyr::mutate(senroll_c = senroll - 533.4148) %&gt;% dplyr::mutate(senroll_ch = senroll_c / 100) %&gt;% # centered AND divided by one hundred dplyr::mutate(ses_t = ses / 10) # JUST divide by ten Fit the final model (see previous chapter) fit_read_11re_s &lt;- lme4::lmer(geread ~ gevocab_c*age_c + gevocab_c*ses_t + # 2 2-way interactions (gevocab_c | school), data = data_achieve_center_scale, REML = TRUE) # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(fit_read_11re_s), custom.model.names = c(&quot;Final&quot;), caption = &quot;MLM: Final Model&quot;, caption.above = TRUE, single.row = TRUE) MLM: Final Model Final (Intercept) 3.94 (0.11)*** gevocab_c 0.67 (0.05)*** age_c -0.00 (0.00) ses_t 0.06 (0.01)*** gevocab_c:age_c 0.01 (0.00)** gevocab_c:ses_t -0.02 (0.01)** AIC 42976.51 BIC 43048.93 Log Likelihood -21478.26 Num. obs. 10320 Num. groups: school 160 Var: school (Intercept) 0.08 Var: school gevocab_c 0.02 Cov: school (Intercept) gevocab_c 0.03 Var: Residual 3.66 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 Now we will show some of the things the sjPlot package can do! 7.1 Plotting Coefficients Select terms that should be plotted. All other term are removed from the output. Note that the term names must match the names of the model’s coefficients. For factors, this means that the variable name is suffixed with the related factor level, and each category counts as one term. E.g. rm.terms = &quot;t_name [2,3]&quot; would remove the terms t_name2 and t_name3 (assuming that the variable t_name is categorical and has at least the factor levels 2 and 3). Another example for the iris-dataset: terms = &quot;Species&quot; would not work, instead you would write terms = &quot;Species [versicolor, virginica]&quot; to remove these two levels, or terms = &quot;Speciesversicolor&quot; if you just want to remove the level versicolor from the plot. 7.1.1 Fixed Effects sjPlot::plot_model(fit_read_11re_s, type = &quot;est&quot;) sjPlot::plot_model(fit_read_11re_s, type = &quot;est&quot;, show.values = TRUE) # Logical, whether values should be plotted or not. sjPlot::plot_model(fit_read_11re_s, type = &quot;std&quot;) Determines in which way estimates are sorted in the plot with the option: sort.est = If NULL (default), no sorting is done and estimates are sorted in the same order as they appear in the model formula. If TRUE, estimates are sorted in descending order, with highest estimate at the top. If sort.est = &quot;sort.all&quot;, estimates are re-sorted for each coefficient (only applies if type = &quot;re&quot; and grid = FALSE), i.e. the estimates of the random effects for each predictor are sorted and plotted to an own plot. If type = &quot;re&quot;, specify a predictor’s / coefficient’s name to sort estimates according to this random effect. sjPlot::plot_model(fit_read_11re_s, type = &quot;std&quot;, sort.est = TRUE) sjPlot::plot_model(fit_read_11re_s, type = &quot;std&quot;, sort.est = TRUE, show.values = TRUE) # Logical, whether values should be plotted or not. Plots standardized beta values, however, standardization follows Gelman (2008) suggestion, rescaling the estimates by dividing them by two standard deviations instead of just one. Resulting coefficients are then directly comparable for untransformed binary predictors. sjPlot::plot_model(fit_read_11re_s, type = &quot;std2&quot;) 7.1.2 Random Effects sjPlot::plot_model(fit_read_11re_s, type = &quot;re&quot;) sjPlot::plot_model(fit_read_11re_s, type = &quot;re&quot;, grid = FALSE, sort.est = TRUE) [[1]] [[2]] 7.2 Plotting Marginal Effects Here terms indicates for which terms marginal effects should be displayed. At least one term is required to calculate effects, maximum length is three terms, where the second and third term indicate the groups, i.e. predictions of first term are grouped by the levels of the second (and third) term. terms may also indicate higher order terms (e.g. interaction terms). Indicating levels in square brackets allows for selecting only specific groups. Term name and levels in brackets must be separated by a whitespace character, e.g. terms = c(&quot;age&quot;, &quot;education [1,3]&quot;). It is also possible to specify a range of numeric values for the predictions with a colon, for instance terms = c(&quot;education [1,3]&quot;, &quot;age [30:50]&quot;). Furthermore, it is possible to specify a function name. Values for predictions will then be transformed, e.g. terms = &quot;income [exp]&quot;. This is useful when model predictors were transformed for fitting the model and should be back-transformed to the original scale for predictions. Finally, using pretty for numeric variables (e.g. terms = &quot;age [pretty]&quot;) calculates a pretty range of values for the term, roughly of proportional length to the term’s value range. For more details, see the documentation for the ggpredict package. 7.2.1 Predicted Values Based on (i.e. is a wrapper for): ggeffects::ggpredict() sjPlot::plot_model(fit_read_11re_s, type = &quot;pred&quot;, terms = c(&quot;gevocab_c&quot;, &quot;ses_t&quot;, &quot;age_c&quot;)) The pred.type = option only applies for Marginal Effects plots with mixed effects models. Indicates whether predicted values should be conditioned on random effects (pred.type = &quot;re&quot;) or fixed effects only (pred.type = &quot;fe&quot;, the default). For details, see documentation of the type-argument in ggpredict() function. sjPlot::plot_model(fit_read_11re_s, type = &quot;pred&quot;, pred.type = &quot;re&quot;, terms = c(&quot;gevocab_c&quot;, &quot;ses_t&quot;, &quot;age_c&quot;)) 7.2.2 Effect Plots Based on (i.e. is a wrapper for): ggeffects::ggeffect() Similar to type = &quot;pred&quot;, however, discrete predictors are held constant at their proportions (not reference level). See the ggeffect package documentation for details. sjPlot::plot_model(fit_read_11re_s, type = &quot;eff&quot;, terms = c(&quot;gevocab_c&quot;, &quot;ses_t&quot;, &quot;age_c&quot;)) 7.2.3 Interaction Plots A shortcut for marginal effects plots, where interaction terms are automatically detected and used as terms-argument. Furthermore, if the moderator variable (the second - and third - term in an interaction) is continuous, type = &quot;int&quot; automatically chooses useful values based on the mdrt.values-argument, which are passed to terms. Then, ggpredict is called. type = &quot;int&quot; plots the interaction term that appears: first in the formula along the x-axis, while the second (and possibly third) variable in an interaction is used as grouping factor(s) (moderating variable). Use type = &quot;pred&quot; or type = &quot;eff&quot; and specify a certain order in the terms-argument to indicate which variable(s) should be used as moderator. sjPlot::plot_model(fit_read_11re_s, type = &quot;int&quot;) [[1]] [[2]] The mdrt.values = option indicates which values of the moderator variable should be used when plotting interaction terms (i.e. type = &quot;int&quot;). minmax (default) minimum and maximum values (lower and upper bounds) of the moderator are used to plot the interaction between independent variable and moderator(s). meansd uses the mean value of the moderator as well as one standard deviation below and above mean value to plot the effect of the moderator on the independent variable (following the convention suggested by Cohen and Cohen and popularized by Aiken and West (1991), i.e. using the mean, the value one standard deviation above, and the value one standard deviation below the mean as values of the moderator, see Grace-Martin K: 3 Tips to Make Interpreting Moderation Effects Easier). zeromax is similar to the minmax option, however, \\(0\\) is always used as minimum value for the moderator. This may be useful for predictors that don’t have an empirical zero-value, but absence of moderation should be simulated by using \\(0\\) as minimum. quart calculates and uses the quartiles (lower, median and upper) of the moderator value. all uses all values of the moderator variable. sjPlot::plot_model(fit_read_11re_s, type = &quot;int&quot;, mdrt.values = &quot;meansd&quot;) [[1]] [[2]] 7.3 Model Diagnostics Note: For mixed models, the diagnostic plots like linear relationship or check for Homoscedasticity, do not take the uncertainty of random effects into account, but is only based on the fixed effects part of the model. 7.3.1 Slope of Coefficentes Slope of coefficients for each single predictor, against the response (linear relationship between each model term and response). sjPlot::plot_model(fit_read_11re_s, type = &quot;slope&quot;) 7.3.2 Residuals Slope of coefficients for each single predictor, against the residuals (linear relationship between each model term and residuals). sjPlot::plot_model(fit_read_11re_s, type = &quot;resid&quot;) 7.3.3 Diagnostics Check model assumptions. sjPlot::plot_model(fit_read_11re_s, type = &quot;diag&quot;) [[1]] [[2]] [[2]]$school [[3]] [[4]] "],
["longitudinal-intro-example-autism.html", "8 Longitudinal Intro: Example - Autism 8.1 Background 8.2 Exploratory Data Analysis 8.3 Model 1: Full model with ‘loaded’ mean structure 8.4 Model 2A: Drop Random Intercepts 8.5 Model 2B: Drop Random Quadratic Slope 8.6 Model 3: Drop Quadratic Time Fixed Effect 8.7 Final Model", " 8 Longitudinal Intro: Example - Autism library(tidyverse) # all things tidy library(pander) # nice looking genderal tabulations library(furniture) # nice table1() descriptives library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(psych) # contains some useful functions, like headTail library(sjstats) # ICC calculations library(sjPlot) # Quick predicitive and diagnostic plots library(effects) # Estimated Marginal Means library(VIM) # Visualization and Imputation of Missing Values library(naniar) # Summaries and Visualisations for Missing Data library(lme4) # Linear, generalized linear, &amp; nonlinear mixed models library(HLMdiag) # package with the dataset 8.1 Background The source: http://www-personal.umich.edu/~kwelch/ This data was collected by researchers at the University of Michigan (Anderson et al. 2007, Anderson et al. (2009)) as part of a prospective longitudinal study of 214 children. The children were divided into three diagnostic groups (bestest2) when they were 2 years old: Autism (autism), Pervasive Developmental Disorder (pdd), and non-spectrum children (none in this sample). The study was designed to collect information on each child at approximately 2, 3, 5, 9, and 13 years of age, although not all children were measured for each age. One of the study objectives was to assess the relative influence of the initial diagnostic category, language proficiency at age 2, and other covariates on the developmental trajectories of the socialization (vsae) of these children. Study participants were children who had had consecutive referrals to one of two autism clinics before the age of 3 years. Social development was assessed at each age using the Vineland Adaptive Behavior Interview survey form, a parent-reported measure of socialization. VSAE (Vineland Socialization Age Equivalent), was a combined score that included assessments of interpersonal relationships, play/leisure time activities, and coping skills. Initial language development was assessed using the Sequenced Inventory of Communication Development (SICD) scale; children were placed into one of three groups (sicdegp) based on their initial SICD scores on the expressive language subscale at age 2. childid Child’s identification number for this study sicdegp Sequenced Inventory of Communication Development group (an assessment of expressive language development) - a factor. Levels are low, med, and high age2 Age (in years) centered around age 2 (age at diagnosis) vsae Vineland Socialization Age Equivalent, Parent-reported measure of socialization, including measures of: interpersonal relationships play/leisure time activities coping skills gender Child’s gender - a factor. Levels are male and female race Child’s race - a factor. Levels are white and nonwhite bestest2 Diagnosis at age 2 - a factor. Levels are autism and pdd (pervasive developmental disorder) data(autism, package = &quot;HLMdiag&quot;) # make the dataset &#39;active&#39; from this package tibble::glimpse(autism) # first look at the dataset and its varaibles Observations: 604 Variables: 7 $ childid &lt;int&gt; 1, 1, 1, 1, 1, 10, 10, 10, 10, 100, 100, 100, 100, 10... $ sicdegp &lt;fct&gt; high, high, high, high, high, low, low, low, low, hig... $ age2 &lt;dbl&gt; 0, 1, 3, 7, 11, 0, 1, 7, 11, 0, 1, 3, 7, 0, 1, 7, 11,... $ vsae &lt;int&gt; 6, 7, 18, 25, 27, 9, 11, 18, 39, 15, 24, 37, 135, 8, ... $ gender &lt;fct&gt; male, male, male, male, male, male, male, male, male,... $ race &lt;fct&gt; white, white, white, white, white, white, white, whit... $ bestest2 &lt;fct&gt; pdd, pdd, pdd, pdd, pdd, autism, autism, autism, auti... 8.1.1 Long Format data_long &lt;- autism %&gt;% # save the dataset as a new name dplyr::mutate(childid = childid %&gt;% factor) %&gt;% # declare grouping var a factor dplyr::mutate(age = 2 + age2) %&gt;% # create the original age variable (unequally spaced) dplyr::mutate(obs = age %&gt;% factor %&gt;% as.numeric) %&gt;% # Observation Number = 1, 2, 3, 4, 5 (equally spaced) dplyr::select(childid, # choose variables and order to keep gender, race, bestest2, sicdegp, obs, age, age2, vsae) %&gt;% dplyr::arrange(childid, age2) # sort observations data_long %&gt;% psych::headTail(top = 11, bottom = 6) childid gender race bestest2 sicdegp obs age age2 vsae 1 1 male white pdd high 1 2 0 6 2 1 male white pdd high 2 3 1 7 3 1 male white pdd high 3 5 3 18 4 1 male white pdd high 4 9 7 25 5 1 male white pdd high 5 13 11 27 6 2 male white autism low 1 2 0 6 7 2 male white autism low 2 3 1 7 8 2 male white autism low 3 5 3 7 9 2 male white autism low 4 9 7 8 10 2 male white autism low 5 13 11 14 11 3 male nonwhite pdd high 1 2 0 17 ... &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ... ... ... ... 599 211 male nonwhite autism high 1 2 0 15 600 212 male white autism med 1 2 0 7 601 212 male white autism med 2 3 1 21 602 212 male white autism med 3 5 3 29 603 212 male white autism med 4 9 7 72 604 212 male white autism med 5 13 11 147 8.1.2 Wide Format data_wide &lt;- data_long %&gt;% # save the dataset as a new name dplyr::select(-age, -obs) %&gt;% # delete (by deselecting) this variable tidyr::spread(key = age2, # name the varaible that specifies how to spread value = vsae, # name the variable to be spread out sep = &quot;_&quot;) %&gt;% # add to combine the key-variables name before its value dplyr::arrange(childid) # sort observations Notice the missing values, displayed as NA. data_wide %&gt;% psych::headTail() childid gender race bestest2 sicdegp age2_0 age2_1 age2_3 age2_7 1 1 male white pdd high 6 7 18 25 2 2 male white autism low 6 7 7 8 3 3 male nonwhite pdd high 17 18 12 18 4 4 male nonwhite autism high 12 14 38 114 ... &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ... ... ... ... 152 209 male white autism med 2 4 &lt;NA&gt; 12 153 210 male white autism low 4 25 &lt;NA&gt; 130 154 211 male nonwhite autism high 15 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 155 212 male white autism med 7 21 29 72 age2_11 1 27 2 14 3 24 4 &lt;NA&gt; ... ... 152 32 153 &lt;NA&gt; 154 &lt;NA&gt; 155 147 8.2 Exploratory Data Analysis 8.2.1 Demographic Summary 8.2.1.1 Using the WIDE formatted dataset Each person’s data is only stored on a single line # Knit to Website: output = &quot;html&quot; # Knit to PDF: output = &quot;latex2&quot; # View on Screen: output = &quot;&quot;text&quot;, or &quot;markdown&quot;, &quot;html&quot; data_wide %&gt;% furniture::table1(bestest2, gender, race, # variables to summarize splitby = ~ sicdegp, # how split into columns test = TRUE, # compare the groups output = &quot;html&quot;) # for website low med high P-Value n = 49 n = 65 n = 41 bestest2 0.018 autism 38 (77.6%) 42 (64.6%) 20 (48.8%) pdd 11 (22.4%) 23 (35.4%) 21 (51.2%) gender 0.706 male 44 (89.8%) 55 (84.6%) 35 (85.4%) female 5 (10.2%) 10 (15.4%) 6 (14.6%) race 0.944 white 31 (63.3%) 43 (66.2%) 27 (65.9%) nonwhite 18 (36.7%) 22 (33.8%) 14 (34.1%) 8.2.1.2 Using the LONG formatted dataset Each person’s data is stored on multiple lines, one for each time point. To ensure the summary table is correct, you must choose a single time point per person. # Knit to Website: output = &quot;html&quot; # Knit to PDF: output = &quot;latex2&quot; # View on Screen: output = &quot;&quot;text&quot;, or &quot;markdown&quot;, &quot;html&quot; data_long %&gt;% dplyr::filter(age == 2) %&gt;% # restrict to one-line per person furniture::table1(bestest2, gender, race, # variables to summarize splitby = ~ sicdegp, # how split into columns test = TRUE, # compare the groups output = &quot;html&quot;) # for website low med high P-Value n = 49 n = 65 n = 40 bestest2 0.025 autism 38 (77.6%) 42 (64.6%) 20 (50%) pdd 11 (22.4%) 23 (35.4%) 20 (50%) gender 0.714 male 44 (89.8%) 55 (84.6%) 35 (87.5%) female 5 (10.2%) 10 (15.4%) 5 (12.5%) race 0.95 white 31 (63.3%) 43 (66.2%) 26 (65%) nonwhite 18 (36.7%) 22 (33.8%) 14 (35%) 8.2.2 Baseline Summary 8.2.2.1 Using the LONG formatted dataset Each person’s data is stored on multiple lines, one for each time point. To ensure the summary table is correct, you must choose a single time point per person. # Knit to Website: output = &quot;html&quot; # Knit to PDF: output = &quot;latex2&quot; # View on Screen: output = &quot;&quot;text&quot;, or &quot;markdown&quot;, &quot;html&quot; data_long %&gt;% dplyr::filter(age == 2) %&gt;% furniture::table1(vsae, splitby = ~ sicdegp, test = TRUE, output = &quot;html&quot;) low med high P-Value n = 49 n = 65 n = 40 vsae &lt;.001 7.1 (2.7) 8.7 (3.5) 12.4 (3.4) 8.2.3 Missing Data &amp; Attrition 8.2.3.1 VIM package Plot the amount of missing vlaues and the amount of each patter of missing values. data_wide %&gt;% VIM::aggr(numbers = TRUE, # shows the number to the far right prop = FALSE) # shows counts instead of proportions 8.2.3.2 naniar package data_wide %&gt;% naniar::vis_miss() data_wide %&gt;% naniar::gg_miss_var() data_wide %&gt;% naniar::gg_miss_var(show_pct = TRUE, # x-axis is PERCENT, not count facet = sicdegp) + # create seperate panels theme_bw() # add ggplot layers as normal data_wide %&gt;% naniar::gg_miss_upset() data_wide %&gt;% naniar::gg_miss_upset(sets = c(&quot;age2_0_NA&quot;, &quot;age2_1_NA&quot;, &quot;age2_3_NA&quot;, &quot;age2_7_NA&quot;, &quot;age2_11_NA&quot;), keep.order = TRUE) 8.2.4 Means Across Time 8.2.4.1 Using the WIDE formatted dataset Default = COMPLETE CASES ONLY!!! Note - the sample size at each time point is the same, but subjects are only included if they have data at every time point # Knit to Website: output = &quot;html&quot; # Knit to PDF: output = &quot;latex2&quot; # View on Screen: output = &quot;&quot;text&quot;, or &quot;markdown&quot;, &quot;html&quot; data_wide %&gt;% furniture::table1(age2_0, age2_1, age2_3, age2_7, age2_11, splitby = ~ sicdegp, test = TRUE, na.rm = TRUE, # default: COMPLETE CASES ONLY!!!!! output = &quot;html&quot;) low med high P-Value n = 10 n = 17 n = 14 age2_0 0.003 6.6 (2.9) 9.5 (4.1) 12.5 (4.1) age2_1 &lt;.001 8.0 (2.1) 14.3 (6.1) 19.5 (6.1) age2_3 &lt;.001 12.5 (5.5) 20.2 (8.6) 33.9 (18.5) age2_7 &lt;.001 12.3 (8.1) 34.4 (22.5) 56.9 (23.5) age2_11 0.009 22.4 (24.4) 57.8 (50.3) 80.9 (44.4) Specify All data: note - that the smaple sizes will be different for each time point # Knit to Website: output = &quot;html&quot; # Knit to PDF: output = &quot;latex2&quot; # View on Screen: output = &quot;&quot;text&quot;, or &quot;markdown&quot;, &quot;html&quot; data_wide %&gt;% furniture::table1(age2_0, age2_1, age2_3, age2_7, age2_11, splitby = ~ sicdegp, test = TRUE, na.rm = FALSE, # INCLUDES ALL DATA !!!! output = &quot;html&quot;) low med high P-Value n = 49 n = 65 n = 41 age2_0 &lt;.001 7.1 (2.7) 8.7 (3.5) 12.4 (3.4) age2_1 &lt;.001 12.0 (6.3) 13.7 (5.4) 21.2 (9.4) age2_3 &lt;.001 15.0 (7.9) 17.7 (8.0) 33.9 (15.8) age2_7 &lt;.001 25.6 (28.4) 32.1 (23.4) 64.1 (34.6) age2_11 &lt;.001 37.1 (35.5) 56.2 (47.9) 88.7 (46.3) 8.2.4.2 Using the LONG formatted dataset Each person’s data is stored on multiple lines, one for each time point. FOR ALL DATA! data_sum_all &lt;- data_long %&gt;% dplyr::group_by(sicdegp, age2) %&gt;% # specify the groups dplyr::summarise(vsae_n = n(), # count of valid scores vsae_mean = mean(vsae), # mean score vsae_sd = sd(vsae), # standard deviation of scores vsae_sem = vsae_sd / sqrt(vsae_n)) # stadard error for the mean of scores data_sum_all # A tibble: 15 x 6 # Groups: sicdegp [?] sicdegp age2 vsae_n vsae_mean vsae_sd vsae_sem &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 low 0 49 7.06 2.73 0.389 2 low 1 46 12.0 6.33 0.934 3 low 3 29 15.0 7.92 1.47 4 low 7 36 25.6 28.4 4.74 5 low 11 28 37.1 35.5 6.72 6 med 0 65 8.74 3.51 0.436 7 med 1 62 13.7 5.42 0.688 8 med 3 36 17.7 8.00 1.33 9 med 7 48 32.1 23.4 3.38 10 med 11 40 56.2 47.9 7.58 11 high 0 40 12.4 3.43 0.542 12 high 1 38 21.2 9.38 1.52 13 high 3 26 33.9 15.8 3.09 14 high 7 35 64.1 34.6 5.85 15 high 11 26 88.7 46.3 9.09 FOR COMPLETE CASES ONLY!!! data_sum_cc &lt;- data_long %&gt;% dplyr::group_by(childid) %&gt;% # group-by child dplyr::mutate(child_vsae_n = n()) %&gt;% # count the number of valid VSAE scores dplyr::filter(child_vsae_n == 5) %&gt;% # restrict to only thoes children with 5 valid scores dplyr::group_by(sicdegp, age2) %&gt;% # specify the groups dplyr::summarise(vsae_n = n(), # count of valid scores vsae_mean = mean(vsae), # mean score vsae_sd = sd(vsae), # standard deviation of scores vsae_sem = vsae_sd / sqrt(vsae_n)) # stadard error for the mean of scores data_sum_cc # A tibble: 15 x 6 # Groups: sicdegp [?] sicdegp age2 vsae_n vsae_mean vsae_sd vsae_sem &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 low 0 10 6.6 2.88 0.909 2 low 1 10 8 2.11 0.667 3 low 3 10 12.5 5.54 1.75 4 low 7 10 12.3 8.07 2.55 5 low 11 10 22.4 24.4 7.73 6 med 0 17 9.47 4.12 1.00 7 med 1 17 14.3 6.08 1.47 8 med 3 17 20.2 8.57 2.08 9 med 7 17 34.4 22.5 5.47 10 med 11 17 57.8 50.3 12.2 11 high 0 14 12.5 4.09 1.09 12 high 1 14 19.5 6.15 1.64 13 high 3 14 33.9 18.5 4.94 14 high 7 14 56.9 23.5 6.29 15 high 11 14 80.9 44.4 11.9 8.2.5 Person Profile Plots Use the data in LONG format 8.2.5.1 Unequally Spaced data_long %&gt;% dplyr::mutate(sicdegp = fct_recode(sicdegp, &quot;Low Communication&quot; = &quot;low&quot;, &quot;Medium Communication&quot; = &quot;med&quot;, &quot;High Communication&quot; = &quot;high&quot;)) %&gt;% ggplot(aes(x = age, y = vsae)) + geom_point(size = 0.75) + geom_line(aes(group = childid), alpha = .5, size = 1) + facet_grid(. ~ sicdegp) + theme_bw() + scale_x_continuous(breaks = c(2, 3, 5, 9, 13)) + labs(x = &quot;Age of Child, Years&quot;, y = &quot;Vineland Socialization Age Equivalent&quot;, color = &quot;Sequenced Inventory of Communication Development&quot;) + geom_smooth(aes(color = &quot;Flexible&quot;), method = &quot;loess&quot;, se = FALSE,) + geom_smooth(aes(color = &quot;Linear&quot;), method = &quot;lm&quot;, se = FALSE) + scale_color_manual(name = &quot;Smoother Type: &quot;, values = c(&quot;Flexible&quot; = &quot;blue&quot;, &quot;Linear&quot; = &quot;red&quot;)) + theme(legend.position = &quot;bottom&quot;, legend.key.width = unit(2, &quot;cm&quot;)) 8.2.5.2 Equally Spaced data_long %&gt;% dplyr::mutate(sicdegp = fct_recode(sicdegp, &quot;Low Communication&quot; = &quot;low&quot;, &quot;Medium Communication&quot; = &quot;med&quot;, &quot;High Communication&quot; = &quot;high&quot;)) %&gt;% ggplot(aes(x = obs, y = vsae)) + geom_point(size = 0.75) + geom_line(aes(group = childid), alpha = .5, size = 1) + facet_grid(. ~ sicdegp) + theme_bw() + labs(x = &quot;Observation Number&quot;, y = &quot;Vineland Socialization Age Equivalent&quot;, color = &quot;Sequenced Inventory of Communication Development&quot;) + geom_smooth(aes(color = &quot;Flexible&quot;), method = &quot;loess&quot;, se = FALSE,) + geom_smooth(aes(color = &quot;Linear&quot;), method = &quot;lm&quot;, se = FALSE) + scale_color_manual(name = &quot;Smoother Type: &quot;, values = c(&quot;Flexible&quot; = &quot;blue&quot;, &quot;Linear&quot; = &quot;red&quot;)) + theme(legend.position = &quot;bottom&quot;, legend.key.width = unit(2, &quot;cm&quot;)) 8.2.6 Side-by-side Boxplots data_long %&gt;% ggplot(aes(x = sicdegp, y = vsae, fill = sicdegp)) + geom_boxplot() + theme_bw() + facet_grid(. ~ age, labeller = &quot;label_both&quot;) + theme(legend.position = &quot;top&quot;) 8.2.7 Means Plots 8.2.7.1 Default stat_summary It is nice that the stat_summary() layer computes the standard error for the mean for you using the data in LONG format data_long %&gt;% ggplot(aes(x = age, y = vsae, color = sicdegp)) + stat_summary() + # default: points at MEAN and extend vertically 1 standard error for the mean stat_summary(fun.y = &quot;mean&quot;, # plot the means geom = &quot;line&quot;) + # ...and connect with lines theme_bw() + scale_x_continuous(breaks = c(2, 3, 5, 9, 13)) + theme(legend.position = c(0, 1), legend.justification = c(-0.25, 1.25), legend.background = element_rect(color = &quot;black&quot;)) data_long %&gt;% ggplot(aes(x = obs, y = vsae, color = sicdegp)) + stat_summary() + # default: points at MEAN and extend vertically 1 standard error for the mean stat_summary(fun.y = &quot;mean&quot;, # plot the means geom = &quot;line&quot;) + # ...and connect with lines theme_bw() + theme(legend.position = c(0, 1), legend.justification = c(-0.25, 1.25), legend.background = element_rect(color = &quot;black&quot;)) 8.2.7.2 Manually Summarized data_sum_all %&gt;% dplyr::mutate(age = age2 + 2) %&gt;% ggplot() + aes(x = age, y = vsae_mean, color = sicdegp) + geom_errorbar(aes(ymin = vsae_mean - vsae_sem, # mean +/- one SE for the mean ymax = vsae_mean + vsae_sem), width = .25) + geom_point(aes(shape = sicdegp), size = 3) + geom_line(aes(group = sicdegp)) + theme_bw() + scale_x_continuous(breaks = c(2, 3, 5, 9, 13)) + labs(x = &quot;Age of Child, Years&quot;, y = &quot;Vineland Socialization Age Equivalent&quot;, color = &quot;Sequenced Inventory of Communication Development:&quot;, shape = &quot;Sequenced Inventory of Communication Development:&quot;, linetype = &quot;Sequenced Inventory of Communication Development:&quot;) + theme(legend.position = &quot;bottom&quot;, legend.key.width = unit(2, &quot;cm&quot;)) 8.3 Model 1: Full model with ‘loaded’ mean structure Take top-down approach: Quadratic regression model, describing vsae as a function of age2 Each child has a unique parabolic trajectory over time, with coefficients that vary randomly around fixed-effects defining a mean growth curve for each SICD group. Since there is no age = 0 in our data, we will use the age2 variables, which is age -2, so that intercept has meaning (mean at baseline age). Fixed-effects age2 age I(age2^2) quadratic age or age-squared, the I() denotes an internal calculated variable sicdegp SICD group (reference group = low) SICD group x age/age-squared interactions Random-effects intercep age and age-squared 8.3.1 Fit the Model fit_lmer_1_re &lt;- lmer(vsae ~ age2*sicdegp + I(age2^2)*sicdegp + (1 + age2 + I(age2^2) | childid), data = data_long, REML = TRUE) 8.3.2 Table of Prameter Estimates # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(fit_lmer_1_re), caption = &quot;MLM: Full Model&quot;, caption.above = TRUE, single.row = TRUE) MLM: Full Model Model 1 (Intercept) 8.40 (0.75)*** age2 2.28 (0.74)** sicdegpmed 1.26 (0.99) sicdegphigh 5.39 (1.11)*** I(age2^2) 0.07 (0.08) age2:sicdegpmed 0.43 (0.98) age2:sicdegphigh 3.31 (1.08)** sicdegpmed:I(age2^2) -0.00 (0.11) sicdegphigh:I(age2^2) 0.14 (0.12) AIC 4586.50 BIC 4656.96 Log Likelihood -2277.25 Num. obs. 604 Num. groups: childid 155 Var: childid (Intercept) 1.27 Var: childid age2 14.00 Var: childid I(age2^2) 0.16 Cov: childid (Intercept) age2 -0.13 Cov: childid (Intercept) I(age2^2) 0.41 Cov: childid age2 I(age2^2) -0.61 Var: Residual 37.20 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 8.3.3 Plot of the Estimated Marginal Means Note: the \\(x-axis\\) is the age2 (age - 2) variable, so it represents time since age 2 (or baseline) sjPlot::plot_model(fit_lmer_1_re, type = &quot;pred&quot;, terms = c(&quot;age2&quot;, &quot;sicdegp&quot;), title = &quot;Model 1: Loaded Means Structure&quot;) 8.4 Model 2A: Drop Random Intercepts Note: There seems to be relatively little variation in baseline measurements of VSAE across individuals in the same SICD group, so the variation at age 2 can be attributed to random error, rather than between-subject variation. This indicates we may want to try removing the random intercepts, while retaining the same fixed- and other random-effects. This new model implies that children have common initial VSAE value at age 2, given their SICD group. 8.4.1 Fit the Model fit_lmer_2a_re &lt;- lmer(vsae ~ age2*sicdegp + I(age2^2)*sicdegp + (-1 + age2 + I(age2^2) | childid), data = data_long, REML = TRUE) 8.4.2 Assess the Signifcance anova(fit_lmer_1_re, fit_lmer_2a_re, refit = FALSE) Data: data_long Models: fit_lmer_2a_re: vsae ~ age2 * sicdegp + I(age2^2) * sicdegp + (-1 + age2 + I(age2^2) | fit_lmer_2a_re: childid) fit_lmer_1_re: vsae ~ age2 * sicdegp + I(age2^2) * sicdegp + (1 + age2 + I(age2^2) | fit_lmer_1_re: childid) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) fit_lmer_2a_re 13 4587.0 4644.2 -2280.5 4561.0 fit_lmer_1_re 16 4586.5 4657.0 -2277.2 4554.5 6.4643 3 0.09108 fit_lmer_2a_re fit_lmer_1_re . --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The more complicated model (including random intercepts) does NOT fit better, thus the random intercepts may be removed from the model. Model 2a is bettern than Model 1 8.5 Model 2B: Drop Random Quadratic Slope We should formally test the necessity of quadratic age random-effect. Comparison of nested models with REML-based LRT using a 50:50 mixture χ2-distribution with 1 and 2 df Difference of 2 covariance parameters 8.5.1 Fit the Model fit_lmer_2b_re &lt;- lmer(vsae ~ age2*sicdegp + I(age2^2)*sicdegp + (-1 + age2 | childid), REML = TRUE, data = data_long) 8.5.2 Assess the Signifcance anova(fit_lmer_2a_re, fit_lmer_2b_re, refit = FALSE) Data: data_long Models: fit_lmer_2b_re: vsae ~ age2 * sicdegp + I(age2^2) * sicdegp + (-1 + age2 | childid) fit_lmer_2a_re: vsae ~ age2 * sicdegp + I(age2^2) * sicdegp + (-1 + age2 + I(age2^2) | fit_lmer_2a_re: childid) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) fit_lmer_2b_re 11 4669.3 4717.7 -2323.7 4647.3 fit_lmer_2a_re 13 4587.0 4644.2 -2280.5 4561.0 86.34 2 &lt; 2.2e-16 fit_lmer_2b_re fit_lmer_2a_re *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The more complicated model (including random intercepts) DOES fit better, thus the random sopes for both the linear AND the quadratic effect of age should be retained in the model. Model 2a is better than model 2b 8.6 Model 3: Drop Quadratic Time Fixed Effect Fit the previous ‘best’ model via ML, not REML to compare nested model that differe in terms of fixed effects only 8.6.1 Fit the Models fit_lmer_2a_ml &lt;- lmer(vsae ~ age2*sicdegp + I(age2^2)*sicdegp + (-1 + age2 + I(age2^2) | childid), data = data_long, REML = FALSE) fit_lmer_3_ml &lt;- lmer(vsae ~ age2*sicdegp + (-1 + age2 + I(age2^2) | childid), data = data_long, REML = FALSE) 8.6.2 Assess the Signifcance anova(fit_lmer_2a_ml, fit_lmer_3_ml) Data: data_long Models: fit_lmer_3_ml: vsae ~ age2 * sicdegp + (-1 + age2 + I(age2^2) | childid) fit_lmer_2a_ml: vsae ~ age2 * sicdegp + I(age2^2) * sicdegp + (-1 + age2 + I(age2^2) | fit_lmer_2a_ml: childid) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) fit_lmer_3_ml 10 4584.4 4628.5 -2282.2 4564.4 fit_lmer_2a_ml 13 4582.1 4639.3 -2278.0 4556.1 8.3704 3 0.03895 fit_lmer_3_ml fit_lmer_2a_ml * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The more complicated model (including fixed interaction between quadratic time and SICD group) DOES fit better, thus the higher level interaction should be retained in the model. Model 2a is better than model 3. 8.7 Final Model 8.7.1 Table of Parameter Esitmates # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(fit_lmer_2a_re), caption = &quot;MLM: Final Model&quot;, caption.above = TRUE, single.row = TRUE, digits = 4) MLM: Final Model Model 1 (Intercept) 8.4085 (0.7370)*** age2 2.2694 (0.7399)** sicdegpmed 1.2644 (0.9741) sicdegphigh 5.3646 (1.0907)*** I(age2^2) 0.0721 (0.0790) age2:sicdegpmed 0.4289 (0.9808) age2:sicdegphigh 3.3259 (1.0760)** sicdegpmed:I(age2^2) 0.0007 (0.1038) sicdegphigh:I(age2^2) 0.1335 (0.1138) AIC 4586.9689 BIC 4644.2154 Log Likelihood -2280.4845 Num. obs. 604 Num. groups: childid 155 Var: childid age2 13.9919 Var: childid I(age2^2) 0.1338 Cov: childid age2 I(age2^2) -0.4436 Var: Residual 37.9867 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 8.7.2 Interpretation of Fixed Effects 8.7.2.1 Reference Group: low SICD group \\(\\gamma_{0}\\) = 8.408 is the estimated marginal mean VSAE score for children in the low SICD, at 2 years of age \\(\\gamma_{a}\\) = 2.269 and \\(\\gamma_{a^2}\\) = 0.072 are the fixed effects for age and age-squared on VSAE for children in the low SICD group (change over time) Thus the equation for the estimated marginal mean VASE trajectory for the low SICD group is: \\[ \\begin{align*} VASE =&amp; \\gamma_{0} + \\gamma_{a} (AGE - 2) + \\gamma_{a^2} (AGE - 2)^2 \\\\ =&amp; 8.408 + 2.269 (AGE - 2) + 0.072 (AGE - 2)^2 \\\\ \\end{align*} \\] 8.7.2.2 First Comparison Group: medium SICD group \\(\\gamma_{med}\\) = 1.264 is the DIFFERENCE in the estimated marginal mean VSAE score for children in the medium vs. the low SICD, at 2 years of age \\(\\gamma_{med:\\;a}\\) = 0.429 and \\(\\gamma_{med:\\;a^2}\\) = 0.001 are the DIFFERENCE in the fixed effects for age and age-squared on VSAE for children in the medium vs. the low SICD group Thus the equation for the estimated marginal mean VASE trajectory for the medium SICD group is: \\[ \\begin{align*} VASE =&amp; (\\gamma_{0} + \\gamma_{med}) + (\\gamma_{a} + \\gamma_{med:\\;a}) (AGE - 2) + (\\gamma_{a^2} + \\gamma_{med:\\;a^2})(AGE - 2)^2 \\\\ =&amp; (8.408 + 1.264) + (2.269 + 0.429) (AGE - 2) + (0.072 + 0.001)(AGE - 2)^2 \\\\ =&amp; 9.673 + 2.698 (AGE - 2) + 0.073 (AGE - 2)^2 \\\\ \\end{align*} \\] 8.7.2.3 Second Comparison Group: high SICD group \\(\\gamma_{hi}\\) = 5.365 is the DIFFERENCE in the estimated marginal mean VSAE score for children in the high vs. the low SICD, at 2 years of age \\(\\gamma_{hi:\\;a}\\) = 3.326 and \\(\\gamma_{hi:\\;a^2}\\) = 0.133 are the DIFFERENCE in the fixed effects for age and age-squared on VSAE for children in the high vs. the low SICD group Thus the equation for the estimated marginal mean VASE trajectory for the high SICD group is: \\[ \\begin{align*} VASE =&amp; (\\gamma_{0} + \\gamma_{hi}) + (\\gamma_{a} + \\gamma_{hi:\\;a}) (AGE - 2) + (\\gamma_{a^2} + \\gamma_{hi:\\;a^2})(AGE - 2)^2 \\\\ =&amp; (8.408 + 5.365) + (2.269 + 3.326) (AGE - 2) + (0.072 + 0.133)(AGE - 2)^2 \\\\ =&amp; 13.773 + 5.595 (AGE - 2) + 0.206 (AGE - 2)^2 \\\\ \\end{align*} \\] 8.7.3 Interpretation of Random Effects lme4::VarCorr(fit_lmer_2a_re)%&gt;% print(comp = c(&quot;Variance&quot;, &quot;Std.Dev&quot;), digits = 3) Groups Name Variance Std.Dev. Corr childid age2 13.992 3.741 I(age2^2) 0.134 0.366 -0.32 Residual 37.987 6.163 Here a group of observations = a CHILD 8.7.3.1 Residual Varaince Within-child-variance \\(e_{ti}\\) the residuals associated with observation at time \\(t\\) on child \\(i\\) \\[ \\sigma^2 = \\sigma^2_e = VAR[e_{ti}] = 37.987 \\] 8.7.3.2 2 Variance Components Between-children slope variances Random LINEAR effect of age variance \\(u_{1i}\\) the DIFFERENCE between child \\(i\\)’s specific linear component for age and the fixed linear component for age, given their SICD group \\[ \\tau_{11} = \\sigma^2_{u1} = VAR[u_{1i}] = 13.99 \\] Random QUADRATIC effect of age variance \\(u_{2i}\\) the DIFFERENCE between child \\(i\\)’s specific quadratic component for age and the fixed quadratic component for age, given their SICD group \\[ \\tau_{22} = \\sigma^2_{u2} = VAR[u_{2i}] = 0.13 \\] 8.7.3.3 1 Covariance (or correlation) Components Slope-slope covariance Random LINEAR and Quadratic effect of age covariance: correlation = -0.32 \\[ \\tau_{12} = \\sigma^2_{u12} = COV[u_{1i}, u_{2i}] = -0.44 \\] 8.7.4 Assumption Checking The residuals are: Assumed to be normally, independently, and identically distributed (conditional on other random-effects) Assumed independent of random-effects \\[ e_{ti} \\sim N(0, \\sigma^2) \\] sjPlot::plot_model(fit_lmer_2a_re, type = &quot;diag&quot;) [[1]] [[2]] [[2]]$childid [[3]] [[4]] 8.7.5 Plot of the Estimated Marginal Means 8.7.5.1 Quick and Default Note: the \\(x-axis\\) is the age2 (age - 2) variable, so it represents time since age 2 (or baseline) sjPlot::plot_model(fit_lmer_2a_re, type = &quot;pred&quot;, # estimated marginal means (no random effects) terms = c(&quot;age2&quot;, &quot;sicdegp&quot;), # 1st = x-axiz, 2nd = seperate lines title = &quot;Final Model (2a)&quot;) # Optional title 8.7.5.2 More Customized - Color This version would look better on a poster or in a slide presentation. effects::Effect(focal.predictors = c(&quot;age2&quot;,&quot;sicdegp&quot;), # variables involved in interactions mod = fit_lmer_2a_re, xlevels = list(age2 = seq(from = 0, to = 11, by = .1))) %&gt;% # add more values to smooth out the prediction lines and ribbons data.frame() %&gt;% dplyr::mutate(age = age2 + 2) %&gt;% # trandform back to standard years dplyr::mutate(sicdegp = factor(sicdegp, levels = c(&quot;high&quot;, &quot;med&quot;, &quot;low&quot;), labels = c(&quot;High&quot;, &quot;Medium&quot;, &quot;Low&quot;))) %&gt;% ggplot(aes(x = age, y = fit, group = sicdegp)) + geom_ribbon(aes(ymin = lower, # 95% Confidence Intervals ymax = upper, fill = sicdegp), alpha = .3) + geom_line(aes(linetype = sicdegp, color = sicdegp), size = 1) + scale_x_continuous(breaks = c(2, 3, 5, 9, 13)) + # mark values that were actually measured scale_y_continuous(breaks = seq(from = 0, to = 120, by = 20)) + scale_linetype_manual(values = c(&quot;solid&quot;, &quot;longdash&quot;, &quot;dotted&quot;)) + theme_bw() + theme(legend.position = c(.2, .8), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(2.5, &quot;cm&quot;)) + labs(x = &quot;Age, in years&quot;, y = &quot;Estimated Marginal Mean\\nVineland Socialization Age Equivalent&quot;, linetype = &quot;Communication&quot;, fill = &quot;Communication&quot;, color = &quot;Communication&quot;) 8.7.5.3 More Customized - Black and White This version would be better for a publication. effects::Effect(focal.predictors = c(&quot;age2&quot;,&quot;sicdegp&quot;), mod = fit_lmer_2a_re, xlevels = list(age2 = seq(from = 0, to = 11, by = .1))) %&gt;% data.frame() %&gt;% dplyr::mutate(age = age2 + 2) %&gt;% dplyr::mutate(sicdegp = factor(sicdegp, levels = c(&quot;high&quot;, &quot;med&quot;, &quot;low&quot;), labels = c(&quot;High&quot;, &quot;Medium&quot;, &quot;Low&quot;))) %&gt;% ggplot(aes(x = age, y = fit, group = sicdegp)) + geom_ribbon(aes(ymin = lower, ymax = upper, fill = sicdegp), alpha = .4) + geom_line(aes(linetype = sicdegp), size = .7) + scale_x_continuous(breaks = c(2, 3, 5, 9, 13)) + scale_y_continuous(breaks = seq(from = 0, to = 120, by = 20)) + scale_linetype_manual(values = c(&quot;solid&quot;, &quot;longdash&quot;, &quot;dotted&quot;)) + scale_fill_manual(values = c(&quot;gray10&quot;, &quot;gray40&quot;, &quot;gray60&quot;)) + theme_bw() + theme(legend.position = c(.15, .8), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(2, &quot;cm&quot;)) + labs(x = &quot;Age, in years&quot;, y = &quot;Estimated Marginal Mean\\nVineland Socialization Age Equivalent&quot;, linetype = &quot;Communication&quot;, fill = &quot;Communication&quot;) 8.7.6 Blups vs. Fixed Effects BLUP = Best Linear Unbiased Predictor A BLUP is the specific prediction for an individual supject, showin by black lines below. This includes the fixed effects as well as the specific random effects for a given individual. Comparatively, the blue lines below display the predictions for fixed effects only. data_long %&gt;% dplyr::mutate(sicdegp = fct_recode(sicdegp, &quot;Low Communication&quot; = &quot;low&quot;, &quot;Medium Communication&quot; = &quot;med&quot;, &quot;High Communication&quot; = &quot;high&quot;)) %&gt;% dplyr::mutate(pred_fixed = predict(fit_lmer_2a_re, re.form = NA)) %&gt;% # fixed effects only dplyr::mutate(pred_wrand = predict(fit_lmer_2a_re)) %&gt;% # fixed and random effects together ggplot(aes(x = age2, y = vsae)) + geom_line(aes(y = pred_wrand, # BLUP = fixed and random effects together group = childid, color = &quot;BLUP&quot;, size = &quot;BLUP&quot;)) + geom_line(aes(y = pred_fixed, # fixed effects only group = sicdegp, color = &quot;Fixed&quot;, size = &quot;Fixed&quot;)) + scale_color_manual(name = &quot;Model: &quot;, values = c(&quot;BLUP&quot; = &quot;black&quot;, &quot;Fixed&quot; = &quot;blue&quot;)) + scale_size_manual(name = &quot;Model: &quot;, values = c(&quot;BLUP&quot; = .5, &quot;Fixed&quot; = 1.5)) + facet_grid(. ~ sicdegp) + theme_bw() + labs(x = &quot;Age, in years&quot;, y = &quot;Estimated Marginal Mean\\nVineland Socialization Age Equivalent&quot;) + theme(legend.position = &quot;bottom&quot;, legend.key.width = unit(1.5, &quot;cm&quot;)) data_long %&gt;% dplyr::mutate(pred_fixed = predict(fit_lmer_2a_re, re.form = NA)) %&gt;% dplyr::mutate(pred_wrand = predict(fit_lmer_2a_re)) %&gt;% dplyr::filter(childid %in% sample(levels(data_long$childid), 25)) %&gt;% # 25 randomly sampled children ggplot(aes(x = age, y = vsae)) + geom_point(aes(color = sicdegp), size = 3) + geom_line(aes(y = pred_wrand, linetype = &quot;BLUP&quot;, size = &quot;BLUP&quot;), color = &quot;black&quot;) + geom_line(aes(y = pred_fixed, color = sicdegp, linetype = &quot;Fixed&quot;, size = &quot;Fixed&quot;)) + scale_linetype_manual(name = &quot;Model: &quot;, values = c(&quot;BLUP&quot; = &quot;longdash&quot;, &quot;Fixed&quot; = &quot;solid&quot;)) + scale_size_manual(name = &quot;Model: &quot;, values = c(&quot;BLUP&quot; = .5, &quot;Fixed&quot; = 1)) + facet_wrap(. ~ childid, labeller = &quot;label_both&quot;) + theme_bw() + theme(legend.position = &quot;bottom&quot;, legend.key.width = unit(1.5, &quot;cm&quot;)) + labs(x = &quot;Age, in years&quot;, y = &quot;Estimated Marginal Mean\\nVineland Socialization Age Equivalent&quot;, color = &quot;Communication:&quot;) "],
["longitudinal-mlm-hedeker-gibbons-depression.html", "9 Longitudinal MLM: Hedeker &amp; Gibbons - Depression 9.1 Background 9.2 Exploratory Data Analysis 9.3 Patterns in the Outcome Over Time 9.4 MLM - Null or Emptly Models 9.5 MLM: Add Random Slope for Time (i.e. Trend) 9.6 MLM: Coding of Time 9.7 MLM: Effect of DIagnosis on Time Trends (Fixed Interaction) 9.8 MLM: Quadratic Trend", " 9 Longitudinal MLM: Hedeker &amp; Gibbons - Depression library(tidyverse) # all things tidy library(pander) # nice looking genderal tabulations library(furniture) # nice table1() descriptives library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(psych) # contains some useful functions, like headTail library(sjstats) # ICC calculationsv library(effects) # Estimated Marginal Means library(VIM) # Visualization and Imputation of Missing Values library(naniar) # Summaries and Visualisations for Missing Data library(corrplot) # Vizualize correlation matrix library(lme4) # Linear, generalized linear, &amp; nonlinear mixed models library(haven) # read in SPSS dataset 9.1 Background Starting in chapter 4, Hedeker and Gibbons (2006) details analysis of a psychiatric study described by Reisby et al. (1977). This study focues on the relationship between Imipramine (IMI) and Desipramine (DMI) plasma levels and clinical response in 66 depressed inpatients (37 endogenous and 29 non-endogenous). The IMI and DMI measures were only taken in the later weeks, but are not used here. Dependent or outcome variable: hamd Hamilton Depression Scores (HD) Independent or predictor variables: endog Prepression Diagnosis endogenous non-endogenous/reactive IMI (imipramine) drug-plasma levels (µg/l) antidepressant given 225 mg/day, weeks 3-6 DMI (desipramine) drug-plasma levels (µg/l) metabolite of imipramine data_raw &lt;- haven::read_spss(&quot;http://www.uic.edu/~hedeker/riesby.sav&quot;) %&gt;% # read from teh webpage dplyr::select(-intrcpt, -endweek) # de-select or delete some variables data_raw %&gt;% psych::headTail(top = 11, bottom = 8) id hamd week endog 1 101 26 0 0 2 101 22 1 0 3 101 18 2 0 4 101 7 3 0 5 101 4 4 0 6 101 3 5 0 7 103 33 0 0 8 103 24 1 0 9 103 15 2 0 10 103 24 3 0 11 103 15 4 0 12 ... ... ... ... 13 360 28 4 1 14 360 33 5 1 15 361 30 0 1 16 361 22 1 1 17 361 11 2 1 18 361 8 3 1 19 361 7 4 1 20 361 19 5 1 9.1.1 Long Format data_long &lt;- data_raw %&gt;% # save the dataset as a new name dplyr::filter(!is.na(hamd)) %&gt;% # remove NA or missing scores dplyr::mutate(id = factor(id)) %&gt;% # declare grouping var a factor dplyr::mutate(endog = factor(endog, # attach labels to a grouping variable levels = c(0, 1), # order of the levels should match levels labels = c(&quot;Reactive&quot;, # order matters! &quot;Endogenous&quot;))) %&gt;% dplyr::select(id, week, endog, hamd) %&gt;% # select the order of variables to include dplyr::arrange(id, week) # sort observations data_long %&gt;% psych::headTail(top = 11, bottom = 8) id week endog hamd 1 101 0 Reactive 26 2 101 1 Reactive 22 3 101 2 Reactive 18 4 101 3 Reactive 7 5 101 4 Reactive 4 6 101 5 Reactive 3 7 103 0 Reactive 33 8 103 1 Reactive 24 9 103 2 Reactive 15 10 103 3 Reactive 24 11 103 4 Reactive 15 12 &lt;NA&gt; ... &lt;NA&gt; ... 13 609 2 Endogenous 22 14 609 3 Endogenous 14 15 609 4 Endogenous 15 16 609 5 Endogenous 2 17 610 0 Endogenous 34 18 610 2 Endogenous 33 19 610 3 Endogenous 23 20 610 5 Endogenous 11 9.1.2 Wide Format data_wide &lt;- data_long %&gt;% # save the dataset as a new name tidyr::spread(key = week, # name the varaible that specifies how to spread value = hamd, # name the variable to be spread out sep = &quot;_&quot;) # add to combine the key-variables name before its value Notice the missing values, displayed as NA. data_wide %&gt;% psych::headTail() id endog week_0 week_1 week_2 week_3 week_4 week_5 1 101 Reactive 26 22 18 7 4 3 2 103 Reactive 33 24 15 24 15 13 3 104 Endogenous 29 22 18 13 19 0 4 105 Reactive 22 12 16 16 13 9 5 &lt;NA&gt; &lt;NA&gt; ... ... ... ... ... ... 6 607 Endogenous 30 39 30 27 20 4 7 608 Reactive 24 19 14 12 3 4 8 609 Endogenous &lt;NA&gt; 25 22 14 15 2 9 610 Endogenous 34 &lt;NA&gt; 33 23 &lt;NA&gt; 11 9.2 Exploratory Data Analysis 9.2.1 Diagnosis Group # Knit to Website: output = &quot;html&quot; # Knit to PDF: output = &quot;latex2&quot; # View on Screen: output = &quot;&quot;text&quot;, or &quot;markdown&quot;, &quot;html&quot; data_long %&gt;% dplyr::filter(week == 0) %&gt;% furniture::table1(&quot;Depression Type&quot; = endog, output = &quot;html&quot;) Mean/Count (SD/%) n = 61 Depression Type Reactive 28 (45.9%) Endogenous 33 (54.1%) 9.2.2 Missing Data &amp; Attrition Plot the amount of missing vlaues and the amount of each patter of missing values. 9.2.2.1 VIM package data_wide %&gt;% VIM::aggr(numbers = TRUE, # shows the number to the far right prop = FALSE) # shows counts instead of proportions 9.2.2.2 naniar package data_wide %&gt;% naniar::gg_miss_upset(sets = c(&quot;week_0_NA&quot;, &quot;week_1_NA&quot;, &quot;week_2_NA&quot;, &quot;week_3_NA&quot;, &quot;week_4_NA&quot;, &quot;week_5_NA&quot;), keep.order = TRUE) 9.2.3 Depression Over Time, by Group 9.2.3.1 Table of Means Default = COMPLETE CASES ONLY!!! Note - the sample size at each time point is the same, but subjects are only included if they have data at every time point # Knit to Website: output = &quot;html&quot; # Knit to PDF: output = &quot;latex2&quot; # View on Screen: output = &quot;&quot;text&quot;, or &quot;markdown&quot;, &quot;html&quot; data_wide %&gt;% furniture::table1(week_0, week_1, week_2, week_3, week_4, week_5, splitby = ~ endog, test = TRUE, na.rm = TRUE, # default: COMPLETE CASES ONLY!!!!! output = &quot;html&quot;) Reactive Endogenous P-Value n = 25 n = 21 week_0 0.196 22.4 (3.9) 24.1 (4.9) week_1 0.01 20.1 (3.7) 23.9 (5.5) week_2 0.306 17.3 (4.3) 19.0 (6.0) week_3 0.406 15.9 (5.8) 17.5 (6.9) week_4 0.509 12.8 (6.7) 14.2 (7.0) week_5 0.48 11.4 (6.5) 13.0 (8.7) Specify All data: note - that the smaple sizes will be different for each time point # Knit to Website: output = &quot;html&quot; # Knit to PDF: output = &quot;latex2&quot; # View on Screen: output = &quot;&quot;text&quot;, or &quot;markdown&quot;, &quot;html&quot; data_wide %&gt;% furniture::table1(week_0, week_1, week_2, week_3, week_4, week_5, splitby = ~ endog, test = TRUE, na.rm = FALSE, # INCLUDES ALL DATA !!!! output = &quot;html&quot;) Reactive Endogenous P-Value n = 29 n = 37 week_0 0.295 22.8 (4.1) 24.0 (4.8) week_1 0.029 20.5 (3.8) 23.0 (5.1) week_2 0.081 17.0 (4.3) 19.3 (6.1) week_3 0.227 15.3 (6.2) 17.3 (6.6) week_4 0.295 12.6 (6.7) 14.5 (7.2) week_5 0.473 11.2 (6.3) 12.6 (8.0) 9.2.3.2 Using the LONG formatted dataset Each person’s data is stored on multiple lines, one for each time point. FOR ALL DATA! data_sum_all &lt;- data_long %&gt;% dplyr::group_by(endog, week) %&gt;% # specify the groups dplyr::summarise(hamd_n = n(), # count of valid scores hamd_mean = mean(hamd), # mean score hamd_sd = sd(hamd), # standard deviation of scores hamd_sem = hamd_sd / sqrt(hamd_n)) # stadard error for the mean of scores data_sum_all # A tibble: 12 x 6 # Groups: endog [?] endog week hamd_n hamd_mean hamd_sd hamd_sem &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Reactive 0 28 22.8 4.12 0.779 2 Reactive 1 29 20.5 3.83 0.712 3 Reactive 2 28 17 4.35 0.821 4 Reactive 3 29 15.3 6.17 1.15 5 Reactive 4 29 12.6 6.72 1.25 6 Reactive 5 27 11.2 6.34 1.22 7 Endogenous 0 33 24 4.85 0.844 8 Endogenous 1 34 23 5.10 0.874 9 Endogenous 2 37 19.3 6.08 1.000 10 Endogenous 3 36 17.3 6.56 1.09 11 Endogenous 4 34 14.5 7.17 1.23 12 Endogenous 5 31 12.6 7.96 1.43 FOR COMPLETE CASES ONLY!!! data_sum_cc &lt;- data_long %&gt;% dplyr::group_by(id) %&gt;% # group-by participant dplyr::mutate(person_vsae_n = n()) %&gt;% # count the number of valid VSAE scores dplyr::filter(person_vsae_n == 6) %&gt;% # restrict to only thoes children with 5 valid scores dplyr::group_by(endog, week) %&gt;% # specify the groups dplyr::summarise(hamd_n = n(), # count of valid scores hamd_mean = mean(hamd), # mean score hamd_sd = sd(hamd), # standard deviation of scores hamd_sem = hamd_sd / sqrt(hamd_n)) # stadard error for the mean of scores data_sum_cc # A tibble: 12 x 6 # Groups: endog [?] endog week hamd_n hamd_mean hamd_sd hamd_sem &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Reactive 0 25 22.4 3.90 0.781 2 Reactive 1 25 20.1 3.68 0.737 3 Reactive 2 25 17.3 4.34 0.867 4 Reactive 3 25 15.9 5.84 1.17 5 Reactive 4 25 12.8 6.68 1.34 6 Reactive 5 25 11.4 6.54 1.31 7 Endogenous 0 21 24.1 4.87 1.06 8 Endogenous 1 21 23.9 5.47 1.19 9 Endogenous 2 21 19.0 6.01 1.31 10 Endogenous 3 21 17.5 6.86 1.50 11 Endogenous 4 21 14.2 6.98 1.52 12 Endogenous 5 21 13.0 8.73 1.90 9.2.3.3 Person-Profile Plot or Spaghetti Plot Use the data in LONG format. A scatterplot of all observations of depression scores over time, joining the dots of each individual’s data. NOTE: Not all lines have a point for every week! data_long %&gt;% ggplot(aes(x = week, y = hamd)) + geom_point() + geom_line(aes(group = id)) # join points that belong to the same &quot;id&quot; data_long %&gt;% ggplot(aes(x = week, y = hamd, color = endog)) + # color points and lines by the &quot;endog&quot; variable geom_point() + geom_line(aes(group = id)) data_long %&gt;% ggplot(aes(x = week, y = hamd)) + geom_point() + geom_line(aes(group = id)) + facet_grid( ~ endog) # side-by-side pandels by the &quot;endog&quot; variable data_long %&gt;% ggplot(aes(x = week %&gt;% factor(), y = hamd)) + geom_boxplot() + # compare center and spread facet_grid( ~ endog) data_long %&gt;% ggplot(aes(x = week %&gt;% factor(), y = hamd)) + geom_violin() + # similar to boxplots to show distribution facet_grid( ~ endog) data_long %&gt;% ggplot(aes(x = week, y = hamd)) + geom_point() + geom_line(aes(group = id)) + facet_grid( ~ endog) + geom_smooth() + # DEFAULTS: method = &quot;loess&quot;, se = TRUE, color = &quot;blue&quot; geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;hot pink&quot;) data_long %&gt;% ggplot(aes(x = week, y = hamd)) + geom_point() + geom_line(aes(group = id)) + facet_grid( ~ endog) + geom_smooth(aes(color = &quot;Flexible&quot;), method = &quot;loess&quot;, se = FALSE,) + geom_smooth(aes(color = &quot;Linear&quot;), method = &quot;lm&quot;, se = FALSE) + scale_color_manual(name = &quot;Smoother Type: &quot;, values = c(&quot;Flexible&quot; = &quot;blue&quot;, &quot;Linear&quot; = &quot;red&quot;)) + theme_bw() + theme(legend.position = &quot;bottom&quot;) 9.3 Patterns in the Outcome Over Time 9.3.1 Variance-Covariance 9.3.1.1 Full Matrix Variances are down the diagonal Increasing variance over time violates the ANOVA assumption of homogeity of variance data_wide %&gt;% dplyr::select(starts_with(&quot;week_&quot;)) %&gt;% # just the outcome(s) cov(use = &quot;complete.obs&quot;) %&gt;% # covariance matrix, LIST-wise deletion round(3) week_0 week_1 week_2 week_3 week_4 week_5 week_0 19.421 10.716 9.523 12.350 9.062 7.376 week_1 10.716 24.236 12.545 15.930 11.592 8.471 week_2 9.523 12.545 26.773 23.848 23.858 20.657 week_3 12.350 15.930 23.848 39.755 33.316 29.728 week_4 9.062 11.592 23.858 33.316 45.943 37.107 week_5 7.376 8.471 20.657 29.728 37.107 57.332 data_wide %&gt;% dplyr::select(starts_with(&quot;week_&quot;)) %&gt;% # just the outcome(s) cov(use = &quot;pairwise.complete.obs&quot;) %&gt;% # covariance matrix, PAIR-wise deletion round(3) week_0 week_1 week_2 week_3 week_4 week_5 week_0 20.551 10.115 10.139 10.086 7.191 6.278 week_1 10.115 22.071 12.277 12.550 10.264 7.720 week_2 10.139 12.277 30.091 25.126 24.626 18.384 week_3 10.086 12.550 25.126 41.153 37.339 23.992 week_4 7.191 10.264 24.626 37.339 48.594 30.513 week_5 6.278 7.720 18.384 23.992 30.513 52.120 9.3.1.2 Just Variances Notice the variance in scores increases over time, which is seen in the side-by-side boxplots. data_wide %&gt;% dplyr::select(starts_with(&quot;week_&quot;)) %&gt;% # just the outcome(s) cov(use = &quot;pairwise.complete.obs&quot;) %&gt;% # covariance matrix, PAIR-wise deletion diag() # extracts just the variances week_0 week_1 week_2 week_3 week_4 week_5 20.55082 22.07117 30.09135 41.15288 48.59447 52.12008 9.3.2 Correlation 9.3.2.1 Full Matrix Pairwise relationships are easier to eye-ball magnitude when presented as correlations, rather than covariances, due to the relative scale. data_wide %&gt;% dplyr::select(starts_with(&quot;week_&quot;)) %&gt;% # just the outcome(s) cor(use = &quot;complete.obs&quot;) %&gt;% # correlation matrix - LIST-wise deletion round(2) week_0 week_1 week_2 week_3 week_4 week_5 week_0 1.00 0.49 0.42 0.44 0.30 0.22 week_1 0.49 1.00 0.49 0.51 0.35 0.23 week_2 0.42 0.49 1.00 0.73 0.68 0.53 week_3 0.44 0.51 0.73 1.00 0.78 0.62 week_4 0.30 0.35 0.68 0.78 1.00 0.72 week_5 0.22 0.23 0.53 0.62 0.72 1.00 data_wide %&gt;% dplyr::select(starts_with(&quot;week_&quot;)) %&gt;% # just the outcome(s) cor(use = &quot;pairwise.complete.obs&quot;) %&gt;% # correlation matrix - PAIR-wise deletion round(2) week_0 week_1 week_2 week_3 week_4 week_5 week_0 1.00 0.49 0.41 0.33 0.23 0.18 week_1 0.49 1.00 0.49 0.41 0.31 0.22 week_2 0.41 0.49 1.00 0.74 0.67 0.46 week_3 0.33 0.41 0.74 1.00 0.82 0.57 week_4 0.23 0.31 0.67 0.82 1.00 0.65 week_5 0.18 0.22 0.46 0.57 0.65 1.00 9.3.2.2 Visualization Looking for patterns is always easier with a plot. All RM or mixed ANOVA assume sphericity or compound symmetry, meaning that all the correlations in the matrix would be the same. This is not the case for these data. Instead we see a classic pattern of corralary decay. Measures taken close in time, say 1 week apart, exhibit the highest degree of correlation. The farther apart in time that two measures are taken, the less correlated they are. Note that the adjacent measures become more highly correlated, too. This can be due to attrition; later time points having a smaller sample size. data_wide %&gt;% dplyr::select(starts_with(&quot;week_&quot;)) %&gt;% # just the outcome(s) cor(use = &quot;pairwise.complete.obs&quot;) %&gt;% # correlation matrix corrplot::corrplot.mixed(upper = &quot;ellipse&quot;) 9.3.3 For Each Group It can be a good ideal to investigate if the groups exhibit a similar pattern in correlation. Reactive Depression data_wide %&gt;% dplyr::filter(endog == &quot;Reactive&quot;) %&gt;% # filter observations for the REACTIVE group dplyr::select(starts_with(&quot;week_&quot;)) %&gt;% cor(use = &quot;pairwise.complete.obs&quot;) %&gt;% corrplot::corrplot.mixed(upper = &quot;ellipse&quot;) Endogenous Depression data_wide %&gt;% dplyr::filter(endog == &quot;Endogenous&quot;) %&gt;% # filter observations for the Endogenous group dplyr::select(starts_with(&quot;week_&quot;)) %&gt;% cor(use = &quot;pairwise.complete.obs&quot;) %&gt;% corrplot::corrplot.mixed(upper = &quot;ellipse&quot;) 9.4 MLM - Null or Emptly Models 9.4.1 Fit the model Random Intercepts, with Fixed Intercept and Time Slope (i.e. Trend)….Hedeker and Gibbons (2006) section 4.3.5, starting on page 55 Since this situation deals with longitudinal data, it is more appropriate to start off including the time variable in the null model as a fixed effect only. fit_lmer_week_RI_reml &lt;- lme4::lmer(hamd ~ week + (1|id), data = data_long, REML = TRUE) 9.4.2 Table of Parameter Estimates # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(fit_lmer_week_RI_reml, single.row = TRUE, caption = &quot;MLM: Random Intercepts Null Model fit w/REML&quot;, caption.above = TRUE, custom.note = &quot;Reproduction of Hedeker&#39;s table 4.3 on page 55, except using REML here instead of ML&quot;) MLM: Random Intercepts Null Model fit w/REML Model 1 (Intercept) 23.55 (0.64)*** week -2.38 (0.14)*** AIC 2294.73 BIC 2310.43 Log Likelihood -1143.36 Num. obs. 375 Num. groups: id 66 Var: id (Intercept) 16.45 Var: Residual 19.10 Reproduction of Hedeker’s table 4.3 on page 55, except using REML here instead of ML On average, patients start off with HDRS scores of 23.55 and then change by -2.38 points each week. This weekly improvement of about 2 points a week is statistically significant via the Wald test. 9.4.3 Estimated Marginal Means Plot Multilevel model on page 55 (Hedeker and Gibbons 2006) \\[ \\hat{y} = 23.552 + -2.376 week \\] The fastest way to plot a model is to use the sjPlot::plot_model() function. sjPlot::plot_model(fit_lmer_week_RI_reml, type = &quot;pred&quot;, terms = c(&quot;week&quot;)) 9.4.4 Estimated Marginal Means and Emperical Bayes Plots With a bit more code we can plot not only the marginal model (fixed effects only), but add the Best Linear Unbiased Predictions (BLUPs) or person-specific specific models (both fixed and random effects). data_long %&gt;% dplyr::mutate(pred_fixed = predict(fit_lmer_week_RI_reml, re.form = NA)) %&gt;% # fixed effects only dplyr::mutate(pred_wrand = predict(fit_lmer_week_RI_reml)) %&gt;% # fixed and random effects together ggplot(aes(x = week, y = hamd, group = id)) + geom_line(aes(y = pred_wrand, color = &quot;BLUP&quot;, size = &quot;BLUP&quot;, linetype = &quot;BLUP&quot;)) + geom_line(aes(y = pred_fixed, color = &quot;Marginal&quot;, size = &quot;Marginal&quot;, linetype = &quot;Marginal&quot;)) + theme_bw() + scale_color_manual(name = &quot;Type of Prediction&quot;, values = c(&quot;BLUP&quot; = &quot;gray50&quot;, &quot;Marginal&quot; = &quot;blue&quot;)) + scale_size_manual(name = &quot;Type of Prediction&quot;, values = c(&quot;BLUP&quot; = .5, &quot;Marginal&quot; = 1.25)) + scale_linetype_manual(name = &quot;Type of Prediction&quot;, values = c(&quot;BLUP&quot; = &quot;longdash&quot;, &quot;Marginal&quot; = &quot;solid&quot;)) + theme(legend.position = c(0, 0), legend.justification = c(-0.1, -0.1), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(1.5, &quot;cm&quot;)) + labs(x = &quot;Weeks Since Baseline&quot;, y = &quot;Hamilton Depression Scores&quot;) Notice that in this model, all the BLUPs are parallel. That is becuase we are only letting the intercept vary from person-to-person while keeping the effect of time (slope) constant. Reproduce Table 4.4 on page 55 (Hedeker and Gibbons 2006) One way to judge a model is to compare the estimated means to the observed means to see how accuratedly they are represented by the model. This excellent fit of the estimated marginal means to the observed data supports the hypothesis that the change in depression across time is LINEAR. obs &lt;- data_long %&gt;% dplyr::group_by(week) %&gt;% dplyr::summarise(observed = mean(hamd, na.rm = TRUE)) effects::Effect(focal.predictors = &quot;week&quot;, mod = fit_lmer_week_RI_reml, xlevels = list(week = 0:5)) %&gt;% data.frame() %&gt;% dplyr::rename(estimated = fit) %&gt;% dplyr::left_join(obs, by = &quot;week&quot;) %&gt;% dplyr::select(week, observed, estimated) %&gt;% dplyr::mutate(diff = observed - estimated) %&gt;% pander::pander(caption = &quot;Observed and Estimated Means&quot;) Observed and Estimated Means week observed estimated diff 0 23 24 -0.11 1 22 21 0.67 2 18 19 -0.49 3 16 16 -0.01 4 14 14 -0.43 5 12 12 0.27 9.4.5 Intra-individual Correlation (ICC) sjstats::icc(fit_lmer_week_RI_reml) Linear mixed model Family : gaussian (identity) Formula: hamd ~ week + (1 | id) ICC (id): 0.4627 Interpretation Nearly HALF of the variance in depression scores not explained by the linear effect of time is attributable to person-to-person differences. Thus, subjects display considerable heterogeneity in depression levels. This value of 46% is an oversimplification of the correlation matrix above. 9.4.6 Compare to the Single-Level Null: No Random Effects Simple Linear Regression, Hedeker and Gibbons (2006) To compare, fit the single level regression model fit_lm_week_ml &lt;- lm(hamd ~ week, data = data_long) 9.4.6.1 Table of Parameter Estimates # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(fit_lm_week_ml, fit_lmer_week_RI_reml), custom.model.names = c(&quot;Single-Level&quot;, &quot;Multilevel&quot;), caption = &quot;MLM: Longitudinal Null Models&quot;, caption.above = TRUE, custom.note = &quot;The singel-level model treats are observations as being independent and unrelated to each other, even if they were made on the same person.&quot;) MLM: Longitudinal Null Models Single-Level Multilevel (Intercept) 23.60*** 23.55*** (0.55) (0.64) week -2.41*** -2.38*** (0.18) (0.14) R2 0.32 Adj. R2 0.32 Num. obs. 375 375 RMSE 5.95 AIC 2294.73 BIC 2310.43 Log Likelihood -1143.36 Num. groups: id 66 Var: id (Intercept) 16.45 Var: Residual 19.10 The singel-level model treats are observations as being independent and unrelated to each other, even if they were made on the same person. For the multilevel model, the Wald tests indicated the fixed intercept is significant (no surprized that the depressions scores are not zero at baseline). More of note is the significance of the fixed effect of time. This signifies that depression scores are declining over time. On average, patients are improving across time, by an average of 2.4 points a week. 9.4.6.2 Residual Variance Note: the fixed estimates are very similar for the two models, but the standard errors are different. Additionally, whereas the single-level regression lumps all remaining variance together (\\(\\sigma^2\\)), the multilevel model seperates it into within-subjects (\\(\\sigma^2_{u0}\\) or \\(\\tau_{00}\\)) and between-subjects variance (\\(\\sigma^2_{e}\\) or \\(\\sigma^2\\)). sigma(fit_lm_week_ml)^2 [1] 35.3997 sjstats::re_var(fit_lmer_week_RI_reml) # in longitudinal data, a group of observations = a participant or person Within-group-variance: 19.099 Between-group-variance: 16.446 (id) “One statistician’s error term is another’s career!” Hedeker and Gibbons (2006), page 56 9.5 MLM: Add Random Slope for Time (i.e. Trend) 9.5.1 Fit the Model fit_lmer_week_RIAS_reml &lt;- lme4::lmer(hamd ~ week + (week|id), # MLM-RIAS data = data_long, REML = TRUE) 9.5.2 Table of Prameter Estimates # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(fit_lmer_week_RI_reml, fit_lmer_week_RIAS_reml), custom.model.names = c(&quot;Random Intercepts&quot;, &quot; And Random Slopes&quot;), caption = &quot;MLM: Null models fit w/REML&quot;, caption.above = TRUE, custom.note = &quot;Hedeker table 4.4 on page 55 and table 4.5 on page 58, except using REML here instead of ML&quot;) MLM: Null models fit w/REML Random Intercepts And Random Slopes (Intercept) 23.55*** 23.58*** (0.64) (0.55) week -2.38*** -2.38*** (0.14) (0.21) AIC 2294.73 2231.92 BIC 2310.43 2255.48 Log Likelihood -1143.36 -1109.96 Num. obs. 375 375 Num. groups: id 66 66 Var: id (Intercept) 16.45 12.94 Var: Residual 19.10 12.21 Var: id week 2.13 Cov: id (Intercept) week -1.48 Hedeker table 4.4 on page 55 and table 4.5 on page 58, except using REML here instead of ML Visually, we can see that the unexplained or residual variance is less (12.21 vs 19.10) for the model that includes person-specific slopes (trajectories over time). Note: the negative covariance between random intercepts and random slopes (\\(\\sigma_{u01} = \\tau_{01} = -1.48\\)): “This suggests that patients who are initially more depressed (i.e. greater intercepts) improve at a greater rate (i.e. more pronounced negative slopes). An alternative explainatio, though,is that of a floor effect due to the HDRS rating scale. Simply put, patients with less depressed intitial scores have a more limited range of lower scores than those with higher initial scores.” Hedeker and Gibbons (2006), page 58 9.5.3 Likelihood Ratio Test anova(fit_lmer_week_RI_reml, fit_lmer_week_RIAS_reml, model.names = c(&quot;RI&quot;, &quot;RIAS&quot;), refit = FALSE) %&gt;% pander::pander(caption = &quot;LRT: Assess Significance of Random Slopes&quot;) LRT: Assess Significance of Random Slopes Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) RI 4 2295 2310 -1143 2287 NA NA NA RIAS 6 2232 2255 -1110 2220 67 2 0 Including the random slope for time significantly improved the model fit via the formal Likelihood Ratio Test. This rejects the assumption of compound symmetry. 9.5.4 Estimated Marginal Means Plot sjPlot::plot_model(fit_lmer_week_RIAS_reml, type = &quot;pred&quot;, terms = c(&quot;week&quot;)) Adding the random slopes didn’t change the estimates for the fixed effects much. 9.5.5 Estimated Marginal Means and Emperical Bayes Plots data_long %&gt;% dplyr::mutate(pred_fixed = predict(fit_lmer_week_RIAS_reml, re.form = NA)) %&gt;% # fixed effects only dplyr::mutate(pred_wrand = predict(fit_lmer_week_RIAS_reml)) %&gt;% # fixed and random effects together ggplot(aes(x = week, y = hamd, group = id)) + geom_line(aes(y = pred_wrand, color = &quot;BLUP&quot;, size = &quot;BLUP&quot;, linetype = &quot;BLUP&quot;)) + geom_line(aes(y = pred_fixed, color = &quot;Marginal&quot;, size = &quot;Marginal&quot;, linetype = &quot;Marginal&quot;)) + theme_bw() + scale_color_manual(name = &quot;Type of Prediction&quot;, values = c(&quot;BLUP&quot; = &quot;gray50&quot;, &quot;Marginal&quot; = &quot;blue&quot;)) + scale_size_manual(name = &quot;Type of Prediction&quot;, values = c(&quot;BLUP&quot; = .5, &quot;Marginal&quot; = 1.25)) + scale_linetype_manual(name = &quot;Type of Prediction&quot;, values = c(&quot;BLUP&quot; = &quot;longdash&quot;, &quot;Marginal&quot; = &quot;solid&quot;)) + theme(legend.position = c(0, 0), legend.justification = c(-0.1, -0.1), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(1.5, &quot;cm&quot;)) + labs(x = &quot;Weeks Since Baseline&quot;, y = &quot;Hamilton Depression Scores&quot;) BLUPs are also refered to as Empirical Bayes Estimates and may be extracted from a model fit. In this cases there will be a specific intercept ((Intercept)) and time slope (week) for each individual or person (id). 9.5.5.1 Fixed Effects Marginal Model = within-subject effects fixef(fit_lmer_week_RIAS_reml) (Intercept) week 23.577044 -2.377047 9.5.5.2 Random Effects between-subjects effects ranef(fit_lmer_week_RIAS_reml)$id %&gt;% head() (Intercept) week 101 1.0571740 -2.1151289 103 3.6707747 -0.4832421 104 2.6727301 -1.5008735 105 -3.0413283 0.2264453 106 0.3154366 1.0254705 107 -0.6149028 -0.4297376 9.5.5.3 BLUPs or Empirical Bayes Estimates fixed effects + random effects coef(fit_lmer_week_RIAS_reml)$id %&gt;% head() (Intercept) week 101 24.63422 -4.492176 103 27.24782 -2.860289 104 26.24977 -3.877921 105 20.53572 -2.150602 106 23.89248 -1.351577 107 22.96214 -2.806785 We can create a scatterplot of these to see the correlation between them. coef(fit_lmer_week_RIAS_reml)$id %&gt;% ggplot(aes(x = week, y = `(Intercept)`)) + geom_point() + geom_hline(yintercept = fixef(fit_lmer_week_RIAS_reml)[&quot;(Intercept)&quot;], linetype = &quot;dashed&quot;) + geom_vline(xintercept = fixef(fit_lmer_week_RIAS_reml)[&quot;week&quot;], linetype = &quot;dashed&quot;) + geom_smooth(method = &quot;lm&quot;) + labs(title = &quot;Hedeker&#39;s Figure 4.4 on page 59&quot;, subtitle = &quot;Reisby data: Estimated random effects&quot;, x = &quot;Week Change in Depression&quot;, y = &quot;Baseline Depression Level&quot;) 9.6 MLM: Coding of Time So far we have used the variable week to denote time as weeks since baseline = week \\(\\in 0, 1, 2, 3, 4, 5\\) data_long_wc &lt;- data_long %&gt;% dplyr::mutate(weekc = week - 2.5) head(data_long_wc) # A tibble: 6 x 5 id week endog hamd weekc &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; 1 101 0 Reactive 26 -2.5 2 101 1 Reactive 22 -1.5 3 101 2 Reactive 18 -0.5 4 101 3 Reactive 7 0.5 5 101 4 Reactive 4 1.5 6 101 5 Reactive 3 2.5 9.6.1 Fit the Model fit_lmer_week_RIAS_reml_wc &lt;- lme4::lmer(hamd ~ weekc + (weekc|id), # MLM-RIAS data = data_long_wc, REML = TRUE) 9.6.2 Table of Parameter Estimates # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(fit_lmer_week_RIAS_reml, fit_lmer_week_RIAS_reml_wc), custom.model.names = c(&quot;Random Intercepts&quot;, &quot; And Random Slopes&quot;), caption = &quot;MLM: Null models fit w/REML&quot;, caption.above = TRUE, custom.note = &quot;Hedeker table table 4.5 on page 58 and table 4.6 on page 61, except using REML here instead of ML&quot;) MLM: Null models fit w/REML Random Intercepts And Random Slopes (Intercept) 23.58*** 17.63*** (0.55) (0.56) week -2.38*** (0.21) weekc -2.38*** (0.21) AIC 2231.92 2231.92 BIC 2255.48 2255.48 Log Likelihood -1109.96 -1109.96 Num. obs. 375 375 Num. groups: id 66 66 Var: id (Intercept) 12.94 18.85 Var: id week 2.13 Cov: id (Intercept) week -1.48 Var: Residual 12.21 12.21 Var: id weekc 2.13 Cov: id (Intercept) weekc 3.84 Hedeker table table 4.5 on page 58 and table 4.6 on page 61, except using REML here instead of ML Unchanged model fit: AIC, BIC, -2LL, residual variance fixed effect of week variance for random intercepts Changed fixed intercept variance for random slopes covariance between random intercepts and random slopes 9.6.3 Estimated Marginal Means and Emperical Bayes Plots data_long_wc %&gt;% dplyr::mutate(pred_fixed = predict(fit_lmer_week_RIAS_reml_wc, re.form = NA)) %&gt;% # fixed effects only dplyr::mutate(pred_wrand = predict(fit_lmer_week_RIAS_reml_wc)) %&gt;% # fixed and random effects together ggplot(aes(x = weekc, y = hamd, group = id)) + geom_line(aes(y = pred_wrand, color = &quot;BLUP&quot;, size = &quot;BLUP&quot;, linetype = &quot;BLUP&quot;)) + geom_line(aes(y = pred_fixed, color = &quot;Marginal&quot;, size = &quot;Marginal&quot;, linetype = &quot;Marginal&quot;)) + theme_bw() + scale_color_manual(name = &quot;Type of Prediction&quot;, values = c(&quot;BLUP&quot; = &quot;gray50&quot;, &quot;Marginal&quot; = &quot;blue&quot;)) + scale_size_manual(name = &quot;Type of Prediction&quot;, values = c(&quot;BLUP&quot; = .5, &quot;Marginal&quot; = 1.25)) + scale_linetype_manual(name = &quot;Type of Prediction&quot;, values = c(&quot;BLUP&quot; = &quot;longdash&quot;, &quot;Marginal&quot; = &quot;solid&quot;)) + theme(legend.position = c(0, 0), legend.justification = c(-0.1, -0.1), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(1.5, &quot;cm&quot;)) + labs(x = &quot;Weeks Centered at Mid-study&quot;, y = &quot;Hamilton Depression Scores&quot;) Again, centering time doesn’t change the interpretation at all. 9.7 MLM: Effect of DIagnosis on Time Trends (Fixed Interaction) The researcher specifically wants to know if the trajectory over time differs for the two types of depression. This translates into a fixed effects interaction between time and group. Start by comapring random intercepts only (RI) to a random intercetps and slopes (RIAS) model. 9.7.1 Fit the Models fit_lmer_week_RIAS_ml &lt;- lme4::lmer(hamd ~ week + (week|id), data = data_long, REML = FALSE) fit_lmer_wkdx_RIAS_ml &lt;- lme4::lmer(hamd ~ week*endog + (week|id), data = data_long, REML = FALSE) 9.7.2 Estimated Marginal Meanse Plot sjPlot::plot_model(fit_lmer_wkdx_RIAS_ml, type = &quot;pred&quot;, terms = c(&quot;week&quot;, &quot;endog&quot;), title = &quot;Hedeker&#39;s Table 4.7 on page 64&quot;) 9.7.3 Likelihood Ratio Test anova(fit_lmer_week_RIAS_ml, fit_lmer_wkdx_RIAS_ml, model.names = c(&quot;Just Time&quot;, &quot;Time X Dx&quot;)) %&gt;% pander::pander(caption = &quot;LRT: Assess Significance of Diagnosis Moderation of Trend&quot;) LRT: Assess Significance of Diagnosis Moderation of Trend Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) Just Time 6 2231 2255 -1110 2219 NA NA NA Time X Dx 8 2231 2262 -1107 2215 4.1 2 0.13 The more complicated model (including the moderating effect of diagnosis) is NOT supported. 9.8 MLM: Quadratic Trend 9.8.1 Fit the Model fit_lmer_quad_RIAS_ml &lt;- lme4::lmer(hamd ~ week + I(week^2) + (week+ I(week^2)|id), data = data_long, REML = FALSE) 9.8.2 Table of Parameter Estimates # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(fit_lmer_week_RIAS_ml, fit_lmer_quad_RIAS_ml), custom.model.names = c(&quot;Linear Trend&quot;, &quot;QUadratic Trend&quot;), caption = &quot;MLM: RIAS models fit w/ML&quot;, caption.above = TRUE, custom.note = &quot;Hedeker table 4.5 on page 58 and table 5.1 on page 84&quot;) MLM: RIAS models fit w/ML Linear Trend QUadratic Trend (Intercept) 23.58*** 23.76*** (0.55) (0.55) week -2.38*** -2.63*** (0.21) (0.48) I(week^2) 0.05 (0.09) AIC 2231.04 2227.65 BIC 2254.60 2266.92 Log Likelihood -1109.52 -1103.82 Num. obs. 375 375 Num. groups: id 66 66 Var: id (Intercept) 12.63 10.44 Var: id week 2.08 6.64 Cov: id (Intercept) week -1.42 -0.92 Var: Residual 12.22 10.52 Var: id I(week^2) 0.19 Cov: id (Intercept) I(week^2) -0.11 Cov: id week I(week^2) -0.94 Hedeker table 4.5 on page 58 and table 5.1 on page 84 9.8.3 Likelihood Ratio Test anova(fit_lmer_week_RIAS_ml, fit_lmer_quad_RIAS_ml) Data: data_long Models: fit_lmer_week_RIAS_ml: hamd ~ week + (week | id) fit_lmer_quad_RIAS_ml: hamd ~ week + I(week^2) + (week + I(week^2) | id) Df AIC BIC logLik deviance Chisq Chi Df fit_lmer_week_RIAS_ml 6 2231.0 2254.6 -1109.5 2219.0 fit_lmer_quad_RIAS_ml 10 2227.7 2266.9 -1103.8 2207.7 11.39 4 Pr(&gt;Chisq) fit_lmer_week_RIAS_ml fit_lmer_quad_RIAS_ml 0.02252 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Even though the Wald test did not find the quadratic fixed time trend to be significant at the population level (marginal), the LRT finds that including the quadratic terms improves the model’s fit. 9.8.4 Estimated Marginal Means Plot fixef(fit_lmer_quad_RIAS_ml) (Intercept) week I(week^2) 23.76024947 -2.63257568 0.05148121 sjPlot::plot_model(fit_lmer_quad_RIAS_ml, type = &quot;pred&quot;, terms = &quot;week&quot;) At the population level, the curviture is very slight. 9.8.5 BLUPs or Emperical Bayes Estimates coef(fit_lmer_quad_RIAS_ml)$id (Intercept) week I(week^2) 101 25.16569 -5.28321743 0.150799875 103 27.50747 -3.43493212 0.119895295 104 25.99802 -3.09403230 -0.176873997 105 21.01143 -2.95260202 0.164660414 106 23.64346 -0.74433551 -0.141870749 107 22.70272 -1.98455923 -0.179103921 108 22.40954 -4.72427828 0.316019058 113 22.70934 -0.40480125 -0.023813241 114 21.71499 -4.51594389 0.305503895 115 21.54596 -2.55734566 0.139467215 117 20.68538 -3.87700294 0.196725834 118 25.37073 -2.57668940 -0.047612151 120 21.54365 -2.02855278 0.175310109 121 22.58018 -1.75375999 -0.038804565 123 19.25421 -2.87312193 0.017542415 302 21.58631 -3.89089252 0.294415476 303 22.46073 -3.40753212 0.044347157 304 24.67540 -0.37917276 -0.166164249 305 21.27401 -3.68029456 -0.010284888 308 23.09149 -2.47011530 0.005956677 309 22.90664 -1.55837862 -0.062498797 310 23.05814 -5.82990167 0.392499942 311 21.32624 -1.62724965 0.056407836 312 20.97724 -0.80435410 -0.285842787 313 21.61301 -4.45108999 0.355815226 315 25.23223 -4.58344529 0.080217069 316 27.75571 -1.45599876 0.069629879 318 20.56469 -0.29405872 -0.235308927 319 22.38192 -4.51712285 0.466349080 322 24.93395 1.26791015 -0.042516264 327 19.82906 -3.17596037 0.466784313 328 23.74555 1.29272720 -0.129018245 331 21.92610 -1.83149273 -0.074415728 333 23.11844 -0.64672711 -0.127987095 334 27.19416 -6.33067334 0.497613741 335 22.72436 -2.57422602 0.010287544 337 25.62004 -2.06605916 -0.118351200 338 22.89845 -0.54279772 -0.095537199 339 24.27807 -4.99290542 0.444972859 344 22.43030 -4.34890671 0.557805239 345 27.22531 -1.03851188 -0.044992524 346 24.66105 -2.09386031 0.138720909 347 20.11424 -3.85553934 0.239871728 348 23.42449 -4.05934642 0.152402180 349 20.49507 -3.30476489 0.269602399 350 23.29831 -3.86555856 0.351772378 351 27.85602 -2.54762823 -0.174397693 352 21.86126 -2.47215664 0.305549810 353 25.44900 -1.25790020 -0.261971685 354 26.94737 0.07804281 -0.289825751 355 24.47061 -2.72957885 -0.141017988 357 25.37351 -0.82942775 -0.114172428 360 24.04647 1.73527605 -0.131112600 361 25.48491 -7.37199538 0.953497998 501 27.83194 -1.46848166 -0.003540939 502 22.99363 -4.51402650 0.038610603 504 20.80202 -2.67923240 0.167675818 505 20.96546 -6.31343523 0.490343799 507 26.25982 0.08284743 -0.277546987 603 25.55712 -3.23921557 0.099755567 604 26.04325 -6.06912697 0.284970707 606 24.47425 -3.73161635 -0.178377319 607 31.08953 1.15555545 -1.091096480 608 23.46558 -4.87963661 0.180136732 609 25.91657 -2.23143281 -0.347100746 610 30.62473 -0.54535125 -0.593019718 For Illustration, two cases have been hand selected: id = 115 and 610. fun_115 &lt;- function(week){ coef(fit_lmer_quad_RIAS_ml)$id[&quot;115&quot;, &quot;(Intercept)&quot;] + coef(fit_lmer_quad_RIAS_ml)$id[&quot;115&quot;, &quot;week&quot;] * week + coef(fit_lmer_quad_RIAS_ml)$id[&quot;115&quot;, &quot;I(week^2)&quot;] * week^2 } fun_610 &lt;- function(week){ coef(fit_lmer_quad_RIAS_ml)$id[&quot;610&quot;, &quot;(Intercept)&quot;] + coef(fit_lmer_quad_RIAS_ml)$id[&quot;610&quot;, &quot;week&quot;] * week + coef(fit_lmer_quad_RIAS_ml)$id[&quot;610&quot;, &quot;I(week^2)&quot;] * week^2 } data_long %&gt;% dplyr::mutate(pred_fixed = predict(fit_lmer_quad_RIAS_ml, re.form = NA)) %&gt;% # fixed effects only dplyr::mutate(pred_wrand = predict(fit_lmer_quad_RIAS_ml)) %&gt;% # fixed and random effects together ggplot(aes(x = week, y = hamd, group = id)) + stat_function(fun = fun_115) + # add cure for ID = 115 stat_function(fun = fun_610) + # add cure for ID = 610 geom_line(aes(y = pred_fixed), color = &quot;blue&quot;, size = 1.25) + theme_bw() + theme(legend.position = c(0, 0), legend.justification = c(-0.1, -0.1), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(1.5, &quot;cm&quot;)) + labs(x = &quot;Weeks Since Baseline&quot;, y = &quot;Hamilton Depression Scores&quot;, title = &quot;Similar to Hedeker&#39;s Figure 5.3 on page 84&quot;, subtitle = &quot;Marginal Mean show in thinker blue&quot;) Figure 9.1: Two Example BLUPS for two different participants These two individuals have quite different curvitures and illustrated how this type of curvatures in person-specific trajectories may end up cancelling each other out to arive at a fairly linear marginal model. 9.8.6 Estimated Marginal Means and Emperical Bayes Plots Note: although the BLUPs are shown for all participants, the predictions are just connects and are therefore slightly jagged and now smoother like the lines on the plot above. data_long %&gt;% dplyr::mutate(pred_fixed = predict(fit_lmer_quad_RIAS_ml, re.form = NA)) %&gt;% # fixed effects only dplyr::mutate(pred_wrand = predict(fit_lmer_quad_RIAS_ml)) %&gt;% # fixed and random effects together ggplot(aes(x = week, y = hamd, group = id)) + geom_line(aes(y = pred_wrand, color = &quot;BLUP&quot;, size = &quot;BLUP&quot;, linetype = &quot;BLUP&quot;)) + geom_line(aes(y = pred_fixed, color = &quot;Marginal&quot;, size = &quot;Marginal&quot;, linetype = &quot;Marginal&quot;)) + theme_bw() + scale_color_manual(name = &quot;Type of Prediction&quot;, values = c(&quot;BLUP&quot; = &quot;gray50&quot;, &quot;Marginal&quot; = &quot;blue&quot;)) + scale_size_manual(name = &quot;Type of Prediction&quot;, values = c(&quot;BLUP&quot; = .5, &quot;Marginal&quot; = 1.25)) + scale_linetype_manual(name = &quot;Type of Prediction&quot;, values = c(&quot;BLUP&quot; = &quot;longdash&quot;, &quot;Marginal&quot; = &quot;solid&quot;)) + theme(legend.position = c(0, 0), legend.justification = c(-0.1, -0.1), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(1.5, &quot;cm&quot;)) + labs(x = &quot;Weeks Since Baseline&quot;, y = &quot;Hamilton Depression Scores&quot;, title = &quot;Hedeker&#39;s Figure 5.4 on page 85&quot;) Figure 9.2: EStimated curvilinear trends At the person-level, the curviture is very diverse (heterogeneous). Some individuals have accelerating downward tend while other have accelerating upward trends. Th improvement that the curvilinear model provides in describing change across time is perhaps modest. "],
["longitudinal-mlm-hox-ch-5-student-gpa.html", "10 Longitudinal MLM: Hox ch 5 - student GPA 10.1 Background 10.2 MLM", " 10 Longitudinal MLM: Hox ch 5 - student GPA library(tidyverse) # all things tidy library(pander) # nice looking genderal tabulations library(furniture) # nice table1() descriptives library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(psych) # contains some useful functions, like headTail library(sjstats) # ICC calculations library(sjPlot) # Visualization for Models library(effects) # Effec displays for Models library(lme4) # non-linear mixed-effects models library(haven) # read in SPSS dataset 10.1 Background The text “Multilevel Analysis: Techniques and Applications, Third Edition” (Hox, Moerbeek, and Van de Schoot 2017) has a companion website which includes links to all the data files used throughout the book (housed on the book’s GitHub repository). The following example is used through out Hox, Moerbeek, and Van de Schoot (2017)’s chapater 5. The GPA for 200 college students were followed for 6 consecutive semesters (simulated). Job status was also measured as number of hours worked for the same size occations. Time-invariant covariates are the student’s gender and high school GPA. The variable admitted will not be used. data_raw &lt;- haven::read_sav(&quot;https://github.com/MultiLevelAnalysis/Datasets-third-edition-Multilevel-book/raw/master/chapter%205/GPA2/gpa2long.sav&quot;) %&gt;% haven::as_factor() # retain the labels from SPSS --&gt; factor tibble::glimpse(data_raw) Observations: 1,200 Variables: 7 $ student &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3,... $ occas &lt;fct&gt; year 1 semester 1, year 1 semester 2, year 2 semester... $ gpa &lt;dbl&gt; 2.3, 2.1, 3.0, 3.0, 3.0, 3.3, 2.2, 2.5, 2.6, 2.6, 3.0... $ job &lt;fct&gt; 2 hours, 2 hours, 2 hours, 2 hours, 2 hours, 2 hours,... $ sex &lt;fct&gt; female, female, female, female, female, female, male,... $ highgpa &lt;dbl&gt; 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.5, 2.5, 2.5, 2.5, 2.5... $ admitted &lt;fct&gt; yes, yes, yes, yes, yes, yes, no, no, no, no, no, no,... data_raw %&gt;% dplyr::select(occas, job) %&gt;% table(useNA = &quot;always&quot;) job occas no job 1 hour 2 hours 3 hours 4 or more hours &lt;NA&gt; year 1 semester 1 0 0 172 28 0 0 year 1 semester 2 0 0 169 31 0 0 year 2 semester 1 0 7 159 34 0 0 year 2 semester 2 0 5 169 26 0 0 year 3 semester 1 0 18 150 32 0 0 year 3 semester 2 0 22 148 30 0 0 &lt;NA&gt; 0 0 0 0 0 0 data_long &lt;- data_raw %&gt;% dplyr::mutate(student = factor(student)) %&gt;% dplyr::mutate(sem = case_when(occas == &quot;year 1 semester 1&quot; ~ 1, occas == &quot;year 1 semester 2&quot; ~ 2, occas == &quot;year 2 semester 1&quot; ~ 3, occas == &quot;year 2 semester 2&quot; ~ 4, occas == &quot;year 3 semester 1&quot; ~ 5, occas == &quot;year 3 semester 2&quot; ~ 6)) %&gt;% dplyr::mutate(semc = sem - 1) %&gt;% dplyr::mutate(job = fct_drop(job)) %&gt;% dplyr::mutate(hrs = case_when(job == &quot;no job&quot; ~ 0, job == &quot;1 hour&quot; ~ 1, job == &quot;2 hours&quot; ~ 2, job == &quot;3 hours&quot; ~ 3, job == &quot;4 or more hours&quot; ~ 4)) %&gt;% dplyr::select(student, sex, highgpa, sem, semc, job, hrs, gpa) %&gt;% dplyr::arrange(student, sem) psych::headTail(data_long, top = 10) student sex highgpa sem semc job hrs gpa 1 1 female 2.8 1 0 2 hours 2 2.3 2 1 female 2.8 2 1 2 hours 2 2.1 3 1 female 2.8 3 2 2 hours 2 3 4 1 female 2.8 4 3 2 hours 2 3 5 1 female 2.8 5 4 2 hours 2 3 6 1 female 2.8 6 5 2 hours 2 3.3 7 2 male 2.5 1 0 2 hours 2 2.2 8 2 male 2.5 2 1 3 hours 3 2.5 9 2 male 2.5 3 2 2 hours 2 2.6 10 2 male 2.5 4 3 2 hours 2 2.6 11 &lt;NA&gt; &lt;NA&gt; ... ... ... &lt;NA&gt; ... ... 12 200 male 3.4 3 2 2 hours 2 3.4 13 200 male 3.4 4 3 2 hours 2 3.5 14 200 male 3.4 5 4 1 hour 1 3.3 15 200 male 3.4 6 5 1 hour 1 3.4 data_wide &lt;- data_long %&gt;% furniture::wide(v.names = c(&quot;job&quot;, &quot;hrs&quot;, &quot;gpa&quot;), timevar = &quot;sem&quot;, id = &quot;student&quot;) psych::headTail(data_wide) student sex highgpa semc job.1 hrs.1 gpa.1 job.2 hrs.2 gpa.2 1 1 female 2.8 0 2 hours 2 2.3 2 hours 2 2.1 7 2 male 2.5 0 2 hours 2 2.2 3 hours 3 2.5 13 3 female 2.5 0 2 hours 2 2.4 2 hours 2 2.9 19 4 male 3.8 0 3 hours 3 2.5 2 hours 2 2.7 ... &lt;NA&gt; &lt;NA&gt; ... ... &lt;NA&gt; ... ... &lt;NA&gt; ... ... 1177 197 female 2.1 0 2 hours 2 2.9 2 hours 2 2.5 1183 198 male 4 0 2 hours 2 2.5 2 hours 2 2.9 1189 199 female 2.3 0 2 hours 2 2.6 2 hours 2 2.3 1195 200 male 3.4 0 2 hours 2 2.8 2 hours 2 3.2 job.3 hrs.3 gpa.3 job.4 hrs.4 gpa.4 job.5 hrs.5 gpa.5 job.6 1 2 hours 2 3 2 hours 2 3 2 hours 2 3 2 hours 7 2 hours 2 2.6 2 hours 2 2.6 2 hours 2 3 2 hours 13 2 hours 2 3 3 hours 3 2.8 2 hours 2 3.3 2 hours 19 2 hours 2 2.4 2 hours 2 2.7 2 hours 2 2.9 2 hours ... &lt;NA&gt; ... ... &lt;NA&gt; ... ... &lt;NA&gt; ... ... &lt;NA&gt; 1177 3 hours 3 2.8 2 hours 2 3.2 2 hours 2 3.3 2 hours 1183 2 hours 2 3 2 hours 2 3.2 1 hour 1 3.3 2 hours 1189 3 hours 3 2.7 2 hours 2 2.7 2 hours 2 2.8 3 hours 1195 2 hours 2 3.4 2 hours 2 3.5 1 hour 1 3.3 1 hour hrs.6 gpa.6 1 2 3.3 7 2 2.8 13 2 3.4 19 2 2.7 ... ... ... 1177 2 3.8 1183 2 3.4 1189 3 2.8 1195 1 3.4 data_wide %&gt;% furniture::table1(highgpa, gpa.1, job.1, hrs.1, splitby = ~ sex, output = &quot;html&quot;, test = TRUE) male female P-Value n = 95 n = 105 highgpa 0.309 3.0 (0.6) 2.9 (0.6) gpa 1 0.095 2.6 (0.3) 2.6 (0.3) job 1 0.192 1 hour 0 (0%) 0 (0%) 2 hours 78 (82.1%) 94 (89.5%) 3 hours 17 (17.9%) 11 (10.5%) hrs 1 0.137 2.2 (0.4) 2.1 (0.3) data_wide %&gt;% furniture::table1(gpa.1, gpa.2, gpa.3, gpa.4, gpa.5, gpa.6, splitby = ~ sex, output = &quot;html&quot;) male female n = 95 n = 105 gpa 1 2.6 (0.3) 2.6 (0.3) gpa 2 2.7 (0.3) 2.8 (0.3) gpa 3 2.7 (0.4) 2.9 (0.3) gpa 4 2.8 (0.4) 3.0 (0.3) gpa 5 2.9 (0.4) 3.1 (0.3) gpa 6 3.0 (0.4) 3.2 (0.4) 10.2 MLM 10.2.1 Null Models and ICC fit_lmer_0_re &lt;- lme4::lmer(gpa ~ 1 + (1|student), data = data_long, REML = TRUE) sjstats::icc(fit_lmer_0_re) Linear mixed model Family : gaussian (identity) Formula: gpa ~ 1 + (1 | student) ICC (student): 0.3693 Over a third of the variance in the 6 GPA measures is variance between individuals, and about two-thirds is variance within individuals across time. fit_lmer_1_re &lt;- lme4::lmer(gpa ~ semc + (1|student), data = data_long, REML = TRUE) sjstats::icc(fit_lmer_1_re) Linear mixed model Family : gaussian (identity) Formula: gpa ~ semc + (1 | student) ICC (student): 0.5231 After accounting for the linear change in GPA over semesters, about half of the remaining variance in GPA scores is attributable person-to-person differences. 10.2.2 Fixed Effects fit_lmer_0_ml &lt;- lme4::lmer(gpa ~ 1 + (1|student), data = data_long, REML = FALSE) fit_lmer_1_ml &lt;- lme4::lmer(gpa ~ semc + (1|student), data = data_long, REML = FALSE) fit_lmer_2_ml &lt;- lme4::lmer(gpa ~ semc + hrs + (1|student), data = data_long, REML = FALSE) fit_lmer_3_ml &lt;- lme4::lmer(gpa ~ semc + hrs + highgpa + sex + (1|student), data = data_long, REML = FALSE) screenreg(list(fit_lmer_0_ml, fit_lmer_1_ml, fit_lmer_2_ml, fit_lmer_3_ml), digits = 3) ================================================================================ Model 1 Model 2 Model 3 Model 4 -------------------------------------------------------------------------------- (Intercept) 2.865 *** 2.599 *** 2.970 *** 2.641 *** (0.019) (0.022) (0.044) (0.098) semc 0.106 *** 0.102 *** 0.102 *** (0.004) (0.004) (0.004) hrs -0.171 *** -0.172 *** (0.018) (0.018) highgpa 0.085 ** (0.028) sexfemale 0.147 *** (0.033) -------------------------------------------------------------------------------- AIC 919.456 401.649 318.399 296.760 BIC 934.726 422.009 343.849 332.390 Log Likelihood -456.728 -196.825 -154.200 -141.380 Num. obs. 1200 1200 1200 1200 Num. groups: student 200 200 200 200 Var: student (Intercept) 0.057 0.063 0.052 0.045 Var: Residual 0.098 0.058 0.055 0.055 ================================================================================ *** p &lt; 0.001, ** p &lt; 0.01, * p &lt; 0.05 anova(fit_lmer_0_ml, fit_lmer_1_ml, fit_lmer_2_ml, fit_lmer_3_ml) Data: data_long Models: fit_lmer_0_ml: gpa ~ 1 + (1 | student) fit_lmer_1_ml: gpa ~ semc + (1 | student) fit_lmer_2_ml: gpa ~ semc + hrs + (1 | student) fit_lmer_3_ml: gpa ~ semc + hrs + highgpa + sex + (1 | student) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) fit_lmer_0_ml 3 919.46 934.73 -456.73 913.46 fit_lmer_1_ml 4 401.65 422.01 -196.82 393.65 519.807 1 &lt; 2.2e-16 fit_lmer_2_ml 5 318.40 343.85 -154.20 308.40 85.250 1 &lt; 2.2e-16 fit_lmer_3_ml 7 296.76 332.39 -141.38 282.76 25.639 2 2.707e-06 fit_lmer_0_ml fit_lmer_1_ml *** fit_lmer_2_ml *** fit_lmer_3_ml *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 10.2.3 Variance Explained by linear TIME at Level ONE sjstats::re_var(fit_lmer_0_ml) # baseilne Within-group-variance: 0.098 Between-group-variance: 0.057 (student) sjstats::re_var(fit_lmer_1_ml) # model to compare Within-group-variance: 0.058 Between-group-variance: 0.063 (student) 10.2.3.1 Raudenbush and Bryk Explained variance is a proportion of first-level variance only A good option when the multilevel sampling process is is close to two-stage simple random sampling Raudenbush and Bryk Approximate Formula - Level 1 approximate \\[ approx \\;R^2_1 = \\frac{\\sigma^2_{e-BL} - \\sigma^2_{e-MC}} {\\sigma^2_{e-BL} } \\tag{Hox 4.8} \\] (0.098 - 0.058) / 0.098 [1] 0.4081633 10.2.3.2 Snijders and Bosker Snijders and Bosker Formula - Level 1 Random Intercepts Models Only, address potential negative \\(R^2\\) issue \\[ R^2_1 = 1 - \\frac{\\sigma^2_{e-MC} + \\sigma^2_{u0-MC}} {\\sigma^2_{e-BL} + \\sigma^2_{u0-BL}} \\] 1 - (0.058 + 0.063)/(0.098 + 0.057) [1] 0.2193548 10.2.4 Variance Explained by linear TIME at Level TWO 10.2.4.1 Raudenbush and Bryk Raudenbush and Bryk Approximate Formula - Level 2 \\[ approx \\; R^2_s = \\frac{\\sigma^2_{u0-BL} - \\sigma^2_{u0-MC}} {\\sigma^2_{u0-BL} } \\tag{Hox 4.9} \\] (0.057 - 0.063)/ 0.057 [1] -0.1052632 YIKES! Negative Variance explained! 10.2.4.2 Snijders and Bosker Snijders and Bosker Formula Extended - Level 2 \\[ R^2_2 = 1 - \\frac{\\frac{\\sigma^2_{e-MC}}{B} + \\sigma^2_{u0-MC}} {\\frac{\\sigma^2_{e-BL}}{B} + \\sigma^2_{u0-BL}} \\] \\(B\\) is the average size of the Level 2 units. Technically, you should use the harmonic mean, but unless the clusters differ greatly in size, it doesn’t make a huge difference. 1 - (0.058/6 + 0.063) / (0.098/6 + 0.057) [1] 0.009090909 Reason: The intercept only model overestimates the variance at the occasion level and underestimates the variance at the subject level (se chapter 4 of Hox, Moerbeek, and Van de Schoot (2017)) 10.2.5 Random Effects fit_lmer_3_re &lt;- lme4::lmer(gpa ~ semc + hrs + highgpa + sex + (1|student), data = data_long, REML = TRUE) fit_lmer_4_re &lt;- lme4::lmer(gpa ~ semc + hrs + highgpa + sex + (semc|student), data = data_long, REML = TRUE) anova(fit_lmer_3_re, fit_lmer_4_re, refit = FALSE) Data: data_long Models: fit_lmer_3_re: gpa ~ semc + hrs + highgpa + sex + (1 | student) fit_lmer_4_re: gpa ~ semc + hrs + highgpa + sex + (semc | student) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) fit_lmer_3_re 7 328.84 364.47 -157.42 314.84 fit_lmer_4_re 9 219.93 265.75 -100.97 201.94 112.9 2 &lt; 2.2e-16 fit_lmer_3_re fit_lmer_4_re *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 10.2.6 Cross-Level Interaction fit_lmer_4_ml &lt;- lme4::lmer(gpa ~ semc + hrs + highgpa + sex + (semc|student), data = data_long, REML = FALSE) fit_lmer_4_re &lt;- lme4::lmer(gpa ~ semc + hrs + highgpa + sex + (semc|student), data = data_long, REML = TRUE) fit_lmer_5_ml &lt;- lme4::lmer(gpa ~ semc + hrs + highgpa + sex + semc:sex + (semc|student), data = data_long, REML = FALSE) fit_lmer_5_re &lt;- lme4::lmer(gpa ~ semc + hrs + highgpa + sex + semc:sex + (semc|student), data = data_long, REML = FALSE) texreg::screenreg(list(fit_lmer_4_ml, fit_lmer_5_ml, fit_lmer_5_re)) ==================================================================== Model 1 Model 2 Model 3 -------------------------------------------------------------------- (Intercept) 2.56 *** 2.58 *** 2.58 *** (0.09) (0.09) (0.09) semc 0.10 *** 0.09 *** 0.09 *** (0.01) (0.01) (0.01) hrs -0.13 *** -0.13 *** -0.13 *** (0.02) (0.02) (0.02) highgpa 0.09 *** 0.09 *** 0.09 *** (0.03) (0.03) (0.03) sexfemale 0.12 *** 0.08 * 0.08 * (0.03) (0.03) (0.03) semc:sexfemale 0.03 ** 0.03 ** (0.01) (0.01) -------------------------------------------------------------------- AIC 188.12 182.97 182.97 BIC 233.93 233.87 233.87 Log Likelihood -85.06 -81.49 -81.49 Num. obs. 1200 1200 1200 Num. groups: student 200 200 200 Var: student (Intercept) 0.04 0.04 0.04 Var: student semc 0.00 0.00 0.00 Cov: student (Intercept) semc -0.00 -0.00 -0.00 Var: Residual 0.04 0.04 0.04 ==================================================================== *** p &lt; 0.001, ** p &lt; 0.01, * p &lt; 0.05 anova(fit_lmer_4_ml, fit_lmer_5_ml) Data: data_long Models: fit_lmer_4_ml: gpa ~ semc + hrs + highgpa + sex + (semc | student) fit_lmer_5_ml: gpa ~ semc + hrs + highgpa + sex + semc:sex + (semc | student) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) fit_lmer_4_ml 9 188.12 233.93 -85.059 170.12 fit_lmer_5_ml 10 182.97 233.87 -81.486 162.97 7.1464 1 0.007511 fit_lmer_4_ml fit_lmer_5_ml ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 sjPlot::plot_model(fit_lmer_5_re, type = &quot;pred&quot;, terms = c(&quot;semc&quot;, &quot;sex&quot;)) "],
["longitudinal-mlm-rct-exercise-and-diet.html", "11 Longitudinal MLM: RCT - Exercise and Diet 11.1 The dataset 11.2 Exploratory Data Analysis 11.3 Multilevel Modeling", " 11 Longitudinal MLM: RCT - Exercise and Diet library(tidyverse) # all things tidy library(pander) # nice looking genderal tabulations library(furniture) # nice table1() descriptives library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(psych) # contains some useful functions, like headTail library(sjstats) # ICC calculations library(sjPlot) # Visualization for Models library(effects) # Effec displays for Models library(lme4) # non-linear mixed-effects models 11.1 The dataset This comes from a Randomized Controled Trial. data_raw &lt;- read.table(&quot;https://raw.githubusercontent.com/CEHS-research/data/master/MLM/exercise_diet.txt&quot;, header = TRUE, sep = &quot;,&quot;) tibble::glimpse(data_raw) Observations: 120 Variables: 5 $ id &lt;int&gt; 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5,... $ exertype &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... $ diet &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... $ pulse &lt;int&gt; 90, 92, 93, 93, 90, 92, 93, 93, 97, 97, 94, 94, 80, 8... $ time &lt;int&gt; 0, 228, 296, 639, 0, 56, 434, 538, 0, 150, 295, 541, ... data_long &lt;- data_raw %&gt;% dplyr::mutate(id = id %&gt;% factor) %&gt;% dplyr::mutate(exertype = exertype %&gt;% factor(levels = 1:3, labels = c(&quot;At Rest&quot;, &quot;Leisurely Walking&quot;, &quot;Moderate Running&quot;))) %&gt;% dplyr::mutate(diet = diet %&gt;% factor(levels = 1:2, labels = c(&quot;low-fat&quot;, &quot;non-fat&quot;))) %&gt;% dplyr::mutate(time_min = time / 60) data_long %&gt;% psych::headTail(top = 10, bottom = 10) %&gt;% pander::pander(caption = &quot;Raw Data&quot;) Raw Data id exertype diet pulse time time_min 1 1 At Rest low-fat 90 0 0 2 1 At Rest low-fat 92 228 3.8 3 1 At Rest low-fat 93 296 4.93 4 1 At Rest low-fat 93 639 10.65 5 2 At Rest low-fat 90 0 0 6 2 At Rest low-fat 92 56 0.93 7 2 At Rest low-fat 93 434 7.23 8 2 At Rest low-fat 93 538 8.97 9 3 At Rest low-fat 97 0 0 10 3 At Rest low-fat 97 150 2.5 … NA NA NA … … … 111 28 Moderate Running non-fat 140 263 4.38 112 28 Moderate Running non-fat 143 588 9.8 113 29 Moderate Running non-fat 94 0 0 114 29 Moderate Running non-fat 135 164 2.73 115 29 Moderate Running non-fat 130 353 5.88 116 29 Moderate Running non-fat 137 560 9.33 117 30 Moderate Running non-fat 99 0 0 118 30 Moderate Running non-fat 111 114 1.9 119 30 Moderate Running non-fat 140 362 6.03 120 30 Moderate Running non-fat 148 501 8.35 11.2 Exploratory Data Analysis 11.2.1 Demographic Summary data_long %&gt;% dplyr::filter(time == 0) %&gt;% furniture::table1(diet, pulse, splitby = ~ exertype, caption = &quot;Baseline Descriptives&quot;, output = &quot;html&quot;, test = TRUE) Table 11.1: Baseline Descriptives At Rest Leisurely Walking Moderate Running P-Value n = 10 n = 10 n = 10 diet 1 low-fat 5 (50%) 5 (50%) 5 (50%) non-fat 5 (50%) 5 (50%) 5 (50%) pulse 0.129 90.7 (6.3) 93.1 (6.3) 96.1 (4.5) 11.2.2 Baseline Summary data_long %&gt;% dplyr::filter(time == 0) %&gt;% dplyr::group_by(exertype, diet) %&gt;% dplyr::summarise(mean = mean(pulse)) %&gt;% tidyr::spread(key = diet, value = mean) %&gt;% pander::pander(caption = &quot;Baseline Pulse, Means&quot;) Baseline Pulse, Means exertype low-fat non-fat At Rest 90 92 Leisurely Walking 91 96 Moderate Running 94 98 11.2.3 Raw Trajectories - Person Profile Plot 11.2.3.1 Connect the dots data_long %&gt;% ggplot(aes(x = time_min, y = pulse)) + geom_point() + geom_line(aes(group = id)) + facet_grid(diet ~ exertype) + theme_bw() 11.2.3.2 Loess - Moving Average Smoothers data_long %&gt;% ggplot(aes(x = time_min, y = pulse, color = diet)) + geom_line(aes(group = id)) + facet_grid(~ exertype) + theme_bw() + geom_smooth(method = &quot;loess&quot;, se = FALSE, size = 2, span = 5) + theme(legend.position = c(0.08, 0.85), legend.background = element_rect(color = &quot;black&quot;)) + labs(title = &quot;Raw Pulse Trajectories&quot;, subtitle = &quot;By Exercise and Diet Groupings&quot;, x = &quot;Time (Minutes Post-Baseline)&quot;, y = &quot;Pulse (Beats per Minute)&quot;, color = &quot;Diet Plan&quot;) 11.3 Multilevel Modeling 11.3.1 Null Model fit_lmer_0re &lt;- lme4::lmer(pulse ~ 1 + (1 | id), data = data_long) texreg::htmlreg(fit_lmer_0re) Statistical models Model 1 (Intercept) 102.13*** (2.54) AIC 963.89 BIC 972.25 Log Likelihood -478.95 Num. obs. 120 Num. groups: id 30 Var: id (Intercept) 165.84 Var: Residual 109.39 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 11.3.2 ICC sjstats::icc(fit_lmer_0re) Linear mixed model Family : gaussian (identity) Formula: pulse ~ 1 + (1 | id) ICC (id): 0.6025 11.3.3 Add fixed effects: level specific 11.3.3.1 Fit nested models # Null Model (random intercept only) fit_lmer_0ml &lt;- lme4::lmer(pulse ~ 1 + (1 | id), data = data_long, REML = FALSE) # Add quadratic time fit_lmer_1ml &lt;- lme4::lmer(pulse ~ time_min + I(time_min^2) + (1 | id), data = data_long, REML = FALSE) # Add main effects for 2 interventions (person-specific, i.e. level-2 factors) fit_lmer_2ml &lt;- lme4::lmer(pulse ~ diet + exertype + time_min + I(time_min^2) + (1 | id), data = data_long, REML = FALSE) # Add interaction between level-2 factors fit_lmer_3ml &lt;- lme4::lmer(pulse ~ diet*exertype + time_min + I(time_min^2) + (1 | id), data = data_long, REML = FALSE) # Add exercise interacting with [time &amp; time-squared] fit_lmer_4ml &lt;- lme4::lmer(pulse ~ diet*exertype + exertype*time_min + exertype*I(time_min^2) + (1 | id), data = data_long, REML = FALSE) # Add diet interacting with [time &amp; time-squared] fit_lmer_5ml &lt;- lme4::lmer(pulse ~ diet*exertype*time_min + diet*exertype*I(time_min^2) + (1 | id), data = data_long, REML = FALSE) texreg::htmlreg(list(fit_lmer_1ml, fit_lmer_2ml, fit_lmer_3ml, fit_lmer_4ml, fit_lmer_5ml)) Statistical models Model 1 Model 2 Model 3 Model 4 Model 5 (Intercept) 94.05*** 79.30*** 82.15*** 89.89*** 89.81*** (2.71) (2.46) (2.64) (2.69) (2.78) time_min 3.57*** 3.58*** 3.44*** 0.24 0.37 (0.65) (0.64) (0.64) (0.62) (0.87) I(time_min^2) -0.21*** -0.21*** -0.20*** -0.01 -0.03 (0.06) (0.06) (0.06) (0.05) (0.09) dietnon-fat 8.36*** 2.89 1.99 2.11 (2.21) (3.36) (3.45) (3.89) exertypeLeisurely Walking 5.20 3.81 0.84 1.40 (2.70) (3.34) (3.78) (3.92) exertypeModerate Running 26.43*** 19.71*** 0.53 5.71 (2.70) (3.34) (3.77) (3.92) dietnon-fat:exertypeLeisurely Walking 2.83 3.70 2.53 (4.73) (4.86) (5.50) dietnon-fat:exertypeModerate Running 13.47** 14.02** 3.99 (4.74) (4.86) (5.50) exertypeLeisurely Walking:time_min 1.17 1.09 (0.87) (1.17) exertypeModerate Running:time_min 8.19*** 5.77*** (0.90) (1.20) exertypeLeisurely Walking:I(time_min^2) -0.07 -0.08 (0.08) (0.11) exertypeModerate Running:I(time_min^2) -0.48*** -0.33** (0.08) (0.11) dietnon-fat:time_min -0.17 (1.14) dietnon-fat:I(time_min^2) 0.02 (0.10) dietnon-fat:exertypeLeisurely Walking:time_min 0.21 (1.56) dietnon-fat:exertypeModerate Running:time_min 4.42** (1.61) dietnon-fat:exertypeLeisurely Walking:I(time_min^2) 0.01 (0.14) dietnon-fat:exertypeModerate Running:I(time_min^2) -0.27 (0.15) AIC 927.70 884.96 881.11 785.34 769.23 BIC 941.64 907.26 908.99 824.36 824.98 Log Likelihood -458.85 -434.48 -430.56 -378.67 -364.62 Num. obs. 120 120 120 120 120 Num. groups: id 30 30 30 30 30 Var: id (Intercept) 167.58 19.46 11.03 24.13 25.64 Var: Residual 67.47 67.47 67.52 20.95 15.32 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 11.3.3.2 Evaluate Model Fit, i.e. variable significance anova(fit_lmer_1ml, fit_lmer_2ml, fit_lmer_3ml, fit_lmer_4ml, fit_lmer_5ml) Data: data_long Models: fit_lmer_1ml: pulse ~ time_min + I(time_min^2) + (1 | id) fit_lmer_2ml: pulse ~ diet + exertype + time_min + I(time_min^2) + (1 | id) fit_lmer_3ml: pulse ~ diet * exertype + time_min + I(time_min^2) + (1 | id) fit_lmer_4ml: pulse ~ diet * exertype + exertype * time_min + exertype * I(time_min^2) + fit_lmer_4ml: (1 | id) fit_lmer_5ml: pulse ~ diet * exertype * time_min + diet * exertype * I(time_min^2) + fit_lmer_5ml: (1 | id) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) fit_lmer_1ml 5 927.70 941.64 -458.85 917.70 fit_lmer_2ml 8 884.96 907.26 -434.48 868.96 48.742 3 1.480e-10 fit_lmer_3ml 10 881.11 908.99 -430.56 861.11 7.847 2 0.01977 fit_lmer_4ml 14 785.34 824.36 -378.67 757.34 103.776 4 &lt; 2.2e-16 fit_lmer_5ml 20 769.23 824.98 -364.62 729.23 28.108 6 8.968e-05 fit_lmer_1ml fit_lmer_2ml *** fit_lmer_3ml * fit_lmer_4ml *** fit_lmer_5ml *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 11.3.4 Final Model Refit via REML fit_lmer_5re &lt;- lme4::lmer(pulse ~ diet*exertype*time_min + diet*exertype*I(time_min^2) + (1 | id), data = data_long, REML = TRUE) 11.3.4.1 Visualize sjPlot::plot_model(fit_lmer_5re, type = &quot;pred&quot;, terms = c(&quot;time_min&quot;, &quot;diet&quot;, &quot;exertype&quot;)) effects::Effect(focal.predictors = c(&quot;diet&quot;, &quot;exertype&quot;, &quot;time_min&quot;), mod = fit_lmer_5re) %&gt;% data.frame %&gt;% ggplot(aes(x = time_min, y = fit, fill = diet, color = diet)) + geom_line(size = 1.5) + theme_bw() + facet_grid(~ exertype) + theme(legend.position = c(0.08, 0.85), legend.background = element_rect(color = &quot;black&quot;)) + labs(title = &quot;Raw Pulse Trajectories&quot;, subtitle = &quot;By Exercise and Diet Groupings&quot;, x = &quot;Time (Minutes Post-Baseline)&quot;, y = &quot;Estimated Marginal Mean\\nPulse (Beats per Minute)&quot;, fill = &quot;Diet Plan&quot;, color = &quot;Diet Plan&quot;) effects::Effect(focal.predictors = c(&quot;diet&quot;, &quot;exertype&quot;, &quot;time_min&quot;), mod = fit_lmer_5re, xlevels = list(&quot;time_min&quot; = seq(from = 0, to = 12, by = 0.5))) %&gt;% data.frame %&gt;% dplyr::mutate(diet = fct_rev(diet)) %&gt;% # reverse the order of the levels ggplot(aes(x = time_min, y = fit)) + geom_ribbon(aes(ymin = fit - se, ymax = fit + se, fill = diet), alpha = 0.3) + geom_line(aes(color = diet, linetype = diet), size = 1) + theme_bw() + facet_grid(~ exertype) + theme(legend.position = c(0.12, 0.85), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(2, &quot;cm&quot;)) + labs(title = &quot;Raw Pulse Trajectories&quot;, subtitle = &quot;By Exercise and Diet Groupings&quot;, x = &quot;Time (Minutes Post-Baseline)&quot;, y = &quot;Estimated Marginal Mean\\nPulse (Beats per Minute)&quot;, fill = &quot;Diet Plan&quot;, color = &quot;Diet Plan&quot;, linetype = &quot;Diet Plan&quot;) + scale_color_manual(values = c(&quot;gray50&quot;, &quot;black&quot;)) + scale_fill_manual(values = c(&quot;gray50&quot;, &quot;black&quot;)) + scale_x_continuous(breaks = seq(from = 0, to = 14, by = 5)) "],
["gee-continuous-outcome-beat-the-blues.html", "12 GEE, Continuous Outcome: Beat the Blues 12.1 Prepare and get to know the dataset 12.2 Multiple Regression (OLS) 12.3 Multilevel Models (MLM) 12.4 General Estimating Equations, GEE", " 12 GEE, Continuous Outcome: Beat the Blues library(tidyverse) # all things tidy library(pander) # nice looking genderal tabulations library(furniture) # nice table1() descriptives library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(psych) # contains some useful functions, like headTail library(lme4) # Linear, generalized linear, &amp; nonlinear mixed models library(corrplot) # Vizualize correlation matrix library(gee) # Genderalized Estimation Equation Solver library(geepack) # Genderalized Estimation Equation Package library(MuMIn) # Multi-Model Inference (caluclate QIC) library(HSAUR) # package with the dataset 12.1 Prepare and get to know the dataset 12.1.1 Read in the data This data set is in the HSAUR package. It is from a clinical trial of an interactive multimedia program called ‘Beat the Blues’. The variables are as follows: drug did the patient take anti-depressant drugs (No or Yes) length the length of the current episode of depression, a factor with levels: “&lt;6m” less than six months “&gt;6m” more than six months treatment treatment group, a factor with levels: “TAU” treatment as usual “BtheB” Beat the Blues bdi.pre Beck Depression Inventory II, before treatment bdi.2m Beck Depression Inventory II, after 2 months bdi.4m Beck Depression Inventory II, after 4 months bdi.6m Beck Depression Inventory II, after 6 months bdi.8m Beck Depression Inventory II, after 8 months data(BtheB, package = &quot;HSAUR&quot;) BtheB %&gt;% psych::headTail() drug length treatment bdi.pre bdi.2m bdi.4m bdi.6m bdi.8m 1 No &gt;6m TAU 29 2 2 &lt;NA&gt; &lt;NA&gt; 2 Yes &gt;6m BtheB 32 16 24 17 20 3 Yes &lt;6m TAU 25 20 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 4 No &gt;6m BtheB 21 17 16 10 9 ... &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ... ... ... ... ... 97 Yes &lt;6m TAU 28 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 98 No &gt;6m BtheB 11 22 9 11 11 99 No &lt;6m TAU 13 5 5 0 6 100 Yes &lt;6m TAU 43 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 12.1.2 Tidy up the dataset btb_wide &lt;- BtheB %&gt;% dplyr::mutate(id = row_number()) %&gt;% # create a new variable to ID participants dplyr::select(id, treatment, # specify that ID variable is first drug, length, bdi.pre, bdi.2m, bdi.4m, bdi.6m, bdi.8m) btb_wide %&gt;% furniture::table1(bdi.pre, bdi.2m, bdi.4m, bdi.6m, bdi.8m, splitby = ~ treatment, test = TRUE, output = &quot;html&quot;) TAU BtheB P-Value n = 25 n = 27 bdi pre 0.427 24.1 (8.1) 22.0 (10.9) bdi 2m 0.001 20.1 (11.5) 10.9 (6.5) bdi 4m 0.01 17.8 (12.2) 10.3 (6.9) bdi 6m 0.038 15.9 (12.8) 9.5 (8.1) bdi 8m 0.074 13.6 (11.5) 8.9 (6.1) 12.1.3 Restructure to long format btb_long &lt;- btb_wide %&gt;% tidyr::gather(key = month, value = bdi, bdi.2m, bdi.4m, bdi.6m, bdi.8m) %&gt;% dplyr::mutate(month = case_when(month == &quot;bdi.2m&quot; ~ 2, month == &quot;bdi.4m&quot; ~ 4, month == &quot;bdi.6m&quot; ~ 6, month == &quot;bdi.8m&quot; ~ 8)) %&gt;% dplyr::filter(complete.cases(id, bdi, treatment, month)) %&gt;% dplyr::arrange(id, month) %&gt;% dplyr::select(id, treatment, drug, length, bdi.pre, month, bdi) btb_long %&gt;% psych::headTail(top = 10, bottom = 10) id treatment drug length bdi.pre month bdi 1 1 TAU No &gt;6m 29 2 2 2 1 TAU No &gt;6m 29 4 2 3 2 BtheB Yes &gt;6m 32 2 16 4 2 BtheB Yes &gt;6m 32 4 24 5 2 BtheB Yes &gt;6m 32 6 17 6 2 BtheB Yes &gt;6m 32 8 20 7 3 TAU Yes &lt;6m 25 2 20 8 4 BtheB No &gt;6m 21 2 17 9 4 BtheB No &gt;6m 21 4 16 10 4 BtheB No &gt;6m 21 6 10 ... ... &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ... ... ... 271 96 BtheB Yes &gt;6m 16 6 10 272 96 BtheB Yes &gt;6m 16 8 8 273 98 BtheB No &gt;6m 11 2 22 274 98 BtheB No &gt;6m 11 4 9 275 98 BtheB No &gt;6m 11 6 11 276 98 BtheB No &gt;6m 11 8 11 277 99 TAU No &lt;6m 13 2 5 278 99 TAU No &lt;6m 13 4 5 279 99 TAU No &lt;6m 13 6 0 280 99 TAU No &lt;6m 13 8 6 12.1.4 Visualize: Person-profile Plots Create spaghetti plots of the raw, observed data btb_long %&gt;% ggplot(aes(x = month, y = bdi)) + geom_point() + geom_line(aes(group = id), size = 1, alpha = 0.3) + geom_smooth(method = &quot;lm&quot;) + theme_bw() + facet_grid(.~ treatment) + labs(title = &quot;BtheB - Observed Data Across Time with LM Smoother&quot;, subtitle = &quot;Seperate by Treatment&quot;) btb_long %&gt;% ggplot(aes(x = month, y = bdi)) + geom_point() + geom_line(aes(group = id), size = 1, alpha = 0.3) + geom_smooth(method = &quot;lm&quot;) + facet_grid(drug~ treatment, labeller = label_both) + theme_bw() + labs(title = &quot;BtheB - Observed Data Across Time with LM Smoother&quot;, subtitle = &quot;Seperate by Treatment &amp; Antidepressant Use&quot;) btb_long %&gt;% ggplot(aes(x = month, y = bdi)) + geom_point() + geom_line(aes(group = id, color = length), size = 1, alpha = 0.3) + geom_smooth(aes(color = length), method = &quot;lm&quot;, size = 1.25, se = FALSE) + facet_grid(drug~ treatment, labeller = label_both) + theme_bw() + labs(title = &quot;BtheB - Observed Data Across Time with LM Smoother&quot;, subtitle = &quot;Seperate by Treatment, Antidepressant Use, &amp; Length of Current Episode&quot;) btb_long %&gt;% ggplot(aes(x = month, y = bdi, color = treatment, fill = treatment)) + geom_smooth(method = &quot;lm&quot;) + theme_bw() + labs(title = &quot;BtheB - Predictions from TWO Seperate Single Simple Linear Models (lm)&quot;, subtitle = &quot;Assumes Independence of Repeated Measures&quot;) 12.1.5 Calculate the Observed Correlation Structure bdi_corr &lt;- btb_wide %&gt;% dplyr::select(starts_with(&quot;bdi&quot;)) %&gt;% stats::cor(use=&quot;pairwise.complete.obs&quot;) bdi_corr bdi.pre bdi.2m bdi.4m bdi.6m bdi.8m bdi.pre 1.0000000 0.6142207 0.5691248 0.5077286 0.3835090 bdi.2m 0.6142207 1.0000000 0.7903346 0.7849188 0.7038158 bdi.4m 0.5691248 0.7903346 1.0000000 0.8166591 0.7220149 bdi.6m 0.5077286 0.7849188 0.8166591 1.0000000 0.8107773 bdi.8m 0.3835090 0.7038158 0.7220149 0.8107773 1.0000000 12.1.6 Plot the correlation matrix to get a better feel for the pattern corrplot::corrplot.mixed(bdi_corr) 12.2 Multiple Regression (OLS) This ignores any correlation between repeated measures on the same individual and treats all observations as independent. 12.2.1 Fit the models btb_lm_1 &lt;- stats::lm(bdi ~ bdi.pre + length + drug + treatment + month, data = btb_long) btb_lm_2 &lt;- stats::lm(bdi ~ bdi.pre + length + drug + treatment*month, data = btb_long) btb_lm_3 &lt;- stats::lm(bdi ~ bdi.pre + length + drug + treatment + drug*month, data = btb_long) btb_lm_4 &lt;- stats::lm(bdi ~ bdi.pre + length + drug*treatment*month, data = btb_long) 12.2.2 Parameter Estimates Table # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(btb_lm_1, btb_lm_2, btb_lm_3, btb_lm_4), caption = &quot;OLS&quot;) OLS Model 1 Model 2 Model 3 Model 4 (Intercept) 7.88*** 7.77*** 7.21*** 7.33** (1.78) (2.08) (2.03) (2.30) bdi.pre 0.57*** 0.57*** 0.57*** 0.56*** (0.05) (0.05) (0.05) (0.05) length&gt;6m 1.75 1.75 1.78 1.86 (1.11) (1.11) (1.11) (1.10) drugYes -3.55** -3.55** -2.10 -2.00 (1.14) (1.15) (2.39) (3.75) treatmentBtheB -3.35** -3.13 -3.36** -3.31 (1.10) (2.36) (1.10) (3.13) month -0.96*** -0.93** -0.82** -0.60 (0.23) (0.34) (0.31) (0.40) treatmentBtheB:month -0.05 -0.56 (0.47) (0.63) drugYes:month -0.32 -1.02 (0.47) (0.73) drugYes:treatmentBtheB -0.23 (4.92) drugYes:treatmentBtheB:month 1.31 (0.98) R2 0.40 0.40 0.40 0.42 Adj. R2 0.39 0.38 0.39 0.40 Num. obs. 280 280 280 280 RMSE 8.65 8.67 8.66 8.58 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 12.2.3 Plot the model predictions effects::Effect(focal.predictors = c(&quot;treatment&quot;, &quot;month&quot;), mod = btb_lm_1) %&gt;% data.frame %&gt;% dplyr::mutate(treatment = fct_reorder2(treatment, month, fit)) %&gt;% ggplot(aes(x = month, y = fit)) + geom_line(aes(color = treatment)) + geom_ribbon(aes(ymin = lower, ymax = upper, fill = treatment), alpha = 0.3) + geom_ribbon(aes(ymin = fit - se, ymax = fit + se, fill = treatment), alpha = 0.3) + theme_bw() + labs(title = &quot;BtheB - Predictions from a Single Linear Model (lm)&quot;, subtitle = &quot;Assumes Independence of Repeated Measures&quot;) + theme(legend.position = c(1, 1), legend.justification = c(1.1, 1.1), legend.background = element_rect(color = &quot;black&quot;)) 12.3 Multilevel Models (MLM) 12.3.1 Fit the models btb_lmer_RI &lt;- lme4::lmer(bdi ~ bdi.pre + length + drug + treatment + month + (1 | id), data = btb_long, REML = TRUE) btb_lmer_RIAS &lt;- lme4::lmer(bdi ~ bdi.pre + length + drug + treatment + month + (month | id), data = btb_long, REML = TRUE) 12.3.2 Parameter Estimates Table # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(btb_lm_1, btb_lmer_RI, btb_lmer_RIAS), custom.model.names = c(&quot;OLS&quot;, &quot;MLM-RI&quot;, &quot;MLM-RIAS&quot;)) Statistical models OLS MLM-RI MLM-RIAS (Intercept) 7.88*** 5.92* 5.94** (1.78) (2.31) (2.30) bdi.pre 0.57*** 0.64*** 0.64*** (0.05) (0.08) (0.08) length&gt;6m 1.75 0.24 0.10 (1.11) (1.68) (1.67) drugYes -3.55** -2.79 -2.89 (1.14) (1.77) (1.76) treatmentBtheB -3.35** -2.36 -2.49 (1.10) (1.71) (1.71) month -0.96*** -0.71*** -0.70*** (0.23) (0.15) (0.16) R2 0.40 Adj. R2 0.39 Num. obs. 280 280 280 RMSE 8.65 AIC 1882.08 1885.16 BIC 1911.16 1921.50 Log Likelihood -933.04 -932.58 Num. groups: id 97 97 Var: id (Intercept) 51.44 50.56 Var: Residual 25.27 23.87 Var: id month 0.23 Cov: id (Intercept) month -0.31 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 12.3.3 Likelihood Ratio Test anova(btb_lmer_RI, btb_lmer_RIAS, refit = FALSE) Data: btb_long Models: btb_lmer_RI: bdi ~ bdi.pre + length + drug + treatment + month + (1 | id) btb_lmer_RIAS: bdi ~ bdi.pre + length + drug + treatment + month + (month | btb_lmer_RIAS: id) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) btb_lmer_RI 8 1882.1 1911.2 -933.04 1866.1 btb_lmer_RIAS 10 1885.2 1921.5 -932.58 1865.2 0.9236 2 0.6301 12.3.4 Plot the model predictions effects::Effect(c(&quot;treatment&quot;, &quot;month&quot;, &quot;drug&quot;), mod = btb_lmer_RI) %&gt;% data.frame %&gt;% dplyr::mutate(treatment = fct_reorder2(treatment, month, fit)) %&gt;% ggplot(aes(x = month, y = fit)) + geom_line(aes(color = treatment)) + geom_ribbon(aes(ymin = lower, ymax = upper, fill = treatment), alpha = 0.3) + theme_bw() + facet_grid(.~ drug, labeller = label_both) + labs(title = &quot;BtheB - Predictions from a Multilevel Model (lmer)&quot;) + theme(legend.position = c(0, 0), legend.justification = c(-0.1, -0.1), legend.background = element_rect(color = &quot;black&quot;)) 12.4 General Estimating Equations, GEE 12.4.1 Fit the models Use the gee() function from the gee package for the results to be used in a texreg() table. btb_gee_in &lt;- gee::gee(bdi ~ bdi.pre + length + drug + treatment + month, data = btb_long, id = id, family = gaussian, corstr = &#39;independence&#39;) (Intercept) bdi.pre length&gt;6m drugYes treatmentBtheB 7.8830747 0.5723729 1.7530800 -3.5460058 -3.3539662 month -0.9608077 btb_gee_ex &lt;- gee::gee(bdi ~ bdi.pre + length + drug + treatment + month, data = btb_long, id = id, family = gaussian, corstr = &#39;exchangeable&#39;) (Intercept) bdi.pre length&gt;6m drugYes treatmentBtheB 7.8830747 0.5723729 1.7530800 -3.5460058 -3.3539662 month -0.9608077 # The AR-1 fails if any subjects have only 1 observation # to use this one, we would need to remove participants with only 1 BDI # btb_gee_ar &lt;- gee(bdi ~ bdi.pre + length + drug + treatment + month, # data = btb_long, # id = id, # family = gaussian, # corstr = &#39;AR-M&#39;, # Mv = 1) btb_gee_un &lt;- gee::gee(bdi ~ bdi.pre + length + drug + treatment + month, data = btb_long, id = id, family = gaussian, corstr = &#39;unstructured&#39;) (Intercept) bdi.pre length&gt;6m drugYes treatmentBtheB 7.8830747 0.5723729 1.7530800 -3.5460058 -3.3539662 month -0.9608077 summary(btb_gee_in) GEE: GENERALIZED LINEAR MODELS FOR DEPENDENT DATA gee S-function, version 4.13 modified 98/01/27 (1998) Model: Link: Identity Variance to Mean Relation: Gaussian Correlation Structure: Independent Call: gee::gee(formula = bdi ~ bdi.pre + length + drug + treatment + month, id = id, data = btb_long, family = gaussian, corstr = &quot;independence&quot;) Summary of Residuals: Min 1Q Median 3Q Max -24.20158432 -5.31202378 0.01101526 5.29503741 27.77789553 Coefficients: Estimate Naive S.E. Naive z Robust S.E. Robust z (Intercept) 7.8830747 1.78048908 4.427477 2.19973944 3.583640 bdi.pre 0.5723729 0.05486079 10.433188 0.08853253 6.465114 length&gt;6m 1.7530800 1.10849861 1.581490 1.41954159 1.234962 drugYes -3.5460058 1.14469086 -3.097785 1.73069664 -2.048889 treatmentBtheB -3.3539662 1.09831939 -3.053726 1.71390982 -1.956909 month -0.9608077 0.23263437 -4.130119 0.17688635 -5.431780 Estimated Scale Parameter: 74.8854 Number of Iterations: 1 Working Correlation [,1] [,2] [,3] [,4] [1,] 1 0 0 0 [2,] 0 1 0 0 [3,] 0 0 1 0 [4,] 0 0 0 1 summary(btb_gee_ex) GEE: GENERALIZED LINEAR MODELS FOR DEPENDENT DATA gee S-function, version 4.13 modified 98/01/27 (1998) Model: Link: Identity Variance to Mean Relation: Gaussian Correlation Structure: Exchangeable Call: gee::gee(formula = bdi ~ bdi.pre + length + drug + treatment + month, id = id, data = btb_long, family = gaussian, corstr = &quot;exchangeable&quot;) Summary of Residuals: Min 1Q Median 3Q Max -25.4478843 -6.3276726 -0.8152833 4.3622258 25.4078115 Coefficients: Estimate Naive S.E. Naive z Robust S.E. Robust z (Intercept) 5.8855129 2.32380381 2.5327065 2.10712166 2.7931529 bdi.pre 0.6399964 0.08033495 7.9665999 0.07931263 8.0692874 length&gt;6m 0.2084783 1.69179766 0.1232288 1.48052530 0.1408137 drugYes -2.7742506 1.78397557 -1.5550945 1.64824318 -1.6831561 treatmentBtheB -2.3360241 1.72621751 -1.3532617 1.66217026 -1.4054060 month -0.7078407 0.14254124 -4.9658660 0.15394156 -4.5981134 Estimated Scale Parameter: 77.14393 Number of Iterations: 5 Working Correlation [,1] [,2] [,3] [,4] [1,] 1.0000000 0.6915241 0.6915241 0.6915241 [2,] 0.6915241 1.0000000 0.6915241 0.6915241 [3,] 0.6915241 0.6915241 1.0000000 0.6915241 [4,] 0.6915241 0.6915241 0.6915241 1.0000000 summary(btb_gee_un) GEE: GENERALIZED LINEAR MODELS FOR DEPENDENT DATA gee S-function, version 4.13 modified 98/01/27 (1998) Model: Link: Identity Variance to Mean Relation: Gaussian Correlation Structure: Unstructured Call: gee::gee(formula = bdi ~ bdi.pre + length + drug + treatment + month, id = id, data = btb_long, family = gaussian, corstr = &quot;unstructured&quot;) Summary of Residuals: Min 1Q Median 3Q Max -25.1527937 -6.1091139 -0.5896205 4.7316139 25.9041542 Coefficients: Estimate Naive S.E. Naive z Robust S.E. Robust z (Intercept) 6.3905215 2.28769760 2.793429 2.15668950 2.9631162 bdi.pre 0.6171798 0.07744569 7.969195 0.08081777 7.6366846 length&gt;6m 0.5834398 1.61626647 0.360980 1.46837275 0.3973377 drugYes -2.7908835 1.69816226 -1.643473 1.63741987 -1.7044398 treatmentBtheB -2.4261698 1.64272613 -1.476917 1.65519523 -1.4657907 month -0.7628336 0.18121518 -4.209546 0.15643591 -4.8763329 Estimated Scale Parameter: 76.40371 Number of Iterations: 5 Working Correlation [,1] [,2] [,3] [,4] [1,] 1.0000000 0.7069560 0.5704892 0.4714744 [2,] 0.7069560 1.0000000 0.6086188 0.4637445 [3,] 0.5704892 0.6086188 1.0000000 0.5454963 [4,] 0.4714744 0.4637445 0.5454963 1.0000000 12.4.2 Parameter Estimates Table # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(btb_lm_1, btb_lmer_RI, btb_gee_in, btb_gee_ex, btb_gee_un), custom.model.names = c(&quot;OLS&quot;, &quot;MLM-RI&quot;, &quot;GEE-in&quot;, &quot;GEE-ex&quot;, &quot;GEE-un&quot;)) Statistical models OLS MLM-RI GEE-in GEE-ex GEE-un (Intercept) 7.88*** 5.92* 7.88*** 5.89** 6.39** (1.78) (2.31) (2.20) (2.11) (2.16) bdi.pre 0.57*** 0.64*** 0.57*** 0.64*** 0.62*** (0.05) (0.08) (0.09) (0.08) (0.08) length&gt;6m 1.75 0.24 1.75 0.21 0.58 (1.11) (1.68) (1.42) (1.48) (1.47) drugYes -3.55** -2.79 -3.55* -2.77 -2.79 (1.14) (1.77) (1.73) (1.65) (1.64) treatmentBtheB -3.35** -2.36 -3.35 -2.34 -2.43 (1.10) (1.71) (1.71) (1.66) (1.66) month -0.96*** -0.71*** -0.96*** -0.71*** -0.76*** (0.23) (0.15) (0.18) (0.15) (0.16) R2 0.40 Adj. R2 0.39 Num. obs. 280 280 280 280 280 RMSE 8.65 AIC 1882.08 BIC 1911.16 Log Likelihood -933.04 Num. groups: id 97 Var: id (Intercept) 51.44 Var: Residual 25.27 Dispersion 74.89 77.14 76.40 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 12.4.3 Re-Fit Models Use the geeglm() function from the geepack package for the results to be used in a anova() table. btb_geeglm_in &lt;- geeglm(bdi ~ bdi.pre + length + drug + treatment + month, data = btb_long, id = id, wave = month, family = gaussian, corstr = &#39;independence&#39;) btb_geeglm_ex &lt;- geeglm(bdi ~ bdi.pre + length + drug + treatment + month, data = btb_long, id = id, wave = month, family = gaussian, corstr = &#39;exchangeable&#39;) btb_geeglm_ar &lt;- geeglm(bdi ~ bdi.pre + length + drug + treatment + month, data = btb_long, id = id, wave = month, family = gaussian, corstr = &#39;ar1&#39;) btb_geeglm_un &lt;- geeglm(bdi ~ bdi.pre + length + drug + treatment + month, data = btb_long, id = id, wave = month, family = gaussian, corstr = &#39;unstructured&#39;) 12.4.4 Can’t Use the Likelihood Ratio Test The anova() function is used to compare nested models for parameters (fixed effects), not correlation structures. anova(btb_geeglm_in, btb_geeglm_ex) Models are identical NULL anova(btb_geeglm_in, btb_geeglm_ar) Models are identical NULL anova(btb_geeglm_in, btb_geeglm_un) Models are identical NULL 12.4.5 Variaous QIC Measures of Fit References: Pan, W. 2001. Akaike’s information criterion in generalized estimating equations. Biometrics 57:120-125. https://onlinelibrary.wiley.com/doi/abs/10.1111/j.0006-341X.2001.00120.x Burnham, K. P. and D. R. Anderson. 2002. Model selection and multimodel inference: a practical information-theoretic approach. Second edition. Springer Science and Business Media, Inc., New York. https://cds.cern.ch/record/1608735/files/9780387953649_TOC.pdf The QIC() is one way to try to measure model fit. You can enter more than one model into a single function call. QIC(I) based on independence model &lt;– suggested by Pan (Biometric, March 2001), asymptotically unbiased estimator (choose the correlation stucture that produces the smallest QIC(I), p122) MuMIn::QIC(btb_geeglm_in, btb_geeglm_ex, btb_geeglm_ar, btb_geeglm_un, typeR = FALSE) # default QIC btb_geeglm_in 1228.962 btb_geeglm_ex 1226.904 btb_geeglm_ar 1227.046 btb_geeglm_un 1226.812 QIC(R) is based on quasi-likelihood of a working correlation R model, can NOT be used to select the working correlation matrix. MuMIn::QIC(btb_geeglm_in, btb_geeglm_ex, btb_geeglm_ar, btb_geeglm_un, typeR = TRUE) # NOT the default QIC btb_geeglm_in 1228.962 btb_geeglm_ex 1235.272 btb_geeglm_ar 1233.507 btb_geeglm_un 1234.753 QIC_U(R) approximates QIC(R), and while both are useful for variable selection, they can NOT be applied to select the working correlation matrix. MuMIn::QICu(btb_geeglm_in, btb_geeglm_ex, btb_geeglm_ar, btb_geeglm_un) QICu btb_geeglm_in 1762.403 btb_geeglm_ex 1762.403 btb_geeglm_ar 1762.403 btb_geeglm_un 1762.403 MuMIn::model.sel(btb_geeglm_in, btb_geeglm_ex, btb_geeglm_ar, btb_geeglm_un, rank = &quot;QIC&quot;) #sorts the best to the TOP, uses QIC(I) Model selection table (Int) bdi.pre drg lng mnt trt corstr qLik QIC btb_geeglm_un 6.068 0.6307 + + -0.7061 + unstrc -10554.42 1226.8 btb_geeglm_ex 5.880 0.6402 + + -0.7070 + exchng -10570.56 1226.9 btb_geeglm_ar 6.620 0.5956 + + -0.7357 + ar1 -10498.78 1227.0 btb_geeglm_in 7.883 0.5724 + + -0.9608 + indpnd -10259.30 1229.0 delta weight btb_geeglm_un 0.00 0.314 btb_geeglm_ex 0.09 0.300 btb_geeglm_ar 0.23 0.279 btb_geeglm_in 2.15 0.107 Abbreviations: corstr: exchng = &#39;exchangeable&#39;, indpnd = &#39;independence&#39;, unstrc = &#39;unstructured&#39; Models ranked by QIC(x) 12.4.6 Plot the model predictions summary(btb_long) id treatment drug length bdi.pre Min. : 1.00 TAU :135 No :156 &lt;6m:122 Min. : 2.00 1st Qu.:22.75 BtheB:145 Yes:124 &gt;6m:158 1st Qu.:15.00 Median :48.00 Median :21.00 Mean :49.55 Mean :22.99 3rd Qu.:76.25 3rd Qu.:31.00 Max. :99.00 Max. :49.00 month bdi Min. :2.000 Min. : 0.00 1st Qu.:2.000 1st Qu.: 6.00 Median :4.000 Median :12.50 Mean :4.464 Mean :14.43 3rd Qu.:6.000 3rd Qu.:21.00 Max. :8.000 Max. :53.00 Do not worry about confidence intervals. expand.grid(bdi.pre = 23, length = &quot;&lt;6m&quot;, drug = &quot;No&quot;, treatment = levels(btb_long$treatment), month = seq(from = 2, to = 8, by = 2)) %&gt;% mutate(fit_in = predict(btb_geeglm_in, newdata = ., type = &quot;response&quot;)) %&gt;% mutate(fit_ex = predict(btb_geeglm_ex, newdata = ., type = &quot;response&quot;)) %&gt;% mutate(fit_ar = predict(btb_geeglm_ar, newdata = ., type = &quot;response&quot;)) %&gt;% mutate(fit_un = predict(btb_geeglm_un, newdata = ., type = &quot;response&quot;)) %&gt;% gather(key = variable, value = fit, starts_with(&quot;fit&quot;)) %&gt;% separate(col = variable, into = c(&quot;junk&quot;, &quot;covR&quot;)) %&gt;% select(-junk) %&gt;% mutate(covR = factor(covR, levels = c(&quot;un&quot;, &quot;ar&quot;, &quot;ex&quot;, &quot;in&quot;), labels = c(&quot;Unstructured&quot;, &quot;Auto-Regressive&quot;, &quot;Compound Symetry&quot;, &quot;Independence&quot;))) %&gt;% ggplot(aes(x = month, y = fit, linetype = treatment)) + geom_line(alpha = 0.6) + theme_bw() + labs(title = &quot;BtheB - Predictions from four GEE models (geeglm)&quot;, linetype = &quot;Treatment&quot;) + scale_linetype_manual(values = c(&quot;solid&quot;, &quot;longdash&quot;)) + scale_size_manual(values = c(2, 1, 1, 1)) + scale_color_manual(values = c(&quot;red&quot;, &quot;dodgerblue&quot;, &quot;blue&quot;, &quot;darkgreen&quot;)) + theme(legend.key.width = unit(1, &quot;cm&quot;)) + facet_wrap(~ covR) "],
["gee-binary-outcome-respiratory-illness.html", "13 GEE, Binary Outcome: Respiratory Illness 13.1 Packages 13.2 Prepare and get to know the dataset 13.3 Exploratory Data Analysis 13.4 Logisitc Regression (GLM) 13.5 Generalized Estimating Equations (GEE)", " 13 GEE, Binary Outcome: Respiratory Illness 13.1 Packages 13.1.1 CRAN library(tidyverse) # all things tidy library(pander) # nice looking genderal tabulations library(furniture) # nice table1() descriptives library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(psych) # contains some useful functions, like headTail library(lme4) # Linear, generalized linear, &amp; nonlinear mixed models library(corrplot) # Vizualize correlation matrix library(gee) # Genderalized Estimation Equation Solver library(geepack) # Genderalized Estimation Equation Package library(MuMIn) # Multi-Model Inference (caluclate QIC) library(HSAUR) # package with the dataset 13.1.2 GitHub Helper extract functions for exponentiating parameters form generalized regression models within a texreg table of model parameters. # install.packages(&quot;devtools&quot;) # library(devtools) # install_github(&quot;SarBearSchwartz/texreghelpr&quot;) library(texreghelpr) 13.2 Prepare and get to know the dataset 13.2.1 Read in the data This data set is in the HSAUR package. In each of two centres, eligible patients were randomly assigned to active treatment or placebo. During the treatment, the respiratory status (categorised poor or good) was determined at each of four, monthly visits. The trial recruited 111 participants (54 in the active group, 57 in the placebo group) and there were no missing data for either the responses or the covariates. The question of interest is to assess whether the treatment is effective and to estimate its effect. Note that the data (555 observations on the following 7 variables) are in long form, i.e, repeated measurments are stored as additional rows in the data frame. Indicators subject the patient ID, a factor with levels 1 to 111 centre the study center, a factor with levels 1 and 2 month the month, each patient was examined at months 0, 1, 2, 3 and 4 Outcome or dependent variable status the respiratory status (response variable), a factor with levels poor and good Main predictor or independent variable of interest treatment the treatment arm, a factor with levels placebo and treatment Time-invariant Covariates sex a factor with levels female and male age the age of the patient data(&quot;respiratory&quot;, package = &quot;HSAUR&quot;) str(respiratory) &#39;data.frame&#39;: 555 obs. of 7 variables: $ centre : Factor w/ 2 levels &quot;1&quot;,&quot;2&quot;: 1 1 1 1 1 1 1 1 1 1 ... $ treatment: Factor w/ 2 levels &quot;placebo&quot;,&quot;treatment&quot;: 1 1 1 1 1 1 1 1 1 1 ... $ sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 1 1 1 1 1 1 1 1 1 1 ... $ age : num 46 46 46 46 46 28 28 28 28 28 ... $ status : Factor w/ 2 levels &quot;poor&quot;,&quot;good&quot;: 1 1 1 1 1 1 1 1 1 1 ... $ month : Ord.factor w/ 5 levels &quot;0&quot;&lt;&quot;1&quot;&lt;&quot;2&quot;&lt;&quot;3&quot;&lt;..: 1 2 3 4 5 1 2 3 4 5 ... $ subject : Factor w/ 111 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 1 1 1 2 2 2 2 2 ... psych::headTail(respiratory) centre treatment sex age status month subject 1 1 placebo female 46 poor 0 1 112 1 placebo female 46 poor 1 1 223 1 placebo female 46 poor 2 1 334 1 placebo female 46 poor 3 1 ... &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ... &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 222 2 treatment female 31 good 1 111 333 2 treatment female 31 good 2 111 444 2 treatment female 31 good 3 111 555 2 treatment female 31 good 4 111 13.2.2 Wide Format data_wide &lt;- respiratory %&gt;% tidyr::spread(key = month, value = status, sep = &quot;_&quot;) %&gt;% dplyr::rename(&quot;BL_status&quot; = &quot;month_0&quot;) %&gt;% dplyr::arrange(subject) %&gt;% dplyr::select(subject, centre, sex, age, treatment, BL_status, starts_with(&quot;month&quot;)) str(data_wide) &#39;data.frame&#39;: 111 obs. of 10 variables: $ subject : Factor w/ 111 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... $ centre : Factor w/ 2 levels &quot;1&quot;,&quot;2&quot;: 1 1 1 1 1 1 1 1 1 1 ... $ sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 1 1 1 1 2 1 1 1 1 1 ... $ age : num 46 28 23 44 13 34 43 28 31 37 ... $ treatment: Factor w/ 2 levels &quot;placebo&quot;,&quot;treatment&quot;: 1 1 2 1 1 2 1 2 2 1 ... $ BL_status: Factor w/ 2 levels &quot;poor&quot;,&quot;good&quot;: 1 1 2 2 2 1 1 1 2 2 ... $ month_1 : Factor w/ 2 levels &quot;poor&quot;,&quot;good&quot;: 1 1 2 2 2 1 2 1 2 1 ... $ month_2 : Factor w/ 2 levels &quot;poor&quot;,&quot;good&quot;: 1 1 2 2 2 1 1 1 2 2 ... $ month_3 : Factor w/ 2 levels &quot;poor&quot;,&quot;good&quot;: 1 1 2 2 2 1 2 1 2 2 ... $ month_4 : Factor w/ 2 levels &quot;poor&quot;,&quot;good&quot;: 1 1 2 1 2 1 2 1 2 1 ... psych::headTail(data_wide) subject centre sex age treatment BL_status month_1 month_2 month_3 1 1 1 female 46 placebo poor poor poor poor 2 2 1 female 28 placebo poor poor poor poor 3 3 1 female 23 treatment good good good good 4 4 1 female 44 placebo good good good good ... &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ... &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 108 108 2 male 39 treatment poor good good good 109 109 2 female 68 treatment poor good good good 110 110 2 male 63 treatment good good good good 111 111 2 female 31 treatment good good good good month_4 1 poor 2 poor 3 good 4 poor ... &lt;NA&gt; 108 good 109 good 110 good 111 good 13.2.3 Long Format data_long &lt;- data_wide%&gt;% tidyr::gather(key = month, value = status, starts_with(&quot;month&quot;)) %&gt;% dplyr::mutate(month = str_sub(month, start = -1) %&gt;% as.numeric) %&gt;% dplyr::mutate(status = case_when(status == &quot;poor&quot; ~ 0, status == &quot;good&quot; ~ 1)) %&gt;% dplyr::arrange(subject, month) %&gt;% dplyr::select(subject, centre, sex, age, treatment, BL_status, month, status) str(data_long) &#39;data.frame&#39;: 444 obs. of 8 variables: $ subject : Factor w/ 111 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 1 1 2 2 2 2 3 3 ... $ centre : Factor w/ 2 levels &quot;1&quot;,&quot;2&quot;: 1 1 1 1 1 1 1 1 1 1 ... $ sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 1 1 1 1 1 1 1 1 1 1 ... $ age : num 46 46 46 46 28 28 28 28 23 23 ... $ treatment: Factor w/ 2 levels &quot;placebo&quot;,&quot;treatment&quot;: 1 1 1 1 1 1 1 1 2 2 ... $ BL_status: Factor w/ 2 levels &quot;poor&quot;,&quot;good&quot;: 1 1 1 1 1 1 1 1 2 2 ... $ month : num 1 2 3 4 1 2 3 4 1 2 ... $ status : num 0 0 0 0 0 0 0 0 1 1 ... psych::headTail(data_long) subject centre sex age treatment BL_status month status 1 1 1 female 46 placebo poor 1 0 2 1 1 female 46 placebo poor 2 0 3 1 1 female 46 placebo poor 3 0 4 1 1 female 46 placebo poor 4 0 ... &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ... &lt;NA&gt; &lt;NA&gt; ... ... 441 111 2 female 31 treatment good 1 1 442 111 2 female 31 treatment good 2 1 443 111 2 female 31 treatment good 3 1 444 111 2 female 31 treatment good 4 1 13.3 Exploratory Data Analysis 13.3.1 Summary Statistics 13.3.1.1 Demographics and Baseline Measure data_wide %&gt;% furniture::table1(centre, sex, age, BL_status, splitby = ~ treatment, output = &quot;html&quot;) placebo treatment n = 57 n = 54 centre 1 29 (50.9%) 27 (50%) 2 28 (49.1%) 27 (50%) sex female 40 (70.2%) 48 (88.9%) male 17 (29.8%) 6 (11.1%) age 33.6 (13.4) 32.9 (14.0) BL_status poor 31 (54.4%) 30 (55.6%) good 26 (45.6%) 24 (44.4%) 13.3.1.2 Status Over Time data_wide %&gt;% furniture::table1(month_1, month_2, month_3, month_4, splitby = ~ treatment, output = &quot;markdown&quot;) placebo treatment n = 57 n = 54 month_1 poor 29 (50.9%) 17 (31.5%) good 28 (49.1%) 37 (68.5%) month_2 poor 35 (61.4%) 16 (29.6%) good 22 (38.6%) 38 (70.4%) month_3 poor 31 (54.4%) 15 (27.8%) good 26 (45.6%) 39 (72.2%) month_4 poor 32 (56.1%) 20 (37%) good 25 (43.9%) 34 (63%) data_month_trt_prop &lt;- data_long %&gt;% dplyr::group_by(treatment, month) %&gt;% dplyr::summarise(n = n(), prop_good = mean(status), prop_sd = sd(status), prop_se = prop_sd/n) psych::headTail(data_month_trt_prop) treatment month n prop_good prop_sd prop_se 1 placebo 1 57 0.49 0.5 0.01 2 placebo 2 57 0.39 0.49 0.01 3 placebo 3 57 0.46 0.5 0.01 4 placebo 4 57 0.44 0.5 0.01 5 &lt;NA&gt; ... ... ... ... ... 6 treatment 1 54 0.69 0.47 0.01 7 treatment 2 54 0.7 0.46 0.01 8 treatment 3 54 0.72 0.45 0.01 9 treatment 4 54 0.63 0.49 0.01 13.3.2 Visualization 13.3.2.1 Status Over Time data_month_trt_prop %&gt;% ggplot(aes(x = month, y = prop_good, group = treatment, color = treatment)) + geom_errorbar(aes(ymin = prop_good - prop_se, ymax = prop_good + prop_se), width = .25) + geom_point() + geom_line() + theme_bw() 13.4 Logisitc Regression (GLM) resp_glm &lt;- glm(status ~ centre + treatment + sex + BL_status + I(age-33) + I((age-33)^2), data = data_long, family = binomial(link = &quot;logit&quot;)) summary(resp_glm) Call: glm(formula = status ~ centre + treatment + sex + BL_status + I(age - 33) + I((age - 33)^2), family = binomial(link = &quot;logit&quot;), data = data_long) Deviance Residuals: Min 1Q Median 3Q Max -2.5965 -0.9178 0.3985 0.8388 2.0988 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -1.9685725 0.2733549 -7.202 5.95e-13 *** centre2 0.5347938 0.2464412 2.170 0.030002 * treatmenttreatment 1.3561814 0.2447533 5.541 3.01e-08 *** sexmale 0.4263433 0.3175081 1.343 0.179343 BL_statusgood 1.9193401 0.2500033 7.677 1.63e-14 *** I(age - 33) -0.0368535 0.0106382 -3.464 0.000532 *** I((age - 33)^2) 0.0025169 0.0006352 3.963 7.41e-05 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 608.93 on 443 degrees of freedom Residual deviance: 465.25 on 437 degrees of freedom AIC: 479.25 Number of Fisher Scoring iterations: 4 sjPlot::tab_model(resp_glm) status Predictors Odds Ratios CI p (Intercept) 0.14 0.08 – 0.23 &lt;0.001 centre: centre 2 1.71 1.05 – 2.77 0.030 treatmenttreatment 3.88 2.42 – 6.33 &lt;0.001 sexmale 1.53 0.83 – 2.87 0.179 BL_statusgood 6.82 4.22 – 11.27 &lt;0.001 I(age-33) 0.96 0.94 – 0.98 0.001 I((age-33)^2) 1.00 1.00 – 1.00 &lt;0.001 Observations 444 Cox &amp; Snell’s R2 / Nagelkerke’s R2 0.276 / 0.370 13.5 Generalized Estimating Equations (GEE) 13.5.1 Indepdendence resp_gee_in &lt;- gee::gee(status ~ centre + treatment + sex + BL_status + I(age-33) + I((age-33)^2), data = data_long, family = binomial(link = &quot;logit&quot;), id = subject, corstr = &quot;independence&quot;, scale.fix = TRUE, scale.value = 1) (Intercept) centre2 treatmenttreatment -1.968572485 0.534793799 1.356181372 sexmale BL_statusgood I(age - 33) 0.426343291 1.919340141 -0.036853528 I((age - 33)^2) 0.002516859 summary(resp_gee_in) GEE: GENERALIZED LINEAR MODELS FOR DEPENDENT DATA gee S-function, version 4.13 modified 98/01/27 (1998) Model: Link: Logit Variance to Mean Relation: Binomial Correlation Structure: Independent Call: gee::gee(formula = status ~ centre + treatment + sex + BL_status + I(age - 33) + I((age - 33)^2), id = subject, data = data_long, family = binomial(link = &quot;logit&quot;), corstr = &quot;independence&quot;, scale.fix = TRUE, scale.value = 1) Summary of Residuals: Min 1Q Median 3Q Max -0.96563962 -0.34372730 0.07631922 0.29658264 0.88947816 Coefficients: Estimate Naive S.E. Naive z Robust S.E. (Intercept) -1.968572493 0.2733635751 -7.201298 0.4457014141 centre2 0.534793799 0.2464443046 2.170039 0.3795759846 treatmenttreatment 1.356181377 0.2447584751 5.540896 0.3777998909 sexmale 0.426343293 0.3175134753 1.342757 0.4832336627 BL_statusgood 1.919340146 0.2500092510 7.677077 0.3772812271 I(age - 33) -0.036853528 0.0106384086 -3.464196 0.0150120266 I((age - 33)^2) 0.002516859 0.0006351834 3.962414 0.0007592432 Robust z (Intercept) -4.4167966 centre2 1.4089242 treatmenttreatment 3.5896818 sexmale 0.8822715 BL_statusgood 5.0872930 I(age - 33) -2.4549336 I((age - 33)^2) 3.3149582 Estimated Scale Parameter: 1 Number of Iterations: 1 Working Correlation [,1] [,2] [,3] [,4] [1,] 1 0 0 0 [2,] 0 1 0 0 [3,] 0 0 1 0 [4,] 0 0 0 1 The results for GEE fit with the independence correlation structure produces results that are nearly identical to the GLM model. The robust (sandwhich) standard errors are however much larger than the naive stadard errors 13.5.2 Exchangeable resp_gee_ex &lt;- gee::gee(status ~ centre + treatment + sex + BL_status + I(age-33) + I((age-33)^2), data = data_long, family = binomial(link = &quot;logit&quot;), id = subject, corstr = &quot;exchangeable&quot;, scale.fix = TRUE, scale.value = 1) (Intercept) centre2 treatmenttreatment -1.968572485 0.534793799 1.356181372 sexmale BL_statusgood I(age - 33) 0.426343291 1.919340141 -0.036853528 I((age - 33)^2) 0.002516859 summary(resp_gee_ex) GEE: GENERALIZED LINEAR MODELS FOR DEPENDENT DATA gee S-function, version 4.13 modified 98/01/27 (1998) Model: Link: Logit Variance to Mean Relation: Binomial Correlation Structure: Exchangeable Call: gee::gee(formula = status ~ centre + treatment + sex + BL_status + I(age - 33) + I((age - 33)^2), id = subject, data = data_long, family = binomial(link = &quot;logit&quot;), corstr = &quot;exchangeable&quot;, scale.fix = TRUE, scale.value = 1) Summary of Residuals: Min 1Q Median 3Q Max -0.96563962 -0.34372730 0.07631922 0.29658264 0.88947816 Coefficients: Estimate Naive S.E. Naive z Robust S.E. (Intercept) -1.968572493 0.379830796 -5.1827617 0.4457014141 centre2 0.534793799 0.342427246 1.5617735 0.3795759846 treatmenttreatment 1.356181377 0.340084835 3.9877737 0.3777998909 sexmale 0.426343293 0.441175807 0.9663796 0.4832336627 BL_statusgood 1.919340146 0.347380636 5.5251789 0.3772812271 I(age - 33) -0.036853528 0.014781762 -2.4931757 0.0150120266 I((age - 33)^2) 0.002516859 0.000882569 2.8517424 0.0007592432 Robust z (Intercept) -4.4167966 centre2 1.4089242 treatmenttreatment 3.5896818 sexmale 0.8822715 BL_statusgood 5.0872930 I(age - 33) -2.4549336 I((age - 33)^2) 3.3149582 Estimated Scale Parameter: 1 Number of Iterations: 1 Working Correlation [,1] [,2] [,3] [,4] [1,] 1.00000 0.31021 0.31021 0.31021 [2,] 0.31021 1.00000 0.31021 0.31021 [3,] 0.31021 0.31021 1.00000 0.31021 [4,] 0.31021 0.31021 0.31021 1.00000 Notice that the naive stadard errors are more similar to the robust (sandwhich) standard errors, supporting that this is a better fitting model 13.5.3 Paramgeter Estimates Table The GEE models display the robus (sandwhich) standard errors. 13.5.3.1 Raw Estimates (Logit Scale) # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(resp_glm, resp_gee_in, resp_gee_ex), custom.model.names = c(&quot;GLM&quot;, &quot;GEE-INDEP&quot;, &quot;GEE-EXCH&quot;), caption = &quot;Estimates on Logit Scale&quot;, digits = 4) Estimates on Logit Scale GLM GEE-INDEP GEE-EXCH (Intercept) -1.9686*** -1.9686*** -1.9686*** (0.2734) (0.4457) (0.4457) centre2 0.5348* 0.5348 0.5348 (0.2464) (0.3796) (0.3796) treatmenttreatment 1.3562*** 1.3562*** 1.3562*** (0.2448) (0.3778) (0.3778) sexmale 0.4263 0.4263 0.4263 (0.3175) (0.4832) (0.4832) BL_statusgood 1.9193*** 1.9193*** 1.9193*** (0.2500) (0.3773) (0.3773) I(age - 33) -0.0369*** -0.0369* -0.0369* (0.0106) (0.0150) (0.0150) I((age - 33)^2) 0.0025*** 0.0025*** 0.0025*** (0.0006) (0.0008) (0.0008) AIC 479.2530 BIC 507.9238 Log Likelihood -232.6265 Deviance 465.2530 Num. obs. 444 444 444 Dispersion 1.0000 1.0000 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 Comparing the two GEE models: parameters are identical and so are the robust (sandwhich) standard errors. 13.5.3.2 Exponentiate the Estimates (odds ratio scale) # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(extract_glm_exp(resp_glm), extract_gee_exp(resp_gee_in), extract_gee_exp(resp_gee_ex)), custom.model.names = c(&quot;GLM&quot;, &quot;GEE-INDEP&quot;, &quot;GEE-EXCH&quot;), caption = &quot;Estimates on Odds-Ratio Scale&quot;, ci.test = 1, digits = 4) Estimates on Odds-Ratio Scale GLM GEE-INDEP GEE-EXCH (Intercept) 0.1397* 0.1397* 0.1397* [0.0801; 0.2343] [0.0583; 0.3345] [0.0583; 0.3345] centre2 1.7071* 1.7071 1.7071 [1.0540; 2.7748] [0.8113; 3.5922] [0.8113; 3.5922] treatmenttreatment 3.8813* 3.8813* 3.8813* [2.4221; 6.3344] [1.8509; 8.1390] [1.8509; 8.1390] sexmale 1.5316 1.5316 1.5316 [0.8257; 2.8745] [0.5940; 3.9491] [0.5940; 3.9491] BL_statusgood 6.8165* 6.8165* 6.8165* [4.2214; 11.2710] [3.2539; 14.2793] [3.2539; 14.2793] I(age - 33) 0.9638* 0.9638* 0.9638* [0.9435; 0.9837] [0.9359; 0.9926] [0.9359; 0.9926] I((age - 33)^2) 1.0025* 1.0025* 1.0025* [1.0013; 1.0038] [1.0010; 1.0040] [1.0010; 1.0040] AIC 479.2530 BIC 507.9238 Log Likelihood -232.6265 Deviance 465.2530 Num. obs. 444 444 444 Dispersion 1.0000 1.0000 * 1 outside the confidence interval 13.5.3.3 Manual Extraction resp_gee_ex %&gt;% coef() %&gt;% exp() (Intercept) centre2 treatmenttreatment 0.1396561 1.7070962 3.8813436 sexmale BL_statusgood I(age - 33) 1.5316465 6.8164591 0.9638173 I((age - 33)^2) 1.0025200 trt_EST &lt;- summary(resp_gee_ex)$coeff[&quot;treatmenttreatment&quot;, &quot;Estimate&quot;] trt_EST [1] 1.356181 exp(trt_EST) [1] 3.881344 Trt_SE &lt;- summary(resp_gee_ex)$coeff[&quot;treatmenttreatment&quot;, &quot;Robust S.E.&quot;] Trt_SE [1] 0.3777999 trt_95ci &lt;- trt_EST + c(-1, +1)*1.96*Trt_SE trt_95ci [1] 0.6156936 2.0966692 exp(trt_95ci) [1] 1.850940 8.139015 13.5.4 Refit with the geepack package resp_geeglm_ex &lt;- geepack::geeglm(status ~ centre + treatment + sex + BL_status + I(age-33) + I((age-33)^2), data = data_long, family = binomial(link = &quot;logit&quot;), id = subject, waves = month, corstr = &quot;exchangeable&quot;) summary(resp_geeglm_ex) Call: geepack::geeglm(formula = status ~ centre + treatment + sex + BL_status + I(age - 33) + I((age - 33)^2), family = binomial(link = &quot;logit&quot;), data = data_long, id = subject, waves = month, corstr = &quot;exchangeable&quot;) Coefficients: Estimate Std.err Wald Pr(&gt;|W|) (Intercept) -1.9685725 0.4457014 19.508 1.00e-05 *** centre2 0.5347938 0.3795760 1.985 0.158858 treatmenttreatment 1.3561814 0.3777999 12.886 0.000331 *** sexmale 0.4263433 0.4832337 0.778 0.377630 BL_statusgood 1.9193401 0.3772812 25.881 3.63e-07 *** I(age - 33) -0.0368535 0.0150120 6.027 0.014091 * I((age - 33)^2) 0.0025169 0.0007592 10.989 0.000917 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Estimated Scale Parameters: Estimate Std.err (Intercept) 1.033 0.4694 Correlation: Structure = exchangeable Link = identity Estimated Correlation Parameters: Estimate Std.err alpha 0.3119 0.1515 Number of clusters: 111 Maximum cluster size: 4 resp_geeglm_ex %&gt;% coef() %&gt;% exp() (Intercept) centre2 treatmenttreatment 0.1397 1.7071 3.8813 sexmale BL_statusgood I(age - 33) 1.5316 6.8165 0.9638 I((age - 33)^2) 1.0025 13.5.5 Visualize the Model 13.5.5.1 Range of Data Values summary(data_long) subject centre sex age treatment 1 : 4 1:224 female:352 Min. :11.0 placebo :228 2 : 4 2:220 male : 92 1st Qu.:23.0 treatment:216 3 : 4 Median :31.0 4 : 4 Mean :33.3 5 : 4 3rd Qu.:43.0 6 : 4 Max. :68.0 (Other):420 BL_status month status poor:244 Min. :1.00 Min. :0.000 good:200 1st Qu.:1.75 1st Qu.:0.000 Median :2.50 Median :1.000 Mean :2.50 Mean :0.561 3rd Qu.:3.25 3rd Qu.:1.000 Max. :4.00 Max. :1.000 13.5.5.2 Females in Center 1 expand.grid(centre = &quot;1&quot;, treatment = levels(data_long$treatment), sex = &quot;female&quot;, age = seq(from = 11, to = 68, by = 1), BL_status = levels(data_long$BL_status)) %&gt;% dplyr::mutate(fit = predict(resp_geeglm_ex, newdata = ., type = &quot;response&quot;)) %&gt;% ggplot(aes(x = age, y = fit, linetype = treatment)) + geom_line() + theme_bw() + facet_grid(.~ BL_status) + labs(x = &quot;Age, years&quot;, y = &quot;Predicted Probability of GOOD Respiratory Status&quot;, title = &quot;For Females at Center 1, by Baseline Status&quot;) expand.grid(centre = &quot;2&quot;, treatment = levels(data_long$treatment), sex = &quot;male&quot;, age = seq(from = 11, to = 68, by = 1), BL_status = levels(data_long$BL_status)) %&gt;% dplyr::mutate(fit = predict(resp_geeglm_ex, newdata = ., type = &quot;response&quot;)) %&gt;% ggplot(aes(x = age, y = fit, linetype = treatment)) + geom_line() + theme_bw() + facet_grid(.~ BL_status) + labs(x = &quot;Age, years&quot;, y = &quot;Predicted Probability of GOOD Respiratory Status&quot;, title = &quot;For Males at Center 2, by Baseline Status&quot;) "],
["gee-count-outcome-epilepsy.html", "14 GEE, Count Outcome: Epilepsy 14.1 Packages 14.2 Prepare and get to know the dataset 14.3 Exploratory Data Analysis 14.4 Poisson Regression (GLM) 14.5 Generalized Estimating Equations (GEE)", " 14 GEE, Count Outcome: Epilepsy 14.1 Packages 14.1.1 CRAN library(tidyverse) # all things tidy library(pander) # nice looking genderal tabulations library(furniture) # nice table1() descriptives library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(psych) # contains some useful functions, like headTail library(lme4) # Linear, generalized linear, &amp; nonlinear mixed models library(corrplot) # Vizualize correlation matrix library(gee) # Genderalized Estimation Equation Solver library(geepack) # Genderalized Estimation Equation Package library(MuMIn) # Multi-Model Inference (caluclate QIC) library(HSAUR) # package with the dataset 14.1.2 GitHub Helper extract functions for exponentiating parameters form generalized regression models within a texreg table of model parameters. # install.packages(&quot;devtools&quot;) # library(devtools) # install_github(&quot;SarBearSchwartz/texreghelpr&quot;) library(texreghelpr) 14.2 Prepare and get to know the dataset This data set is in the HSAUR package. In this clinical trial, 59 patients suffering from epilepsy were randomized to groups receiving either the anti-epileptic drug “Progabide”&quot; or a placebo in addition to standard chemotherapy. The numbers of seizures suffered in each of four, two-week periods were recorded for each patient along with a baseline seizure count for the 8 weeks prior to being randomized to treatment and age. The main question of interest is whether taking progabide reduced the number of epileptic seizures compared with placebo. Indicators subject the patient ID, a factor with levels 1 to 59 period treatment period, an ordered factor with levels 1 to 4 Outcome or dependent variable +seizure.rate the number of seizures (2-weeks) Main predictor or independent variable of interest treatment the treatment group, a factor with levels placebo and Progabide Time-invariant Covariates age the age of the patient base the number of seizures before the trial (8 weeks) 14.2.1 Read in the data data(&quot;epilepsy&quot;, package = &quot;HSAUR&quot;) Problem: The outcome (seizure.rate) were counts over a TWO-week period and we would like to interpret the results in terms of effects on the WEEKLY rate. If we divide the values by TWO to get weekly rates, the outcome might be a decimal number The Poisson distribution may only be used for whole numbers (not deciamls) Solution: We need to include an offset term in the model that indicates the LOG DURATION of each period. Every observation period is exactly 2 weeks in this experiment Create a variable in the original dataset that is equal to the LOG DURATION (per = log(2)) To the formula for the glm() or gee(), add: + offset(per) 14.2.2 Long Format data_long &lt;- epilepsy %&gt;% dplyr::select(subject, age, treatment, base, period, seizure.rate) %&gt;% dplyr::mutate(per = log(2)) %&gt;% # new variable to use with the offset dplyr::mutate(base_wk = base/8) str(data_long) &#39;data.frame&#39;: 236 obs. of 8 variables: $ subject : Factor w/ 59 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 1 1 2 2 2 2 3 3 ... $ age : int 31 31 31 31 30 30 30 30 25 25 ... $ treatment : Factor w/ 2 levels &quot;placebo&quot;,&quot;Progabide&quot;: 1 1 1 1 1 1 1 1 1 1 ... $ base : int 11 11 11 11 11 11 11 11 6 6 ... $ period : Ord.factor w/ 4 levels &quot;1&quot;&lt;&quot;2&quot;&lt;&quot;3&quot;&lt;&quot;4&quot;: 1 2 3 4 1 2 3 4 1 2 ... $ seizure.rate: int 5 3 3 3 3 5 3 3 2 4 ... $ per : num 0.693 0.693 0.693 0.693 0.693 ... $ base_wk : num 1.38 1.38 1.38 1.38 1.38 ... psych::headTail(data_long, top = 10, bottom = 6) subject age treatment base period seizure.rate per base_wk 1 1 31 placebo 11 1 5 0.69 1.38 2 1 31 placebo 11 2 3 0.69 1.38 3 1 31 placebo 11 3 3 0.69 1.38 4 1 31 placebo 11 4 3 0.69 1.38 5 2 30 placebo 11 1 3 0.69 1.38 6 2 30 placebo 11 2 5 0.69 1.38 7 2 30 placebo 11 3 3 0.69 1.38 8 2 30 placebo 11 4 3 0.69 1.38 9 3 25 placebo 6 1 2 0.69 0.75 10 3 25 placebo 6 2 4 0.69 0.75 ... &lt;NA&gt; ... &lt;NA&gt; ... &lt;NA&gt; ... ... ... 231 58 36 Progabide 13 3 0 0.69 1.62 232 58 36 Progabide 13 4 0 0.69 1.62 233 59 37 Progabide 12 1 1 0.69 1.5 234 59 37 Progabide 12 2 4 0.69 1.5 235 59 37 Progabide 12 3 3 0.69 1.5 236 59 37 Progabide 12 4 2 0.69 1.5 14.2.3 Wide Format data_wide &lt;- data_long %&gt;% tidyr::spread(key = period, value = seizure.rate, sep = &quot;_&quot;) %&gt;% dplyr::arrange(subject) str(data_wide) &#39;data.frame&#39;: 59 obs. of 10 variables: $ subject : Factor w/ 59 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... $ age : int 31 30 25 36 22 29 31 42 37 28 ... $ treatment: Factor w/ 2 levels &quot;placebo&quot;,&quot;Progabide&quot;: 1 1 1 1 1 1 1 1 1 1 ... $ base : int 11 11 6 8 66 27 12 52 23 10 ... $ per : num 0.693 0.693 0.693 0.693 0.693 ... $ base_wk : num 1.38 1.38 0.75 1 8.25 ... $ period_1 : int 5 3 2 4 7 5 6 40 5 14 ... $ period_2 : int 3 5 4 4 18 2 4 20 6 13 ... $ period_3 : int 3 3 0 1 9 8 0 23 6 6 ... $ period_4 : int 3 3 5 4 21 7 2 12 5 0 ... psych::headTail(data_wide) subject age treatment base per base_wk period_1 period_2 period_3 1 1 31 placebo 11 0.69 1.38 5 3 3 2 2 30 placebo 11 0.69 1.38 3 5 3 3 3 25 placebo 6 0.69 0.75 2 4 0 4 4 36 placebo 8 0.69 1 4 4 1 ... &lt;NA&gt; ... &lt;NA&gt; ... ... ... ... ... ... 56 56 26 Progabide 22 0.69 2.75 1 23 19 57 57 21 Progabide 25 0.69 3.12 2 3 0 58 58 36 Progabide 13 0.69 1.62 0 0 0 59 59 37 Progabide 12 0.69 1.5 1 4 3 period_4 1 3 2 3 3 5 4 4 ... ... 56 8 57 1 58 0 59 2 14.3 Exploratory Data Analysis 14.3.1 Summarize 14.3.1.1 Demographics and Baseline data_wide %&gt;% furniture::table1(age, base, base_wk, splitby = ~ treatment, test = TRUE, type = &quot;full&quot;, output = &quot;markdown&quot;) placebo Progabide Test P-Value n = 28 n = 31 age T-Test: 0.77 0.446 29.0 (6.0) 27.7 (6.6) base T-Test: -0.12 0.907 30.8 (26.1) 31.6 (28.0) base_wk T-Test: -0.12 0.907 3.8 (3.3) 4.0 (3.5) 14.3.1.2 Outcome Across Time Note: The Poisson distribution specifies that the MEAN = VARIANCE In this dataset, the variance is much larger than the mean, at all time points for both groups. This is evidence of overdispersion and suggest the scale parameter should be greater than one. data_long %&gt;% dplyr::group_by(treatment, period) %&gt;% dplyr::summarise(N = n(), M = mean(seizure.rate), VAR = var(seizure.rate), SD = sd(seizure.rate)) %&gt;% pander::pander() treatment period N M VAR SD placebo 1 28 9.4 103 10.1 placebo 2 28 8.3 67 8.2 placebo 3 28 8.8 215 14.7 placebo 4 28 8.0 58 7.6 Progabide 1 31 8.6 333 18.2 Progabide 2 31 8.4 141 11.9 Progabide 3 31 8.1 193 13.9 Progabide 4 31 6.7 127 11.3 14.3.1.3 Correlation Across Time Raw Scale data_long %&gt;% dplyr::select(subject, period, seizure.rate ) %&gt;% tidyr::spread(key = period, value = seizure.rate ) %&gt;% dplyr::select(-subject) %&gt;% cor() %&gt;% corrplot::corrplot.mixed() Log Scale data_long %&gt;% dplyr::mutate(rate_wk = log(seizure.rate + 1)) %&gt;% dplyr::select(subject, period, rate_wk) %&gt;% tidyr::spread(key = period, value = rate_wk) %&gt;% dplyr::select(-subject) %&gt;% cor() %&gt;% corrplot::corrplot.mixed() 14.3.2 Visualize 14.3.2.1 Oucome on the Raw Scale There appear to be quite a few extreme values or outliers, particularly for the Progabide group during period one. Since the outcome is truely a COUNT, we will model it with a Poisson distribution combined with a LOG link. data_long %&gt;% ggplot(aes(x = period, y = seizure.rate)) + geom_boxplot() + theme_bw() + facet_grid(.~ treatment) To investigate possible outliers, we should transform the outcome with the log function first. Note: Since some participants reported no seizures during a two week period and the log(0) is unndefinded, we must add some amount to the values before transforming. Here we have chosen to add the value of \\(1\\). data_long %&gt;% ggplot(aes(x = period, y = log(seizure.rate + 1))) + geom_boxplot() + theme_bw() + facet_grid(.~ treatment) data_long %&gt;% ggplot(aes(x = period, y = log(seizure.rate + 1))) + geom_line(aes(group = subject)) + theme_bw() + facet_grid(.~ treatment) data_long %&gt;% ggplot(aes(x = period, y = log(seizure.rate + 1))) + geom_smooth(aes(group = subject), method = &quot;lm&quot;, se = FALSE) + geom_smooth(aes(group = 1), color = &quot;red&quot;, size = 1.5, method = &quot;lm&quot;, se = FALSE) + theme_bw() + facet_grid(.~ treatment) 14.4 Poisson Regression (GLM) fit_glm &lt;- glm(seizure.rate ~ base + age + treatment + offset(per), data = data_long, family = poisson(link = &quot;log&quot;)) summary(fit_glm) Call: glm(formula = seizure.rate ~ base + age + treatment + offset(per), family = poisson(link = &quot;log&quot;), data = data_long) Deviance Residuals: Min 1Q Median 3Q Max -4.436 -1.403 -0.503 0.484 12.322 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -0.130616 0.135619 -0.96 0.3355 base 0.022652 0.000509 44.48 &lt; 2e-16 *** age 0.022740 0.004024 5.65 1.6e-08 *** treatmentProgabide -0.152701 0.047805 -3.19 0.0014 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for poisson family taken to be 1) Null deviance: 2521.75 on 235 degrees of freedom Residual deviance: 958.46 on 232 degrees of freedom AIC: 1732 Number of Fisher Scoring iterations: 5 14.5 Generalized Estimating Equations (GEE) 14.5.1 Match Poisson Regresssion (GLM) correlation structure: independence scale parameter = \\(1\\) fit_gee_ind_s1 &lt;- gee::gee(seizure.rate ~ base + age + treatment + offset(per), data = data_long, family = poisson(link = &quot;log&quot;), id = subject, corstr = &quot;independence&quot;, scale.fix = TRUE, scale.value = 1) (Intercept) base age -0.13062 0.02265 0.02274 treatmentProgabide -0.15270 summary(fit_gee_ind_s1) GEE: GENERALIZED LINEAR MODELS FOR DEPENDENT DATA gee S-function, version 4.13 modified 98/01/27 (1998) Model: Link: Logarithm Variance to Mean Relation: Poisson Correlation Structure: Independent Call: gee::gee(formula = seizure.rate ~ base + age + treatment + offset(per), id = subject, data = data_long, family = poisson(link = &quot;log&quot;), corstr = &quot;independence&quot;, scale.fix = TRUE, scale.value = 1) Summary of Residuals: Min 1Q Median 3Q Max -4.9195 0.1808 1.7073 4.8851 69.9659 Coefficients: Estimate Naive S.E. Naive z Robust S.E. Robust z (Intercept) -0.13062 0.1356191 -0.9631 0.365148 -0.3577 base 0.02265 0.0005093 44.4761 0.001236 18.3316 age 0.02274 0.0040240 5.6511 0.011580 1.9637 treatmentProgabide -0.15270 0.0478051 -3.1942 0.171109 -0.8924 Estimated Scale Parameter: 1 Number of Iterations: 1 Working Correlation [,1] [,2] [,3] [,4] [1,] 1 0 0 0 [2,] 0 1 0 0 [3,] 0 0 1 0 [4,] 0 0 0 1 The estimates and the naive standard errors match the GLM exactly. The naive SE’s are much smaller (half) than the robust (sandwich) SE’s, suggesting a poor fit. 14.5.2 Change Correlation Sturucture correlation structure: exchangeable scale parameter = \\(1\\) fit_gee_exc_s1 &lt;- gee::gee(seizure.rate ~ base + age + treatment + offset(per), data = data_long, family = poisson(link = &quot;log&quot;), id = subject, corstr = &quot;exchangeable&quot;, scale.fix = TRUE, scale.value = 1) (Intercept) base age -0.13062 0.02265 0.02274 treatmentProgabide -0.15270 summary(fit_gee_exc_s1) GEE: GENERALIZED LINEAR MODELS FOR DEPENDENT DATA gee S-function, version 4.13 modified 98/01/27 (1998) Model: Link: Logarithm Variance to Mean Relation: Poisson Correlation Structure: Exchangeable Call: gee::gee(formula = seizure.rate ~ base + age + treatment + offset(per), id = subject, data = data_long, family = poisson(link = &quot;log&quot;), corstr = &quot;exchangeable&quot;, scale.fix = TRUE, scale.value = 1) Summary of Residuals: Min 1Q Median 3Q Max -4.9195 0.1808 1.7073 4.8851 69.9659 Coefficients: Estimate Naive S.E. Naive z Robust S.E. Robust z (Intercept) -0.13062 0.2004417 -0.6516 0.365148 -0.3577 base 0.02265 0.0007527 30.0926 0.001236 18.3316 age 0.02274 0.0059474 3.8236 0.011580 1.9637 treatmentProgabide -0.15270 0.0706547 -2.1612 0.171109 -0.8924 Estimated Scale Parameter: 1 Number of Iterations: 1 Working Correlation [,1] [,2] [,3] [,4] [1,] 1.0000 0.3948 0.3948 0.3948 [2,] 0.3948 1.0000 0.3948 0.3948 [3,] 0.3948 0.3948 1.0000 0.3948 [4,] 0.3948 0.3948 0.3948 1.0000 Although the estimated beta parameters are not much different, the naive SE’s are some closer to the robust SE’s. 14.5.3 Estimate the Additional Scale Parameter correlation structure: exchangeable scale parameter = freely estimated fit_gee_exc_sf &lt;- gee::gee(seizure.rate ~ base + age + treatment + offset(per), data = data_long, family = poisson(link = &quot;log&quot;), id = subject, corstr = &quot;exchangeable&quot;, scale.fix = FALSE) (Intercept) base age -0.13062 0.02265 0.02274 treatmentProgabide -0.15270 summary(fit_gee_exc_sf) GEE: GENERALIZED LINEAR MODELS FOR DEPENDENT DATA gee S-function, version 4.13 modified 98/01/27 (1998) Model: Link: Logarithm Variance to Mean Relation: Poisson Correlation Structure: Exchangeable Call: gee::gee(formula = seizure.rate ~ base + age + treatment + offset(per), id = subject, data = data_long, family = poisson(link = &quot;log&quot;), corstr = &quot;exchangeable&quot;, scale.fix = FALSE) Summary of Residuals: Min 1Q Median 3Q Max -4.9195 0.1808 1.7073 4.8851 69.9659 Coefficients: Estimate Naive S.E. Naive z Robust S.E. Robust z (Intercept) -0.13062 0.452200 -0.2888 0.365148 -0.3577 base 0.02265 0.001698 13.3388 0.001236 18.3316 age 0.02274 0.013417 1.6948 0.011580 1.9637 treatmentProgabide -0.15270 0.159398 -0.9580 0.171109 -0.8924 Estimated Scale Parameter: 5.09 Number of Iterations: 1 Working Correlation [,1] [,2] [,3] [,4] [1,] 1.0000 0.3948 0.3948 0.3948 [2,] 0.3948 1.0000 0.3948 0.3948 [3,] 0.3948 0.3948 1.0000 0.3948 [4,] 0.3948 0.3948 0.3948 1.0000 The naive SE’s are much closer inline with the robust SE’s. The sclae parameter is estimated to be much larger than \\(1\\). 14.5.4 Compare Models 14.5.4.1 Raw Estimates (logit scale) # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(fit_glm, fit_gee_ind_s1, fit_gee_exc_s1, fit_gee_exc_sf), custom.model.names = c(&quot;GLM&quot;, &quot;GEE-Indep(1)&quot;, &quot;GEE-Exchg(1)&quot;, &quot;GEE-Exchg(free)&quot;), caption = &quot;Estimates on Logit Scale&quot;, digits = 3) Estimates on Logit Scale GLM GEE-Indep(1) GEE-Exchg(1) GEE-Exchg(free) (Intercept) -0.131 -0.131 -0.131 -0.131 (0.136) (0.365) (0.365) (0.365) base 0.023*** 0.023*** 0.023*** 0.023*** (0.001) (0.001) (0.001) (0.001) age 0.023*** 0.023* 0.023* 0.023* (0.004) (0.012) (0.012) (0.012) treatmentProgabide -0.153** -0.153 -0.153 -0.153 (0.048) (0.171) (0.171) (0.171) AIC 1732.459 BIC 1746.314 Log Likelihood -862.229 Deviance 958.464 Num. obs. 236 236 236 236 Dispersion 1.000 1.000 5.090 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 14.5.4.2 Exponentiate the Estimates (odds ratio scale) # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(extract_glm_exp(fit_glm), extract_gee_exp(fit_gee_ind_s1), extract_gee_exp(fit_gee_exc_s1), extract_gee_exp(fit_gee_exc_sf)), custom.model.names = c(&quot;GLM&quot;, &quot;GEE-Indep(1)&quot;, &quot;GEE-Exchg(1)&quot;, &quot;GEE-Exchg(free)&quot;), digits = 3, caption = &quot;Estimates on Odds-Ratio Scale&quot;, caption.above = TRUE, ci.test = 1) Estimates on Odds-Ratio Scale GLM GEE-Indep(1) GEE-Exchg(1) GEE-Exchg(free) (Intercept) 0.878 0.878 0.878 0.878 [0.672; 1.144] [0.429; 1.795] [0.429; 1.795] [0.429; 1.795] base 1.023* 1.023* 1.023* 1.023* [1.022; 1.024] [1.020; 1.025] [1.020; 1.025] [1.020; 1.025] age 1.023* 1.023* 1.023* 1.023* [1.015; 1.031] [1.000; 1.046] [1.000; 1.046] [1.000; 1.046] treatmentProgabide 0.858* 0.858 0.858 0.858 [0.782; 0.943] [0.614; 1.200] [0.614; 1.200] [0.614; 1.200] AIC 1732.459 BIC 1746.314 Log Likelihood -862.229 Deviance 958.464 Num. obs. 236 236 236 236 Dispersion 1.000 1.000 5.090 * 1 outside the confidence interval CONCLUSION: There is no evidence that Progabide effects the weekly rate of epileptic seizures differently than placebo. "],
["gee-count-outcome-antibiotics-for-leprosy.html", "15 GEE, Count Outcome: Antibiotics for Leprosy 15.1 Packages 15.2 Data 15.3 Exploratory Data Analysis 15.4 Generalized Estimating Equations (GEE) 15.5 Follow-up Analysis", " 15 GEE, Count Outcome: Antibiotics for Leprosy 15.1 Packages 15.1.1 CRAN library(tidyverse) # all things tidy library(pander) # nice looking genderal tabulations library(furniture) # nice table1() descriptives library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(psych) # contains some useful functions, like headTail library(lme4) # Linear, generalized linear, &amp; nonlinear mixed models library(corrplot) # Vizualize correlation matrix library(gee) # Genderalized Estimation Equation Solver library(geepack) # Genderalized Estimation Equation Package library(MuMIn) # Multi-Model Inference (caluclate QIC) 15.1.2 GitHub Helper extract functions for exponentiating parameters form generalized regression models within a texreg table of model parameters. # install.packages(&quot;devtools&quot;) # library(devtools) # install_github(&quot;SarBearSchwartz/texreghelpr&quot;) library(texreghelpr) 15.2 Data 15.2.1 Import The following example is presented in the textbook: “Applied Longitudinal Analysis” by Garrett Fitzmaurice, Nan Laird &amp; James Ware The dataset maybe downloaded from: https://content.sph.harvard.edu/fitzmaur/ala/ Data on count of leprosy bacilli pre- and post-treatment from a clinical trial of antibiotics for leprosy. Source: Table 14.2.1 (page 422) in Snedecor, G.W. and Cochran, W.G. (1967). Statistical Methods, (6th edn). Ames, Iowa: Iowa State University Press With permission of Iowa State University Press. Reference: Snedecor, G.W. and Cochran, W.G. (1967). Statistical Methods, (6th edn). Ames, Iowa: Iowa State University Press Description: The dataset consists of count data from a placebo-controlled clinical trial of 30 patients with leprosy at the Eversley Childs Sanitorium in the Philippines. Participants in the study were randomized to either of two antibiotics (denoted treatment drug A and B) or to a placebo (denoted treatment drug C). Prior to receiving treatment, baseline data on the number of leprosy bacilli at six sites of the body where the bacilli tend to congregate were recorded for each patient. After several months of treatment, the number of leprosy bacilli at six sites of the body were recorded a second time. The outcome variable is the total count of the number of leprosy bacilli at the six sites. In this study, the question of main scientific interest is whether treatment with antibiotics (drugs A and B) reduces the abundance of leprosy bacilli at the six sites of the body when compared to placebo (drug C). Variables: Outcome or dependent variable(s) count.pre Pre-Treatment Bacilli Count count.post Post-Treatment Bacilli Count Main predictor or independent variable of interest drug the treatment group: antibiotics (drugs A and B) or placebo (drug C) data_raw &lt;- tibble::tribble( ~drug, ~count_pre, ~count_post, &quot;A&quot;, 11, 6, &quot;B&quot;, 6, 0, &quot;C&quot;, 16, 13, &quot;A&quot;, 8, 0, &quot;B&quot;, 6, 2, &quot;C&quot;, 13, 10, &quot;A&quot;, 5, 2, &quot;B&quot;, 7, 3, &quot;C&quot;, 11, 18, &quot;A&quot;, 14, 8, &quot;B&quot;, 8, 1, &quot;C&quot;, 9, 5, &quot;A&quot;, 19, 11, &quot;B&quot;, 18, 18, &quot;C&quot;, 21, 23, &quot;A&quot;, 6, 4, &quot;B&quot;, 8, 4, &quot;C&quot;, 16, 12, &quot;A&quot;, 10, 13, &quot;B&quot;, 19, 14, &quot;C&quot;, 12, 5, &quot;A&quot;, 6, 1, &quot;B&quot;, 8, 9, &quot;C&quot;, 12, 16, &quot;A&quot;, 11, 8, &quot;B&quot;, 5, 1, &quot;C&quot;, 7, 1, &quot;A&quot;, 3, 0, &quot;B&quot;, 15, 9, &quot;C&quot;, 12, 20) 15.2.2 Wide Format data_wide &lt;- data_raw %&gt;% dplyr::mutate(drug = factor(drug)) %&gt;% dplyr::mutate(id = row_number()) %&gt;% dplyr::select(id, drug, count_pre, count_post) str(data_wide) Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 30 obs. of 4 variables: $ id : int 1 2 3 4 5 6 7 8 9 10 ... $ drug : Factor w/ 3 levels &quot;A&quot;,&quot;B&quot;,&quot;C&quot;: 1 2 3 1 2 3 1 2 3 1 ... $ count_pre : num 11 6 16 8 6 13 5 7 11 14 ... $ count_post: num 6 0 13 0 2 10 2 3 18 8 ... psych::headTail(data_wide) id drug count_pre count_post 1 1 A 11 6 2 2 B 6 0 3 3 C 16 13 4 4 A 8 0 5 ... &lt;NA&gt; ... ... 6 27 C 7 1 7 28 A 3 0 8 29 B 15 9 9 30 C 12 20 15.2.3 Long Format data_long &lt;- data_wide %&gt;% tidyr::gather(key = obs, value = count, starts_with(&quot;count&quot;)) %&gt;% dplyr::mutate(time = case_when(obs == &quot;count_pre&quot; ~ 0, obs == &quot;count_post&quot; ~ 1)) %&gt;% dplyr::select(id, drug, time, count) %&gt;% dplyr::arrange(id, time) str(data_long) Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 60 obs. of 4 variables: $ id : int 1 1 2 2 3 3 4 4 5 5 ... $ drug : Factor w/ 3 levels &quot;A&quot;,&quot;B&quot;,&quot;C&quot;: 1 1 2 2 3 3 1 1 2 2 ... $ time : num 0 1 0 1 0 1 0 1 0 1 ... $ count: num 11 6 6 0 16 13 8 0 6 2 ... psych::headTail(data_long) id drug time count 1 1 A 0 11 2 1 A 1 6 3 2 B 0 6 4 2 B 1 0 5 ... &lt;NA&gt; ... ... 6 29 B 0 15 7 29 B 1 9 8 30 C 0 12 9 30 C 1 20 15.3 Exploratory Data Analysis 15.3.1 Summary Statistics data_long %&gt;% dplyr::group_by(drug, time) %&gt;% dplyr::summarise(N = n(), M = mean(count), VAR = var(count), SD = sd(count)) %&gt;% pander::pander() drug time N M VAR SD A 0 10 9.3 23 4.8 A 1 10 5.3 22 4.6 B 0 10 10.0 28 5.2 B 1 10 6.1 38 6.2 C 0 10 12.9 16 4.0 C 1 10 12.3 51 7.2 15.3.2 Visualize data_long %&gt;% dplyr::mutate(time_name = case_when(time == 0 ~ &quot;Pre&quot;, time == 1 ~ &quot;Post&quot;) %&gt;% factor(levels = c(&quot;Pre&quot;, &quot;Post&quot;))) %&gt;% dplyr::mutate(drug_name = fct_recode(drug, &quot;Antibiotic A&quot; = &quot;A&quot;, &quot;Antibiotic B&quot; = &quot;B&quot;, &quot;Placebo&quot; = &quot;C&quot;)) %&gt;% ggplot(aes(x = time_name, y = count)) + geom_line(aes(group = id)) + facet_grid(.~ drug_name) + theme_bw() + labs(x = NULL, y = &quot;Number of Leprosy Bacilli at Six Sites of the Body&quot;) data_long %&gt;% dplyr::mutate(time_name = case_when(time == 0 ~ &quot;Pre&quot;, time == 1 ~ &quot;Post&quot;) %&gt;% factor(levels = c(&quot;Pre&quot;, &quot;Post&quot;))) %&gt;% dplyr::mutate(drug_name = fct_recode(drug, &quot;Antibiotic A&quot; = &quot;A&quot;, &quot;Antibiotic B&quot; = &quot;B&quot;, &quot;Placebo&quot; = &quot;C&quot;)) %&gt;% ggplot(aes(x = time, y = count)) + geom_line(aes(group = id), color = &quot;gray&quot;) + geom_smooth(aes(group = drug), method = &quot;lm&quot;) + facet_grid(.~ drug_name) + theme_bw() + labs(x = NULL, y = &quot;Number of Leprosy Bacilli at Six Sites of the Body&quot;) data_long %&gt;% dplyr::mutate(time_name = case_when(time == 0 ~ &quot;Pre&quot;, time == 1 ~ &quot;Post&quot;) %&gt;% factor(levels = c(&quot;Pre&quot;, &quot;Post&quot;))) %&gt;% dplyr::mutate(drug_name = fct_recode(drug, &quot;Antibiotic A&quot; = &quot;A&quot;, &quot;Antibiotic B&quot; = &quot;B&quot;, &quot;Placebo&quot; = &quot;C&quot;)) %&gt;% ggplot(aes(x = time, y = count)) + geom_smooth(aes(group = drug, color = drug_name, fill = drug_name), method = &quot;lm&quot;, alpha = .2) + theme_bw() + labs(x = NULL, y = &quot;Number of Leprosy Bacilli at Six Sites of the Body&quot;, color = NULL, fill = NULL) + scale_x_continuous(breaks = 0:1, labels = c(&quot;Pre-Treatment&quot;, &quot;Post-Treatment&quot;)) 15.4 Generalized Estimating Equations (GEE) 15.4.0.1 The gee() function in the gee package mod_gee_ind &lt;- gee::gee(count ~ drug*time, data = data_long, family = poisson(link = &quot;log&quot;), id = id, corstr = &quot;independence&quot;) (Intercept) drugB drugC time drugB:time drugC:time 2.23001 0.07257 0.32721 -0.56231 0.06801 0.51468 mod_gee_exc &lt;- gee::gee(count ~ drug*time, data = data_long, family = poisson(link = &quot;log&quot;), id = id, corstr = &quot;exchangeable&quot;) (Intercept) drugB drugC time drugB:time drugC:time 2.23001 0.07257 0.32721 -0.56231 0.06801 0.51468 mod_gee_uns &lt;- gee::gee(count ~ drug*time, data = data_long, family = poisson(link = &quot;log&quot;), id = id, corstr = &quot;unstructured&quot;) (Intercept) drugB drugC time drugB:time drugC:time 2.23001 0.07257 0.32721 -0.56231 0.06801 0.51468 15.4.1 Compare Models 15.4.1.1 Raw Estimates (log-rate scale) # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(mod_gee_ind, mod_gee_exc, mod_gee_uns), custom.model.names = c(&quot;Independence&quot;, &quot;Exchangeable&quot;, &quot;Unstructured&quot;), single.row = TRUE, digits = 3, caption = &quot;Estimates on Log Scale&quot;) Estimates on Log Scale Independence Exchangeable Unstructured (Intercept) 2.230 (0.154)*** 2.230 (0.154)*** 2.230 (0.154)*** drugB 0.073 (0.220) 0.073 (0.220) 0.073 (0.220) drugC 0.327 (0.179) 0.327 (0.179) 0.327 (0.179) time -0.562 (0.176)** -0.562 (0.176)** -0.562 (0.176)** drugB:time 0.068 (0.246) 0.068 (0.246) 0.068 (0.246) drugC:time 0.515 (0.221)* 0.515 (0.221)* 0.515 (0.221)* Dispersion 3.474 3.474 3.474 Num. obs. 60 60 60 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 15.4.1.2 Exponentiate the Estimates (risk scale) # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(extract_gee_exp(mod_gee_ind), extract_gee_exp(mod_gee_exc), extract_gee_exp(mod_gee_uns)), custom.model.names = c(&quot;Independence&quot;, &quot;Exchangeable&quot;, &quot;Unstructured&quot;), single.row = TRUE, digits = 3, ci.test = 1, caption = &quot;Estimates on Count Scale&quot;) Estimates on Count Scale Independence Exchangeable Unstructured (Intercept) 9.300 [6.882; 12.567]* 9.300 [6.882; 12.567]* 9.300 [6.882; 12.567]* drugB 1.075 [0.699; 1.655] 1.075 [0.699; 1.655] 1.075 [0.699; 1.655] drugC 1.387 [0.977; 1.970] 1.387 [0.977; 1.970] 1.387 [0.977; 1.970] time 0.570 [0.404; 0.805]* 0.570 [0.404; 0.805]* 0.570 [0.404; 0.805]* drugB:time 1.070 [0.661; 1.734] 1.070 [0.661; 1.734] 1.070 [0.661; 1.734] drugC:time 1.673 [1.086; 2.578]* 1.673 [1.086; 2.578]* 1.673 [1.086; 2.578]* Dispersion 3.474 3.474 3.474 Num. obs. 60 60 60 * 1 outside the confidence interval 15.4.1.3 Manual Extraction mod_gee_exc %&gt;% coef() %&gt;% exp() (Intercept) drugB drugC time drugB:time drugC:time 9.3000 1.0753 1.3871 0.5699 1.0704 1.6731 0.5699*1.6731 [1] 0.9535 Interpretation Antibiotic A Group: Starts with mean of 9.3 and drops by 45% (nearly cut in half) over the course of treatment. Antibiotic B Group: Starts at about the same mean at Antibiotic A group and experiences the same decrease. Control Group (C): Starts at about the same mean at Antibiotic A group BUT experiences a less than a 5% decrease over the student period while on the placebo pills. 15.4.2 Visualize the Final Model 15.4.2.1 Refit with the geeglm() function in the geepack package mod_geeglm_exc &lt;- geepack::geeglm(count ~ drug*time, data = data_long, family = poisson(link = &quot;log&quot;), id = id, corstr = &quot;exchangeable&quot;) summary(mod_geeglm_exc) Call: geepack::geeglm(formula = count ~ drug * time, family = poisson(link = &quot;log&quot;), data = data_long, id = id, corstr = &quot;exchangeable&quot;) Coefficients: Estimate Std.err Wald Pr(&gt;|W|) (Intercept) 2.2300 0.1536 210.74 &lt;2e-16 *** drugB 0.0726 0.2200 0.11 0.7415 drugC 0.3272 0.1791 3.34 0.0677 . time -0.5623 0.1760 10.21 0.0014 ** drugB:time 0.0680 0.2460 0.08 0.7822 drugC:time 0.5147 0.2206 5.45 0.0196 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Estimated Scale Parameters: Estimate Std.err (Intercept) 3.13 0.513 Correlation: Structure = exchangeable Link = identity Estimated Correlation Parameters: Estimate Std.err alpha 0.735 0.081 Number of clusters: 30 Maximum cluster size: 2 15.4.2.2 Predict over a manual grid of predictors expand.grid(drug = levels(data_long$drug), time = 0:1) %&gt;% dplyr::mutate(fit = predict(mod_geeglm_exc, newdata = ., type = &quot;response&quot;)) drug time fit 1 A 0 9.3 2 B 0 10.0 3 C 0 12.9 4 A 1 5.3 5 B 1 6.1 6 C 1 12.3 expand.grid(drug = levels(data_long$drug), time = 0:1) %&gt;% dplyr::mutate(fit = predict(mod_geeglm_exc, newdata = ., type = &quot;response&quot;)) %&gt;% dplyr::mutate(time_name = case_when(time == 0 ~ &quot;Pre&quot;, time == 1 ~ &quot;Post&quot;) %&gt;% factor(levels = c(&quot;Pre&quot;, &quot;Post&quot;))) %&gt;% dplyr::mutate(drug_name = fct_recode(drug, &quot;Antibiotic A&quot; = &quot;A&quot;, &quot;Antibiotic B&quot; = &quot;B&quot;, &quot;Placebo&quot; = &quot;C&quot;)) %&gt;% ggplot(aes(x = time_name, y = fit, group = drug_name %&gt;% fct_rev, color = drug_name %&gt;% fct_rev)) + geom_point() + geom_line() + theme_bw() + labs(x = NULL, y = &quot;Estimated Marginal Mean\\nNumber of Leprosy Bacilli at Six Sites of the Body&quot;, color = NULL) + theme(legend.position = c(0, 0), legend.justification = c(-0.1, -0.1), legend.background = element_rect(color = &quot;black&quot;)) 15.5 Follow-up Analysis 15.5.1 Collapse the Predictor data_remodel &lt;- data_long %&gt;% dplyr::mutate(antibiotic = fct_collapse(drug, yes = c(&quot;A&quot;, &quot;B&quot;), no = c(&quot;C&quot;))) 15.5.2 Reduce the Model - gee::gee() mod_gee_exc2 &lt;- gee::gee(count ~ antibiotic:time , data = data_remodel, family = poisson(link = &quot;log&quot;), id = id, corstr = &quot;exchangeable&quot;) (Intercept) antibioticyes:time antibioticno:time 2.373 -0.633 0.136 summary(mod_gee_exc2) GEE: GENERALIZED LINEAR MODELS FOR DEPENDENT DATA gee S-function, version 4.13 modified 98/01/27 (1998) Model: Link: Logarithm Variance to Mean Relation: Poisson Correlation Structure: Exchangeable Call: gee::gee(formula = count ~ antibiotic:time, id = id, data = data_remodel, family = poisson(link = &quot;log&quot;), corstr = &quot;exchangeable&quot;) Summary of Residuals: Min 1Q Median 3Q Max -9.618 -4.733 -0.484 3.517 12.382 Coefficients: Estimate Naive S.E. Naive z Robust S.E. Robust z (Intercept) 2.3734 0.103 23.0780 0.0801 29.6159 antibioticyes:time -0.5249 0.102 -5.1243 0.1112 -4.7183 antibioticno:time -0.0108 0.114 -0.0942 0.1572 -0.0684 Estimated Scale Parameter: 3.41 Number of Iterations: 5 Working Correlation [,1] [,2] [1,] 1.00 0.78 [2,] 0.78 1.00 15.5.3 Compare Parameters # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(extract_gee_exp(mod_gee_exc), extract_gee_exp(mod_gee_exc2)), custom.model.names = c(&quot;Original&quot;, &quot;Refit&quot;), single.row = TRUE, digits = 3, ci.test = 1, caption = &quot;Estimates on Count Scale (Exchangeable)&quot;) Estimates on Count Scale (Exchangeable) Original Refit (Intercept) 9.300 [6.882; 12.567]* 10.733 [9.173; 12.559]* drugB 1.075 [0.699; 1.655] drugC 1.387 [0.977; 1.970] time 0.570 [0.404; 0.805]* drugB:time 1.070 [0.661; 1.734] drugC:time 1.673 [1.086; 2.578]* antibioticyes:time 0.592 [0.476; 0.736]* antibioticno:time 0.989 [0.727; 1.346] Dispersion 3.474 3.406 Num. obs. 60 60 * 1 outside the confidence interval Interpretation Grand mean is 10.73 at pre-treatment. The mean count dropped by about 40% among thoes on antibiotics, but there was no decrease for thoes on placebo pills. 15.5.4 Visualize 15.5.4.1 Refit with geepack::geeglm() mod_geeglm_exc2 &lt;- geepack::geeglm(count ~ antibiotic:time, data = data_remodel, family = poisson(link = &quot;log&quot;), id = id, corstr = &quot;exchangeable&quot;) 15.5.4.2 Predict over a manual grid expand.grid(antibiotic = levels(data_remodel$antibiotic), time = 0:1) %&gt;% dplyr::mutate(fit = predict(mod_geeglm_exc2, newdata = ., type = &quot;response&quot;)) %&gt;% dplyr::mutate(time_name = case_when(time == 0 ~ &quot;Pre&quot;, time == 1 ~ &quot;Post&quot;) %&gt;% factor(levels = c(&quot;Pre&quot;, &quot;Post&quot;))) %&gt;% ggplot(aes(x = time_name, y = fit, group = antibiotic %&gt;% fct_rev, color = antibiotic %&gt;% fct_rev)) + geom_point() + geom_line() + theme_bw() + labs(x = NULL, y = &quot;Estimated Marginal Mean\\nNumber of Leprosy Bacilli at Six Sites of the Body&quot;, color = &quot;Antibiotic&quot;) + theme(legend.position = c(0, 0), legend.justification = c(-0.1, -0.1), legend.background = element_rect(color = &quot;black&quot;)) "],
["glmm-binary-outcome-antibiotics-for-leprosy.html", "16 GLMM, Binary Outcome: Antibiotics for Leprosy 16.1 Packages 16.2 Data Prep 16.3 Exploratory Data Analysis 16.4 GLMM - Basic 16.5 GLMM - Optimizers 16.6 Quadratic Time?", " 16 GLMM, Binary Outcome: Antibiotics for Leprosy 16.1 Packages 16.1.1 CRAN library(tidyverse) # all things tidy library(pander) # nice looking genderal tabulations library(furniture) # nice table1() descriptives library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(psych) # contains some useful functions, like headTail library(lme4) # Linear, generalized linear, &amp; nonlinear mixed models library(effects) # Plotting estimated marginal means library(optimx) # Unify and streamline optimization capabilities in R 16.1.2 GitHub Helper extract functions for exponentiating parameters form generalized regression models within a texreg table of model parameters. # install.packages(&quot;devtools&quot;) # library(devtools) # install_github(&quot;SarBearSchwartz/texreghelpr&quot;) library(texreghelpr) 16.2 Data Prep Data on Amenorrhea from Clinical Trial of Contracepting Women. Source: Table 1 (page 168) of Machin et al. (1988). With permission of Elsevier. Reference: Machin D, Farley T, Busca B, Campbell M and d’Arcangues C. (1988). Assessing changes in vaginal bleeding patterns in contracepting women. Contraception, 38, 165-179. Description: The data are from a longitudinal clinical trial of contracepting women. In this trial women received an injection of either 100 mg or 150 mg of depot-medroxyprogesterone acetate (DMPA) on the day of randomization and three additional injections at 90-day intervals. There was a final follow-up visit 90 days after the fourth injection, i.e., one year after the first injection. Throughout the study each woman completed a menstrual diary that recorded any vaginal bleeding pattern disturbances. The diary data were used to determine whether a women experienced amenorrhea, the absence of menstrual bleeding for a specified number of days. A total of 1151 women completed the menstrual diaries and the diary data were used to generate a binary sequence for each woman according to whether or not she had experienced amenorrhea in the four successive three month intervals. In clinical trials of modern hormonal contraceptives, pregnancy is exceedingly rare (and would be regarded as a failure of the contraceptive method), and is not the main outcome of interest in this study. Instead, the outcome of interest is a binary response indicating whether a woman experienced amenorrhea in the four successive three month intervals. A feature of this clinical trial is that there was substantial dropout. More than one third of the women dropped out before the completion of the trial. Variable List: Indicators id participant identification occasion denotes the four 90-day periods Outcome or dependent variable amenorrhea Amenorrhea Status: 1=Amenorrhea, 0=No Amenorrhea Main predictor or independent variable of interest dose 0 = Low (100 mg), 1 = High (150 mg) 16.2.1 Import data_raw &lt;- read.table(&quot;https://raw.githubusercontent.com/CEHS-research/data/master/MLM/RCTcontraception.txt&quot;, header=TRUE) str(data_raw) &#39;data.frame&#39;: 4604 obs. of 4 variables: $ id : int 1 1 1 1 2 2 2 2 3 3 ... $ dose : int 0 0 0 0 0 0 0 0 0 0 ... $ occasion : int 1 2 3 4 1 2 3 4 1 2 ... $ amenorrhea: Factor w/ 3 levels &quot;.&quot;,&quot;0&quot;,&quot;1&quot;: 2 1 1 1 2 1 1 1 2 1 ... psych::headTail(data_raw, top = 10) id dose occasion amenorrhea 1 1 0 1 0 2 1 0 2 . 3 1 0 3 . 4 1 0 4 . 5 2 0 1 0 6 2 0 2 . 7 2 0 3 . 8 2 0 4 . 9 3 0 1 0 10 3 0 2 . ... ... ... ... &lt;NA&gt; 4601 1151 1 1 1 4602 1151 1 2 1 4603 1151 1 3 1 4604 1151 1 4 1 16.2.2 Long Format data_long &lt;- data_raw %&gt;% dplyr::mutate(id = factor(id)) %&gt;% dplyr::mutate(dose = factor(dose, levels = c(&quot;0&quot;, &quot;1&quot;), labels = c(&quot;Low&quot;, &quot;High&quot;))) %&gt;% dplyr::mutate(time = occasion - 1) %&gt;% dplyr::mutate(amenorrhea = amenorrhea %&gt;% # outcome needs to be numeric as.character() %&gt;% as.numeric()) %&gt;% dplyr::filter(complete.cases(amenorrhea)) %&gt;% # dump missing occations dplyr::arrange(id, time) str(data_long) &#39;data.frame&#39;: 3616 obs. of 5 variables: $ id : Factor w/ 1151 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... $ dose : Factor w/ 2 levels &quot;Low&quot;,&quot;High&quot;: 1 1 1 1 1 1 1 1 1 1 ... $ occasion : int 1 1 1 1 1 1 1 1 1 1 ... $ amenorrhea: num 0 0 0 0 0 0 0 0 0 0 ... $ time : num 0 0 0 0 0 0 0 0 0 0 ... psych::headTail(data_long, bottom = 10) id dose occasion amenorrhea time 1 1 Low 1 0 0 2 2 Low 1 0 0 3 3 Low 1 0 0 4 4 Low 1 0 0 ... &lt;NA&gt; &lt;NA&gt; ... ... ... 3607 1149 High 3 1 2 3608 1149 High 4 1 3 3609 1150 High 1 1 0 3610 1150 High 2 1 1 3611 1150 High 3 1 2 3612 1150 High 4 1 3 3613 1151 High 1 1 0 3614 1151 High 2 1 1 3615 1151 High 3 1 2 3616 1151 High 4 1 3 16.2.3 Wide Format data_wide &lt;- data_long %&gt;% dplyr::select(-time) %&gt;% tidyr::spread(key = occasion, value = amenorrhea, sep = &quot;_&quot;) str(data_wide) &#39;data.frame&#39;: 1151 obs. of 6 variables: $ id : Factor w/ 1151 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... $ dose : Factor w/ 2 levels &quot;Low&quot;,&quot;High&quot;: 1 1 1 1 1 1 1 1 1 1 ... $ occasion_1: num 0 0 0 0 0 0 0 0 0 0 ... $ occasion_2: num NA NA NA NA NA NA NA NA NA NA ... $ occasion_3: num NA NA NA NA NA NA NA NA NA NA ... $ occasion_4: num NA NA NA NA NA NA NA NA NA NA ... psych::headTail(data_wide, bottom = 10) id dose occasion_1 occasion_2 occasion_3 occasion_4 1 1 Low 0 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 2 2 Low 0 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 3 3 Low 0 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 4 4 Low 0 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ... &lt;NA&gt; &lt;NA&gt; ... ... ... ... 1142 1142 High 1 1 1 1 1143 1143 High 1 1 1 1 1144 1144 High 1 1 1 1 1145 1145 High 1 1 1 1 1146 1146 High 1 1 1 1 1147 1147 High 1 1 1 1 1148 1148 High 1 1 1 1 1149 1149 High 1 1 1 1 1150 1150 High 1 1 1 1 1151 1151 High 1 1 1 1 16.3 Exploratory Data Analysis 16.3.1 Summary Statistics data_summary &lt;- data_long %&gt;% dplyr::group_by(dose, occasion) %&gt;% dplyr::summarise(N = n(), M = mean(amenorrhea), SD = sd(amenorrhea), SE = SD/sqrt(N)) data_summary # A tibble: 8 x 6 # Groups: dose [?] dose occasion N M SD SE &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Low 1 576 0.186 0.389 0.0162 2 Low 2 477 0.262 0.440 0.0202 3 Low 3 409 0.389 0.488 0.0241 4 Low 4 361 0.501 0.501 0.0264 5 High 1 575 0.205 0.404 0.0169 6 High 2 476 0.336 0.473 0.0217 7 High 3 389 0.494 0.501 0.0254 8 High 4 353 0.535 0.499 0.0266 16.3.2 Visualize data_summary %&gt;% ggplot(aes(x = occasion, y = M, fill = dose)) + geom_col(position = &quot;dodge&quot;) + theme_bw() + theme(legend.position = c(0, 1), legend.justification = c(-0.1, 1.1), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(1.5, &quot;cm&quot;)) + labs(x = &quot;90-day windows&quot;, y = &quot;Observed Proportion of Amenorrhea&quot;, fill = &quot;Dosage&quot;) + scale_x_continuous(breaks = 1:4, labels = c(&quot;First&quot;, &quot;Second&quot;, &quot;Third&quot;, &quot;Fourth&quot;)) data_summary %&gt;% ggplot(aes(x = occasion, y = M, color = dose %&gt;% fct_rev())) + geom_errorbar(aes(ymin = M - SE, ymax = M + SE), width = .3, position = position_dodge(width = .25)) + geom_point(position = position_dodge(width = .25)) + geom_line(position = position_dodge(width = .25)) + theme_bw() + theme(legend.position = c(0, 1), legend.justification = c(-0.1, 1.1), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(1.5, &quot;cm&quot;)) + labs(x = &quot;90-day windows&quot;, y = &quot;Observed Proportion of Amenorrhea&quot;, color = &quot;Dosage&quot;) + scale_x_continuous(breaks = 1:4, labels = c(&quot;First&quot;, &quot;Second&quot;, &quot;Third&quot;, &quot;Fourth&quot;)) 16.4 GLMM - Basic 16.4.1 Fit Models fit_1 &lt;- lme4::glmer(amenorrhea ~ time*dose + (1 | id), data = data_long, family = binomial(link = &quot;logit&quot;)) fit_2 &lt;- lme4::glmer(amenorrhea ~ time + dose + (1 | id), data = data_long, family = binomial(link = &quot;logit&quot;)) 16.4.1.1 Compare via LRT Should the interaction be included? No. anova(fit_1, fit_2) Data: data_long Models: fit_2: amenorrhea ~ time + dose + (1 | id) fit_1: amenorrhea ~ time * dose + (1 | id) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) fit_2 4 3931 3956 -1961 3923 fit_1 5 3932 3963 -1961 3922 0.69 1 0.41 16.4.2 Model Parameter Tables 16.4.2.1 Logit Scale # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(fit_1, fit_2), custom.model.names = c(&quot;with&quot;, &quot;without&quot;), single.row = TRUE, caption = &quot;MLM Parameter Estimates: Inclusion of Interaction (SE and p-values)&quot;) MLM Parameter Estimates: Inclusion of Interaction (SE and p-values) with without (Intercept) -2.55 (0.17)*** -2.61 (0.16)*** time 0.87 (0.07)*** 0.91 (0.05)*** doseHigh 0.39 (0.21) 0.50 (0.16)** time:doseHigh 0.08 (0.09) AIC 3932.14 3930.83 BIC 3963.11 3955.61 Log Likelihood -1961.07 -1961.42 Num. obs. 3616 3616 Num. groups: id 1151 1151 Var: id (Intercept) 4.26 4.24 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(fit_1, fit_2), custom.model.names = c(&quot;with&quot;, &quot;without&quot;), ci.force = TRUE, single.row = TRUE, caption = &quot;MLM Parameter Estimates: Inclusion of Interaction (95% CI&#39;s)&quot;) MLM Parameter Estimates: Inclusion of Interaction (95% CI’s) with without (Intercept) -2.55 [-2.89; -2.22]* -2.61 [-2.92; -2.30]* time 0.87 [0.74; 1.01]* 0.91 [0.81; 1.02]* doseHigh 0.39 [-0.01; 0.79] 0.50 [0.17; 0.82]* time:doseHigh 0.08 [-0.10; 0.26] AIC 3932.14 3930.83 BIC 3963.11 3955.61 Log Likelihood -1961.07 -1961.42 Num. obs. 3616 3616 Num. groups: id 1151 1151 Var: id (Intercept) 4.26 4.24 * 0 outside the confidence interval 16.4.2.2 Odds ratio scale # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(extract_glmer_exp(fit_1), extract_glmer_exp(fit_2)), custom.model.names = c(&quot;with&quot;, &quot;without&quot;), ci.test = 1, ci.force = TRUE, single.row = TRUE, caption = &quot;MLM Parameter Estimates: Inclusion of Interaction (95% CI&#39;s)&quot;) MLM Parameter Estimates: Inclusion of Interaction (95% CI’s) with without (Intercept) 0.08 [0.06; 0.11]* 0.07 [0.05; 0.10]* time 2.40 [2.08; 2.75]* 2.49 [2.24; 2.77]* doseHigh 1.48 [0.98; 2.22] 1.64 [1.19; 2.27]* time:doseHigh 1.08 [0.90; 1.30] AIC 3932.14 3930.83 BIC 3963.11 3955.61 Log Likelihood -1961.07 -1961.42 Num. obs. 3616 3616 Num. groups: id 1151 1151 Var: id (Intercept) 4.26 4.24 * 1 outside the confidence interval 16.4.3 Visualize the Model 16.4.3.1 Scale = Logit effects::Effect(focal.predictors = c(&quot;dose&quot;, &quot;time&quot;), xlevels = list(time = seq(from = 0, to = 3, by = .1)), mod = fit_2, transform = NULL) %&gt;% data.frame %&gt;% ggplot(aes(x = time, y = fit)) + geom_hline(yintercept = 0, # reference line color = &quot;gray&quot;, size = 1.5) + geom_ribbon(aes(ymin = fit - se, # Mean +/- 1 SEM ymax = fit + se, fill = dose), alpha = .2) + geom_line(aes(color = dose), size = 1.5) + theme_bw() + labs(y = &quot;Logit Scale&quot;) 16.4.3.2 Scale = Probability effects::Effect(focal.predictors = c(&quot;dose&quot;, &quot;time&quot;), xlevels = list(time = seq(from = 0, to = 3, by = .1)), mod = fit_2) %&gt;% data.frame %&gt;% ggplot(aes(x = time, y = fit)) + geom_hline(yintercept = c(0, 0.5, 1), # reference lines color = &quot;gray&quot;, size = 1.5) + geom_ribbon(aes(ymin = fit - se, ymax = fit + se, fill = dose), alpha = .2) + geom_line(aes(color = dose), size = 1.5) + theme_bw() + labs(y = &quot;Predicted Probability&quot;) Remove the error bands: effects::Effect(focal.predictors = c(&quot;dose&quot;, &quot;time&quot;), xlevels = list(time = seq(from = 0, to = 3, by = .1)), mod = fit_2) %&gt;% data.frame %&gt;% ggplot(aes(x = time, y = fit)) + geom_hline(yintercept = c(0, 0.5), color = &quot;gray&quot;, size = 1.5) + geom_line(aes(linetype = dose), size = 1) + theme_bw() + theme(legend.position = c(0, 1), legend.justification = c(-0.1, 1.1), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(1.5, &quot;cm&quot;)) + labs(x = &quot;90-day Window&quot;, y = &quot;Predicted Probability of Amenorrhea&quot;, linetype = &quot;Dosage:&quot;) + scale_x_continuous(breaks = 0:3, labels = c(&quot;First&quot;, &quot;Second&quot;, &quot;Third&quot;, &quot;Fourth&quot;)) 16.5 GLMM - Optimizers From the documentation: The lme4::glmer() function fits a generalized linear mixed model, which incorporates both fixed-effects parameters and random effects in a linear predictor, via maximum likelihood. The linear predictor is related to the conditional mean of the response through the inverse link function defined in the GLM family. The expression for the likelihood of a mixed-effects model is an integral over the random effects space. For a linear mixed-effects model (LMM), as fit by lmer, this integral can be evaluated exactly. For a GLMM the integral must be approximated. The most reliable approximation for GLMMs is adaptive Gauss-Hermite quadrature, at present implemented only for models with a single scalar random effect. The nAGQ argument controls the number of nodes in the quadrature formula. A model with a single, scalar random-effects term could reasonably use up to 25 quadrature points per scalar integral. The lme4::lmerControl() function includes an argument for the optimizer, which is the name of a optimizing function(s). IT is a character vector or list of functions: length 1 for lmer or glmer, possibly length 2 for glmer). The built-in optimizers are Nelder_Mead and bobyqa (from the minqa package). Other minimizing functions are allows (constraints do apply). Special provisions are made for bobyqa, Nelder_Mead, and optimizers wrapped in the optimx package; to use the optimx optimizers (including L-BFGS-B from base optim and nlminb), pass the method argument to optim in the optCtrl argument (you may also need to load the optimx package manually using library(optimx). 16.5.1 Adaptive Gauss-Hermite Quadrature: Increase the number of quadrature points nAGQ (integer scalar) the number of points per axis for evaluating the adaptive Gauss-Hermite approximation to the log-likelihood. Defaults to 1, corresponding to the Laplace approximation. Values greater than 1 produce greater accuracy in the evaluation of the log-likelihood at the expense of speed. A value of zero uses a faster but less exact form of parameter estimation for GLMMs by optimizing the random effects and the fixed-effects coefficients in the penalized iteratively reweighted least squares step. (See Details.) fit_3a &lt;- lme4::glmer(amenorrhea ~ time + I(time^2) + time:dose + I(time^2):dose + (1 | id), data = data_long, nAGQ = 50, # increase the number of points family = binomial) 16.5.2 Laplace Approximation: switch to the Nelder_Mead optimizer fit_3b &lt;- lme4::glmer(amenorrhea ~ time + I(time^2) + time:dose + I(time^2):dose + (1 | id), data = data_long, control = glmerControl(optimizer =&quot;Nelder_Mead&quot;), family = binomial) 16.5.3 Laplace Approximation: Switch to the L-BFGS-B method fit_3c &lt;- lme4::glmer(amenorrhea ~ time + I(time^2) + time:dose + I(time^2):dose + (1 | id), data = data_long, control = glmerControl(optimizer =&#39;optimx&#39;, optCtrl=list(method=&#39;L-BFGS-B&#39;)), family = binomial) 16.5.4 Laplace Approximation: Switch to the nlminb method fit_3d &lt;- lme4::glmer(amenorrhea ~ time + I(time^2) + time:dose + I(time^2):dose + (1 | id), data = data_long, control = glmerControl(optimizer =&#39;optimx&#39;, optCtrl=list(method=&#39;nlminb&#39;)), family = binomial) 16.6 Quadratic Time? Assess need for quadratic time with the LRT anova(fit_2, fit_3d) Data: data_long Models: fit_2: amenorrhea ~ time + dose + (1 | id) fit_3d: amenorrhea ~ time + I(time^2) + time:dose + I(time^2):dose + fit_3d: (1 | id) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) fit_2 4 3931 3956 -1961 3923 fit_3d 6 3925 3962 -1957 3913 9.72 2 0.0078 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 texreg::htmlreg(list(fit_3a, fit_3b, fit_3c, fit_3d), custom.model.names = c(&quot;nAGQ&quot;, &quot;Nelder_Mead&quot;, &quot;L BFGS B&quot;, &quot;nlminb&quot;), caption = &quot;GLMM: Various methods of ML approximation&quot;, digits = 4) GLMM: Various methods of ML approximation nAGQ Nelder_Mead L BFGS B nlminb (Intercept) -2.4829*** -2.4604*** -2.4601*** -2.4604*** (0.1416) (0.1397) (0.1397) (0.1397) time 0.7714*** 0.7561*** 0.7558*** 0.7561*** (0.2026) (0.1985) (0.1985) (0.1985) I(time^2) 0.0346 0.0340 0.0341 0.0340 (0.0667) (0.0655) (0.0655) (0.0655) time:doseHigh 0.8920*** 0.8861*** 0.8860*** 0.8861*** (0.2574) (0.2513) (0.2513) (0.2513) I(time^2):doseHigh -0.2599** -0.2579** -0.2579** -0.2579** (0.0895) (0.0879) (0.0879) (0.0879) AIC 3879.4906 3925.1127 3925.1128 3925.1127 BIC 3916.6493 3962.2715 3962.2715 3962.2715 Log Likelihood -1933.7453 -1956.5564 -1956.5564 -1956.5564 Num. obs. 3616 3616 3616 3616 Num. groups: id 1151 1151 1151 1151 Var: id (Intercept) 5.0794 4.3478 4.3494 4.3479 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 effects::Effect(focal.predictors = c(&quot;dose&quot;, &quot;time&quot;), xlevels = list(time = seq(from = 0, to = 3, by = .1)), mod = fit_3d) %&gt;% data.frame %&gt;% ggplot(aes(x = time, y = fit)) + geom_hline(yintercept = c(0, 0.5), color = &quot;gray&quot;, size = 1.5) + geom_line(aes(linetype = dose), size = 1) + theme_bw() + theme(legend.position = c(0, 1), legend.justification = c(-0.1, 1.1), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(1.5, &quot;cm&quot;)) + labs(x = &quot;90-day Window&quot;, y = &quot;Predicted Probability of Amenorrhea&quot;, linetype = &quot;Dosage:&quot;) + scale_x_continuous(breaks = 0:3, labels = c(&quot;First&quot;, &quot;Second&quot;, &quot;Third&quot;, &quot;Fourth&quot;)) "],
["glmm-binary-outcome-antibiotics-for-leprosy-1.html", "17 GLMM, Binary Outcome: Antibiotics for Leprosy 17.1 Packages 17.2 Data Prep 17.3 Exploratory Data Analysis 17.4 Analysis Goal 17.5 GLM Analysis 17.6 GEE Analysis 17.7 GLMM Analysis 17.8 Compare Methods", " 17 GLMM, Binary Outcome: Antibiotics for Leprosy 17.1 Packages 17.1.1 CRAN library(tidyverse) # all things tidy library(pander) # nice looking genderal tabulations library(furniture) # nice table1() descriptives library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(psych) # contains some useful functions, like headTail library(lme4) # Linear, generalized linear, &amp; nonlinear mixed models library(gee) # Generalized Estimating Equations library(effects) # Plotting estimated marginal means library(optimx) # Unify and streamline optimization capabilities in R 17.1.2 GitHub Helper extract functions for exponentiating parameters form generalized regression models within a texreg table of model parameters. # install.packages(&quot;devtools&quot;) # library(devtools) # install_github(&quot;SarBearSchwartz/texreghelpr&quot;) library(texreghelpr) 17.2 Data Prep Data on Obesity from the Muscatine Coronary Risk Factor Study. Source: Table 10 (page 96) in Woolson and Clarke (1984). With permission of Blackwell Publishing. Reference: Woolson, R.F. and Clarke, W.R. (1984). Analysis of categorical incompletel longitudinal data. Journal of the Royal Statistical Society, Series A, 147, 87-99. Description: The Muscatine Coronary Risk Factor Study (MCRFS) was a longitudinal study of coronary risk factors in school children in Muscatine, Iowa (Woolson and Clarke 1984; Ekholm and Skinner 1998). Five cohorts of children were measured for height and weight in 1977, 1979, and 1981. Relative weight was calculated as the ratio of a child’s observed weight to the median weight for their age-sex-height group. Children with a relative weight greater than 110% of the median weight for their respective stratum were classified as obese. The analysis of this study involves binary data (1 = obese, 0 = not obese) collected at successive time points. This data was also using in an article title “Missing data methods in longitudinal studies: a review” (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3016756/). Variable List: Indicators id Child’s unique identification number occas Occasion number: 1, 2, 3 Outcome or dependent variable obesity Obesity Status, 0 = no, 1 = yes Main predictor or independent variable of interest gender 0 = Male, 1 = Female baseage Baseline Age, mid-point of age-cohort currage Current Age, mid-point of age-cohort 17.2.1 Import data_raw &lt;- read.table(&quot;https://raw.githubusercontent.com/CEHS-research/data/master/MLM/Muscatine.txt&quot;, header=TRUE) str(data_raw) &#39;data.frame&#39;: 14568 obs. of 6 variables: $ id : int 1 1 1 2 2 2 3 3 3 4 ... $ gender : int 0 0 0 0 0 0 0 0 0 0 ... $ baseage: int 6 6 6 6 6 6 6 6 6 6 ... $ currage: int 6 8 10 6 8 10 6 8 10 6 ... $ occas : int 1 2 3 1 2 3 1 2 3 1 ... $ obesity: Factor w/ 3 levels &quot;.&quot;,&quot;0&quot;,&quot;1&quot;: 3 3 3 3 3 3 3 3 3 3 ... psych::headTail(data_raw, top = 10) id gender baseage currage occas obesity 1 1 0 6 6 1 1 2 1 0 6 8 2 1 3 1 0 6 10 3 1 4 2 0 6 6 1 1 5 2 0 6 8 2 1 6 2 0 6 10 3 1 7 3 0 6 6 1 1 8 3 0 6 8 2 1 9 3 0 6 10 3 1 10 4 0 6 6 1 1 ... ... ... ... ... ... &lt;NA&gt; 14565 4855 1 14 18 3 0 14566 4856 1 14 14 1 . 14567 4856 1 14 16 2 . 14568 4856 1 14 18 3 0 17.2.2 Restrict to 350ID’s of children with complete data for Class Demonstration Dealing with missing-ness and its implications are beyond the score of this class. Instead we are going to restrict our class analysis to a subset of 350 children who have complete data I am using the set.seed() function so that I can replicate the restults later. complete_ids &lt;- data_raw %&gt;% dplyr::filter(obesity %in% c(&quot;0&quot;, &quot;1&quot;)) %&gt;% dplyr::group_by(id) %&gt;% dplyr::summarise(n = n()) %&gt;% dplyr::filter(n == 3) %&gt;% dplyr::pull(id) set.seed(8892) use_ids &lt;- complete_ids %&gt;% sample(350) head(use_ids) [1] 2577 2859 364 954 47 3487 17.2.3 Long Format data_long &lt;- data_raw %&gt;% dplyr::filter(id %in% use_ids) %&gt;% mutate(id = id %&gt;% factor) %&gt;% mutate(gender = gender %&gt;% factor(levels = 0:1, labels = c(&quot;Male&quot;, &quot;Female&quot;))) %&gt;% mutate(age_base = baseage %&gt;% factor) %&gt;% mutate(age_curr = currage %&gt;% factor) %&gt;% mutate(occation = occas %&gt;% factor) %&gt;% mutate(obesity = obesity %&gt;% factor(levels = 0:1, labels = c(&quot;No&quot;, &quot;Yes&quot;))) %&gt;% select(id, gender, age_base, age_curr, occation, obesity) str(data_long) &#39;data.frame&#39;: 1050 obs. of 6 variables: $ id : Factor w/ 350 levels &quot;4&quot;,&quot;6&quot;,&quot;8&quot;,&quot;13&quot;,..: 1 1 1 2 2 2 3 3 3 4 ... $ gender : Factor w/ 2 levels &quot;Male&quot;,&quot;Female&quot;: 1 1 1 1 1 1 1 1 1 1 ... $ age_base: Factor w/ 5 levels &quot;6&quot;,&quot;8&quot;,&quot;10&quot;,&quot;12&quot;,..: 1 1 1 1 1 1 1 1 1 2 ... $ age_curr: Factor w/ 7 levels &quot;6&quot;,&quot;8&quot;,&quot;10&quot;,&quot;12&quot;,..: 1 2 3 1 2 3 1 2 3 2 ... $ occation: Factor w/ 3 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;: 1 2 3 1 2 3 1 2 3 1 ... $ obesity : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 2 2 2 2 2 2 2 2 ... psych::headTail(data_long, top = 10) id gender age_base age_curr occation obesity 1 4 Male 6 6 1 Yes 2 4 Male 6 8 2 Yes 3 4 Male 6 10 3 Yes 4 6 Male 6 6 1 Yes 5 6 Male 6 8 2 Yes 6 6 Male 6 10 3 Yes 7 8 Male 6 6 1 Yes 8 8 Male 6 8 2 Yes 9 8 Male 6 10 3 Yes 10 13 Male 8 8 1 Yes ... &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 1047 3572 Female 14 18 3 No 1048 3576 Female 14 14 1 No 1049 3576 Female 14 16 2 No 1050 3576 Female 14 18 3 No 17.2.4 Wide Format data_wide &lt;- data_long %&gt;% gather(key = var, value = val, age_curr, obesity) %&gt;% unite(col = var_occ, var, occation) %&gt;% spread(key = var_occ, value = val) %&gt;% mutate_if(is.character, factor)%&gt;% group_by(id) %&gt;% mutate(num_miss = sum(is.na(c(obesity_1, obesity_2, obesity_3)))) %&gt;% ungroup() %&gt;% mutate(num_miss = as.factor(num_miss)) str(data_wide) Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 350 obs. of 10 variables: $ id : Factor w/ 350 levels &quot;4&quot;,&quot;6&quot;,&quot;8&quot;,&quot;13&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... $ gender : Factor w/ 2 levels &quot;Male&quot;,&quot;Female&quot;: 1 1 1 1 1 1 1 1 1 1 ... $ age_base : Factor w/ 5 levels &quot;6&quot;,&quot;8&quot;,&quot;10&quot;,&quot;12&quot;,..: 1 1 1 2 2 2 2 2 2 2 ... $ age_curr_1: Factor w/ 5 levels &quot;10&quot;,&quot;12&quot;,&quot;14&quot;,..: 4 4 4 5 5 5 5 5 5 5 ... $ age_curr_2: Factor w/ 5 levels &quot;10&quot;,&quot;12&quot;,&quot;14&quot;,..: 5 5 5 1 1 1 1 1 1 1 ... $ age_curr_3: Factor w/ 5 levels &quot;10&quot;,&quot;12&quot;,&quot;14&quot;,..: 1 1 1 2 2 2 2 2 2 2 ... $ obesity_1 : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 2 2 2 2 2 2 2 2 ... $ obesity_2 : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 2 2 2 2 2 2 2 2 ... $ obesity_3 : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 2 2 2 2 2 2 2 2 ... $ num_miss : Factor w/ 1 level &quot;0&quot;: 1 1 1 1 1 1 1 1 1 1 ... psych::headTail(data_wide, top = 10) id gender age_base age_curr_1 age_curr_2 age_curr_3 obesity_1 1 4 Male 6 6 8 10 Yes 2 6 Male 6 6 8 10 Yes 3 8 Male 6 6 8 10 Yes 4 13 Male 8 8 10 12 Yes 5 16 Male 8 8 10 12 Yes 6 21 Male 8 8 10 12 Yes 7 23 Male 8 8 10 12 Yes 8 24 Male 8 8 10 12 Yes 9 25 Male 8 8 10 12 Yes 10 26 Male 8 8 10 12 Yes 11 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 12 3559 Female 14 14 16 18 No 13 3570 Female 14 14 16 18 No 14 3572 Female 14 14 16 18 No 15 3576 Female 14 14 16 18 No obesity_2 obesity_3 num_miss 1 Yes Yes 0 2 Yes Yes 0 3 Yes Yes 0 4 Yes Yes 0 5 Yes Yes 0 6 Yes Yes 0 7 Yes Yes 0 8 Yes Yes 0 9 Yes Yes 0 10 Yes Yes 0 11 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 12 No No 0 13 No No 0 14 No No 0 15 No No 0 17.3 Exploratory Data Analysis 17.3.1 Summary Statistics 17.3.1.1 Demographics and Baseline data_wide %&gt;% furniture::table1(age_base, obesity_1, splitby = ~ gender, test = TRUE, na.rm = FALSE, output = &quot;html&quot;) Male Female P-Value n = 186 n = 164 age_base 0.864 6 28 (15.1%) 24 (14.6%) 8 49 (26.3%) 45 (27.4%) 10 45 (24.2%) 37 (22.6%) 12 37 (19.9%) 28 (17.1%) 14 27 (14.5%) 30 (18.3%) NA 0 (0%) 0 (0%) obesity_1 1 No 148 (79.6%) 130 (79.3%) Yes 38 (20.4%) 34 (20.7%) NA 0 (0%) 0 (0%) 17.3.1.2 Status over Time data_summary &lt;- data_long %&gt;% dplyr::group_by(gender, age_curr) %&gt;% dplyr::mutate(obesityN = case_when(obesity == &quot;Yes&quot; ~ 1, obesity == &quot;No&quot; ~ 0)) %&gt;% dplyr::filter(complete.cases(gender, age_curr, obesityN)) %&gt;% dplyr::summarise(n = n(), prob_est = mean(obesityN), prob_SD = sd(obesityN), prob_SE = prob_SD/sqrt(n)) data_summary # A tibble: 14 x 6 # Groups: gender [?] gender age_curr n prob_est prob_SD prob_SE &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Male 6 28 0.107 0.315 0.0595 2 Male 8 77 0.182 0.388 0.0442 3 Male 10 122 0.213 0.411 0.0372 4 Male 12 131 0.267 0.444 0.0388 5 Male 14 109 0.284 0.453 0.0434 6 Male 16 64 0.219 0.417 0.0521 7 Male 18 27 0.259 0.447 0.0859 8 Female 6 24 0.125 0.338 0.0690 9 Female 8 69 0.174 0.382 0.0460 10 Female 10 106 0.292 0.457 0.0444 11 Female 12 110 0.282 0.452 0.0431 12 Female 14 95 0.284 0.453 0.0465 13 Female 16 58 0.259 0.442 0.0580 14 Female 18 30 0.133 0.346 0.0631 17.3.2 Visualize 17.3.2.1 By cohort and gender data_long %&gt;% dplyr::group_by(gender, age_base, age_curr) %&gt;% dplyr::mutate(obesityN = case_when(obesity == &quot;Yes&quot; ~ 1, obesity == &quot;No&quot; ~ 0)) %&gt;% dplyr::filter(complete.cases(gender, age_curr, obesityN)) %&gt;% dplyr::summarise(n = n(), prob_est = mean(obesityN), prob_SD = sd(obesityN), prob_SE = prob_SD/sqrt(n)) %&gt;% ggplot(aes(x = age_curr, y = prob_est, group = age_base, color = age_base)) + geom_point() + geom_line() + theme_bw() + labs(x = &quot;Child&#39;s Age, years&quot;, y = &quot;Proportion Obese&quot;) + facet_grid(. ~ gender) 17.3.2.2 BY only gender data_summary %&gt;% ggplot(aes(x = age_curr, y = prob_est, group = gender)) + geom_ribbon(aes(ymin = prob_est - prob_SE, ymax = prob_est + prob_SE, fill = gender), alpha = .3) + geom_point(aes(color = gender, shape = gender)) + geom_line(aes(linetype = gender, color = gender)) + theme_bw() + scale_color_manual(values = c(&quot;dodger blue&quot;, &quot;hot pink&quot;)) + scale_fill_manual(values = c(&quot;dodger blue&quot;, &quot;hot pink&quot;)) + labs(x = &quot;Child&#39;s Age, years&quot;, y = &quot;Proportion Obese&quot;) Smooth out the trends data_summary %&gt;% ggplot(aes(x = age_curr, y = prob_est, group = gender, color = gender)) + geom_smooth(method = &quot;lm&quot;, formula = y ~ poly(x, 2), se = FALSE) + theme_bw() + scale_color_manual(values = c(&quot;dodger blue&quot;, &quot;hot pink&quot;)) + scale_fill_manual(values = c(&quot;dodger blue&quot;, &quot;hot pink&quot;)) + labs(x = &quot;Child&#39;s Age, years&quot;, y = &quot;Proportion Obese&quot;) 17.4 Analysis Goal Does risk of obesity increase with age and are patterns of change similar for both sexes? There are 5 age cohorts that were measured each for 3 years, baseage and currage are age midpoints of those cohort groups. Which to include, current age or occasion? Assume no cohort effects. If you do think this is an issue, include baseline age (age_base) and current age minus baseline age (time) in model. data_long %&gt;% group_by(gender, age_base, occation) %&gt;% summarise(n = n(), count = sum(obesity == &quot;Yes&quot;), prop = mean(obesity == &quot;Yes&quot;), se = sd(obesity == &quot;Yes&quot;)/sqrt(n)) %&gt;% mutate(time = (occation %&gt;% as.numeric) * 2 - 2) %&gt;% ggplot(aes(x = time, y = prop, color = gender, fill = gender)) + geom_ribbon(aes(ymin = prop - se, ymax = prop + se), alpha = 0.2) + geom_point() + geom_line() + theme_bw() + facet_wrap(~ age_base, labeller = label_both) + labs(title = &quot;Observed Obesity Rates, by Gender within Cohort&quot;, subtitle = &quot;Subset of 350 children with complete data&quot;, x = &quot;Time, years from 1977&quot;, y = &quot;Proportion of Children Characterized as Obese&quot;) + scale_fill_manual(values = c(&quot;dodgerblue3&quot;, &quot;red&quot;)) + scale_color_manual(values = c(&quot;dodgerblue3&quot;, &quot;red&quot;)) + scale_x_continuous(breaks = seq(from = 0, to = 4, by = 2)) + theme(legend.position = c(1, 0), legend.justification = c(1, 0), legend.background = element_rect(color = &quot;black&quot;)) data_long %&gt;% group_by(gender, age_curr) %&gt;% summarise(n = n(), count = sum(obesity == &quot;Yes&quot;), prop = mean(obesity == &quot;Yes&quot;), se = sd(obesity == &quot;Yes&quot;)/sqrt(n)) %&gt;% ggplot(aes(x = age_curr %&gt;% as.character %&gt;% as.numeric, y = prop, group = gender, color = gender, fill = gender)) + geom_ribbon(aes(ymin = prop - se, ymax = prop + se), alpha = 0.2) + geom_point() + geom_line() + theme_bw() + geom_vline(xintercept = 12, linetype = &quot;dashed&quot;, size = 1, color = &quot;navyblue&quot;) + labs(title = &quot;Observed Obesity Rates, by Gender (collapsing cohorts)&quot;, subtitle = &quot;Subset of 350 children with complete data&quot;, x = &quot;Age of Child, years&quot;, y = &quot;Proportion of Children Characterized as Obese&quot;) + scale_fill_manual(values = c(&quot;dodgerblue3&quot;, &quot;red&quot;)) + scale_color_manual(values = c(&quot;dodgerblue3&quot;, &quot;red&quot;)) + scale_x_continuous(breaks = seq(from = 6, to = 18, by = 2)) + theme(legend.position = c(0, 1), legend.justification = c(-0.05, 1.05), legend.background = element_rect(color = &quot;black&quot;)) 17.4.1 Center time at twelve years old data_long &lt;- data_long %&gt;% dplyr::mutate(age_center = age_curr %&gt;% as.character %&gt;% as.numeric -12) %&gt;% dplyr::mutate(obesity_num = obesity %&gt;% as.numeric - 1) psych::headTail(data_long) id gender age_base age_curr occation obesity age_center obesity_num 1 4 Male 6 6 1 Yes -6 1 2 4 Male 6 8 2 Yes -4 1 3 4 Male 6 10 3 Yes -2 1 4 6 Male 6 6 1 Yes -6 1 ... &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ... ... 1047 3572 Female 14 18 3 No 6 0 1048 3576 Female 14 14 1 No 2 0 1049 3576 Female 14 16 2 No 4 0 1050 3576 Female 14 18 3 No 6 0 17.5 GLM Analysis 17.5.1 Standard logistic regression fit_glm_1 &lt;- glm(obesity_num ~ gender*age_center + gender*I(age_center^2), data = data_long, family = binomial) fit_glm_2 &lt;- glm(obesity_num ~ gender + age_center + I(age_center^2), data = data_long, family = binomial) texreg::screenreg(list(extract_glm_exp(fit_glm_1), extract_glm_exp(fit_glm_2)), custom.model.names = c(&quot;Interaction&quot;, &quot;Main Effects&quot;), caption = &quot;GLM: Parameter EStimates&quot;, single.row = TRUE, ci.test = 1) ============================================================================ Interaction Main Effects ---------------------------------------------------------------------------- (Intercept) 0.35 [0.27; 0.45] * 0.37 [0.29; 0.46] * genderFemale 1.23 [0.85; 1.78] 1.11 [0.83; 1.47] age_center 1.07 [1.00; 1.14] 1.05 [1.00; 1.10] I(age_center^2) 0.98 [0.96; 1.00] 0.98 [0.96; 0.99] * genderFemale:age_center 0.96 [0.87; 1.07] genderFemale:I(age_center^2) 0.99 [0.96; 1.02] ---------------------------------------------------------------------------- AIC 1157.07 1154.39 BIC 1186.81 1174.21 Log Likelihood -572.54 -573.19 Deviance 1145.07 1146.39 Num. obs. 1050 1050 ============================================================================ * 1 outside the confidence interval plot_pred_glm &lt;- Effect(c(&quot;gender&quot;, &quot;age_center&quot;), fit_glm_2, xlevels = list(age_center = seq(from = -6, to = 6, by = 0.25))) %&gt;% data.frame %&gt;% mutate(age = age_center + 12) %&gt;% ggplot(aes(x = age, y = fit, group = gender, linetype = gender, fill = gender, color = gender)) + geom_ribbon(aes(ymin = fit - se, ymax = fit + se), alpha = .3) + geom_line(size = 1.5) + theme_bw() + labs(title = &quot;Generalized Linear Model: Model #2&quot;, subtitle = &quot;Predicted Probability of Obesity&quot;, x = &quot;Child&#39;s Age, years&quot;, y = &quot;Probability&quot;, linetype = &quot;Gender&quot;, fill = &quot;Gender&quot;, color = &quot;Gender&quot;) + theme(legend.position = c(0, 1), legend.justification = c(-0.05, 1.05), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(2, &quot;cm&quot;)) + scale_x_continuous(breaks = seq(from = 6, to = 18, by = 2)) plot_pred_glm 17.6 GEE Analysis ALWAYS: fix the scale parameter to 1 with binomial data!!! fit_gee_1in &lt;- gee::gee(obesity_num ~ gender*age_center + gender*I(age_center^2), id = id, data = data_long, family = binomial, corstr = &#39;independence&#39;, scale.fix = TRUE, scale.value = 1) (Intercept) genderFemale -1.0470 0.2053 age_center I(age_center^2) 0.0643 -0.0166 genderFemale:age_center genderFemale:I(age_center^2) -0.0357 -0.0124 fit_gee_1ex &lt;- gee::gee(obesity_num ~ gender*age_center + gender*I(age_center^2), id = id, data = data_long, family = binomial, corstr = &#39;exchangeable&#39;, scale.fix = TRUE, scale.value = 1) (Intercept) genderFemale -1.0470 0.2053 age_center I(age_center^2) 0.0643 -0.0166 genderFemale:age_center genderFemale:I(age_center^2) -0.0357 -0.0124 fit_gee_1un &lt;- gee::gee(obesity_num ~ gender*age_center + gender*I(age_center^2), id = id, data = data_long, family = binomial, corstr = &#39;unstructured&#39;, scale.fix = TRUE, scale.value = 1) (Intercept) genderFemale -1.0470 0.2053 age_center I(age_center^2) 0.0643 -0.0166 genderFemale:age_center genderFemale:I(age_center^2) -0.0357 -0.0124 texreg::screenreg(list(extract_gee_exp(fit_gee_1in), extract_gee_exp(fit_gee_1ex), extract_gee_exp(fit_gee_1un)), custom.model.names = c(&quot;Independent&quot;, &quot;Exchangable&quot;, &quot;Unstructured&quot;), caption = &quot;Gee Model Parameters: With Interactions&quot;, single.row = TRUE, ci.test = 1) ==================================================================================================== Independent Exchangable Unstructured ---------------------------------------------------------------------------------------------------- (Intercept) 0.35 [0.25; 0.49] * 0.34 [0.25; 0.47] * 0.35 [0.25; 0.47] * genderFemale 1.23 [0.76; 1.97] 1.23 [0.79; 1.92] 1.21 [0.78; 1.88] age_center 1.07 [0.97; 1.17] 1.08 [1.01; 1.17] * 1.09 [1.01; 1.17] * I(age_center^2) 0.98 [0.96; 1.01] 0.99 [0.97; 1.00] 0.99 [0.97; 1.00] genderFemale:age_center 0.96 [0.85; 1.10] 0.96 [0.86; 1.07] 0.95 [0.85; 1.06] genderFemale:I(age_center^2) 0.99 [0.96; 1.02] 0.99 [0.96; 1.01] 0.99 [0.96; 1.01] ---------------------------------------------------------------------------------------------------- Dispersion 1.00 1.00 1.00 Num. obs. 1050 1050 1050 ==================================================================================================== * 1 outside the confidence interval 17.6.1 Drop the interaction with gender. fit_gee_2in &lt;- gee::gee(obesity_num ~ gender + age_center + I(age_center^2), id = id, data = data_long, family = binomial, corstr = &#39;independence&#39;, scale.fix = TRUE, scale.value = 1) (Intercept) genderFemale age_center I(age_center^2) -0.9961 0.1005 0.0469 -0.0227 fit_gee_2ex &lt;- gee::gee(obesity_num ~ gender + age_center + I(age_center^2), id = id, data = data_long, family = binomial, corstr = &#39;exchangeable&#39;, scale.fix = TRUE, scale.value = 1) (Intercept) genderFemale age_center I(age_center^2) -0.9961 0.1005 0.0469 -0.0227 fit_gee_2un &lt;- gee::gee(obesity_num ~ gender + age_center + I(age_center^2), id = id, data = data_long, family = binomial, corstr = &#39;unstructured&#39;, scale.fix = TRUE, scale.value = 1) (Intercept) genderFemale age_center I(age_center^2) -0.9961 0.1005 0.0469 -0.0227 texreg::screenreg(list(extract_gee_exp(fit_gee_2in), extract_gee_exp(fit_gee_2ex), extract_gee_exp(fit_gee_2un)), custom.model.names = c(&quot;Independent&quot;, &quot;Exchangable&quot;, &quot;Unstructured&quot;), caption = &quot;Gee Model Parameters: Main Effects Only&quot;, single.row = TRUE, ci.test = 1) ======================================================================================= Independent Exchangable Unstructured --------------------------------------------------------------------------------------- (Intercept) 0.37 [0.27; 0.50] * 0.36 [0.27; 0.48] * 0.36 [0.27; 0.49] * genderFemale 1.11 [0.73; 1.67] 1.12 [0.74; 1.69] 1.09 [0.72; 1.64] age_center 1.05 [0.98; 1.12] 1.06 [1.01; 1.12] * 1.06 [1.00; 1.12] * I(age_center^2) 0.98 [0.96; 0.99] * 0.98 [0.97; 0.99] * 0.98 [0.97; 0.99] * --------------------------------------------------------------------------------------- Dispersion 1.00 1.00 1.00 Num. obs. 1050 1050 1050 ======================================================================================= * 1 outside the confidence interval 17.6.2 Select the “final” model. fit_geeglm_2un &lt;- geepack::geeglm(obesity_num ~ gender + age_center + I(age_center^2), id = id, data = data_long, family = binomial, corstr = &#39;unstructured&#39;) plot_pred_gee &lt;- expand.grid(gender = data_long$gender %&gt;% levels, age_center = seq(from = -6, to = 6, by = .1)) %&gt;% mutate(fit = predict(fit_geeglm_2un, newdata = ., type = &quot;response&quot;)) %&gt;% mutate(gender = fct_rev(gender)) %&gt;% mutate(age = age_center + 12) %&gt;% ggplot(aes(x = age, y = fit, group = gender, linetype = gender, color = gender)) + geom_line(size = 1.5) + theme_bw() + labs(title = &quot;Generalized Estimating Equation: Model #2, unstructured&quot;, subtitle = &quot;Predicted Probability of Obesity&quot;, x = &quot;Child&#39;s Age, years&quot;, y = &quot;Probability&quot;, linetype = &quot;Gender&quot;, color = &quot;Gender&quot;) + theme(legend.position = c(0, 1), legend.justification = c(-0.05, 1.05), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(2, &quot;cm&quot;)) + scale_x_continuous(breaks = seq(from = 6, to = 18, by = 3)) plot_pred_gee 17.7 GLMM Analysis IT IS GENERALLY NOT RECOMMENDED THAT RANDOM-SLOPES BE ESTIMATED FOR BINOMIAL GLMMS fit_glmm_1 &lt;- lme4::glmer(obesity_num ~ age_center*gender + I(age_center^2)*gender + (1|id), data = data_long, family = binomial) fit_glmm_2 &lt;- lme4::glmer(obesity_num ~ gender + age_center + I(age_center^2) + (1|id), data = data_long, family = binomial) Indicates smaller model is better, no interaction at higher level necessary anova(fit_glmm_1, fit_glmm_2) Data: data_long Models: fit_glmm_2: obesity_num ~ gender + age_center + I(age_center^2) + (1 | id) fit_glmm_1: obesity_num ~ age_center * gender + I(age_center^2) * gender + fit_glmm_1: (1 | id) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) fit_glmm_2 5 919 944 -455 909 fit_glmm_1 7 921 956 -453 907 2.27 2 0.32 texreg::screenreg(list(extract_glmer_exp(fit_glmm_1), extract_glmer_exp(fit_glmm_2)), custom.model.names = c(&quot;Interaction&quot;, &quot;Main Effects&quot;), caption = &quot;GLMM: Parameter EStimates&quot;, single.row = TRUE, ci.test = 1) ============================================================================ Interaction Main Effects ---------------------------------------------------------------------------- (Intercept) 0.00 [0.00; 0.01] * 0.00 [0.00; 0.01] * age_center 1.32 [1.07; 1.62] * 1.24 [1.07; 1.44] * genderFemale 1.65 [0.45; 6.03] 1.29 [0.38; 4.40] I(age_center^2) 0.95 [0.91; 1.00] * 0.94 [0.91; 0.97] * age_center:genderFemale 0.87 [0.65; 1.17] genderFemale:I(age_center^2) 0.97 [0.91; 1.03] ---------------------------------------------------------------------------- AIC 920.90 919.17 BIC 955.59 943.95 Log Likelihood -453.45 -454.59 Num. obs. 1050 1050 Num. groups: id 350 350 Var: id (Intercept) 55.06 54.21 ============================================================================ * 1 outside the confidence interval Effect(c(&quot;gender&quot;, &quot;age_center&quot;),fit_glmm_2) %&gt;% data.frame %&gt;% mutate(fit_exp = exp(fit)) gender age_center fit se lower upper fit_exp 1 Male -6 7.48e-05 1.260 6.33e-06 0.000884 1 2 Female -6 9.66e-05 1.288 7.73e-06 0.001205 1 3 Male -3 8.32e-04 0.896 1.44e-04 0.004801 1 4 Female -3 1.07e-03 0.935 1.72e-04 0.006670 1 5 Male 0 2.84e-03 0.785 6.12e-04 0.013096 1 6 Female 0 3.67e-03 0.828 7.26e-04 0.018292 1 7 Male 3 2.99e-03 0.799 6.28e-04 0.014164 1 8 Female 3 3.86e-03 0.840 7.46e-04 0.019720 1 9 Male 6 9.73e-04 1.025 1.31e-04 0.007216 1 10 Female 6 1.26e-03 1.058 1.58e-04 0.009890 1 plot_pred_glmm &lt;- Effect(c(&quot;gender&quot;, &quot;age_center&quot;), fit_glmm_2, xlevels = list(age_center = seq(from = -6, to = 6, by = 0.25))) %&gt;% data.frame %&gt;% mutate(age = age_center + 12) %&gt;% ggplot(aes(x = age, y = fit, group = gender, linetype = gender, color = gender)) + geom_line(size = 1.5) + theme_bw() + labs(title = &quot;Generalized Linear Mixed Models: Model #2&quot;, subtitle = &quot;Predicted Probability of Obesity&quot;, x = &quot;Child&#39;s Age, years&quot;, y = &quot;Probability&quot;, linetype = &quot;Gender&quot;, color = &quot;Gender&quot;) + theme(legend.position = c(0, 1), legend.justification = c(-0.05, 1.05), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(2, &quot;cm&quot;)) + scale_x_continuous(breaks = seq(from = 6, to = 18, by = 3)) plot_pred_glmm 17.8 Compare Methods texreg::screenreg(list(extract_glm_exp(fit_glm_2), extract_gee_exp(fit_gee_2un), extract_glmer_exp(fit_glmm_2)), custom.model.names = c(&quot;GLM&quot;, &quot;GEE&quot;, &quot;GLMM&quot;), caption = &quot;Compare Methods: Parameter EStimates&quot;, single.row = TRUE, ci.test = 1) =========================================================================================== GLM GEE GLMM ------------------------------------------------------------------------------------------- (Intercept) 0.37 [0.29; 0.46] * 0.36 [0.27; 0.49] * 0.00 [0.00; 0.01] * genderFemale 1.11 [0.83; 1.47] 1.09 [0.72; 1.64] 1.29 [0.38; 4.40] age_center 1.05 [1.00; 1.10] 1.06 [1.00; 1.12] * 1.24 [1.07; 1.44] * I(age_center^2) 0.98 [0.96; 0.99] * 0.98 [0.97; 0.99] * 0.94 [0.91; 0.97] * ------------------------------------------------------------------------------------------- AIC 1154.39 919.17 BIC 1174.21 943.95 Log Likelihood -573.19 -454.59 Deviance 1146.39 Num. obs. 1050 1050 1050 Dispersion 1.00 Num. groups: id 350 Var: id (Intercept) 54.21 =========================================================================================== * 1 outside the confidence interval gridExtra::grid.arrange(plot_pred_glm, plot_pred_gee, plot_pred_glmm) "],
["references.html", "References", " References "]
]
