[
["index.html", "Encyclopedia of Quantitative Methods in R, vol. 5: Multilevel Models Welcome Blocked Notes Code and Output The Authors", " Encyclopedia of Quantitative Methods in R, vol. 5: Multilevel Models Sarah Schwartz &amp; Tyson Barrett Last updated: 2018-09-13 Welcome Backgroup and links to other volumes of this encyclopedia may be found at the Encyclopedia’s Home Website. Blocked Notes Thoughout all the eBooks in this encyclopedia, several small secitons will be blocked out in the following ways: These blocks denote an area UNDER CONSTRUCTION, so check back often. This massive undertaking started during the summer of 2018 and is far from complete. The outline of seven volumes is given above despite any one being complete. Feedback is welcome via either author’s email. These blocks denote something EXTREMELY IMPORTANT. Do NOT skip these notes as they will be used very sparingly. These blocks denote something to DOWNLOAD. This may include software installations, example datasets, or notebook code files. These blocks denote something INTERESTING. These point out information we found of interest or added value. These blocks denote LINKS to other websites. This may include instructional video clips, articles, or blog posts. We are all about NOT re-creating the wheel. If somebody else has described or illustrated a topic well, we celebrate it! Code and Output This is how \\(R\\) code is shown: 1 + 1 This is what the output of the \\(R\\) code above will look: ## [1] 2 The Authors Dr. Sarah Schwartz Dr. Tyson Barrett www.SarahSchwartzStats.com www.TysonBarrett.com Sarah.Schwartz@usu.edu Tyson.Barrett@usu.edu Statistical Consulting Studio Data Science and Discover Unit Why choose R ? Check it out: an article from Fall 2016… No more excuses: R is better than SPSS for psychology undergrads, and students agree FYI This entire encyclopedia is written in \\(R Markdown\\), using \\(R Studio\\) as the text editor and the bookdown package to turn a collection of markdown documents into a coherent whole. The book’s source code is hosted on GitHub. If you notice typos or other issues, feel free to email either of the authors. This work is licensed under the Attribution-NonCommercial-NoDerivatives 4.0 International License. "],
["hox-chap-2-classroom-popularity.html", "1 Hox, chap 2. - Classroom Popularity 1.1 Background 1.2 Exploratory Data Analysis 1.3 Notation for the Data 1.4 Single-level Regression Analysis 1.5 Multi-level Regression Analysis", " 1 Hox, chap 2. - Classroom Popularity library(tidyverse) library(haven) # read in SPSS dataset library(furniture) # nice table1() descriptives library(stargazer) # display nice tables: summary &amp; regression library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(RColorBrewer) # nice color palettes for plots library(gridExtra) # place ggplots together as one plot library(psych) # contains some useful functions, like headTail library(car) # Companion to Applied Regression library(nlme) # non-linear mixed-effects models library(lme4) # Linear, generalized linear, &amp; nonlinear mixed models library(lmerTest) # Tests on lmer objects library(HLMdiag) # Diagnostic Tools for for nlme &amp; lmer4 library(sjstats) # ICC calculations library(optimx) # Different optimizers to solve mlm&#39;s 1.1 Background The text “Multilevel Analysis: Techniques and Applications, Third Edition” (Hox, Moerbeek, and Van de Schoot 2017) has a companion website which includes links to all the data files used throughout the book (housed on the book’s GitHub repository). From Appendix E: The popularity data in popular2.sav are simulated data for 2000 pupils in 100 schools. The purpose is to offer a very simple example for multilevel regression analysis. The main outcome variable is the pupil popularity, a popularity rating on a scale of 1-10 derived by a sociometric procedure. Typically, a sociometric procedure asks all pupils in a class to rate all the other pupils, and then assigns the average received popularity rating to each pupil. Because of the sociometric procedure, group effects as apparent from higher level variance components are rather strong. There is a second outcome variable: pupil popularity as rated by their teacher, on a scale from 1-10. The explanatory variables are pupil gender (boy=0, girl=1), pupil extraversion (10-point scale) and teacher experience in years. The popularity data have been generated to be a ‘nice’ well-behaved data set: the sample sizes at both levels are sufficient, the residuals have a normal distribution, and the multilevel effects are strong.* data_raw &lt;- haven::read_sav(&quot;https://github.com/MultiLevelAnalysis/Datasets-third-edition-Multilevel-book/raw/master/chapter%202/popularity/SPSS/popular2.sav&quot;) %&gt;% haven::as_factor() # retain the labels from SPSS --&gt; factor tibble::glimpse(raw) function (length = 0L) 1.1.1 Unique Identifiers We will restrict ourselves to a few of the variables and create a distinct identifier variable for each student. data_pop &lt;- data_raw %&gt;% dplyr::mutate(id = paste(class, pupil, sep = &quot;_&quot;) %&gt;% # create a unique id for each student (char) factor()) %&gt;% # declare id is a factor dplyr::select(id, pupil:popteach) # reduce the variables included tibble::glimpse(data_pop) Observations: 2,000 Variables: 8 $ id &lt;fct&gt; 1_1, 1_2, 1_3, 1_4, 1_5, 1_6, 1_7, 1_8, 1_9, 1_10, 1_... $ pupil &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16... $ class &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... $ extrav &lt;dbl&gt; 5, 7, 4, 3, 5, 4, 5, 4, 5, 5, 5, 5, 5, 5, 5, 6, 4, 4,... $ sex &lt;fct&gt; girl, boy, girl, girl, girl, boy, boy, boy, boy, boy,... $ texp &lt;dbl&gt; 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 2... $ popular &lt;dbl&gt; 6.3, 4.9, 5.3, 4.7, 6.0, 4.7, 5.9, 4.2, 5.2, 3.9, 5.7... $ popteach &lt;dbl&gt; 6, 5, 6, 5, 6, 5, 5, 5, 5, 3, 5, 5, 5, 6, 5, 5, 2, 3,... 1.1.2 Scope out the structure and variables Its a good idea to visually inspect the first few lines in the datast to get a sense of how it is organized. data_pop %&gt;% psych::headTail(top = 25, bottom = 5) %&gt;% pander::pander() id pupil class extrav sex texp popular popteach 1_1 1 1 5 girl 24 6.3 6 1_2 2 1 7 boy 24 4.9 5 1_3 3 1 4 girl 24 5.3 6 1_4 4 1 3 girl 24 4.7 5 1_5 5 1 5 girl 24 6 6 1_6 6 1 4 boy 24 4.7 5 1_7 7 1 5 boy 24 5.9 5 1_8 8 1 4 boy 24 4.2 5 1_9 9 1 5 boy 24 5.2 5 1_10 10 1 5 boy 24 3.9 3 1_11 11 1 5 girl 24 5.7 5 1_12 12 1 5 girl 24 4.8 5 1_13 13 1 5 boy 24 5 5 1_14 14 1 5 girl 24 5.5 6 1_15 15 1 5 girl 24 6 5 1_16 16 1 6 girl 24 5.7 5 1_17 17 1 4 boy 24 3.2 2 1_18 18 1 4 boy 24 3.1 3 1_19 19 1 7 girl 24 6.6 7 1_20 20 1 4 boy 24 4.8 4 2_1 1 2 8 girl 14 6.4 6 2_2 2 2 4 boy 14 2.4 3 2_3 3 2 6 boy 14 3.7 4 2_4 4 2 5 girl 14 4.4 4 2_5 5 2 5 girl 14 4.3 4 NA … … … NA … … … 100_16 16 100 4 girl 7 4.3 5 100_17 17 100 4 boy 7 2.6 2 100_18 18 100 8 girl 7 6.7 7 100_19 19 100 5 boy 7 2.9 3 100_20 20 100 9 boy 7 5.3 5 Visual inspection reveals that most of the variables are measurements at level 1 and apply to specific pupils (extrav, sex, popular, and popteach), while the teacher’s years of experiene is a level 2 variable since it applies to the entire class. Notice how the texp variable is identical for all pupils in the same class. This is call Disaggregated data. 1.2 Exploratory Data Analysis 1.2.1 Summarize Descriptive Statistics Most posters, journal articles, and reports start with a table of descriptive statistics. Since it tends to come first, this type of table is often refered to as Table 1. The stargazer() function can be used to create such a table, but only for the entire dataset (Hlavac 2018). I haven’t been able to find a way to get it to summarize subsamples and compare them in the standard format. data_pop %&gt;% dplyr::select(-id) %&gt;% data.frame() %&gt;% stargazer::stargazer(title = &quot;Descriptive statistics, aggregate over entire sample&quot;, header = FALSE, type = &quot;html&quot;) Descriptive statistics, aggregate over entire sample Statistic N Mean St. Dev. Min Pctl(25) Pctl(75) Max pupil 2,000 10.649 5.968 1 6 16 26 class 2,000 50.370 29.078 1 25 76 100 extrav 2,000 5.215 1.262 1 4 6 10 texp 2,000 14.263 6.552 2 8 20 25 popular 2,000 5.076 1.383 0.000 4.100 6.000 9.500 popteach 2,000 5.061 1.404 1 4 6 10 1.2.2 Summarize Descriptive Statistics - furniture Tyson Barrett’s furniture package includes the extremely useful function table1() which simplifies the common task of creating a stratified, comparative table of descriptive statistics. Full documentation can be accessed by executing ?furniture::table1. data_pop %&gt;% table1(&quot;Pupil&#39;s Extraversion (10 pt)&quot; = extrav, &quot;Teacher&#39;s Experience (years)&quot; = texp, &quot;Popularity, Sociometric Score&quot; = popular, &quot;Popularity, Teacher Evaluated&quot; = popteach, splitby = ~ sex, # var to divide sample by test = TRUE, # test groups different? output = &quot;html&quot;, # output for latex align = c(&quot;l&quot;, &quot;r&quot;, &quot;r&quot;, &quot;r&quot;), # column alignment caption = &quot;Compare genders on four main variables&quot;) # title Table 1.1: Compare genders on four main variables boy girl P-Value n = 989 n = 1011 Pupil.s.Extraversion..10.pt. &lt;.001 5.1 (1.2) 5.3 (1.3) Teacher.s.Experience..years. 0.001 13.8 (6.3) 14.7 (6.8) Popularity..Sociometric.Score &lt;.001 4.3 (1.1) 5.9 (1.1) Popularity..Teacher.Evaluated &lt;.001 4.3 (1.2) 5.8 (1.2) 1.2.3 Visualizations of Raw Data For a first look, its useful to plot all the data points on a single scatterplot as displayed in Figure 1.1. Due to ganularity in the rating scale, many points end up being plotted on top of each other (overplotted), so its a good idea to use geom_count() rather than geom_point() so the size of the dot can convey the number of points at that location (Wickham et al. 2018). # Disaggregate: pupil (level 1) only, ignore level 2&#39;s existance # Extraversion treated: continuous measure data_pop %&gt;% ggplot() + aes(x = extrav, # x-axis variable y = popular) + # y-axis variable geom_count() + # POINTS w/ SIZE = COUNT geom_smooth(method = &quot;lm&quot;) + # linear regression line theme_bw() + # white background labs(x = &quot;Extraversion (10 pt scale)&quot;, # x-axis label y = &quot;Popularity, Sociometric Score&quot;, # y-axis label size = &quot;Count&quot;) + # legend key&#39;s title theme(legend.position = c(0.9, 0.2), # key at legend.background = element_rect(color = &quot;black&quot;)) + # key box scale_x_continuous(breaks = seq(from = 0, to = 10, by = 1)) + # x-ticks scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) # y-ticks Figure 1.1: Disaggregate: pupil level only with extraversion treated as an continuous measure. When the degree of overplotting as high as it is in Figure 1.1, it can be useful to represent the data with density contours as seen in Figure 1.2. I’ve chosen to leave the points displayed in this redition, but color them much lighter so that they are present, but do not detract from the pattern of association. # visualize all the data - another way data_pop %&gt;% ggplot() + aes(x = extrav, # x-axis variable y = popular) + # y-axis variable geom_count(color = &quot;gray&quot;) + # POINTS w/ SIZE = COUNT geom_density2d() + # DENSITY CURVES geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;) + # linear regression line theme_bw() + # white background labs(x = &quot;Extraversion (10 pt scale)&quot;, # x-axis label y = &quot;Popularity, Sociometric Score&quot;) + # y-axis label guides(size = FALSE) + # don&#39;t include a legend scale_x_continuous(breaks = seq(from = 0, to = 10, by = 1)) + # x-ticks scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) # y-ticks Figure 1.2: Disaggregate: pupil level only with extraversion treated as an continuous measure. The argument could be made that the extraversion score should be treated as an ordinal factor instead of as a truely continuous scale since the only valid values are the whole number 1 through 10 and there is no assurance that these category assignments represent a true ratio measurement scale. However, we must keep in mind that this was an observational study, ans as such, the number of pupils assignment each level of extraversion is not equal. # count the number of pupils in assigned each Extraversion value, 1:10 table &lt;- data_pop %&gt;% group_by(extrav) %&gt;% summarise(count = n_distinct(id), percent = 100 * count / 2000) table %&gt;% stargazer(summary = FALSE, rownames = FALSE, header = FALSE, type = &quot;html&quot;, title = &quot;Distribution of extraversion in pupils&quot;) Distribution of extraversion in pupils extrav count percent 1 3 0.15 2 13 0.65 3 119 5.95 4 423 21.15 5 688 34.4 6 478 23.9 7 194 9.7 8 58 2.9 9 18 0.9 10 6 0.3 Figure 1.3 displays the same data as Figure 1.1, but uses boxplots for the distribution of scores at each level of extraversion. On one extreme, the lowest extraversion score possible was a value of “one”, but only 3 pupils or 0.15% of the 2000 pupils recieved this value. On the other extreme, the middle value of “five” was applied to 688 pupils or a wopping 34.4%. The option varwidth=TRUE in the geom_boxplot() function helps reflect such unbalanced sample sizes by allowing the width of the boxes to be proportional to the square-roots of the number of observations each box represents. # Disaggregate: pupil (level 1) only, ignore level 2&#39;s existance # Extraversion treated: ordinal factor ggplot(data_pop, # dataset&#39;s name aes(x = factor(extrav), # x-axis values - make factor! y = popular, # y-axis values fill = factor(extrav))) + # makes seperate boxes geom_boxplot(varwidth = TRUE) + # draw boxplots instead of points theme_bw() + # white background guides(fill = FALSE) + # don&#39;t include a legend scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) + # y-ticks labs(x = &quot;Extraversion (10 pt scale)&quot;, # x-axis label y = &quot;Popularity, Sociometric Score&quot;) + # y-axis label scale_fill_brewer(palette = &quot;Spectral&quot;, direction = 1) # select color Figure 1.3: Disaggregate: pupil level only with extraversion treated as an ordinal factor. The width of the boxes are proportional to the square-roots of the number of observations each box represents. Up to this point, all investigation of this dataset has been only at the pupil level and any nesting or clustering within classes has been ignored. Plotting is a good was to start to get an idea of the class-to-class variability. # compare the first 9 classrooms becuase all of there are too many at once data_pop %&gt;% dplyr::filter(class &lt;= 9) %&gt;% # select ONLY NINE classes ggplot(aes(x = extrav, # x-axis values y = popular)) + # y-axis values geom_count() + # POINTS w/ SIZE = COUNT geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;) + # linear regression line theme_bw() + # white background labs(x = &quot;Extraversion (10 pt scale)&quot;, # x-axis label y = &quot;Popularity, Sociometric Score&quot;, # y-axis label size = &quot;Count&quot;) + # legend key&#39;s title guides(size = FALSE) + # don&#39;t include a legend scale_x_continuous(breaks = seq(from = 0, to = 10, by = 3)) + # x-ticks scale_y_continuous(breaks = seq(from = 0, to = 10, by = 3)) + # y-ticks facet_wrap(~ class, labeller = label_both) + theme(strip.background = element_rect(colour = NA, fill = NA)) Figure 1.4: Illustration of the degree of class level variability in the association between extraversion and popularity. Each panel represents a class and each point a pupil in that class. First nice classes shown. # select specific classes by number for illustration purposes data_pop %&gt;% dplyr::filter(class %in% c(15, 25, 33, 35, 51, 64, 76, 94, 100)) %&gt;% ggplot(aes(x = extrav, # x-axis values y = popular)) + # y-axis values geom_count() + # POINTS w/ SIZE = COUNT geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;) + # linear regression line theme_bw() + # white background labs(x = &quot;Extraversion (10 pt scale)&quot;, # x-axis label y = &quot;Popularity, Sociometric Score&quot;, # y-axis label size = &quot;Count&quot;) + # legend key&#39;s title guides(size = FALSE) + # don&#39;t include a legend scale_x_continuous(breaks = seq(from = 0, to = 10, by = 3)) + # x-ticks scale_y_continuous(breaks = seq(from = 0, to = 10, by = 3)) + # y-ticks facet_wrap(~ class) + theme(strip.background = element_blank(), strip.text = element_blank()) Figure 1.5: Illustration of the degree of class level variability in the association between extraversion and popularity. Each panel represents a class and each point a pupil in that class. A set of nine classes was chosen to show a sampling of variability. The facet labels are not shown as the identification number probably would not be advisable for a general publication. # compare all 100 classrooms via linear model for each ggplot(data_pop, aes(x = extrav, # x-axis values y = popular, # y-axis values group = class)) + # GROUPs for LINES geom_smooth(method = &quot;lm&quot;, # linear regression line color = &quot;gray40&quot;, size = 0.4, se = FALSE) + theme_bw() + # white background labs(x = &quot;Extraversion (10 pt scale)&quot;, # x-axis label y = &quot;Popularity, Sociometric Score&quot;) + # y-axis label scale_x_continuous(breaks = seq(from = 0, to = 10, by = 2)) + # x-ticks scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) # y-ticks Figure 1.6: Spaghetti plot of seperate, independent linear models for each of the 100 classes. A helpful resource for choosing colors to use in plots: R color cheatsheet # compare all 100 classrooms via independent linear models data_pop %&gt;% dplyr::mutate(texp3 = cut(texp, breaks = c(0, 10, 18, 30)) %&gt;% factor(labels = c(&quot;&lt; 10 yrs&quot;, &quot;10 - 18 yrs&quot;, &quot;&gt; 18 yrs&quot;))) %&gt;% ggplot(aes(x = extrav, # x-axis values y = popular, # y-axis values group = class)) + # GROUPs for LINES geom_smooth(aes(color = sex), size = 0.3, method = &quot;lm&quot;, # linear regression line se = FALSE) + theme_bw() + # white background labs(x = &quot;Extraversion (10 pt scale)&quot;, # x-axis label y = &quot;Popularity, Sociometric Score&quot;) + # y-axis label guides(color = FALSE) + # don&#39;t include a legend scale_x_continuous(breaks = seq(from = 0, to = 10, by = 3)) + # x-ticks scale_y_continuous(breaks = seq(from = 0, to = 10, by = 3)) + # y-ticks scale_color_manual(values = c(&quot;dodgerblue&quot;, &quot;maroon1&quot;)) + facet_grid(texp3 ~ sex) Figure 1.7: Spaghetti plot of seperate, independent linear models for each of the 100 classes. Seperate panels are used to untangle the ‘hairball’ in the previous figure. The columns are seperated by the pupils’ gender and the rows by the teacher’s experince in years. R markdown is a user friendly, simplified language that allows for more complex formating utilizing standard \\(\\LaTeX\\) code. A great resource for learning how to many common tasks in \\(\\LaTeX\\) is the Sharewebsite. Specific mathematical equation documentation may be found on the Mathematical Expressions subpage. There are also many websites that offer Point-n-click interfaces to build \\(\\LaTeX\\) equations, including: Host Math, Code Cogs, LaTeX 4 Technics, and Sci-Weavers 1.3 Notation for the Data Sample Sizes: \\(n_j\\) = number of pupils in class \\(j\\) \\(N\\) = number of classes Indicators: \\(i \\in (1, 2, \\dots, n_j)\\) = index for pupil number \\(j \\in (1, 2, \\dots, N)\\) = index for class number Level Type of Variable Name in Data Name in EQ Symbol pupil \\(i\\) in class \\(j\\) 1 Outcome (Dependent) popular \\(POP\\) \\(Y\\) \\(Y_{ij}\\) 1 Predictor (Independent) sex \\(GEN\\) \\(X_1\\) \\(X_{1ij}\\) 1 Predictor (Independent) extrav \\(EXT\\) \\(X_2\\) \\(X_{2ij}\\) 2 Predictor (Independent) texp \\(YRS\\) \\(Z\\) \\(Z_j\\) 1.4 Single-level Regression Analysis 1.4.1 Generic Model Notation Since we are don’t have or are ignoring clustering, there is only one level. Single-Level Regression Equation - Generic \\[ \\overbrace{Y_{ij}}^{\\text{outcome}} = \\underbrace{\\beta_{0}}_{\\text{fixed intercept}} + \\underbrace{\\beta_{1}}_{\\text{fixed slope for } X_1} \\overbrace{X_{1ij}}^{\\text{predictor 1}} + \\underbrace{\\beta_{2}}_{\\text{fixed slope for } X_2} \\overbrace{X_{2ij}}^{\\text{predictor 2}} + \\underbrace{e_{ij}}_{\\text{random residuals}} \\tag{Hox Eq 2.1} \\] Parameters Type Parameter of Interest Estimates This Fixed Intercept \\(\\beta_{0}\\) Fixed Slope of \\(X_1\\) \\(\\beta_{1}\\) Fixed Slope of \\(X_2\\) \\(\\beta_{2}\\) Random Residual Variance \\(var[e_{ij}]\\) \\(\\sigma^2_{e}\\) Assumptions to Check The \\(e_{ij}\\)’s follow a normal distribution with a mean of \\(0\\) The \\(e_{ij}\\)’s have a constant variance (homoscedasticity) 1.4.2 Intercept-only or Null Model In a Null or Empty model, no predictors are included Single-Level Regression Equation - Null Model \\[ POP_{ij} = \\overbrace{\\beta_{0}}^{intercept} + \\underbrace{e_{ij}}_{\\text{residuals}} \\] Parameters Type Parameter of Interest Estimates This Fixed Intercept, Grand Mean \\(\\beta_{0}\\) Random Residual Variance \\(var[e_{ij}]\\) \\(\\sigma^2_{e}\\) Fit the model pop_lm_0 &lt;- lm(popular ~ 1, data = data_pop) View the summary information summary(pop_lm_0) Call: lm(formula = popular ~ 1, data = data_pop) Residuals: Min 1Q Median 3Q Max -5.0765 -0.9765 0.0235 0.9236 4.4235 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.07645 0.03091 164.2 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.383 on 1999 degrees of freedom \\(\\hat{\\beta_0}\\) = 5.08 is the grand mean Extract the residual variance sigma(pop_lm_0) # standard deviation of the residuals [1] 1.382522 sigma(pop_lm_0)^2 # variance of the residuals [1] 1.911366 \\(\\hat{\\sigma_e^2}\\) = 1.9114 is residual variance (RMSE is sigma) Assess model fit summary(pop_lm_0)$r.squared [1] 0 \\(R^2\\) = 0 is the proportion of variance in popularity that is explained by the grand mean alone. Interpretation The grand average popularity of all pupils in all the classes is 5.08, and there is strong evidence that it is statistically significantly different than zero, \\(p&lt;.0001\\). The mean alone accounts for none of the variance in popularity. The residual variance is the same as the total variance in popularity, 1.9114. Just to make sure… mean(data_pop$popular) [1] 5.07645 var(data_pop$popular) [1] 1.911366 1.4.3 Add Predictors to the Model Predictors at the Pupil Level: \\(X_1 = GEN\\) = pupils’s gender, (girl vs. boy) \\(X_2 = EXT\\) = pupil’s extraversion, (scale: 1-10) \\[ POP_{ij} = \\beta_{0} + \\beta_{1} GEN_{ij} + \\beta_{2} EXT_{ij} + e_{ij} \\tag{Hox Eq 2.2} \\] Parameters to be Estimated: \\(\\beta_0\\) = mean for pupils with \\(GEN =\\) boy (the reference category) and \\(EXT = 0\\) (not even a possible score) \\(\\beta_1\\) = mean difference in \\(POP\\) for girls vs. boys with the same \\(EXT\\) score \\(\\beta_2\\) = mean difference in \\(POP\\) for a 1-point higher \\(EXT\\) but the same gender \\(\\sigma_e^2 = var(e_{ij})\\) = residual variance Fit the model pop_lm_1 &lt;- lm(popular ~ sex + extrav, data = data_pop) View the summary information summary(pop_lm_1) Call: lm(formula = popular ~ sex + extrav, data = data_pop) Residuals: Min 1Q Median 3Q Max -4.2527 -0.6652 -0.0454 0.7422 3.0473 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 2.78954 0.10355 26.94 &lt;2e-16 *** sexgirl 1.50508 0.04836 31.12 &lt;2e-16 *** extrav 0.29263 0.01916 15.28 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.077 on 1997 degrees of freedom Multiple R-squared: 0.3938, Adjusted R-squared: 0.3932 F-statistic: 648.6 on 2 and 1997 DF, p-value: &lt; 2.2e-16 \\(\\hat{\\beta_0}\\) = 2.79 is the extrapolated mean for boys with an extroversion score of 0. \\(\\hat{\\beta_1}\\) = 1.51 is the mean difference between girls and boys with the same extroversion score. \\(\\hat{\\beta_2}\\) = 0.29 is the mean difference for pupils of the same gender that differ in extroversion by one point. Extract the residual variance sigma(pop_lm_1) # standard deviation of the residuals [1] 1.076985 sigma(pop_lm_1)^2 # variance of the residuals [1] 1.159898 \\(\\hat{\\sigma_e^2}\\) = 1.1599 is residual variance (RMSE is sigma) Assess model fit summary(pop_lm_1)$r.squared [1] 0.393765 \\(R^2\\) = 0.394 is the proportion of variance in popularity that is explained by tha pupils gender and extroversion score. Interpretation On average, girls were rated 1.51 points more popular than boys with the same extroversion score, \\(p&lt;.0001\\). One point higher extroversion scores were associated with 0.29 points higher popularity, within each gender, \\(p&lt;.0001\\). Together, these two factors account for 39.38% of the variance in populartiy. Table to compare the two models: texreg::htmlreg(list(pop_lm_0, pop_lm_1), custom.model.names = c(&quot;Null Model&quot;, &quot;With Predictors&quot;), caption = &quot;Single Level Models&quot;, single.row = TRUE, digits = 4) Single Level Models Null Model With Predictors (Intercept) 5.0764 (0.0309)*** 2.7895 (0.1036)*** sexgirl 1.5051 (0.0484)*** extrav 0.2926 (0.0192)*** R2 0.0000 0.3938 Adj. R2 0.0000 0.3932 Num. obs. 2000 2000 RMSE 1.3825 1.0770 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 1.5 Multi-level Regression Analysis 1.5.1 Generic Model Notation Generic Level 1 Regression Equation Continue taking into account fixed slopes for two Level 1 variables, \\(X_1\\) and \\(X_2\\). \\[ Y_{ij} = \\beta_{0j} + \\beta_{1j} X_{1ij} + \\beta_{2j} X_{2ij} + e_{ij} \\tag{Hox Eq 2.1} \\] Generic Level 2 Regression Equations Now we take clustering into account and include random intercepts (\\(\\beta_{0j}\\)) and slopes (\\(\\beta_{1j}, \\beta_{2j}\\)), as well as including a single Level 2 variable, \\(Z\\) that is predictive fo the random inntercetps and slopes. Random Intercepts Equation: \\[ \\beta_{0j} = \\gamma_{00} + \\gamma_{01} Z_{j} + u_{0j} \\tag{Hox Eq 2.3} \\] Random Slopes Equations: \\[ \\beta_{1j} = \\gamma_{10} + \\gamma_{11} Z_{j} + u_{1j} \\\\ \\beta_{2j} = \\gamma_{20} + \\gamma_{21} Z_{j} + u_{2j} \\tag{Hox Eq 2.4} \\] Generic Multilevel Regression Equation Start with Level 1 equation (2.1) and allow the \\(\\beta\\)’s to be varry for each class. \\[ Y_{ij} = \\underbrace{\\beta_{0j}}_{Random \\; Intercept} + \\underbrace{\\beta_{1j}}_{Random \\; Slope \\; for \\; X_1} X_{1ij} + \\underbrace{\\beta_{2j}}_{Random \\; Slope \\; for \\; X_2} X_{2ij} + e_{ij} \\] Plug in the level 2 equations (2.3 and 2.4) into the level 1 equation (2.1) to make the combined equation. \\[ Y_{ij} = \\overbrace{(\\gamma_{00} + \\gamma_{01} Z_{j} + u_{0j})}^{\\beta_{0j}} + \\overbrace{(\\gamma_{10} + \\gamma_{11} Z_{j} + u_{1j})}^{\\beta_{1j}} X_{1ij} + \\overbrace{(\\gamma_{20} + \\gamma_{21} Z_{j} + u_{2j})}^{\\beta_{2j}} X_{2ij} + e_{ij} \\] Use the distributive property of multiplication to get rid of the parentheses. \\[ Y_{ij} = \\overbrace{\\gamma_{00} + \\gamma_{01} Z_{j} + u_{0j}}^{\\beta_{0j}} + \\overbrace{\\gamma_{10} X_{1ij} + \\gamma_{11} Z_{j} X_{1ij} + u_{1j} X_{1ij}}^{\\beta_{1j} \\times X_{1ij}} + \\overbrace{\\gamma_{20} X_{2ij} + \\gamma_{21} Z_{j} X_{2ij} + u_{2j} X_{2ij}}^{\\beta_{2j} \\times X_{2ij}} + e_{ij} \\] Collect ‘like-terms’ (i.e. get the \\(\\gamma\\)’s together and the \\(u\\)’s together) \\[ \\begin{align*} Y_{ij} =&amp; \\overbrace{\\gamma_{00} + \\gamma_{10} X_{1ij} + \\gamma_{20} X_{2ij} + \\gamma_{01} Z_{j} + \\gamma_{11} Z_{j} X_{1ij} + \\gamma_{21} Z_{j} X_{2ij}}^{FIXED \\; i.e. \\; deterministic \\; part} + \\\\ &amp; \\underbrace{u_{0j} + u_{1j} X_{1ij} + u_{2j} X_{2ij} + e_{ij} }_{RANDOM \\; i.e. \\; stochastic \\; part} \\end{align*} \\tag{Hox Eq 2.5} \\] Parameters to Estimate: \\(\\gamma_{00}\\) = fixed intercept \\(\\gamma_{10}\\) = fixed slope for the first level 1 predictor, \\(X_1\\) \\(\\gamma_{20}\\) = fixed slope for the second level 1 predictor, \\(X_2\\) \\(\\gamma_{01}\\) = fixed slope for the only level 2 predictor, \\(Z\\) \\(\\gamma_{11}\\) = cross-level interaction, \\(X_1 \\times Z\\) \\(\\gamma_{21}\\) = cross-level interaction, \\(X_2 \\times Z\\) \\(var(u_{0j})\\) = variance of random intercepts \\(var(u_{1j})\\) = variance of random slope of the first level 1 predictor, \\(X_1\\) \\(var(u_{2j})\\) = variance of random slope of the second level 1 predictor, \\(X_2\\) \\(cov(u_{0j},u_{1j})\\) = covariance between the random intercetps and random effect of \\(X_1\\) \\(cov(u_{0j},u_{2j})\\) = covariance between the random intercetps and random effect of \\(X_2\\) \\(cov(u_{1j},u_{2j})\\) = covariance between the random effect of \\(X_1\\) and \\(X_2\\) \\(var(e_{ij})\\) = variance of the residuals (residual variance) Assumptions to Check: The \\(e_{ij}\\)’s follow a normal distribution with a mean of \\(0\\) The \\(e_{ij}\\)’s have a constant variance (homoscedasticity) The \\(u_{1j}\\) and \\(u_{2j}\\) terms allow for heteroscedasticity by fitting different error terms for different values of \\(X_1\\) and \\(X_2\\). The HOV assumption is that AFTER accounting for this, the remaining residuals are HOV. 1.5.2 Intercept-only or Null Model “The intercept-only model is useful as a null-model that serves as a benchmark with which other models are compared.” Hox, Moerbeek, and Van de Schoot (2017), page 13 The Set of Level-Specific Model Equations: Level 1 Model Equation (\\(i^{th}\\) pupil in the \\(j^{th}\\) class) \\[ Y_{ij} = \\beta_{0j} + e_{ij} \\tag{Hox Eq 2.6} \\] Level 2 Model Equation (\\(j^{th}\\) class) Random Intercepts Only: \\[ \\beta_{0j} = \\gamma_{00} + u_{0j} \\tag{Hox Eq 2.7} \\] The Single, Multilevel Model Equation: \\[ Y_{ij} = \\gamma_{00} + u_{0j} + e_{ij} \\tag{Hox 2.8} \\] Parameters: Fixed Effects \\[ intercept \\longrightarrow \\gamma_{00} \\] Random Effects \\[ \\begin{align*} class (intercept) \\longrightarrow \\sigma^2_{u_0} &amp; = var[u_{0j}] \\\\ residual \\longrightarrow \\sigma^2_{e} &amp; = var[e_{ij}] \\\\ \\end{align*} \\] Hox, Moerbeek, and Van de Schoot (2017) labeled the Null model for this dataset “\\(M_0\\)” in chapter 2: Predictors: none! \\[ POP_{ij} = \\gamma_{00} + u_{0j} + e_{ij} \\tag{M0: intercept only} \\] Parameters to be Estimated: \\(\\gamma_{00}\\) = intercept \\(\\sigma^2_{u_0}\\) = var(u_{0j}) = variance between classes \\(\\sigma_e^2 = var(e_{ij})\\) = residual variance pop_lmer_0 &lt;- lme4::lmer(popular ~ 1 + (1|class), data = data_pop) summary(pop_lmer_0) Linear mixed model fit by REML [&#39;lmerMod&#39;] Formula: popular ~ 1 + (1 | class) Data: data_pop REML criterion at convergence: 6330.5 Scaled residuals: Min 1Q Median 3Q Max -3.5655 -0.6975 0.0020 0.6758 3.3175 Random effects: Groups Name Variance Std.Dev. class (Intercept) 0.7021 0.8379 Residual 1.2218 1.1053 Number of obs: 2000, groups: class, 100 Fixed effects: Estimate Std. Error t value (Intercept) 5.07786 0.08739 58.1 The entire population of all students in all classes has a grand-average of 5.0779 and the individual classes have popularity averages that vary around that. 1.5.3 Intraclass Correlation (ICC) Although the Null model above does not explain any variance in the dependent variable (popularity), since there are no independent variables, it does decompose (i.e. divide up) the variance into two pieces. We can compute the amount of total variance in popularity that is attribute to the clustering of students in classes verses the residual variance. \\[ \\rho = \\frac{\\sigma^2_{u0}}{\\sigma^2_{u0}+\\sigma^2_{e}} \\tag{Hox Eq 2.9} \\] The VarCorr() function in the lme4 package returns the standard deviations, not the variances (\\(var = SD^2\\)) for a model fit via the lme4::lmer() function. The summary() function reports both the variances and the stadard deviations. lme4::VarCorr(pop_lmer_0) Groups Name Std.Dev. class (Intercept) 0.83792 Residual 1.10535 \\[ \\sigma^2_{u0} = 0.83792^2 = 0.7021\\\\ \\sigma^2_{e} = 1.10535^2 = 1.2218\\\\ \\] Calculate the ICC by hand: \\[ \\rho = \\frac{\\sigma^2_{u0}} {\\sigma^2_{u0}+\\sigma^2_{e}} = \\frac{0.7021} {0.7021+1.2218} = \\frac{0.7021} {1.9239} = 0.3649358 \\] Calculate the ICC with the icc() fucntion in the sjstats package: sjstats::icc(pop_lmer_0) Linear mixed model Family : gaussian (identity) Formula: popular ~ 1 + (1 | class) ICC (class): 0.3649 Interpretation: 36.5% of the variance of the popularity scores is at the group level, which is very high for social science data. The ICC should be based on a Null (intercept only) model fit via REML (restricted maximum likelihood) estimation. This is the default for the ‘lme4::lmer()’ function. In chapter 2, Hox, Moerbeek, and Van de Schoot (2017) presents the numbers based on fitting the model via ML (maximum likelihood) estimation and thus does not match the presentation above exactly (not just rounding error). This is probably because: (1) estimation methods (REML &amp; ML) are not discussed until chapter 3 and (2) due to the Null model also being used for model fit comparisons in Table 2.1 on the top of page 14. 1.5.4 Add Predictors to the Model Hox, Moerbeek, and Van de Schoot (2017) labeled this as “\\(M_1\\)” in chapter 2 for their Table 2.1 on page 14, but adjusted it for Tables 2.2 &amp; 2.3 on pages 15 and 17. Predictors at the Pupil Level: \\(X_1 = GEN\\) = pupils’s gender, (girl vs. boy) \\(X_2 = EXT\\) = pupil’s extraversion, (scale: 1-10 points) Predictors at the Class Level: \\(Z = YRS\\) = teacher’s experience (range of 2-25 years) The Set of Level-Specific Model Equations: Level 1 Model Equation (\\(i^{th}\\) pupil in the \\(j^{th}\\) class) \\[ POP_{ij} = \\beta_{0j} + \\beta_{1j} GEN_{ij} + \\beta_{2j} EXT_{ij} + e_{ij} \\] Level 2 Model Equation (\\(j^{th}\\) class) Include a random intercepts and slopes (\\(X_1\\) and \\(X_2\\)), but NO cross level interactions \\[ \\begin{align*} \\beta_{0j} &amp; = \\gamma_{00} + \\gamma_{01} YRS{j} + u_{0j} \\\\ \\beta_{1j} &amp; = \\gamma_{10} + u_{1j} \\\\ \\beta_{2j} &amp; = \\gamma_{20} + u_{2j} \\end{align*} \\] The Single, Multilevel Model Equation: \\[ POP_{ij} = \\gamma_{00} + \\gamma_{10} GEN_{ij} + \\gamma_{20} EXT_{ij} + \\gamma_{01} YRS_j + u_{0j} + u_{1j} + u_{2j} + e_{ij} \\tag{M1: with predictors} \\] Parameters: Fixed Effects \\[ \\begin{align*} intercept \\longrightarrow &amp; \\gamma_{00}\\\\ pupil \\; gender \\longrightarrow &amp; \\gamma_{10}\\\\ pupil \\; extravert \\longrightarrow &amp; \\gamma_{20}\\\\ teacher \\; experience \\longrightarrow &amp; \\gamma_{01} \\end{align*} \\] Random Effects (ignore covariances for now) \\[ \\begin{align*} intercept \\longrightarrow \\sigma^2_{u_0 } &amp; = var[u_{0j}] \\\\ slope \\; for \\; pupil \\; gender \\longrightarrow \\sigma^2_{u_1} &amp; = var[u_{1j}] \\\\ slope \\; for \\; pupil \\; extravert \\longrightarrow \\sigma^2_{u_2} &amp; = var[u_{2j}] \\\\ residual \\longrightarrow \\sigma^2_{e} &amp; = var[e_{ij}] \\end{align*} \\] Troubleshooting lme4 Linear Mixed-Effects Models website. This website attempts to summarize some of the common problems with fitting lmer models and how to troubleshoot them. This is a helpful post on Stack Exchange regarding using differen t optimizers to get the lme4::lmer() function to converge. Note: Convergence issues MAY signify problems in the model specification. pop_lmer_0_ml &lt;- lme4::lmer(popular ~ 1 + (1|class), data = data_pop, REML = FALSE) pop_lmer_1_ml &lt;- lme4::lmer(popular ~ sex + extrav + texp + (sex + extrav|class), data = data_pop, REML = FALSE, control = lmerControl(optimizer =&quot;Nelder_Mead&quot;)) Reproduce Table 2.1 on the top of page 14 (Hox, Moerbeek, and Van de Schoot 2017) texreg::htmlreg(list(pop_lm_0, pop_lmer_0_ml, pop_lmer_1_ml), custom.model.names = c(&quot;Single-level&quot;, &quot;M0: int only&quot;, &quot;M1: w pred&quot;), caption = &quot;Hox Table 2.1 on the top of page 14&quot;) Hox Table 2.1 on the top of page 14 Single-level M0: int only M1: w pred (Intercept) 5.08*** 5.08*** 0.76*** (0.03) (0.09) (0.20) sexgirl 1.25*** (0.04) extrav 0.45*** (0.02) texp 0.09*** (0.01) R2 0.00 Adj. R2 0.00 Num. obs. 2000 2000 2000 RMSE 1.38 AIC 6333.47 4833.29 BIC 6350.27 4894.90 Log Likelihood -3163.73 -2405.64 Num. groups: class 100 100 Var: class (Intercept) 0.69 1.32 Var: Residual 1.22 0.55 Var: class sexgirl 0.00 Var: class extrav 0.03 Cov: class (Intercept) sexgirl -0.02 Cov: class (Intercept) extrav -0.19 Cov: class sexgirl extrav -0.00 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 "],
["references.html", "References", " References "]
]
