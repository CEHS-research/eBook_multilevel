[
["index.html", "Encyclopedia of Quantitative Methods in R, vol. 5: Multilevel Models Welcome Blocked Notes Code and Output The Authors", " Encyclopedia of Quantitative Methods in R, vol. 5: Multilevel Models Sarah Schwartz &amp; Tyson Barrett Last updated: 2018-10-02 Welcome Backgroup and links to other volumes of this encyclopedia may be found at the Encyclopedia’s Home Website. Blocked Notes Thoughout all the eBooks in this encyclopedia, several small secitons will be blocked out in the following ways: These blocks denote an area UNDER CONSTRUCTION, so check back often. This massive undertaking started during the summer of 2018 and is far from complete. The outline of seven volumes is given above despite any one being complete. Feedback is welcome via either author’s email. These blocks denote something EXTREMELY IMPORTANT. Do NOT skip these notes as they will be used very sparingly. These blocks denote something to DOWNLOAD. This may include software installations, example datasets, or notebook code files. These blocks denote something INTERESTING. These point out information we found of interest or added value. These blocks denote LINKS to other websites. This may include instructional video clips, articles, or blog posts. We are all about NOT re-creating the wheel. If somebody else has described or illustrated a topic well, we celebrate it! Code and Output This is how \\(R\\) code is shown: 1 + 1 This is what the output of the \\(R\\) code above will look: ## [1] 2 The Authors Dr. Sarah Schwartz Dr. Tyson Barrett www.SarahSchwartzStats.com www.TysonBarrett.com Sarah.Schwartz@usu.edu Tyson.Barrett@usu.edu Statistical Consulting Studio Data Science and Discover Unit Why choose R ? Check it out: an article from Fall 2016… No more excuses: R is better than SPSS for psychology undergrads, and students agree FYI This entire encyclopedia is written in \\(R Markdown\\), using \\(R Studio\\) as the text editor and the bookdown package to turn a collection of markdown documents into a coherent whole. The book’s source code is hosted on GitHub. If you notice typos or other issues, feel free to email either of the authors. This work is licensed under the Attribution-NonCommercial-NoDerivatives 4.0 International License. "],
["formula-warehouse.html", "1 Formula Warehouse 1.1 Data Notation 1.2 Single-level Regression Analysis 1.3 Multi-level Regression Analysis 1.4 Intraclass Correlation (ICC) 1.5 Proporion of Variance Explianed 1.6 Using \\(\\LaTeX\\) for Equation Typesetting", " 1 Formula Warehouse This is the home for notation and formulas used thorugh this eBook. Most important equations will be located here. 1.1 Data Notation Sample Sizes: \\(n_j\\) = number of pupils in class \\(j\\) \\(N\\) = number of classes Indicators: \\(i \\in (1, 2, \\dots, n_j)\\) = index for pupil number \\(j \\in (1, 2, \\dots, N)\\) = index for class number Level Type of Variable Symbol pupil \\(i\\) in class \\(j\\) 1 Outcome (Dependent) \\(Y\\) \\(Y_{ij}\\) 1 Predictor (Independent) \\(X_1\\) \\(X_{1ij}\\) 1 Predictor (Independent) \\(X_2\\) \\(X_{2ij}\\) 2 Predictor (Independent) \\(Z\\) \\(Z_j\\) 1.2 Single-level Regression Analysis 1.2.1 The Only Equation Since we are don’t have or are ignoring clustering, there is only one level. Single-Level Regression Equation \\[ \\overbrace{Y_{ij}}^{Outcome} = \\underbrace{\\beta_{0}}_{\\text{Fixed}\\atop\\text{intercept}} + \\underbrace{\\beta_{1}}_{\\text{Fixed}\\atop\\text{slope } X_1} \\overbrace{X_{1ij}}^{\\text{Predictor 1}} + \\underbrace{\\beta_{2}}_{\\text{Fixed}\\atop\\text{slope } X_2} \\overbrace{X_{2ij}}^{\\text{Predictor 2}} + \\underbrace{e_{ij}}_{\\text{Random}\\atop\\text{residuals}} \\tag{Hox 2.1} \\] 1.2.2 Parameters Type Parameter of Interest Estimates This Fixed Intercept \\(\\beta_{0}\\) Fixed Slope or main effect of \\(X_1\\) \\(\\beta_{1}\\) Fixed Slope or main effect of \\(X_2\\) \\(\\beta_{2}\\) Random Residual Variance \\(var[e_{ij}]\\) \\(\\sigma^2_{e}\\) 1.2.3 Assumptions to Check The \\(e_{ij}\\)’s follow a normal distribution with a mean of \\(0\\) The \\(e_{ij}\\)’s have a constant variance (homoscedasticity) 1.3 Multi-level Regression Analysis Continue taking into account fixed slopes for two Level 1 variables, \\(X_1\\) and \\(X_2\\). 1.3.1 Level 1 Regression Equation* \\[ \\overbrace{Y_{ij}}^{\\text{Level 1}\\atop\\text{Outcome}} = \\underbrace{\\beta_{0j}}_{\\text{Level 2}\\atop\\text{intercepts}} + \\underbrace{\\beta_{1j}}_{\\text{Level 2}\\atop\\text{slopes}} \\overbrace{X_{1ij}}^{\\text{Level 1}\\atop\\text{Predictor 1}} + \\underbrace{\\beta_{2j}}_{\\text{Level 2}\\atop\\text{slopes}} \\overbrace{X_{2ij}}^{\\text{Level 1}\\atop\\text{Predictor 2}} + \\underbrace{e_{ij}}_{\\text{Random}\\atop\\text{residuals}} \\tag{Hox 2.1} \\] Now we take clustering into account and include random intercepts (\\(\\beta_{0j}\\)) and slopes (\\(\\beta_{1j}, \\beta_{2j}\\)), as well as including a single Level 2 variable, \\(Z\\) that interacts with both Level 1 variables. 1.3.2 Level 2 Regression Equations 1.3.2.1 Random Intercepts: \\[ \\overbrace{\\beta_{0j}}^{\\text{Level 2}\\atop\\text{intercepts}} = \\underbrace{\\gamma_{00}}_{\\text{Fixed}\\atop\\text{intercept}} + \\underbrace{\\gamma_{01}}_{\\text{Fixed}\\atop\\text{slope } Z} \\overbrace{Z_{j}}^{\\text{Level 2}\\atop\\text{Predictor 3}} + \\underbrace{u_{0j}}_{\\text{Intercept}\\atop\\text{residual}} \\tag{Hox 2.3} \\] 1.3.2.2 Random Slopes For the first predictor, \\(X_1\\): \\[ \\overbrace{\\beta_{1j}}^{\\text{Level 2}\\atop\\text{slopes}} = \\underbrace{\\gamma_{10}}_{\\text{Fixed}\\atop\\text{Slope } X_1} + \\underbrace{\\gamma_{11}}_{\\text{Fixed}\\atop X_1 \\times Z} \\overbrace{Z_{j}}^{\\text{Level 2}\\atop\\text{Predictor 3}} + \\underbrace{u_{1j}}_{\\text{Slope } X_1\\atop\\text{residual}} \\tag{Hox 2.4a} \\] For the second predictor, \\(X_2\\): \\[ \\overbrace{\\beta_{2j}}^{\\text{Level 2}\\atop\\text{slopes}} = \\underbrace{\\gamma_{20}}_{\\text{Fixed}\\atop\\text{Slope } X_2} + \\underbrace{\\gamma_{21}}_{\\text{Fixed}\\atop X_2 \\times Z} \\overbrace{Z_{j}}^{\\text{Level 2}\\atop\\text{Predictor 3}} + \\underbrace{u_{2j}}_{\\text{Slope } X_2\\atop\\text{residual}} \\tag{Hox 2.4a} \\] 1.3.2.3 Merging the Equations Starting with Level 1 equation (2.1) and allow the \\(\\beta\\)’s to be varry for each class and plug in the level 2 equations (2.3 and 2.4) into the level 1 equation (2.1) to make the combined equation. \\[ Y_{ij} = \\overbrace{(\\gamma_{00} + \\gamma_{01} Z_{j} + u_{0j})}^{\\beta_{0j}} + \\overbrace{(\\gamma_{10} + \\gamma_{11} Z_{j} + u_{1j})}^{\\beta_{1j}} X_{1ij} + \\overbrace{(\\gamma_{20} + \\gamma_{21} Z_{j} + u_{2j})}^{\\beta_{2j}} X_{2ij} + e_{ij} \\] Use the distributive property of multiplication to get rid of the parentheses. \\[ Y_{ij} = \\overbrace{\\gamma_{00} + \\gamma_{01} Z_{j} + u_{0j}}^{\\beta_{0j}} + \\overbrace{\\gamma_{10} X_{1ij} + \\gamma_{11} Z_{j} X_{1ij} + u_{1j} X_{1ij}}^{\\beta_{1j} \\times X_{1ij}} + \\overbrace{\\gamma_{20} X_{2ij} + \\gamma_{21} Z_{j} X_{2ij} + u_{2j} X_{2ij}}^{\\beta_{2j} \\times X_{2ij}} + e_{ij} \\] 1.3.3 Combinded, Multilevel Regression Equation Collect ‘like-terms’ (i.e. get the \\(\\gamma\\)’s together and the \\(u\\)’s together) Combinded, Multilevel Regression Equation - Generic \\[ Y_{ij} = \\overbrace{\\gamma_{00} + \\gamma_{10} X_{1ij} + \\gamma_{20} X_{2ij} + \\gamma_{01} Z_{j} + \\gamma_{11} Z_{j} X_{1ij} + \\gamma_{21} Z_{j} X_{2ij}}^{\\text{Fixed part}\\atop\\text{Deterministic}} + \\\\ \\underbrace{u_{0j} + u_{1j} X_{1ij} + u_{2j} X_{2ij} + e_{ij} }_{\\text{Random part}\\atop\\text{Stochastic}} \\tag{Hox 2.5} \\] 1.3.4 Parameters Type Parameter of Interest Estimates This Fixed Intercept \\(\\gamma_{00}\\) Fixed Main Effect of \\(X_1\\) \\(\\gamma_{10}\\) Fixed Main Effect of \\(X_2\\) \\(\\gamma_{20}\\) Fixed Main Effect of \\(Z\\) \\(\\gamma_{01}\\) Fixed Cross-Level interaction between \\(X_1\\) and \\(Z\\) \\(\\gamma_{11}\\) Fixed Cross-Level interaction between \\(X_2\\) and \\(Z\\) \\(\\gamma_{21}\\) Random Variance in random intercepts, \\(var[u_{0j}]\\) \\(\\sigma^2_{u0}\\) Random Variance in random slope of \\(X_1\\), \\(var[u_{1j}]\\) \\(\\sigma^2_{u1}\\) Random Variance in random slope of \\(X_2\\), \\(var[u_{2j}]\\) \\(\\sigma^2_{u2}\\) Random Covariance between random intercepts and random slope of \\(X_1\\), \\(cov[u_{0j}, u_{1j}]\\) \\(\\sigma^2_{u01}\\) Random Covariance between random intercepts and random slope of \\(X_2\\), \\(cov[u_{0j}, u_{2j}]\\) \\(\\sigma^2_{u02}\\) Random Covariance between random slopes of \\(X_1\\) and \\(X_2\\), \\(cov[u_{1j}, u_{2j}]\\) \\(\\sigma^2_{u12}\\) Random Residual Variance \\(var[e_{ij}]\\) \\(\\sigma^2_{e}\\) The \\(u_{1j}\\) and \\(u_{2j}\\) terms allow for heteroscedasticity by fitting different error terms for different values of \\(X_1\\) and \\(X_2\\). The HOV assumption is that AFTER accounting for this, the remaining residuals are HOV. 1.4 Intraclass Correlation (ICC) 1.4.1 Two Level Models Combined, Multilevel Model Equation - Null Model, 2 levels \\[ \\overbrace{Y_{ij}}^{Outcome} = \\underbrace{\\gamma_{00}}_{\\text{Fixed}\\atop\\text{intercept}} + \\underbrace{u_{0j}}_{\\text{Random}\\atop\\text{intercepts}} + \\underbrace{e_{ij}}_{\\text{Random}\\atop\\text{residuals}} \\tag{Hox 2.8} \\] Although the Null model above does not explain any variance in the dependent variable, since there are no independent variables, it does decompose (i.e. divide up) the variance into two pieces. We can compute the amount of total variance in the outcome that is attribute to the clustering of Level 1 untis (micro-units) into clusters of Level 2 units (macro-units) verses the total variance. Intraclass Correlation (ICC) Formula, 2 level model \\[ \\overbrace{\\rho}^{\\text{ICC}} = \\frac{\\overbrace{\\sigma^2_{u0}}^{\\text{Random Intercept}\\atop\\text{Variance}}} {\\underbrace{\\sigma^2_{u0}+\\sigma^2_{e}}_{\\text{Total}\\atop\\text{Variance}}} \\tag{Hox 2.9} \\] 1.4.2 Three Level Models Indicators: \\(i\\) = index for units in the lowest level (Level 1) \\(j\\) = index for units in the middle level (Level 2) \\(k\\) = index for units in the highest level (Level 3) Combined, Multilevel Model Equation - Null Model, 3 levels \\[ \\overbrace{Y_{ijk}}^{Outcome} = \\underbrace{\\gamma_{000}}_{\\text{Fixed}\\atop\\text{intercept}} + \\underbrace{v_{0k }}_{\\text{Random Intercepts}\\atop\\text{Level 3}} + \\underbrace{u_{0jk}}_{\\text{Random Intercepts}\\atop\\text{Level 2}} + \\underbrace{e_{ijk}}_{\\text{Random}\\atop\\text{residuals}} \\tag{Hox 2.15} \\] If you are interested in teh decomposition of variance across all levels, use the Davis and Scott method: Intraclass Correlation (ICC) Formula, 3 level model - Davis and Scott Method \\[ \\overbrace{\\rho_{mid}}^{\\text{ICC}\\atop\\text{at level 2}} = \\frac{\\overbrace{\\sigma^2_{u0}}^{\\text{Random Intercept}\\atop\\text{Variance Level 2}}} {\\underbrace{\\sigma^2_{v0}+\\sigma^2_{u0}+\\sigma^2_{e}}_{\\text{Total}\\atop\\text{Variance}}} \\tag{Hox 2.16} \\] \\[ \\overbrace{\\rho_{top}}^{\\text{ICC}\\atop\\text{ at level 3}} = \\frac{\\overbrace{\\sigma^2_{u0}}^{\\text{Random Intercept}\\atop\\text{Variance Level 3}}} {\\underbrace{\\sigma^2_{v0}+\\sigma^2_{u0}+\\sigma^2_{e}}_{\\text{Total}\\atop\\text{Variance}}} \\tag{Hox 2.17} \\] If you would like to estimate the expected (population) correlation between two randomly chosen elements of the same group: Intraclass Correlation (ICC) Formula, 3 level model - Siddiqui Method \\[ \\overbrace{\\rho_{mid}}^{\\text{ICC}\\atop\\text{at level 2}} = \\frac{\\overbrace{\\sigma^2_{v0}+\\sigma^2_{u0}}^{\\text{Random variance}\\atop\\text{at levels 2 &amp;amp; 3}}} {\\underbrace{\\sigma^2_{v0}+\\sigma^2_{u0}+\\sigma^2_{e}}_{\\text{Total}\\atop\\text{Variance}}} \\tag{Hox 2.18} \\] \\[ \\overbrace{\\rho_{top}}^{\\text{ICC}\\atop\\text{ at level 3}} = \\frac{\\overbrace{\\sigma^2_{u0}}^{\\text{Random variance}\\atop\\text{at only level 3}}} {\\underbrace{\\sigma^2_{v0}+\\sigma^2_{u0}+\\sigma^2_{e}}_{\\text{Total}\\atop\\text{Variance}}} \\tag{Hox 2.19} \\] 1.5 Proporion of Variance Explianed See pages 61-63 of Hox, Moerbeek, and Van de Schoot (2017) http://journals.sagepub.com/doi/10.1177/1094428114541701 Analogous to multiple \\(R^2\\) - done seperately by level \\(BL\\) = Baseline model (Null) \\(MC\\) = Model to Compare to 1.5.1 Level 1 Variance Explained MODELS SHOULD NOT INCLUDE ANY RANOM EFFECTS, OTHER THAN RANDOM INTERCEPTS. Different approaches differ in values and meaning. 1.5.1.1 Snijders and Bosker Explained variance is a proportion of the total variance, because in principle first-level variables can explain all variation, including the variation at the second level. Correction removes the spurious increase in \\(R^2\\) when random slopes are added to a model Snijders and Bosker Formula - Level 1 Random Intercepts Models Only, address potential negative \\(R^2\\) issue \\[ R^2_1 = 1 - \\frac{\\sigma^2_{e-MC} + \\sigma^2_{u0-MC}} {\\sigma^2_{e-BL} + \\sigma^2_{u0-BL}} \\] 1.5.1.2 Raudenbush and Bryk Explained variance is a proportion of first-level variance only A good option when the multilevel sampling process is is close to two-stage simple random sampling Raudenbush and Bryk Approximate Formula - Level 1 approximate \\[ approx \\;R^2_1 = \\frac{\\sigma^2_{e-BL} - \\sigma^2_{e-MC}} {\\sigma^2_{e-BL} } \\tag{Hox 4.8} \\] 1.5.2 Level 2 Variance Explined 1.5.2.1 Snijders and Bosker Snijders and Bosker Formula Extended - Level 2 \\[ R^2_2 = 1 - \\frac{\\frac{\\sigma^2_{e-MC}}{B} + \\sigma^2_{u0-MC}} {\\frac{\\sigma^2_{e-BL}}{B} + \\sigma^2_{u0-BL}} \\] \\(B\\) is the average size of the Level 2 units (schools). Technically, you should use the harmonic mean, but unless the clusters differ greatly in size, it doesn’t make a huge difference. 1.5.2.2 Raudenbush and Bryk Raudenbush and Bryk Approximate Formula - Level 2 \\[ approx \\; R^2_s = \\frac{\\sigma^2_{u0-BL} - \\sigma^2_{u0-MC}} {\\sigma^2_{u0-BL} } \\tag{Hox 4.9} \\] 1.6 Using \\(\\LaTeX\\) for Equation Typesetting R markdown is a user friendly, simplified language that allows for more complex formating utilizing standard \\(\\LaTeX\\) code. A great resource for learning how to many common tasks in \\(\\LaTeX\\) is the Sharewebsite. Specific mathematical equation documentation may be found on the Mathematical Expressions subpage. There are also many websites that offer Point-n-click interfaces to build \\(\\LaTeX\\) equations, including: Host Math, Code Cogs, LaTeX 4 Technics, and Sci-Weavers "],
["intro-2-level-model-example-pupil-popularity.html", "2 Intro 2-level Model Example: Pupil Popularity 2.1 Background 2.2 Exploratory Data Analysis 2.3 Single-level Regression Analysis 2.4 Multi-level Regression Analysis", " 2 Intro 2-level Model Example: Pupil Popularity library(tidyverse) library(haven) # read in SPSS dataset library(furniture) # nice table1() descriptives library(stargazer) # display nice tables: summary &amp; regression library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(RColorBrewer) # nice color palettes for plots library(gridExtra) # place ggplots together as one plot library(psych) # contains some useful functions, like headTail library(car) # Companion to Applied Regression library(nlme) # non-linear mixed-effects models library(lme4) # Linear, generalized linear, &amp; nonlinear mixed models library(lmerTest) # Tests on lmer objects library(HLMdiag) # Diagnostic Tools for for nlme &amp; lmer4 library(sjstats) # ICC calculations library(optimx) # Different optimizers to solve mlm&#39;s 2.1 Background The text “Multilevel Analysis: Techniques and Applications, Third Edition” (Hox, Moerbeek, and Van de Schoot 2017) has a companion website which includes links to all the data files used throughout the book (housed on the book’s GitHub repository). The following example is used through out Hox, Moerbeek, and Van de Schoot (2017)’s chapater 2. From Appendix E: The popularity data in popular2.sav are simulated data for 2000 pupils in 100 schools. The purpose is to offer a very simple example for multilevel regression analysis. The main outcome variable is the pupil popularity, a popularity rating on a scale of 1-10 derived by a sociometric procedure. Typically, a sociometric procedure asks all pupils in a class to rate all the other pupils, and then assigns the average received popularity rating to each pupil. Because of the sociometric procedure, group effects as apparent from higher level variance components are rather strong. There is a second outcome variable: pupil popularity as rated by their teacher, on a scale from 1-10. The explanatory variables are pupil gender (boy=0, girl=1), pupil extraversion (10-point scale) and teacher experience in years. The popularity data have been generated to be a ‘nice’ well-behaved data set: the sample sizes at both levels are sufficient, the residuals have a normal distribution, and the multilevel effects are strong. data_raw &lt;- haven::read_sav(&quot;https://github.com/MultiLevelAnalysis/Datasets-third-edition-Multilevel-book/raw/master/chapter%202/popularity/SPSS/popular2.sav&quot;) %&gt;% haven::as_factor() # retain the labels from SPSS --&gt; factor tibble::glimpse(data_raw) Observations: 2,000 Variables: 15 $ pupil &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1... $ class &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1... $ extrav &lt;dbl&gt; 5, 7, 4, 3, 5, 4, 5, 4, 5, 5, 5, 5, 5, 5, 5, 6, 4, 4... $ sex &lt;fct&gt; girl, boy, girl, girl, girl, boy, boy, boy, boy, boy... $ texp &lt;dbl&gt; 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, ... $ popular &lt;dbl&gt; 6.3, 4.9, 5.3, 4.7, 6.0, 4.7, 5.9, 4.2, 5.2, 3.9, 5.... $ popteach &lt;dbl&gt; 6, 5, 6, 5, 6, 5, 5, 5, 5, 3, 5, 5, 5, 6, 5, 5, 2, 3... $ Zextrav &lt;dbl&gt; -0.1703149, 1.4140098, -0.9624772, -1.7546396, -0.17... $ Zsex &lt;dbl&gt; 0.9888125, -1.0108084, 0.9888125, 0.9888125, 0.98881... $ Ztexp &lt;dbl&gt; 1.48615283, 1.48615283, 1.48615283, 1.48615283, 1.48... $ Zpopular &lt;dbl&gt; 0.88501327, -0.12762911, 0.16169729, -0.27229230, 0.... $ Zpopteach &lt;dbl&gt; 0.66905609, -0.04308451, 0.66905609, -0.04308451, 0.... $ Cextrav &lt;dbl&gt; -0.215, 1.785, -1.215, -2.215, -0.215, -1.215, -0.21... $ Ctexp &lt;dbl&gt; 9.737, 9.737, 9.737, 9.737, 9.737, 9.737, 9.737, 9.7... $ Csex &lt;dbl&gt; 0.5, -0.5, 0.5, 0.5, 0.5, -0.5, -0.5, -0.5, -0.5, -0... 2.1.1 Unique Identifiers We will restrict ourselves to a few of the variables and create a distinct identifier variable for each student. data_pop &lt;- data_raw %&gt;% dplyr::mutate(id = paste(class, pupil, sep = &quot;_&quot;) %&gt;% # create a unique id for each student (char) factor()) %&gt;% # declare id is a factor dplyr::select(id, pupil:popteach) # reduce the variables included tibble::glimpse(data_pop) Observations: 2,000 Variables: 8 $ id &lt;fct&gt; 1_1, 1_2, 1_3, 1_4, 1_5, 1_6, 1_7, 1_8, 1_9, 1_10, 1_... $ pupil &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16... $ class &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... $ extrav &lt;dbl&gt; 5, 7, 4, 3, 5, 4, 5, 4, 5, 5, 5, 5, 5, 5, 5, 6, 4, 4,... $ sex &lt;fct&gt; girl, boy, girl, girl, girl, boy, boy, boy, boy, boy,... $ texp &lt;dbl&gt; 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 2... $ popular &lt;dbl&gt; 6.3, 4.9, 5.3, 4.7, 6.0, 4.7, 5.9, 4.2, 5.2, 3.9, 5.7... $ popteach &lt;dbl&gt; 6, 5, 6, 5, 6, 5, 5, 5, 5, 3, 5, 5, 5, 6, 5, 5, 2, 3,... 2.1.2 Structure and variables Its a good idea to visually inspect the first few lines in the datast to get a sense of how it is organized. data_pop %&gt;% psych::headTail(top = 25, bottom = 5) %&gt;% pander::pander() id pupil class extrav sex texp popular popteach 1_1 1 1 5 girl 24 6.3 6 1_2 2 1 7 boy 24 4.9 5 1_3 3 1 4 girl 24 5.3 6 1_4 4 1 3 girl 24 4.7 5 1_5 5 1 5 girl 24 6 6 1_6 6 1 4 boy 24 4.7 5 1_7 7 1 5 boy 24 5.9 5 1_8 8 1 4 boy 24 4.2 5 1_9 9 1 5 boy 24 5.2 5 1_10 10 1 5 boy 24 3.9 3 1_11 11 1 5 girl 24 5.7 5 1_12 12 1 5 girl 24 4.8 5 1_13 13 1 5 boy 24 5 5 1_14 14 1 5 girl 24 5.5 6 1_15 15 1 5 girl 24 6 5 1_16 16 1 6 girl 24 5.7 5 1_17 17 1 4 boy 24 3.2 2 1_18 18 1 4 boy 24 3.1 3 1_19 19 1 7 girl 24 6.6 7 1_20 20 1 4 boy 24 4.8 4 2_1 1 2 8 girl 14 6.4 6 2_2 2 2 4 boy 14 2.4 3 2_3 3 2 6 boy 14 3.7 4 2_4 4 2 5 girl 14 4.4 4 2_5 5 2 5 girl 14 4.3 4 NA … … … NA … … … 100_16 16 100 4 girl 7 4.3 5 100_17 17 100 4 boy 7 2.6 2 100_18 18 100 8 girl 7 6.7 7 100_19 19 100 5 boy 7 2.9 3 100_20 20 100 9 boy 7 5.3 5 Visual inspection reveals that most of the variables are measurements at level 1 and apply to specific pupils (extrav, sex, popular, and popteach), while the teacher’s years of experiene is a level 2 variable since it applies to the entire class. Notice how the texp variable is identical for all pupils in the same class. This is call Disaggregated data. 2.2 Exploratory Data Analysis 2.2.1 Summarize Descriptive Statistics 2.2.1.1 The stargazer package Most posters, journal articles, and reports start with a table of descriptive statistics. Since it tends to come first, this type of table is often refered to as Table 1. The stargazer() function can be used to create such a table, but only for the entire dataset (Hlavac 2018). I haven’t been able to find a way to get it to summarize subsamples and compare them in the standard format. # Knit to Website: type = &quot;html&quot; # Knit to PDF: type = &quot;latex&quot; # View on Screen: type = &quot;text&quot; data_pop %&gt;% dplyr::select(extrav, texp, popular) %&gt;% data.frame() %&gt;% stargazer::stargazer(title = &quot;Descriptive statistics, aggregate over entire sample&quot;, header = FALSE, type = &quot;html&quot;) Descriptive statistics, aggregate over entire sample Statistic N Mean St. Dev. Min Pctl(25) Pctl(75) Max extrav 2,000 5.215 1.262 1 4 6 10 texp 2,000 14.263 6.552 2 8 20 25 popular 2,000 5.076 1.383 0.000 4.100 6.000 9.500 2.2.1.2 The furniture package Tyson Barrett’s furniture package includes the extremely useful function table1() which simplifies the common task of creating a stratified, comparative table of descriptive statistics. Full documentation can be accessed by executing ?furniture::table1. # Knit to Website: output = &quot;html&quot; # Knit to PDF: output = &quot;latex2&quot; # View on Screen: output = &quot;&quot;text&quot;, or &quot;markdown&quot;, &quot;html&quot; data_pop %&gt;% furniture::table1(&quot;Pupil&#39;s Extraversion (10 pt)&quot; = extrav, &quot;Teacher&#39;s Experience (years)&quot; = texp, &quot;Popularity, Sociometric Score&quot; = popular, &quot;Popularity, Teacher Evaluated&quot; = popteach, splitby = ~ sex, # divide sample into columns by... test = TRUE, # test groups different? output = &quot;html&quot;, # output for latex caption = &quot;Compare genders on four main variables&quot;) # title Table 2.1: Compare genders on four main variables boy girl P-Value n = 989 n = 1011 Pupil’s Extraversion (10 pt) &lt;.001 5.1 (1.2) 5.3 (1.3) Teacher’s Experience (years) 0.001 13.8 (6.3) 14.7 (6.8) Popularity, Sociometric Score &lt;.001 4.3 (1.1) 5.9 (1.1) Popularity, Teacher Evaluated &lt;.001 4.3 (1.2) 5.8 (1.2) 2.2.2 Visualizations of Raw Data 2.2.2.1 Ignore Clustering 2.2.2.1.1 Scatterplots For a first look, its useful to plot all the data points on a single scatterplot as displayed in Figure 2.1. Due to ganularity in the rating scale, many points end up being plotted on top of each other (overplotted), so its a good idea to use geom_count() rather than geom_point() so the size of the dot can convey the number of points at that location (Wickham et al. 2018). # Disaggregate: pupil (level 1) only, ignore level 2&#39;s existance # Extraversion treated: continuous measure data_pop %&gt;% ggplot() + aes(x = extrav, # x-axis variable y = popular) + # y-axis variable geom_count() + # POINTS w/ SIZE = COUNT geom_smooth(method = &quot;lm&quot;) + # linear regression line theme_bw() + # white background labs(x = &quot;Extraversion (10 pt scale)&quot;, # x-axis label y = &quot;Popularity, Sociometric Score&quot;, # y-axis label size = &quot;Count&quot;) + # legend key&#39;s title theme(legend.position = c(0.9, 0.2), # key at legend.background = element_rect(color = &quot;black&quot;)) + # key box scale_x_continuous(breaks = seq(from = 0, to = 10, by = 1)) + # x-ticks scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) # y-ticks Figure 2.1: Disaggregate: pupil level only with extraversion treated as an continuous measure. 2.2.2.1.2 Density Plots When the degree of overplotting as high as it is in Figure 2.1, it can be useful to represent the data with density contours as seen in Figure 2.2. I’ve chosen to leave the points displayed in this redition, but color them much lighter so that they are present, but do not detract from the pattern of association. # visualize all the data - another way data_pop %&gt;% ggplot() + aes(x = extrav, # x-axis variable y = popular) + # y-axis variable geom_count(color = &quot;gray&quot;) + # POINTS w/ SIZE = COUNT geom_density2d() + # DENSITY CURVES geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;) + # linear regression line theme_bw() + # white background labs(x = &quot;Extraversion (10 pt scale)&quot;, # x-axis label y = &quot;Popularity, Sociometric Score&quot;) + # y-axis label guides(size = FALSE) + # don&#39;t include a legend scale_x_continuous(breaks = seq(from = 0, to = 10, by = 1)) + # x-ticks scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) # y-ticks Figure 2.2: Disaggregate: pupil level only with extraversion treated as an continuous measure. The argument could be made that the extraversion score should be treated as an ordinal factor instead of as a truely continuous scale since the only valid values are the whole number 1 through 10 and there is no assurance that these category assignments represent a true ratio measurement scale. However, we must keep in mind that this was an observational study, ans as such, the number of pupils assignment each level of extraversion is not equal. # count the number of pupils in assigned each Extraversion value, 1:10 table &lt;- data_pop %&gt;% group_by(extrav) %&gt;% summarise(count = n_distinct(id), percent = 100 * count / 2000) # Knit to Website: type = &quot;html&quot; # Knit to PDF: type = &quot;latex&quot; # View on Screen: type = &quot;text&quot; table %&gt;% stargazer(summary = FALSE, rownames = FALSE, header = FALSE, type = &quot;html&quot;, title = &quot;Distribution of extraversion in pupils&quot;) Distribution of extraversion in pupils extrav count percent 1 3 0.15 2 13 0.65 3 119 5.95 4 423 21.15 5 688 34.4 6 478 23.9 7 194 9.7 8 58 2.9 9 18 0.9 10 6 0.3 2.2.2.1.3 Boxplots Figure 2.3 displays the same data as Figure 2.1, but uses boxplots for the distribution of scores at each level of extraversion. On one extreme, the lowest extraversion score possible was a value of “one”, but only 3 pupils or 0.15% of the 2000 pupils recieved this value. On the other extreme, the middle value of “five” was applied to 688 pupils or a wopping 34.4%. The option varwidth=TRUE in the geom_boxplot() function helps reflect such unbalanced sample sizes by allowing the width of the boxes to be proportional to the square-roots of the number of observations each box represents. # Disaggregate: pupil (level 1) only, ignore level 2&#39;s existance # Extraversion treated: ordinal factor ggplot(data_pop, # dataset&#39;s name aes(x = factor(extrav), # x-axis values - make factor! y = popular, # y-axis values fill = factor(extrav))) + # makes seperate boxes geom_boxplot(varwidth = TRUE) + # draw boxplots instead of points theme_bw() + # white background guides(fill = FALSE) + # don&#39;t include a legend scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) + # y-ticks labs(x = &quot;Extraversion (10 pt scale)&quot;, # x-axis label y = &quot;Popularity, Sociometric Score&quot;) + # y-axis label scale_fill_brewer(palette = &quot;Spectral&quot;, direction = 1) # select color Figure 2.3: Disaggregate: pupil level only with extraversion treated as an ordinal factor. The width of the boxes are proportional to the square-roots of the number of observations each box represents. 2.2.3 Consider Clustering 2.2.3.1 Scatterplots Up to this point, all investigation of this dataset has been only at the pupil level and any nesting or clustering within classes has been ignored. Plotting is a good was to start to get an idea of the class-to-class variability. # compare the first 9 classrooms becuase all of there are too many at once data_pop %&gt;% dplyr::filter(class &lt;= 9) %&gt;% # select ONLY NINE classes ggplot(aes(x = extrav, # x-axis values y = popular)) + # y-axis values geom_count() + # POINTS w/ SIZE = COUNT geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;) + # linear regression line theme_bw() + # white background labs(x = &quot;Extraversion (10 pt scale)&quot;, # x-axis label y = &quot;Popularity, Sociometric Score&quot;, # y-axis label size = &quot;Count&quot;) + # legend key&#39;s title guides(size = FALSE) + # don&#39;t include a legend scale_x_continuous(breaks = seq(from = 0, to = 10, by = 3)) + # x-ticks scale_y_continuous(breaks = seq(from = 0, to = 10, by = 3)) + # y-ticks facet_wrap(~ class, labeller = label_both) + theme(strip.background = element_rect(colour = NA, fill = NA)) Figure 2.4: Illustration of the degree of class level variability in the association between extraversion and popularity. Each panel represents a class and each point a pupil in that class. First nice classes shown. # select specific classes by number for illustration purposes data_pop %&gt;% dplyr::filter(class %in% c(15, 25, 33, 35, 51, 64, 76, 94, 100)) %&gt;% ggplot(aes(x = extrav, # x-axis values y = popular)) + # y-axis values geom_count() + # POINTS w/ SIZE = COUNT geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;) + # linear regression line theme_bw() + # white background labs(x = &quot;Extraversion (10 pt scale)&quot;, # x-axis label y = &quot;Popularity, Sociometric Score&quot;, # y-axis label size = &quot;Count&quot;) + # legend key&#39;s title guides(size = FALSE) + # don&#39;t include a legend scale_x_continuous(breaks = seq(from = 0, to = 10, by = 3)) + # x-ticks scale_y_continuous(breaks = seq(from = 0, to = 10, by = 3)) + # y-ticks facet_wrap(~ class) + theme(strip.background = element_blank(), strip.text = element_blank()) Figure 2.5: Illustration of the degree of class level variability in the association between extraversion and popularity. Each panel represents a class and each point a pupil in that class. A set of nine classes was chosen to show a sampling of variability. The facet labels are not shown as the identification number probably would not be advisable for a general publication. 2.2.3.2 Cluster-wise Regression # compare all 100 classrooms via linear model for each ggplot(data_pop, aes(x = extrav, # x-axis values y = popular, # y-axis values group = class)) + # GROUPs for LINES geom_smooth(method = &quot;lm&quot;, # linear regression line color = &quot;gray40&quot;, size = 0.4, se = FALSE) + theme_bw() + # white background labs(x = &quot;Extraversion (10 pt scale)&quot;, # x-axis label y = &quot;Popularity, Sociometric Score&quot;) + # y-axis label scale_x_continuous(breaks = seq(from = 0, to = 10, by = 2)) + # x-ticks scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) # y-ticks Figure 2.6: Spaghetti plot of seperate, independent linear models for each of the 100 classes. A helpful resource for choosing colors to use in plots: R color cheatsheet # compare all 100 classrooms via independent linear models data_pop %&gt;% dplyr::mutate(texp3 = cut(texp, breaks = c(0, 10, 18, 30)) %&gt;% factor(labels = c(&quot;&lt; 10 yrs&quot;, &quot;10 - 18 yrs&quot;, &quot;&gt; 18 yrs&quot;))) %&gt;% ggplot(aes(x = extrav, # x-axis values y = popular, # y-axis values group = class)) + # GROUPs for LINES geom_smooth(aes(color = sex), size = 0.3, method = &quot;lm&quot;, # linear regression line se = FALSE) + theme_bw() + # white background labs(x = &quot;Extraversion (10 pt scale)&quot;, # x-axis label y = &quot;Popularity, Sociometric Score&quot;) + # y-axis label guides(color = FALSE) + # don&#39;t include a legend scale_x_continuous(breaks = seq(from = 0, to = 10, by = 3)) + # x-ticks scale_y_continuous(breaks = seq(from = 0, to = 10, by = 3)) + # y-ticks scale_color_manual(values = c(&quot;dodgerblue&quot;, &quot;maroon1&quot;)) + facet_grid(texp3 ~ sex) Figure 2.7: Spaghetti plot of seperate, independent linear models for each of the 100 classes. Seperate panels are used to untangle the ‘hairball’ in the previous figure. The columns are seperated by the pupils’ gender and the rows by the teacher’s experince in years. 2.3 Single-level Regression Analysis 2.3.1 Null Model In a Null, intercept-only, or Empty model, no predictors are included. 2.3.1.1 Equations Single-Level Regression Equation - Null Model \\[ \\overbrace{POP_{ij}}^{Outcome} = \\underbrace{\\beta_{0}}_{\\text{Fixed}\\atop\\text{intercept}} + \\underbrace{e_{ij}}_{\\text{Random}\\atop\\text{residuals}} \\] 2.3.1.2 Parameters Type Parameter of Interest Estimates This Fixed Intercept \\(\\beta_{0}\\) Random Residual Variance \\(var[e_{ij}]\\) \\(\\sigma^2_{e}\\) 2.3.1.3 Fit the Model pop_lm_0 &lt;- lm(popular ~ 1, # The 1 represents the intercept data = data_pop) summary(pop_lm_0) Call: lm(formula = popular ~ 1, data = data_pop) Residuals: Min 1Q Median 3Q Max -5.0765 -0.9765 0.0235 0.9236 4.4235 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.07645 0.03091 164.2 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.383 on 1999 degrees of freedom \\(\\hat{\\beta_0}\\) = 5.08 is the grand mean 2.3.1.4 Model Fit Residual variance: sigma(pop_lm_0) # standard deviation of the residuals [1] 1.382522 sigma(pop_lm_0)^2 # variance of the residuals [1] 1.911366 \\(\\hat{\\sigma_e^2}\\) = 1.9114 is residual variance (RMSE is sigma = 1.3825) Variance Explained: summary(pop_lm_0)$r.squared [1] 0 \\(R^2\\) = 0 is the proportion of variance in popularity that is explained by the grand mean alone. Deviance: -2 * logLik(pop_lm_0) &#39;log Lik.&#39; 6970.39 (df=2) 2.3.1.5 Interpretation The grand average popularity of all pupils in all the classes is 5.08, and there is strong evidence that it is statistically significantly different than zero, \\(p&lt;.0001\\). The mean alone accounts for none of the variance in popularity. The residual variance is the same as the total variance in popularity, 1.9114. Just to make sure… mean(data_pop$popular) [1] 5.07645 var(data_pop$popular) [1] 1.911366 2.3.2 Add Predictors to the Model 2.3.2.1 Equations LEVEL 1: Student-specific predictors: \\(X_1 = GEN\\), pupils’s gender (girl vs. boy) \\(X_2 = EXT\\), pupil’s extraversion (scale: 1-10) Single-Level Regression Equation \\[ \\overbrace{POP_{ij}}^{Outcome} = \\underbrace{\\beta_{0}}_{\\text{Fixed}\\atop\\text{intercept}} + \\underbrace{\\beta_{1}}_{\\text{Fixed}\\atop\\text{slope}} \\overbrace{GEN_{ij}}^{\\text{Predictor 1}} + \\underbrace{\\beta_{2}}_{\\text{Fixed}\\atop\\text{slope}} \\overbrace{EXT_{ij}}^{\\text{Predictor 2}} + \\underbrace{e_{ij}}_{\\text{Random}\\atop\\text{residuals}} \\tag{Hox 2.1} \\] 2.3.2.2 Parameters Type Parameter of Interest Estimates This Fixed Intercept \\(\\beta_{0}\\) Fixed Slope or effect of sex \\(\\beta_{1}\\) Fixed Slope or effect of extrav \\(\\beta_{2}\\) Random Residual Variance \\(var[e_{ij}]\\) \\(\\sigma^2_{e}\\) 2.3.2.3 Fit the Model pop_lm_1 &lt;- lm(popular ~ sex + extrav, # implies: 1 + sex + extrav data = data_pop) summary(pop_lm_1) Call: lm(formula = popular ~ sex + extrav, data = data_pop) Residuals: Min 1Q Median 3Q Max -4.2527 -0.6652 -0.0454 0.7422 3.0473 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 2.78954 0.10355 26.94 &lt;2e-16 *** sexgirl 1.50508 0.04836 31.12 &lt;2e-16 *** extrav 0.29263 0.01916 15.28 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.077 on 1997 degrees of freedom Multiple R-squared: 0.3938, Adjusted R-squared: 0.3932 F-statistic: 648.6 on 2 and 1997 DF, p-value: &lt; 2.2e-16 \\(\\hat{\\beta_0}\\) = 2.79 is the extrapolated mean for boys with an extroversion score of 0. \\(\\hat{\\beta_1}\\) = 1.51 is the mean difference between girls and boys with the same extroversion score. \\(\\hat{\\beta_2}\\) = 0.29 is the mean difference for pupils of the same gender that differ in extroversion by one point. 2.3.2.4 Model Fit Residual variance: sigma(pop_lm_1) # standard deviation of the residuals [1] 1.076985 sigma(pop_lm_1)^2 # variance of the residuals [1] 1.159898 \\(\\hat{\\sigma_e^2}\\) = 1.1599 is residual variance (RMSE is sigma) Variance Explained: summary(pop_lm_1)$r.squared [1] 0.393765 Deviance: -2 * logLik(pop_lm_1) &#39;log Lik.&#39; 5969.415 (df=4) \\(R^2\\) = 0.394 is the proportion of variance in popularity that is explained by tha pupils gender and extroversion score. 2.3.2.5 Interpretation On average, girls were rated 1.51 points more popular than boys with the same extroversion score, \\(p&lt;.0001\\). One point higher extroversion scores were associated with 0.29 points higher popularity, within each gender, \\(p&lt;.0001\\). Together, these two factors account for 39.38% of the variance in populartiy. 2.3.3 Compare Fixed Effects 2.3.3.1 Compare Nested Models Create a table to compare the two nested models: # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(pop_lm_0, pop_lm_1), custom.model.names = c(&quot;Null Model&quot;, &quot;With Predictors&quot;), caption = &quot;Single Level Models&quot;, caption.above = TRUE, single.row = TRUE) Single Level Models Null Model With Predictors (Intercept) 5.08 (0.03)*** 2.79 (0.10)*** sexgirl 1.51 (0.05)*** extrav 0.29 (0.02)*** R2 0.00 0.39 Adj. R2 0.00 0.39 Num. obs. 2000 2000 RMSE 1.38 1.08 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 When comparing the fit of two single-level models fit via the lm() function, the anova() function runs an F-test where the test statistic is the difference in RSS. anova(pop_lm_0, pop_lm_1) Analysis of Variance Table Model 1: popular ~ 1 Model 2: popular ~ sex + extrav Res.Df RSS Df Sum of Sq F Pr(&gt;F) 1 1999 3820.8 2 1997 2316.3 2 1504.5 648.55 &lt; 2.2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Obviously the model with predictors fits better than the model with no predictors. 2.3.3.2 Terminology The following terminology applies to single-level models fit with ordinary least-squared estimation (the lm() function in \\(R\\)). Values are calculated below for the NULL model. Mean squared error (MSE) is the MEAN of the square of the residuals: mse &lt;- mean(residuals(pop_lm_0)^2) mse [1] 1.91041 Root mean squared error (RMSE) which is the SQUARE ROOT of MSE: rmse &lt;- sqrt(mse) rmse [1] 1.382176 Residual sum of squares (RSS) is the SUM of the squared residuals: rss &lt;- sum(residuals(pop_lm_0)^2) rss [1] 3820.821 Residual standard error (RSE) is the SQUARE ROOT of (RSS / degrees of freedom): rse &lt;- sqrt( sum(residuals(pop_lm_0)^2) / pop_lm_0$df.residual ) rse [1] 1.382522 The same calculation, may be simplified with the previously calculated RSS: sqrt(rss / pop_lm_0$df.residual) [1] 1.382522 When the ‘deviance()’ function is applied to a single-level model fit via ‘lm()’, the Residual sum of squares (RSS) is returned, not the deviance as defined as twice the negative log likelihood (-2LL). deviance(pop_lm_0) # returns the RSS, not deviance = -2LL [1] 3820.821 -2 * logLik(pop_lm_0) # this is how get deviance = -2LL &#39;log Lik.&#39; 6970.39 (df=2) 2.4 Multi-level Regression Analysis 2.4.1 Intercept-only or Null Model In a Null, intercept-only, or Empty model, no predictors are included. “The intercept-only model is useful as a null-model that serves as a benchmark with which other models are compared.” Hox, Moerbeek, and Van de Schoot (2017), page 13 2.4.1.1 Equations Level 1 Model Equation: \\[ \\overbrace{Y_{ij}}^{Outcome} = \\underbrace{\\beta_{0j}}_{\\text{Level 2}\\atop\\text{intercepts}} + \\underbrace{e_{ij}}_{\\text{Random}\\atop\\text{residuals}} \\tag{Hox 2.6} \\] Level 2 Model Equation: \\[ \\overbrace{\\beta_{0j}}^{\\text{Level 2}\\atop\\text{intercepts}} = \\underbrace{\\gamma_{00}}_{\\text{Fixed}\\atop\\text{intercept}} + \\underbrace{u_{0j}}_{\\text{Random}\\atop\\text{intercepts}} \\tag{Hox 2.7} \\] Substitute equation (2.7) into equation (2.6): Combined, Multilevel Model Equation - Null Model \\[ \\overbrace{Y_{ij}}^{Outcome} = \\underbrace{\\gamma_{00}}_{\\text{Fixed}\\atop\\text{intercept}} + \\underbrace{u_{0j}}_{\\text{Random}\\atop\\text{intercepts}} + \\underbrace{e_{ij}}_{\\text{Random}\\atop\\text{residuals}} \\tag{Hox 2.8} \\] 2.4.1.2 Parameters Type Parameter of Interest Estimates This Fixed Intercept \\(\\gamma_{00}\\) Random Variance in random intercepts, \\(var[u_{0j}]\\) \\(\\sigma^2_{u0}\\) Random Residual Variance \\(var[e_{ij}]\\) \\(\\sigma^2_{e}\\) Hox, Moerbeek, and Van de Schoot (2017) labeled the Null model for this dataset “\\(M_0\\)” in chapter 2: Combined, Multilevel Model Equation - Popularity, Random Intercepts Only! \\[ \\overbrace{POP_{ij}}^{Outcome} = \\underbrace{\\gamma_{00}}_{\\text{Fixed}\\atop\\text{intercept}} + \\underbrace{u_{0j}}_{\\text{Random}\\atop\\text{intercepts}} + \\underbrace{e_{ij}}_{\\text{Random}\\atop\\text{residuals}} \\tag{M0: intercept only} \\] 2.4.1.3 Fit the Model Fit the model to the data. pop_lmer_0_re &lt;- lme4::lmer(popular ~ 1 + (1|class), # include a fixed and random intercept data = data_pop, REML = TRUE) # fit via REML (the default) for ICC calculations summary(pop_lmer_0_re) Linear mixed model fit by REML [&#39;lmerMod&#39;] Formula: popular ~ 1 + (1 | class) Data: data_pop REML criterion at convergence: 6330.5 Scaled residuals: Min 1Q Median 3Q Max -3.5655 -0.6975 0.0020 0.6758 3.3175 Random effects: Groups Name Variance Std.Dev. class (Intercept) 0.7021 0.8379 Residual 1.2218 1.1053 Number of obs: 2000, groups: class, 100 Fixed effects: Estimate Std. Error t value (Intercept) 5.07786 0.08739 58.1 Estimation Methods Multilevel models may be fit by various methods. The most commonly used (and availabel in ‘lme4’) optimize various criterions: Maximum Likelihood (ML) -or- Restricted Maximum Likelihood (REML). Hox, Moerbeek, and Van de Schoot (2017) discusses these and other methods in chapter 3. At the end of chapter 2, the authors’ second note staes that the details of estimation methods are glossed over in the current example in an effort to simplfy the introductory. Here we follow these guidelines: Use ML for fitting: nested models that differ only by inclusion/exclusion of FIXED effects, to test parameter significance via a likelihood ratio test Use REML for fitting: the NULL model, on which to base ICC calculations nested models that differ only by inclusion/exclusion of RANDOM effects, to test parameter significance via a likelihood ratio test the FINAL model This often leads to refitting identical models via BOTH estimation methods. 2.4.1.4 Interpretation The grand average popularity of all students is 5.0779 and the class averages tend to vary by about 0.8333 points above or below that. 2.4.2 Intraclass Correlation (ICC) Although the Null model above does not explain any variance in the dependent variable (popularity), since there are no independent variables, it does decompose (i.e. divide up) the variance into two pieces. We can compute the amount of total variance in popularity that is attribute to the clustering of students in classes verses the residual variance. Intraclass Correlation (ICC) Formula \\[ \\overbrace{\\rho}^{\\text{ICC}} = \\frac{\\overbrace{\\sigma^2_{u0}}^{\\text{Random Intercept}\\atop\\text{Variance}}} {\\underbrace{\\sigma^2_{u0}+\\sigma^2_{e}}_{\\text{Total}\\atop\\text{Variance}}} \\tag{Hox 2.9} \\] The VarCorr() function in the lme4 package returns the standard deviations, not the variances (\\(var = SD^2\\)) for a model fit via the lme4::lmer() function. The summary() function reports both the variances and the stadard deviations. lme4::VarCorr(pop_lmer_0_re) # extract random compondent: varrainces and correlations Groups Name Std.Dev. class (Intercept) 0.83792 Residual 1.10535 sjstats::re_var(pop_lmer_0_re) Within-group-variance: 1.222 Between-group-variance: 0.702 (class) \\[ \\begin{align*} \\text{classes} \\rightarrow \\; &amp; \\sigma^2_{u0} = 0.83792^2 = 0.702\\\\ \\text{pupils within classes} \\rightarrow \\; &amp; \\sigma^2_{e} = 1.10535^2 = 1.222\\\\ \\end{align*} \\] 2.4.2.1 By Hand Calculate the ICC by hand: \\[ \\overbrace{\\rho}^{\\text{ICC}} = \\frac{\\overbrace{\\sigma^2_{u0}}^{\\text{Random Intercept}\\atop\\text{Variance}}} {\\underbrace{\\sigma^2_{u0}+\\sigma^2_{e}}_{\\text{Total}\\atop\\text{Variance}}} = \\frac{0.702} {0.702+1.222} = \\frac{0.702} {1.924} = 0.3648649 \\] 0.702 / (0.702 + 1.222) [1] 0.3648649 2.4.2.2 The sjstats package Calculate the ICC with the icc() fucntion in the sjstats package: sjstats::icc(pop_lmer_0_re) Linear mixed model Family : gaussian (identity) Formula: popular ~ 1 + (1 | class) ICC (class): 0.3649 2.4.2.3 Interpretation WOW! 36.5% of the variance of the popularity scores is at the group level, which is very high for social science data. The ICC should be based on a Null (intercept only) model fit via REML (restricted maximum likelihood) estimation. This is the default for the ‘lme4::lmer()’ function. In chapter 2, Hox, Moerbeek, and Van de Schoot (2017) presents the numbers based on fitting the model via ML (maximum likelihood) estimation and thus does not match the presentation above exactly (not just rounding error). This is because: (1) estimation methods (REML &amp; ML) are not discussed until chapter 3 and (2) due to the Null model also being used for model fit comparisons in Table 2.1 on the top of page 14. Here we will fit the empty model twice, above by ML and below by REML 2.4.3 Add Predictors to the Model Hox, Moerbeek, and Van de Schoot (2017) labeled this as “\\(M_1\\)” in chapter 2 for their Table 2.1 (page 14), but adjusted it for Tables 2.2 (page 15) and 2.3 (page 17). LEVEL 1: Student-specific predictors: \\(X_1 = GEN\\), pupils’s gender (girl vs. boy) \\(X_2 = EXT\\), pupil’s extraversion (scale: 1-10) LEVEL 2: Class-specific Predictors: \\(Z = YRS\\), teacher’s experience (range of 2-25 years) 2.4.3.1 Equations Level 1 Model Equation: Include main effects for sex and extrav \\[ \\overbrace{POP_{ij}}^{Outcome} = \\underbrace{\\beta_{0j}}_{\\text{Level 2}\\atop\\text{intercept}} + \\underbrace{\\beta_{1j}}_{\\text{Level 2}\\atop\\text{slopes}} \\overbrace{GEN_{ij}}^{\\text{Level 1}\\atop\\text{Predictor 1}} + \\underbrace{\\beta_{2j}}_{\\text{Level 2}\\atop\\text{slopes}} \\overbrace{EXT_{ij}}^{\\text{Level 1}\\atop\\text{Predictor 2}} + \\underbrace{e_{ij}}_{\\text{Random}\\atop\\text{residuals}} \\] Level 2 Model Equations: Include a random intercepts and random slopes for both for sex and extrav, but NO cross level interactions for now. We will assume this is due to some theoretical reasoning to be our starting point after the fitting of the null model. Random Intercepts: \\[ \\overbrace{\\beta_{0j}}^{\\text{Level 2}\\atop\\text{intercepts}} = \\underbrace{\\gamma_{00}}_{\\text{Fixed}\\atop\\text{intercept}} + \\underbrace{\\gamma_{01}}_{\\text{Fixed}\\atop\\text{slope } Z} \\overbrace{YRS_{j}}^{\\text{Level 2}\\atop\\text{Predictor 3}} + \\underbrace{u_{0j}}_{\\text{Intercept}\\atop\\text{residual}} \\] Random Slopes, for the first predictor, sex: \\[ \\overbrace{\\beta_{1j}}^{\\text{Level 2}\\atop\\text{slopes}} = \\underbrace{\\gamma_{10}}_{\\text{Fixed}\\atop\\text{Slope } X_1} + \\underbrace{u_{1j}}_{\\text{Slope } X_1\\atop\\text{residual}} \\] Random Slopes, for the second predictor, extrav: \\[ \\overbrace{\\beta_{2j}}^{\\text{Level 2}\\atop\\text{slopes}} = \\underbrace{\\gamma_{20}}_{\\text{Fixed}\\atop\\text{Slope } X_2} + \\underbrace{u_{2j}}_{\\text{Slope } X_2\\atop\\text{residual}} \\] Substitute the level 2 equations into the level 1 equation: Combined, Multilevel Model Equation - Popularity, Include Predictors (no cross-level interactions) \\[ \\overbrace{POP_{ij}}^{Outcome} = \\overbrace{\\gamma_{00} + \\gamma_{10} GEN_{ij} + \\gamma_{20} EXT_{ij} + \\gamma_{01} YRS_{j}}^{\\text{Fixed part}\\atop\\text{Deterministic}} + \\\\ \\underbrace{u_{0j} + u_{1j} GEN_{ij} + u_{2j} EXT_{ij} + e_{ij} }_{\\text{Random part}\\atop\\text{Stochastic}} \\tag{M1} \\] 2.4.3.2 Parameters Type Parameter of Interest Estimates This Fixed Intercept \\(\\gamma_{00}\\) Fixed Main Effect of sex \\(\\gamma_{10}\\) Fixed Main Effect of extrav \\(\\gamma_{20}\\) Fixed Main Effect of texp \\(\\gamma_{01}\\) Random Variance in random intercepts, \\(var[u_{0j}]\\) \\(\\sigma^2_{u0}\\) Random Variance in random slope of sex, \\(var[u_{1j}]\\) \\(\\sigma^2_{u1}\\) Random Variance in random slope of extrav, \\(var[u_{2j}]\\) \\(\\sigma^2_{u2}\\) Random Covariance between random intercepts and random slope of sex, \\(cov[u_{0j}, u_{1j}]\\) \\(\\sigma^2_{u01}\\) Random Covariance between random intercepts and random slope of extrav, \\(cov[u_{0j}, u_{2j}]\\) \\(\\sigma^2_{u02}\\) Random Covariance between random slopes of sex and extrav, \\(cov[u_{1j}, u_{2j}]\\) \\(\\sigma^2_{u12}\\) Random Residual Variance \\(var[e_{ij}]\\) \\(\\sigma^2_{e}\\) Troubleshooting ‘lme4’ Linear Mixed-Effects Models website. This website attempts to summarize some of the common problems with fitting lmer models and how to troubleshoot them. This is a helpful post on Stack Exchange regarding using differen t optimizers to get the ‘lme4::lmer()’ function to converge. Note: Convergence issues MAY signify problems in the model specification. 2.4.3.3 Fit the Model pop_lmer_0_ml &lt;- lme4::lmer(popular ~ 1 + (1|class), data = data_pop, REML = FALSE) # refit via ML to compare the model below to pop_lmer_1_ml &lt;- lme4::lmer(popular ~ sex + extrav + texp + (sex + extrav|class), data = data_pop, REML = FALSE, control = lmerControl(optimizer =&quot;Nelder_Mead&quot;)) #helps converge summary(pop_lmer_1_ml) Linear mixed model fit by maximum likelihood [&#39;lmerMod&#39;] Formula: popular ~ sex + extrav + texp + (sex + extrav | class) Data: data_pop Control: lmerControl(optimizer = &quot;Nelder_Mead&quot;) AIC BIC logLik deviance df.resid 4833.3 4894.9 -2405.6 4811.3 1989 Scaled residuals: Min 1Q Median 3Q Max -3.1686 -0.6550 -0.0227 0.6728 2.9571 Random effects: Groups Name Variance Std.Dev. Corr class (Intercept) 1.319429 1.14866 sexgirl 0.002389 0.04888 -0.40 extrav 0.034115 0.18470 -0.88 -0.09 Residual 0.551144 0.74239 Number of obs: 2000, groups: class, 100 Fixed effects: Estimate Std. Error t value (Intercept) 0.760097 0.195930 3.879 sexgirl 1.251022 0.036921 33.884 extrav 0.452877 0.024511 18.477 texp 0.089417 0.008533 10.480 Correlation of Fixed Effects: (Intr) sexgrl extrav sexgirl -0.063 extrav -0.720 -0.066 texp -0.683 -0.040 0.090 2.4.3.4 Interpretation After accounting for the heiarchical nesting of students in classes, girls were rated 1.25 points more popular on average, than boys with the same extroversion score. One point higher extroversion scores were associated with 0.45 points higher popularity, within each gender. Reproduce Table 2.1 on the top of page 14 (Hox, Moerbeek, and Van de Schoot 2017) # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(pop_lm_0, pop_lmer_0_ml, pop_lmer_1_ml), custom.model.names = c(&quot;Single-level&quot;, &quot;M0: int only&quot;, &quot;M1: w pred&quot;), caption = &quot;Hox Table 2.1 on the top of page 14&quot;, caption.above = TRUE, single.row = TRUE) Hox Table 2.1 on the top of page 14 Single-level M0: int only M1: w pred (Intercept) 5.08 (0.03)*** 5.08 (0.09)*** 0.76 (0.20)*** sexgirl 1.25 (0.04)*** extrav 0.45 (0.02)*** texp 0.09 (0.01)*** R2 0.00 Adj. R2 0.00 Num. obs. 2000 2000 2000 RMSE 1.38 AIC 6333.47 4833.29 BIC 6350.27 4894.90 Log Likelihood -3163.73 -2405.64 Num. groups: class 100 100 Var: class (Intercept) 0.69 1.32 Var: Residual 1.22 0.55 Var: class sexgirl 0.00 Var: class extrav 0.03 Cov: class (Intercept) sexgirl -0.02 Cov: class (Intercept) extrav -0.19 Cov: class sexgirl extrav -0.00 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 The regression tables from the texreg package include estimates of the covariances between random components. “These covarianes are rarely interpreted (for an exception see Chapter 5 and Chapter 16 where growth models are discussed), and for that reason they are often not included in the reported tables. However, as Table 2.2 demonstrates, they can be quite large adn significant, so as a rule they are always included in the model.” Hox, Moerbeek, and Van de Schoot (2017), Chapter 2, pages 15-16 Comparing Model Fit Residual Variance in the Residuals In single-level regression, the Root Mean Squared Error (RMSE) is usually reported. It is the standard deviation of the residuals and is called “Residual standard error” in the R output of summary() function applied to an model fit via lm. In multi-level regression, residual variance is reported as \\(\\sigma_e^2\\). \\[ {\\text{RMSE}}^2 = MSE = \\sigma_e^2 \\] Deviance In single-level regression, the model is fit in such a way as to make the sum of the squared residuals as small as possible. Deviance is the sum of the squared residuals. In multi-level regression, the model is fit via a method called ‘Maximum Likelihood’. \\[ \\text{Deviance} = -2LL = -2 \\times log(likelihood) \\] 2.4.4 Testing Random Components In Hox’s table 2.1 (page 14) we see that the MLM with predictors (\\(M_0\\)) includes a random compondnt with virtually no variance. This is likely why the model didn’t easily converge (a different optimizer was employed). It makes sence to remove the random slope component for gender and refit the model. While we are at it, we will also fit a third model dropping the second random slope component for extraversion. 2.4.4.1 Fit Nested Models Since we are going to compare models that are nested on random effects (identical except for inclusing/exclusing of random components, we will specify the REML = TRUE option. pop_lmer_1_re &lt;- lme4::lmer(popular ~ sex + extrav + texp + (sex + extrav|class), data = data_pop, REML = TRUE, control = lmerControl(optimizer =&quot;Nelder_Mead&quot;)) #helps converge pop_lmer_1a_re &lt;- lme4::lmer(popular ~ sex + extrav + texp + (extrav|class), data = data_pop, REML = TRUE) pop_lmer_1b_re &lt;- lme4::lmer(popular ~ sex + extrav + texp + (1 |class), data = data_pop, REML = TRUE) Create a table to compare the three nested models: The middle column below reproduces Hox’s Table 2.2 found on the bottom of page 15 (Hox, Moerbeek, and Van de Schoot 2017), except the values differ slightly becuase here the model was fit via REML where as in the text, Hox used ML. # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(pop_lmer_1_re, pop_lmer_1a_re, pop_lmer_1b_re), custom.model.names = c(&quot;M1&quot;, &quot;M1a&quot;, &quot;M1b&quot;), caption = &quot;Assessing Significance of Random Slopes&quot;, caption.above = TRUE, single.row = TRUE) Assessing Significance of Random Slopes M1 M1a M1b (Intercept) 0.76 (0.20)*** 0.74 (0.20)*** 0.81 (0.17)*** sexgirl 1.25 (0.04)*** 1.25 (0.04)*** 1.25 (0.04)*** extrav 0.45 (0.02)*** 0.45 (0.02)*** 0.45 (0.02)*** texp 0.09 (0.01)*** 0.09 (0.01)*** 0.09 (0.01)*** AIC 4855.26 4850.77 4897.02 BIC 4916.87 4895.58 4930.63 Log Likelihood -2416.63 -2417.38 -2442.51 Num. obs. 2000 2000 2000 Num. groups: class 100 100 100 Var: class (Intercept) 1.34 1.30 0.30 Var: class sexgirl 0.00 Var: class extrav 0.03 0.03 Cov: class (Intercept) sexgirl -0.02 Cov: class (Intercept) extrav -0.19 -0.19 Cov: class sexgirl extrav -0.00 Var: Residual 0.55 0.55 0.59 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 2.4.4.2 Compare Fit Likelihood Ratio Test (LRT) of Nested MLM Models When comparing the fit of two multi-level models fit via the lme::lmer() function, the anova() function runs an Chi-squared test where the test statistic is the difference in -2LL (deviances). Likelihood Ratio Test (LRT) for Random Effects When using the ‘anova()’ function to conduct a LRT for RANDOM effects, make sure: the nested models have identical FIXED effects never test models that differ in fixed and random effects at the same time the models were fit with ‘REML = TRUE’ this results in the best variance/covariance component estimation add the ‘refit = FALSE’ option to the ‘anova()’ call without this \\(R\\) re-runs the models with ‘REML = FALSE’ for you Investigate dropping the random slope component for sex These two models are identical, except for the inclusing/exclusion of the random specification of the level 1 sex predictor. Note, both models were fit with REML. Although we are dropping only ONE variance component, we are also dropping TWO covariances (sex paired with both the random intercept and random slope for extrav). This results in a \\(\\chi^2\\) test with THREE degrees of freedom. anova(pop_lmer_1_re, pop_lmer_1a_re, refit = FALSE) # don&#39;t let it refit the models via LM Data: data_pop Models: pop_lmer_1a_re: popular ~ sex + extrav + texp + (extrav | class) pop_lmer_1_re: popular ~ sex + extrav + texp + (sex + extrav | class) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) pop_lmer_1a_re 8 4850.8 4895.6 -2417.4 4834.8 pop_lmer_1_re 11 4855.3 4916.9 -2416.6 4833.3 1.5133 3 0.6792 The NON-significance likelihood ratio test (LRT: \\(\\chi^2(3) = 1.51\\), \\(p = .679\\)) conveys that the more complex model does NOT fit the data better. Thus the more SIMPLE model does just as good of a job. This is evidence for the EXCLUSION of sex as a random component. Investigate dropping the random slope component for extrav These two models are identical, except for the inclusing/exclusion of the random specification of the level 1 extrav predictor. Note, both models were fit with REML. Although we are dropping only ONE variance component, we are also dropping ONE covariances (extrav paired with the random intercept). This results in a \\(\\chi^2\\) test with TWO degrees of freedom. anova(pop_lmer_1a_re, pop_lmer_1b_re, refit = FALSE) # don&#39;t let it refit the models via LM Data: data_pop Models: pop_lmer_1b_re: popular ~ sex + extrav + texp + (1 | class) pop_lmer_1a_re: popular ~ sex + extrav + texp + (extrav | class) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) pop_lmer_1b_re 6 4897.0 4930.6 -2442.5 4885.0 pop_lmer_1a_re 8 4850.8 4895.6 -2417.4 4834.8 50.256 2 1.222e-11 pop_lmer_1b_re pop_lmer_1a_re *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The significance likelihood ratio test (LRT: \\(\\chi^2(2) = 50.26\\), \\(p &lt; .0001\\)) conveys that the more complex model DOES fit the data better. Thus the more COMPLEX model does just as good of a job. This is evidence for the INCLUSION of extrav as a random component. 2.4.5 Testing Cross-Level Interactions We have already seen formulas of this form for a NULL or emply models, as well as for intercept implied models of main effects: intercept only Y ~ 1 intercept implied Y ~ A = Y ~ 1 + A Y ~ A + B = Y ~ 1 + A + B Including Interactions in Formulas If we wish to include an interaction between the two predictors, we signify this with a colon (:) between the two predictor names. A shortcut may also be employed to signify the including of the main effects and the interaction at the same time by placing an astric (*) between the two variable names. Both of the following specify the outcome is being predicted by an intercept (implied), the main effects for 2 predictors, and the interaction between the two predictors Y ~ A + B + A:B Y ~ A*B Examples 2-way: A*B = A + B + A:B 3-way: A*B*C = A + B + C + A:B + A:C + B:C + A:B:C 4-way: A*B*C*D = A + B + C + D + A:B + A:C + A:D + B:C + B:D + A:B:C + A:B:D+ A:C:D + B:C:D + A:B:C:D 2.4.5.1 Fit Nested Models “Given the significant variance of the regression coefficient of pupil extraversion across the classes, it is attractive to attempt to predict its variation using class-level variables. We have one class-level variable: teacher experience.” Hox, Moerbeek, and Van de Schoot (2017), Chapter 2, page 16 Now that we wish to compare nested that will differ only in terms of the inclusing/exclusion of a FIXED effect, the estimation method should be standard maximum likelihood (REML = FALSE). pop_lmer_1a_ml &lt;- lme4::lmer(popular ~ sex + extrav + texp + (extrav|class), # main effects only data = data_pop, REML = FALSE) pop_lmer_2_ml &lt;- lme4::lmer(popular ~ sex + extrav*texp + (extrav|class), # include cross-level interaction data = data_pop, REML = FALSE) pop_lmer_3_ml &lt;- lme4::lmer(popular ~ extrav*texp + sex*texp + sex*extrav + (extrav|class), data = data_pop, REML = FALSE) pop_lmer_4_ml &lt;- lme4::lmer(popular ~ extrav*texp*sex + (extrav|class), data = data_pop, REML = FALSE) Create a table to compare the two nested models: # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(pop_lmer_1a_ml, pop_lmer_2_ml), custom.model.names = c(&quot;M1a: Main Effects&quot;, &quot;M2: With Interaction&quot;), caption = &quot;Hox Table 2.3 on page 17&quot;, caption.above = TRUE, single.row = TRUE) Hox Table 2.3 on page 17 M1a: Main Effects M2: With Interaction (Intercept) 0.74 (0.20)*** -1.21 (0.27)*** sexgirl 1.25 (0.04)*** 1.24 (0.04)*** extrav 0.45 (0.02)*** 0.80 (0.04)*** texp 0.09 (0.01)*** 0.23 (0.02)*** extrav:texp -0.02 (0.00)*** AIC 4828.81 4765.62 BIC 4873.61 4816.03 Log Likelihood -2406.40 -2373.81 Num. obs. 2000 2000 Num. groups: class 100 100 Var: class (Intercept) 1.28 0.45 Var: class extrav 0.03 0.00 Cov: class (Intercept) extrav -0.18 -0.03 Var: Residual 0.55 0.55 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 Investigate further interactions, not shown in by Hox, Moerbeek, and Van de Schoot (2017). # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(pop_lmer_1a_ml, pop_lmer_2_ml, pop_lmer_3_ml, pop_lmer_4_ml), custom.model.names = c(&quot;M1a: Main Effects&quot;, &quot;M2: With Interaction&quot;, &quot;Add 2-way Inter&quot;, &quot;Add 3-way Interaction&quot;), caption = &quot;Hox Table 2.3 on page 17&quot;, caption.above = TRUE, single.row = TRUE) Hox Table 2.3 on page 17 M1a: Main Effects M2: With Interaction Add 2-way Inter Add 3-way Interaction (Intercept) 0.74 (0.20)*** -1.21 (0.27)*** -1.09 (0.28)*** -0.94 (0.33)** sexgirl 1.25 (0.04)*** 1.24 (0.04)*** 0.96 (0.21)*** 0.66 (0.38) extrav 0.45 (0.02)*** 0.80 (0.04)*** 0.78 (0.04)*** 0.75 (0.05)*** texp 0.09 (0.01)*** 0.23 (0.02)*** 0.23 (0.02)*** 0.22 (0.02)*** extrav:texp -0.02 (0.00)*** -0.02 (0.00)*** -0.02 (0.00)*** texp:sexgirl 0.00 (0.01) 0.02 (0.02) extrav:sexgirl 0.05 (0.03) 0.10 (0.06) extrav:texp:sexgirl -0.00 (0.00) AIC 4828.81 4765.62 4767.17 4768.26 BIC 4873.61 4816.03 4828.78 4835.47 Log Likelihood -2406.40 -2373.81 -2372.58 -2372.13 Num. obs. 2000 2000 2000 2000 Num. groups: class 100 100 100 100 Var: class (Intercept) 1.28 0.45 0.49 0.49 Var: class extrav 0.03 0.00 0.01 0.01 Cov: class (Intercept) extrav -0.18 -0.03 -0.03 -0.03 Var: Residual 0.55 0.55 0.55 0.55 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 2.4.5.2 Compare Fit Since these two models only differe by the inclusing/exclusing of a FIXED effect, they both employed ML estimation. Thus we do not need worry about the anova() function refitting the models prior to conduction the LRT. anova(pop_lmer_1a_ml, pop_lmer_2_ml) Data: data_pop Models: pop_lmer_1a_ml: popular ~ sex + extrav + texp + (extrav | class) pop_lmer_2_ml: popular ~ sex + extrav * texp + (extrav | class) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) pop_lmer_1a_ml 8 4828.8 4873.6 -2406.4 4812.8 pop_lmer_2_ml 9 4765.6 4816.0 -2373.8 4747.6 65.183 1 6.827e-16 pop_lmer_1a_ml pop_lmer_2_ml *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The significance likelihood ratio test (LRT: \\(\\chi^2(1) = 65.18\\), \\(p &lt; .0001\\)) conveys that the more complex model DOES fit the data better. Thus the more COMPLEX model does just as good of a job. This is evidence for the INCLUSION of cross-level interaction between extrav and texp as a fixed component. anova(pop_lmer_2_ml, pop_lmer_3_ml) Data: data_pop Models: pop_lmer_2_ml: popular ~ sex + extrav * texp + (extrav | class) pop_lmer_3_ml: popular ~ extrav * texp + sex * texp + sex * extrav + (extrav | pop_lmer_3_ml: class) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) pop_lmer_2_ml 9 4765.6 4816.0 -2373.8 4747.6 pop_lmer_3_ml 11 4767.2 4828.8 -2372.6 4745.2 2.4552 2 0.293 The significance likelihood ratio test (LRT: \\(\\chi^2(2) = 2.46\\), \\(p=.293\\)) conveys that the more complex model does NOT fit the data better. Thus the more SIMPLE model does just as good of a job. This is evidence for the EXCLUSION of the additional 2-way interactions as a fixed components. anova(pop_lmer_2_ml, pop_lmer_4_ml) Data: data_pop Models: pop_lmer_2_ml: popular ~ sex + extrav * texp + (extrav | class) pop_lmer_4_ml: popular ~ extrav * texp * sex + (extrav | class) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) pop_lmer_2_ml 9 4765.6 4816.0 -2373.8 4747.6 pop_lmer_4_ml 12 4768.3 4835.5 -2372.1 4744.3 3.3636 3 0.3389 The significance likelihood ratio test (LRT: \\(\\chi^2(3) = 3.36\\), \\(p=.339\\)) conveys that the more complex model does NOT fit the data better. Thus the more SIMPLE model does just as good of a job. This is evidence for the EXCLUSION of the additional 3-way interactions as a fixed components. 2.4.6 Final Model 2.4.6.1 Refit with REML pop_lmer_2_re &lt;- lme4::lmer(popular ~ sex + extrav*texp + (extrav|class), data = data_pop, REML = TRUE) # re-fit the final model via REML 2.4.6.2 Parameters # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(pop_lmer_2_ml), custom.model.names = c(&quot;Final Model&quot;), caption = &quot;MLM for Popularity&quot;, caption.above = TRUE, single.row = TRUE) MLM for Popularity Final Model (Intercept) -1.21 (0.27)*** sexgirl 1.24 (0.04)*** extrav 0.80 (0.04)*** texp 0.23 (0.02)*** extrav:texp -0.02 (0.00)*** AIC 4765.62 BIC 4816.03 Log Likelihood -2373.81 Num. obs. 2000 Num. groups: class 100 Var: class (Intercept) 0.45 Var: class extrav 0.00 Cov: class (Intercept) extrav -0.03 Var: Residual 0.55 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 2.4.6.3 Visualization Get Estimated Marginal Means - default ‘nice’ predictor values: Focal predictors: All combinations of… * sex categorical, both levels * extrav continuous 1-10, default: 1, 3, 6, 8, 10 * texp continuous, default: 2.0, 7.8, 14.0, 19.0, 25.0 Always followed by: * fit estimated marginal mean * se standard error for the marginal mean * lower lower end of the 95% confidence interval around the estimated marginal mean * upper upper end of the 95% confidence interval around the estimated marginal mean effects::Effect(focal.predictors = c(&quot;sex&quot;, &quot;extrav&quot;, &quot;texp&quot;), mod = pop_lmer_2_re) %&gt;% data.frame() %&gt;% head(n = 12) sex extrav texp fit se lower upper 1 boy 1 2.0 -0.003093328 0.2113423 -0.4175680 0.4113813 2 girl 1 2.0 1.237604358 0.2138286 0.8182537 1.6569551 3 boy 3 2.0 1.505151652 0.1580324 1.1952259 1.8150774 4 girl 3 2.0 2.745849338 0.1602424 2.4315894 3.0601092 5 boy 6 2.0 3.767519123 0.1201265 3.5319325 4.0031058 6 girl 6 2.0 5.008216809 0.1208436 4.7712238 5.2452098 7 boy 8 2.0 5.275764103 0.1416448 4.9979769 5.5535513 8 girl 8 2.0 6.516461789 0.1410047 6.2399299 6.7929937 9 boy 10 2.0 6.784009084 0.1892770 6.4128078 7.1552104 10 girl 10 2.0 8.024706769 0.1878594 7.6562857 8.3931279 11 boy 1 7.8 1.165428795 0.1405330 0.8898220 1.4410356 12 girl 1 7.8 2.406126481 0.1432180 2.1252539 2.6869991 Pick ‘nicer’ illustrative values for texp effects::Effect(focal.predictors = c(&quot;sex&quot;, &quot;extrav&quot;, &quot;texp&quot;), mod = pop_lmer_2_re, xlevels = list(texp = c(5, 15, 25))) %&gt;% data.frame() %&gt;% head(n = 12) sex extrav texp fit se lower upper 1 boy 1 5 0.6013147 0.17311707 0.2618055 0.9408239 2 girl 1 5 1.8420124 0.17570978 1.4974184 2.1866063 3 boy 3 5 1.9611908 0.12950817 1.7072054 2.2151762 4 girl 3 5 3.2018885 0.13175720 2.9434923 3.4602846 5 boy 6 5 4.0010050 0.09857046 3.8076931 4.1943168 6 girl 6 5 5.2417027 0.09913884 5.0472761 5.4361292 7 boy 8 5 5.3608811 0.11621095 5.1329735 5.5887886 8 girl 8 5 6.6015788 0.11532799 6.3754029 6.8277547 9 boy 10 5 6.7207572 0.15520385 6.4163786 7.0251358 10 girl 10 5 7.9614549 0.15351541 7.6603876 8.2625222 11 boy 1 15 2.6160080 0.09471181 2.4302636 2.8017524 12 girl 1 15 3.8567057 0.09677671 3.6669116 4.0464997 Basic, default plot Other than selecting three illustrative values for the teacher extraversion rating, most everything is left to default. effects::Effect(focal.predictors = c(&quot;sex&quot;, &quot;extrav&quot;, &quot;texp&quot;), mod = pop_lmer_2_re, xlevels = list(texp = c(5, 15, 25))) %&gt;% data.frame() %&gt;% dplyr::mutate(texp = factor(texp)) %&gt;% ggplot() + aes(x = extrav, y = fit, fill = texp, linetype = texp) + geom_ribbon(aes(ymin = lower, ymax = upper), alpha = .3) + geom_line(aes(color = texp)) + facet_grid(.~ sex) More Clean Plot There are many ways to clean up a plot, including labeling the axes. effects::Effect(focal.predictors = c(&quot;sex&quot;, &quot;extrav&quot;, &quot;texp&quot;), mod = pop_lmer_2_re, xlevels = list(texp = c(5, 15, 25))) %&gt;% data.frame() %&gt;% dplyr::mutate(texp = factor(texp)) %&gt;% dplyr::mutate(sex = sex %&gt;% forcats::fct_recode(&quot;Amoung Boys&quot; = &quot;boy&quot;, &quot;Among Girls&quot; = &quot;girl&quot;)) %&gt;% ggplot() + aes(x = extrav, y = fit, fill = texp, linetype = texp) + geom_ribbon(aes(ymin = lower, ymax = upper), alpha = .3) + geom_line(aes(color = texp)) + theme_bw() + facet_grid(.~ sex) + labs(x = &quot;Pupil&#39;s Extroversion, Rated by Teacher&quot;, y = &quot;Estimated Marginal Mean\\nPupil Popularity, Mean Rating of Classroom Peers&quot;, color = &quot;Teacher&#39;s Experience, Years&quot;, linetype = &quot;Teacher&#39;s Experience, Years&quot;, fill = &quot;Teacher&#39;s Experience, Years&quot;) + theme(legend.position = &quot;bottom&quot;) + scale_x_continuous(breaks = seq(from = 0, to = 10, by = 2)) Publishable Plot Since gender only exhibited a main effect and is not involved in any interactions, if would be a better use of space to not muddy the water with seperate panels. The Effect() function will estimate the marginal means using the reference category for categorical variables and the mean for continuous variables. effects::Effect(focal.predictors = c(&quot;extrav&quot;, &quot;texp&quot;), # choose not to investigate sex (the reference category will be used) mod = pop_lmer_2_re, xlevels = list(texp = c(5, 15, 25))) %&gt;% data.frame() %&gt;% dplyr::mutate(texp = factor(texp) %&gt;% forcats::fct_rev()) %&gt;% ggplot() + aes(x = extrav, y = fit, linetype = texp) + geom_ribbon(aes(ymin = lower, ymax = upper), fill = &quot;black&quot;, alpha = .3) + geom_line() + theme_bw() + labs(x = &quot;Pupil&#39;s Extroversion, Rated by Teacher&quot;, y = &quot;Estimated Marginal Mean\\nPupil Popularity, Mean Rating of Classroom Peers&quot;, color = &quot;Teacher&#39;s\\nExperience,\\nYears&quot;, linetype = &quot;Teacher&#39;s\\nExperience,\\nYears&quot;, alpha = &quot;Teacher&#39;s\\nExperience,\\nYears&quot;) + theme(legend.key.width = unit(2, &quot;cm&quot;), legend.background = element_rect(color = &quot;Black&quot;), legend.position = c(1, 0), legend.justification = c(1, 0)) + scale_linetype_manual(values = c(&quot;solid&quot;, &quot;dashed&quot;, &quot;dotted&quot;)) + scale_x_continuous(breaks = seq(from = 0, to = 10, by = 2)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 1)) 2.4.6.4 Interpretation After accounting for class-to-class variation and the effect of gender, a positive association was found between teacher rated extroversion and peer rated popularity. This relationship was more marked for less experienced teachers. "],
["intro-3-level-model-example-nurses-stress-intervention.html", "3 Intro 3-Level Model Example: Nurse’s Stress Intervention 3.1 Background 3.2 Exploratory Data Analysis 3.3 MLM: Null Model 3.4 Estimate the ICC 3.5 MLM: Add Fixed Effects 3.6 MLM: Add Random Slope 3.7 MLM: Add Cross-Level Interaction 3.8 Final Model 3.9 Interpretation 3.10 Reproduction of Table 2.5", " 3 Intro 3-Level Model Example: Nurse’s Stress Intervention library(tidyverse) library(haven) # read in SPSS dataset library(furniture) # nice table1() descriptives library(stargazer) # display nice tables: summary &amp; regression library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(psych) # contains some useful functions, like headTail library(car) # Companion to Applied Regression library(lme4) # Linear, generalized linear, &amp; nonlinear mixed models library(lmerTest) # Tests on lmer objects library(sjstats) # ICC calculations library(effects) # Effects for regression models library(optimx) # Different optimizers to solve mlm&#39;s 3.1 Background The text “Multilevel Analysis: Techniques and Applications, Third Edition” (Hox, Moerbeek, and Van de Schoot 2017) has a companion website which includes links to all the data files used throughout the book (housed on the book’s GitHub repository). The following example is used through out Hox, Moerbeek, and Van de Schoot (2017)’s chapater 2. From Appendix E: The nurses.sav file contains three-level simulated data from a hypothetical study on stress in hospitals. The data are from nurses working in wards nested within hospitals. It is a cluster-randomized experiment. In each of 25 hospitals, four wards are selected and randomly assigned to an experimental and a control condition. In the experimental condition, a training program is offered to all nurses to cope with job-related stress. After the program is completed, a sample of about 10 nurses from each ward is given a test that measures job-related stress. Additional variables (covariates) are: nurse age (years), nurse experience (years), nurse gender (0=male, 1 = female), type of ward (0=general care, 1=special care), and hospital size (0=small, 1 = medium, 2=large). The data have been generated to illustrate three-level analysis with a random slope for the effect of the intervention. Here the data is read in and the SPSS variables with labels are converted to \\(R\\) factors. data_raw &lt;- haven::read_sav(&quot;https://github.com/MultiLevelAnalysis/Datasets-third-edition-Multilevel-book/raw/master/chapter%202/Nurses/SPSS/Nurses.sav&quot;) %&gt;% haven::as_factor() # retain the labels from SPSS --&gt; factor 3.1.1 Unique Identifiers All standardized (starts with “Z”) and mean centered (starts with “C”) variables will be remove so that their creation may be shown later. A new indicator varible for nurses with be created by combining the hospital, ward, and nurse indicators. Having a unique, distinct identifier variable for each of the units on lower (Level 1 and 2) levels is helpful for multilevel anlayses. data_nurse &lt;- data_raw %&gt;% dplyr::mutate(genderF = factor(gender, labels = c(&quot;Male&quot;, &quot;Female&quot;))) %&gt;% # apply factor labels dplyr::mutate(id = paste(hospital, ward, nurse, sep = &quot;_&quot;) %&gt;% # cunique id for each student factor()) %&gt;% # declare id is a factor dplyr::mutate_at(vars(hospital, ward, wardid, nurse), factor) %&gt;% # declare to be factors dplyr::mutate(age = age %&gt;% as.character %&gt;% as.numeric) %&gt;% # declare to be numeric dplyr::select(id, wardid, nurse, ward, hospital, age, gender, genderF, experien, wardtype, hospsize, expcon, stress) # reduce variables included tibble::glimpse(data_nurse) Observations: 1,000 Variables: 13 $ id &lt;fct&gt; 1_1_1, 1_1_2, 1_1_3, 1_1_4, 1_1_5, 1_1_6, 1_1_7, 1_1_... $ wardid &lt;fct&gt; 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 1... $ nurse &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16... $ ward &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2,... $ hospital &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... $ age &lt;dbl&gt; 36, 45, 32, 57, 46, 60, 23, 32, 60, 45, 57, 47, 32, 4... $ gender &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,... $ genderF &lt;fct&gt; Male, Male, Male, Female, Female, Female, Female, Fem... $ experien &lt;dbl&gt; 11, 20, 7, 25, 22, 22, 13, 13, 17, 21, 24, 24, 14, 13... $ wardtype &lt;fct&gt; general care, general care, general care, general car... $ hospsize &lt;fct&gt; large, large, large, large, large, large, large, larg... $ expcon &lt;fct&gt; experiment, experiment, experiment, experiment, exper... $ stress &lt;dbl&gt; 7, 7, 7, 6, 6, 6, 6, 7, 7, 6, 6, 6, 6, 6, 6, 5, 5, 6,... 3.1.2 Centering Variables When variables are involved in an interaction, it may be advantageous to center the variables. Hox, Moerbeek, and Van de Schoot (2017) covers this in chapter 4. To center categorical variables: 1. Convert then to integers, starting with zero: \\(0, 1, \\dots\\) 2. Subtract the mean data_nurse %&gt;% dplyr::mutate(expconN = as.numeric(expcon) - 1) %&gt;% # Numeric Version of experimental condition dplyr::mutate(hospsizeN = as.numeric(hospsize) - 1) %&gt;% # Numeric Version of hospital size dplyr::select(expcon, expconN, hospsize, hospsizeN) %&gt;% summary() expcon expconN hospsize hospsizeN control :496 Min. :0.000 small :374 Min. :0.000 experiment:504 1st Qu.:0.000 medium:476 1st Qu.:0.000 Median :1.000 large :150 Median :1.000 Mean :0.504 Mean :0.776 3rd Qu.:1.000 3rd Qu.:1.000 Max. :1.000 Max. :2.000 data_nurse &lt;- data_nurse %&gt;% dplyr::mutate(expconN = as.numeric(expcon) - 1) %&gt;% # Numeric Version of experimental condition dplyr::mutate(hospsizeN = as.numeric(hospsize) - 1) %&gt;% # Numeric Version of hospital size dplyr::mutate(expconNG = expconN - 0.504) %&gt;% # Grand-Mean Centered version of experimental condition dplyr::mutate(hospsizeNG = hospsizeN - 0.776) # Grand-Mean Centered version of ehospital size data_nurse %&gt;% dplyr::select(expcon, expconNG) %&gt;% table() expconNG expcon -0.504 0.496 control 496 0 experiment 0 504 data_nurse %&gt;% dplyr::select(hospsize, hospsizeNG) %&gt;% table() hospsizeNG hospsize -0.776 0.224 1.224 small 374 0 0 medium 0 476 0 large 0 0 150 3.2 Exploratory Data Analysis 3.2.1 Summarize Descriptive Statistics 3.2.1.1 The stargazer package Most posters, journal articles, and reports start with a table of descriptive statistics. Since it tends to come first, this type of table is often refered to as Table 1. The stargazer() function can be used to create such a table, but only for the entire dataset (Hlavac 2018). I haven’t been able to find a way to get it to summarize subsamples and compare them in the standard format. # Knit to Website: type = &quot;html&quot; # Knit to PDF: type = &quot;latex&quot; # View on Screen: type = &quot;text&quot; data_nurse %&gt;% data.frame() %&gt;% stargazer::stargazer(title = &quot;Descriptive statistics, aggregate over entire sample&quot;, header = FALSE, type = &quot;html&quot;) Descriptive statistics, aggregate over entire sample Statistic N Mean St. Dev. Min Pctl(25) Pctl(75) Max age 1,000 43.005 12.042 23 33 53 64 gender 1,000 0.735 0.442 0 0 1 1 experien 1,000 17.057 6.042 1 13 21 38 stress 1,000 4.977 0.980 1 4 6 7 expconN 1,000 0.504 0.500 0 0 1 1 hospsizeN 1,000 0.776 0.689 0 0 1 2 expconNG 1,000 -0.000 0.500 -0.504 -0.504 0.496 0.496 hospsizeNG 1,000 -0.000 0.689 -0.776 -0.776 0.224 1.224 3.2.1.2 The furniture package Tyson Barrett’s furniture package includes the extremely useful function table1() which simplifies the common task of creating a stratified, comparative table of descriptive statistics. Full documentation can be accessed by executing ?furniture::table1. # Knit to Website: output = &quot;html&quot; # Knit to PDF: output = &quot;latex2&quot; # View on Screen: output = &quot;&quot;text&quot;, or &quot;markdown&quot;, &quot;html&quot; data_nurse %&gt;% furniture::table1(age, genderF, experien, wardtype, hospsize, hospsizeN, hospsizeNG, splitby = ~ expcon, # var to divide sample by test = TRUE, # test groups different? type = &quot;full&quot;, # give the test statistic output = &quot;html&quot;, # output for html caption = &quot;Compare Intervention groups on five main variables&quot;) # title Table 3.1: Compare Intervention groups on five main variables control experiment Test P-Value n = 496 n = 504 age T-Test: 0.82 0.411 43.3 (11.6) 42.7 (12.5) genderF Chi Square: 0.19 0.661 Male 135 (27.2%) 130 (25.8%) Female 361 (72.8%) 374 (74.2%) experien T-Test: 0.69 0.491 17.2 (5.8) 16.9 (6.3) wardtype Chi Square: 0 1 general care 247 (49.8%) 252 (50%) special care 249 (50.2%) 252 (50%) hospsize Chi Square: 0.01 0.993 small 185 (37.3%) 189 (37.5%) medium 237 (47.8%) 239 (47.4%) large 74 (14.9%) 76 (15.1%) hospsizeN T-Test: 0.01 0.992 0.8 (0.7) 0.8 (0.7) hospsizeNG T-Test: 0.01 0.992 0.0 (0.7) -0.0 (0.7) The t-test performed by the furniture::table1() function will always assume indepent groups and that HOV is not violated. This may or may not be appropriate. 3.3 MLM: Null Model In a Null, intercept-only, or Empty model, no predictors are included. 3.3.0.1 Fit the Model Fit the model to the data, with both ML and REML. nurse_lmer_0_re &lt;- lme4::lmer(stress ~ 1 + # Fixed Intercept for all nurses (1|hospital/ward), # Random Intercepts for wards within hospitals data = data_nurse, REML = TRUE) # fit via REML (the default) for ICC calculations nurse_lmer_0_ml &lt;- lme4::lmer(stress ~ 1 + # Fixed Intercept for all nurses (1|hospital/ward), # Random Intercepts for wards within hospitals data = data_nurse, REML = FALSE) # fit via ML for comparing FIXED effects inclusion # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(nurse_lmer_0_ml, nurse_lmer_0_re), custom.model.names = c(&quot;M0: Null, ML&quot;, &quot;M0: Null, REML&quot;), caption = &quot;NULL Model: different estimation methods&quot;, caption.above = TRUE, single.row = TRUE) NULL Model: different estimation methods M0: Null, ML M0: Null, REML (Intercept) 5.00 (0.11)*** 5.00 (0.11)*** AIC 1950.36 1952.95 BIC 1969.99 1972.58 Log Likelihood -971.18 -972.48 Num. obs. 1000 1000 Num. groups: ward:hospital 100 100 Num. groups: hospital 25 25 Var: ward:hospital (Intercept) 0.49 0.49 Var: hospital (Intercept) 0.16 0.17 Var: Residual 0.30 0.30 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 3.4 Estimate the ICC The ICC is calculated by dividing the between-group-variance (random intercept variance) by the total variance (i.e. sum of between-group-variance and within-group (residual) variance). lme4::VarCorr(nurse_lmer_0_re) Groups Name Std.Dev. ward:hospital (Intercept) 0.69916 hospital (Intercept) 0.41749 Residual 0.54887 lme4::VarCorr(nurse_lmer_0_re) %&gt;% print(comp = c(&quot;Variance&quot;, &quot;Std.Dev&quot;), digits = 3) Groups Name Variance Std.Dev. ward:hospital (Intercept) 0.489 0.699 hospital (Intercept) 0.174 0.417 Residual 0.301 0.549 vc &lt;- lme4::VarCorr(nurse_lmer_0_re) %&gt;% data.frame() pie(x = vc$vcov, labels = vc$grp) The sjstats package has a few really helpful funcitons: sjstats::re_var(nurse_lmer_0_re) Within-group-variance: 0.301 Between-group-variance: 0.489 (ward:hospital) Between-group-variance: 0.174 (hospital) \\[ \\begin{align*} \\text{hospitals} \\rightarrow \\; &amp; \\sigma^2_{v0} = 0.417^2 = 0.174\\\\ \\text{wards within hospitals} \\rightarrow \\; &amp; \\sigma^2_{u0} = 0.699^2 = 0.489\\\\ \\text{nurses within wards within hospitals} \\rightarrow \\; &amp; \\sigma^2_{e} = 0.549^2 = 0.301\\\\ \\end{align*} \\] Intraclass Correlation (ICC) Formula, 3 level model - Davis and Scott Method \\[ \\overbrace{\\rho_{mid}}^{\\text{ICC}\\atop\\text{at level 2}} = \\frac{\\overbrace{\\sigma^2_{u0}}^{\\text{Random Intercept}\\atop\\text{Variance Level 2}}} {\\underbrace{\\sigma^2_{v0}+\\sigma^2_{u0}+\\sigma^2_{e}}_{\\text{Total}\\atop\\text{Variance}}} \\tag{Hox 2.16} \\] \\[ \\overbrace{\\rho_{top}}^{\\text{ICC}\\atop\\text{ at level 3}} = \\frac{\\overbrace{\\sigma^2_{u0}}^{\\text{Random Intercept}\\atop\\text{Variance Level 3}}} {\\underbrace{\\sigma^2_{v0}+\\sigma^2_{u0}+\\sigma^2_{e}}_{\\text{Total}\\atop\\text{Variance}}} \\tag{Hox 2.17} \\] 0.489 / (0.174 + 0.489 + 0.301) # middle level (wards) [1] 0.5072614 0.174 / (0.174 + 0.489 + 0.301) # top level (hospitals) [1] 0.1804979 For more than two levels, the ‘sjstats::icc()’ function computes ICC’s by the Davis and Scott method. sjstats::icc(nurse_lmer_0_re) Linear mixed model Family : gaussian (identity) Formula: stress ~ 1 + (1 | hospital/ward) ICC (ward:hospital): 0.5069 ICC (hospital): 0.1807 The proportion of variange in nurse stress level is 0.51 at the ward level and 0.18 at the hospital level. To test if the three level model is justified statistically, compare the null models with and without the nesting of wards in hospitals. nurse_lmer_0_re_2level &lt;- lme4::lmer(stress ~ 1 + (1|wardid), # each hospital contains several wards data = data_nurse, REML = TRUE) # fit via REML (the default) for ICC calculations # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(nurse_lmer_0_re_2level, nurse_lmer_0_re), custom.model.names = c(&quot;2 levels&quot;, &quot;3 levels&quot;), caption = &quot;MLM: Two or Three Levels?&quot;, caption.above = TRUE, single.row = TRUE) MLM: Two or Three Levels? 2 levels 3 levels (Intercept) 5.00 (0.08)*** 5.00 (0.11)*** AIC 1958.43 1952.95 BIC 1973.15 1972.58 Log Likelihood -976.21 -972.48 Num. obs. 1000 1000 Num. groups: wardid 100 Var: wardid (Intercept) 0.66 Var: Residual 0.30 0.30 Num. groups: ward:hospital 100 Num. groups: hospital 25 Var: ward:hospital (Intercept) 0.49 Var: hospital (Intercept) 0.17 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 The deviance test or likelihood-ratio test shows that the inclusion of the nesting of wards within hospitals better explains the variance in nurse stress levels. anova(nurse_lmer_0_re, nurse_lmer_0_re_2level, refit = FALSE) Data: data_nurse Models: nurse_lmer_0_re_2level: stress ~ 1 + (1 | wardid) nurse_lmer_0_re: stress ~ 1 + (1 | hospital/ward) Df AIC BIC logLik deviance Chisq Chi Df nurse_lmer_0_re_2level 3 1958.4 1973.2 -976.21 1952.4 nurse_lmer_0_re 4 1953.0 1972.6 -972.48 1945.0 7.4738 1 Pr(&gt;Chisq) nurse_lmer_0_re_2level nurse_lmer_0_re 0.00626 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 3.5 MLM: Add Fixed Effects 3.5.1 Fit the Model Hox, Moerbeek, and Van de Schoot (2017), page 22: “In this example, the variable expcon is of main interest, and the other variables are covariates. Their funciton is to control for differences between the groups, which can occur even if randomization is used, especially with small samples, and to explain variance in the outcome variable stress. To the extent that these variables successfully explain the variance, the power of the test for the effect of expcon will be increased.” nurse_lmer_1_ml &lt;- lme4::lmer(stress ~ expcon + # experimental condition = CATEGORICAL FACTOR age + gender + experien + # level 1 covariates wardtype + # level 2 covariates hospsize + # level 3 covariates, hospital size = CATEGORICAL FACTOR (1|hospital/ward), # Random Intercepts for wards within hospitals data = data_nurse, REML = FALSE) # fit via ML for nested FIXED effects # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(nurse_lmer_0_ml, nurse_lmer_1_ml), custom.model.names = c(&quot;M0: null&quot;, &quot;M1: fixed pred&quot;), caption = &quot;Nested Models: Fixed effects via ML&quot;, caption.above = TRUE, single.row = TRUE) Nested Models: Fixed effects via ML M0: null M1: fixed pred (Intercept) 5.00 (0.11)*** 5.38 (0.18)*** expconexperiment -0.70 (0.12)*** age 0.02 (0.00)*** gender -0.45 (0.03)*** experien -0.06 (0.00)*** wardtypespecial care 0.05 (0.12) hospsizemedium 0.49 (0.19)** hospsizelarge 0.90 (0.26)*** AIC 1950.36 1626.32 BIC 1969.99 1680.30 Log Likelihood -971.18 -802.16 Num. obs. 1000 1000 Num. groups: ward:hospital 100 100 Num. groups: hospital 25 25 Var: ward:hospital (Intercept) 0.49 0.33 Var: hospital (Intercept) 0.16 0.10 Var: Residual 0.30 0.22 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 3.5.2 Assess Significance anova(nurse_lmer_0_ml, nurse_lmer_1_ml) Data: data_nurse Models: nurse_lmer_0_ml: stress ~ 1 + (1 | hospital/ward) nurse_lmer_1_ml: stress ~ expcon + age + gender + experien + wardtype + hospsize + nurse_lmer_1_ml: (1 | hospital/ward) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) nurse_lmer_0_ml 4 1950.4 1970.0 -971.18 1942.4 nurse_lmer_1_ml 11 1626.3 1680.3 -802.16 1604.3 338.04 7 &lt; 2.2e-16 nurse_lmer_0_ml nurse_lmer_1_ml *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 It is clear that the inclusing of these fixed, main effects improves the model’s fit, but it is questionable that the type of ward is significant (Wald test is non-significant). Rather than test it directly, we will leave it in the model. This is common practice to show that an expected variable is not significant. 3.5.3 Centering Variables Because we will will find that the experimental condition is moderated by hospital size (in other words there is a significant interaction between expcon and hospsize), Hox, Moerbeek, and Van de Schoot (2017) presents the models fit with centered values for these two variables. Let us see how this changes the model. (1) Experimental Condition Experimental conditon is a BINARY or two-level factor. When it is alternatively coded as a numeric, continuous variable taking the values of zero (\\(0\\)) for the reference category and one (\\(1\\)) for the other category, the model estimates are exactly the same, including the paramters for the variables and the intercept, AND the model fit statistics. When the numeric, continuous variable is further grand-mean centered by additionally subtraction the MEAN of the numberic version, the value of the intercept is the only estimate that changes. nurse_lmer_1a_ml &lt;- lme4::lmer(stress ~ expconN + # experimental condition = CONTINUOUS CODED 0/1 age + gender + experien + wardtype + hospsize + # hospital size = CATEGORICAL FACTOR (1|hospital/ward), data = data_nurse, REML = FALSE) nurse_lmer_1b_ml &lt;- lme4::lmer(stress ~ expconNG + # experimental condition = CONTINUOUS GRAND-MEAN CENTERED age + gender + experien + wardtype + hospsize + # hospital size = CATEGORICAL FACTOR (1|hospital/ward), data = data_nurse, REML = FALSE) # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(nurse_lmer_1_ml, nurse_lmer_1a_ml, nurse_lmer_1b_ml), custom.model.names = c(&quot;Factor&quot;, &quot;0 vs 1&quot;, &quot;Centered&quot;), caption = &quot;MLM: Model 1 - Expereimental Condiditon Coding (2-levels)&quot;, caption.above = TRUE, single.row = TRUE) MLM: Model 1 - Expereimental Condiditon Coding (2-levels) Factor 0 vs 1 Centered (Intercept) 5.38 (0.18)*** 5.38 (0.18)*** 5.03 (0.17)*** expconexperiment -0.70 (0.12)*** age 0.02 (0.00)*** 0.02 (0.00)*** 0.02 (0.00)*** gender -0.45 (0.03)*** -0.45 (0.03)*** -0.45 (0.03)*** experien -0.06 (0.00)*** -0.06 (0.00)*** -0.06 (0.00)*** wardtypespecial care 0.05 (0.12) 0.05 (0.12) 0.05 (0.12) hospsizemedium 0.49 (0.19)** 0.49 (0.19)** 0.49 (0.19)** hospsizelarge 0.90 (0.26)*** 0.90 (0.26)*** 0.90 (0.26)*** expconN -0.70 (0.12)*** expconNG -0.70 (0.12)*** AIC 1626.32 1626.32 1626.32 BIC 1680.30 1680.30 1680.30 Log Likelihood -802.16 -802.16 -802.16 Num. obs. 1000 1000 1000 Num. groups: ward:hospital 100 100 100 Num. groups: hospital 25 25 25 Var: ward:hospital (Intercept) 0.33 0.33 0.33 Var: hospital (Intercept) 0.10 0.10 0.10 Var: Residual 0.22 0.22 0.22 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 (2) Hospital Size Experimental conditon is a three-level factor. When it is alternatively coded as a numeric, continuous variables taking the values of whole numbers, starting with zero (\\(0, 1, 2, \\dots\\)), the model there will only be ONE parameter estimated instead of several (one less than the number of categories). This is becuase the levels are treated as being equally different from each other in terms of the outcome. This treats the effect of the categorical variable as if it is linear, which may or may not be appropriate. User beware! When the numeric, continuous variable is further grand-mean centered by additionally subtraction the MEAN of the numberic version, the value of the intercept is the only estimate that changes. nurse_lmer_1c_ml &lt;- lme4::lmer(stress ~ expconNG + # experimental condition = CONTINUOUS GRAND-MEAN CENTERED age + gender + experien + wardtype + hospsizeN + # hospital size = CONTINUOUS CODED 0/1 (1|hospital/ward), data = data_nurse, REML = FALSE) nurse_lmer_1d_ml &lt;- lme4::lmer(stress ~ expconNG + # experimental condition = CONTINUOUS GRAND-MEAN CENTERED age + gender + experien + wardtype + hospsizeNG + # hospital size = CONTINUOUS GRAND-MEAN CENTERED (1|hospital/ward), data = data_nurse, REML = FALSE) # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(nurse_lmer_1b_ml, nurse_lmer_1c_ml, nurse_lmer_1d_ml), custom.model.names = c(&quot;Factor&quot;, &quot;0 vs 1&quot;, &quot;Centered&quot;), caption = &quot;MLM: Model 1 - Hospital Coding (3-levels)&quot;, caption.above = TRUE, single.row = TRUE) MLM: Model 1 - Hospital Coding (3-levels) Factor 0 vs 1 Centered (Intercept) 5.03 (0.17)*** 5.04 (0.16)*** 5.40 (0.12)*** expconNG -0.70 (0.12)*** -0.70 (0.12)*** -0.70 (0.12)*** age 0.02 (0.00)*** 0.02 (0.00)*** 0.02 (0.00)*** gender -0.45 (0.03)*** -0.45 (0.03)*** -0.45 (0.03)*** experien -0.06 (0.00)*** -0.06 (0.00)*** -0.06 (0.00)*** wardtypespecial care 0.05 (0.12) 0.05 (0.12) 0.05 (0.12) hospsizemedium 0.49 (0.19)** hospsizelarge 0.90 (0.26)*** hospsizeN 0.46 (0.12)*** hospsizeNG 0.46 (0.12)*** AIC 1626.32 1624.36 1624.36 BIC 1680.30 1673.44 1673.44 Log Likelihood -802.16 -802.18 -802.18 Num. obs. 1000 1000 1000 Num. groups: ward:hospital 100 100 100 Num. groups: hospital 25 25 25 Var: ward:hospital (Intercept) 0.33 0.33 0.33 Var: hospital (Intercept) 0.10 0.10 0.10 Var: Residual 0.22 0.22 0.22 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 3.6 MLM: Add Random Slope Hox, Moerbeek, and Van de Schoot (2017), page 22: “Although logically we can test if explanatory variables at the first level have random coefficients (slopes) at the second or third level, and if explanatory variables at teh second level have random coefficients (slopes) at the third level, these possibilities are not pursued. We DO test a model with a random coefficient (slope) for expcon at the third level, where there turns out to be significant slope variation.” 3.6.1 Fit the Model nurse_lmer_1d_re &lt;- lme4::lmer(stress ~ expconNG + # experimental condition = CONTINUOUS GRAND-MEAN CENTERED age + gender + experien + # level 1 covariates wardtype + # level 2 covariate hospsizeNG + # level 3 covariate, hospital size = CONTINUOUS GRAND-MEAN CENTERED (1|hospital/ward), # Random Intercepts for wards within hospitals data = data_nurse, REML = TRUE) # fit via REML for nested Random Effects nurse_lmer_2_re &lt;- lme4::lmer(stress ~ expconNG + # experimental condition = CONTINUOUS GRAND-MEAN CENTERED age + gender + experien + # level 1 covariates wardtype + # level 2 covariate hospsizeNG + # level 3 covariate, hospital size = CONTINUOUS GRAND-MEAN CENTERED (1|hospital/ward) + # Random Intercepts for wards within hospitals (0 + expconNG|hospital), # RANDOM SLOPES for exp cond within hospital (does not vary witin a ward!) data = data_nurse, REML = TRUE) # fit via REML for nested Random Effects # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(nurse_lmer_1d_re, nurse_lmer_2_re), custom.model.names = c(&quot;M1: RI&quot;, &quot;M2: RIAS&quot;), caption = &quot;Nested Models: Random Slope via REML&quot;, caption.above = TRUE, single.row = TRUE) Nested Models: Random Slope via REML M1: RI M2: RIAS (Intercept) 5.40 (0.12)*** 5.39 (0.11)*** expconNG -0.70 (0.12)*** -0.70 (0.18)*** age 0.02 (0.00)*** 0.02 (0.00)*** gender -0.45 (0.03)*** -0.45 (0.03)*** experien -0.06 (0.00)*** -0.06 (0.00)*** wardtypespecial care 0.05 (0.12) 0.05 (0.07) hospsizeNG 0.46 (0.13)*** 0.46 (0.13)*** AIC 1659.89 1633.18 BIC 1708.97 1687.17 Log Likelihood -819.94 -805.59 Num. obs. 1000 1000 Num. groups: ward:hospital 100 100 Num. groups: hospital 25 25 Var: ward:hospital (Intercept) 0.34 Var: hospital (Intercept) 0.11 0.17 Var: Residual 0.22 0.22 Var: ward.hospital (Intercept) 0.11 Var: hospital.1 expconNG 0.69 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 3.6.2 Assess Significance anova(nurse_lmer_1d_re, nurse_lmer_2_re, refit = FALSE) Data: data_nurse Models: nurse_lmer_1d_re: stress ~ expconNG + age + gender + experien + wardtype + hospsizeNG + nurse_lmer_1d_re: (1 | hospital/ward) nurse_lmer_2_re: stress ~ expconNG + age + gender + experien + wardtype + hospsizeNG + nurse_lmer_2_re: (1 | hospital/ward) + (0 + expconNG | hospital) Df AIC BIC logLik deviance Chisq Chi Df nurse_lmer_1d_re 10 1659.9 1709.0 -819.94 1639.9 nurse_lmer_2_re 11 1633.2 1687.2 -805.59 1611.2 28.708 1 Pr(&gt;Chisq) nurse_lmer_1d_re nurse_lmer_2_re 8.417e-08 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The inclusion of a random slope effect for the experimental condition expcon significantly improves the models’s fit, thus is should be retained. 3.7 MLM: Add Cross-Level Interaction Hox, Moerbeek, and Van de Schoot (2017), page 22: “The varying slope can be predicted by adding a cross-level interaction between the variables expcon and hospsize. In view of this interaction, the variables expcon and hospsize have been centered on tehir overal means.” 3.7.1 Fit the Model nurse_lmer_2_ml &lt;- lme4::lmer(stress ~ expconNG + # experimental condition = CONTINUOUS GRAND-MEAN CENTERED age + gender + experien + # level 1 covariates wardtype + # level 2 covariate hospsizeNG + # level 3 covariate, hospital size = CONTINUOUS GRAND-MEAN CENTERED (1|hospital/ward) + # Random Intercepts for wards within hospitals (0 + expconNG|hospital), # RANDOM SLOPES for exp cond within hospital (does not vary witin a ward!) data = data_nurse, REML = FALSE) # fit via ML for nested FIXED Effects nurse_lmer_3_ml &lt;- lme4::lmer(stress ~ expconNG + # experimental condition = CONTINUOUS GRAND-MEAN CENTERED age + gender + experien + # level 1 covariates wardtype + # level 2 covariate hospsizeNG + # level 3 covariate, hospital size = CONTINUOUS GRAND-MEAN CENTERED expconNG*hospsizeNG + # CROSS-LEVEL interaction (1|hospital/ward) + # Random Intercepts for wards within hospitals (0 + expconNG|hospital), # RANDOM SLOPES for exp cond within hospital (does not vary witin a ward!) data = data_nurse, REML = FALSE) # fit via ML for nested FIXED Effects # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(nurse_lmer_2_ml, nurse_lmer_3_ml), custom.model.names = c(&quot;M2: RAIS&quot;, &quot;M3: Xlevel Int&quot;), caption = &quot;Nested Models: Fixed Cross-Level Interaction via ML&quot;, caption.above = TRUE, single.row = TRUE) Nested Models: Fixed Cross-Level Interaction via ML M2: RAIS M3: Xlevel Int (Intercept) 5.39 (0.11)*** 5.39 (0.11)*** expconNG -0.70 (0.18)*** -0.72 (0.11)*** age 0.02 (0.00)*** 0.02 (0.00)*** gender -0.46 (0.03)*** -0.46 (0.03)*** experien -0.06 (0.00)*** -0.06 (0.00)*** wardtypespecial care 0.05 (0.07) 0.05 (0.07) hospsizeNG 0.46 (0.12)*** 0.46 (0.12)*** expconNG:hospsizeNG 1.00 (0.16)*** AIC 1597.48 1576.07 BIC 1651.47 1634.96 Log Likelihood -787.74 -776.03 Num. obs. 1000 1000 Num. groups: ward:hospital 100 100 Num. groups: hospital 25 25 Var: ward.hospital (Intercept) 0.11 0.11 Var: hospital (Intercept) 0.15 0.15 Var: hospital.1 expconNG 0.66 0.18 Var: Residual 0.22 0.22 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 3.7.2 Assess Significance anova(nurse_lmer_2_ml, nurse_lmer_3_ml) Data: data_nurse Models: nurse_lmer_2_ml: stress ~ expconNG + age + gender + experien + wardtype + hospsizeNG + nurse_lmer_2_ml: (1 | hospital/ward) + (0 + expconNG | hospital) nurse_lmer_3_ml: stress ~ expconNG + age + gender + experien + wardtype + hospsizeNG + nurse_lmer_3_ml: expconNG * hospsizeNG + (1 | hospital/ward) + (0 + expconNG | nurse_lmer_3_ml: hospital) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) nurse_lmer_2_ml 11 1597.5 1651.5 -787.74 1575.5 nurse_lmer_3_ml 12 1576.1 1635.0 -776.03 1552.1 23.413 1 1.307e-06 nurse_lmer_2_ml nurse_lmer_3_ml *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 There is evidence that hospital size moderated the effect of the intervention. We will want to plot the estimated marginal means to interpret the meaning of this interaction. 3.8 Final Model 3.8.1 Fit the model The final model should be fit via REML. nurse_lmer_3_re &lt;- lme4::lmer(stress ~ expconNG + # experimental condition = CONTINUOUS GRAND-MEAN CENTERED age + gender + experien + # level 1 covariates wardtype + # level 2 covariate hospsizeNG + # level 3 covariate, hospital size = CONTINUOUS GRAND-MEAN CENTERED expconNG*hospsizeNG + # CROSS-LEVEL interaction (1|hospital/ward) + # Random Intercepts for wards within hospitals (0 + expconNG|hospital), # RANDOM SLOPES for exp cond within hospital (does not vary witin a ward!) data = data_nurse, REML = TRUE) # fit via REML for final model 3.8.2 Table of Estimated Parameters # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(nurse_lmer_3_re), custom.model.names = c(&quot;M3: Xlevel Int&quot;), caption = &quot;Final Model: with REML&quot;, caption.above = TRUE, single.row = TRUE) Final Model: with REML M3: Xlevel Int (Intercept) 5.39 (0.11)*** expconNG -0.72 (0.12)*** age 0.02 (0.00)*** gender -0.46 (0.03)*** experien -0.06 (0.00)*** wardtypespecial care 0.05 (0.07) hospsizeNG 0.46 (0.13)*** expconNG:hospsizeNG 1.00 (0.17)*** AIC 1614.47 BIC 1673.36 Log Likelihood -795.23 Num. obs. 1000 Num. groups: ward:hospital 100 Num. groups: hospital 25 Var: ward.hospital (Intercept) 0.11 Var: hospital (Intercept) 0.17 Var: hospital.1 expconNG 0.20 Var: Residual 0.22 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 ICC for unconditional and conditional model Usually, the ICC is calculated for the null model (“unconditional model”). However, according to Raudenbush and Bryk (2002) or Rabe-Hesketh and Skrondal (2012) it is also feasible to compute the ICC for full models with covariates (“conditional models”) and compare how much a level-2 variable explains the portion of variation in the grouping structure (random intercept). ICC for random-slope models Caution!!! For models with random slopes and random intercepts, the ICC would differ at each unit of the predictors. Hence, the ICC for these kind of models cannot be understood simply as proportion of variance (see Goldstein et al. 2010). For convenience reasons, as the ‘icc()’ function also extracts the different random effects variances, the ICC for random-slope-intercept-models is reported nonetheless, but it is usually not a meaningful summary of the proportion of variances. ICC for models with multiple or nested random effects Caution: By default, for three-level-models, depending on the nested structure of the model, or for models with multiple random effects, ‘icc()’ only reports the proportion of variance explained for each grouping level. Use ‘adjusted = TRUE’ to calculate the adjusted and conditional ICC. sjstats::icc(nurse_lmer_3_re) Linear mixed model Family : gaussian (identity) Formula: stress ~ expconNG + age + gender + experien + wardtype + hospsizeNG + expconNG * hospsizeNG + (1 | hospital/ward) + (0 + expconNG | hospital) ICC (ward.hospital): 0.1595 ICC (hospital): 0.2395 ICC (hospital.1): 0.2898 3.8.3 Visualization: Estimated Marginal Means Plot Although there are many variables in this model, only two are involved in any interaction(s). For this reason, we will choose to display the estimated marginal means across only experimental condition and hospital size. For this illustration, all other continuous predictors are taken to be at their mean and categorical predictors at their reference category. sjPlot::plot_model(nurse_lmer_3_re, type = &quot;pred&quot;, terms = c(&quot;hospsizeNG&quot;, &quot;expconNG&quot;)) sjPlot::plot_model(nurse_lmer_3_re, type = &quot;pred&quot;, terms = c(&quot;hospsizeNG [-0.776, 0.224, 1.224]&quot;, &quot;expconNG [-0.504, 0.496]&quot;)) + scale_x_continuous(breaks = c(-0.776, 0.224, 1.224), labels = c(&quot;small&quot;, &quot;medium&quot;, &quot;large&quot;)) + scale_color_manual(labels = c(&quot;Control&quot;, &quot;Intervention&quot;), values = c(&quot;red&quot;, &quot;blue&quot;)) + labs(title = &quot;Multilevel Modeling of Hospital Nurse Stress Intervention&quot;, subtitle = &quot;Ribbons display 95% Confidene Intervals&quot;, x = &quot;Hospital Size&quot;, y = &quot;Estimated Marginal Mean\\nNorse&#39;s Stress Score&quot;, color = &quot;Condition&quot;) + theme_bw() Get Estimated Marginal Means - default ‘nice’ predictor values: Focal predictors: All combinations of… expcon categorical, both levels control and experiment hospsize categorical, all three levelssmall , medium, large Always followed by: fit estimated marginal mean se standard error for the marginal mean lower lower end of the 95% confidence interval around the estimated marginal mean upper upper end of the 95% confidence interval around the estimated marginal mean effects::Effect(focal.predictors = c(&quot;expconNG&quot;, &quot;hospsizeNG&quot;), mod = nurse_lmer_3_re, xlevels = list(expconNG = c(-0.504, 0.496), hospsizeNG = c(-0.776, 0.224, 1.224))) %&gt;% data.frame() expconNG hospsizeNG fit se lower upper 1 -0.504 -0.776 5.382989 0.1631614 5.062808 5.703170 2 0.496 -0.776 3.885142 0.1626431 3.565978 4.204305 3 -0.504 0.224 5.341652 0.1112654 5.123309 5.559994 4 0.496 0.224 4.842621 0.1109172 4.624961 5.060280 5 -0.504 1.224 5.300315 0.2138891 4.880588 5.720042 6 0.496 1.224 5.800100 0.2131947 5.381735 6.218464 effects::Effect(focal.predictors = c(&quot;expconNG&quot;, &quot;hospsizeNG&quot;), mod = nurse_lmer_3_re, xlevels = list(expconNG = c(-0.504, 0.496), hospsizeNG = c(-0.776, 0.224, 1.224))) %&gt;% data.frame() %&gt;% dplyr::mutate(expcon = factor(expconNG + 0.504, labels = c(&quot;Control&quot;, &quot;Intervention&quot;))) %&gt;% dplyr::mutate(hopsize = factor(hospsizeNG + 0.776, labels = c(&quot;Small&quot;, &quot;Medium&quot;, &quot;Large&quot;))) expconNG hospsizeNG fit se lower upper expcon 1 -0.504 -0.776 5.382989 0.1631614 5.062808 5.703170 Control 2 0.496 -0.776 3.885142 0.1626431 3.565978 4.204305 Intervention 3 -0.504 0.224 5.341652 0.1112654 5.123309 5.559994 Control 4 0.496 0.224 4.842621 0.1109172 4.624961 5.060280 Intervention 5 -0.504 1.224 5.300315 0.2138891 4.880588 5.720042 Control 6 0.496 1.224 5.800100 0.2131947 5.381735 6.218464 Intervention hopsize 1 Small 2 Small 3 Medium 4 Medium 5 Large 6 Large effects::Effect(focal.predictors = c(&quot;expconNG&quot;, &quot;hospsizeNG&quot;), mod = nurse_lmer_3_re, xlevels = list(expconNG = c(-0.504, 0.496), hospsizeNG = c(-0.776, 0.224, 1.224))) %&gt;% data.frame() %&gt;% dplyr::mutate(expcon = factor(expconNG + 0.504, labels = c(&quot;Control&quot;, &quot;Intervention&quot;))) %&gt;% dplyr::mutate(hospsize = factor(hospsizeNG + 0.776, labels = c(&quot;Small&quot;, &quot;Medium&quot;, &quot;Large&quot;))) %&gt;% ggplot() + aes(x = hospsize, y = fit, group = expcon, shape = expcon, color = expcon) + geom_errorbar(aes(ymin = fit - se, # mean plus/minus one Std Error ymax = fit + se), width = .4, position = position_dodge(width = .2)) + geom_errorbar(aes(ymin = lower, # 95% CIs ymax = upper), width = .2, position = position_dodge(width = .2)) + geom_line(aes(linetype = expcon), position = position_dodge(width = .2)) + geom_point(size = 4, position = position_dodge(width = .2)) + theme_bw() + labs(x = &quot;Hospital Size&quot;, y = &quot;Estimated Marginal Mean, Stress&quot;, shape = &quot;Condition&quot;, color = &quot;Condition&quot;, linetype = &quot;Condition&quot;) + theme(legend.key.width = unit(2, &quot;cm&quot;), legend.background = element_rect(color = &quot;black&quot;), legend.position = c(1, 0), legend.justification = c(1, 0)) This plot illustrates the estimated marginal means among male (gender’s reference category) nurses at the overall mean age (43.01 years), with the mean level experience (17.06 years), since thoes variables were not included as focal.predictors in the effects::Effect() function. Different values for thoes predictors would yield the exact sample plot, shifted as a whole either up or down. 3.9 Interpretation There is evidence this intervention lowered stress among nurses working in small hospitals and to a smaller degree in medium sized hospitals. The intervention did not exhibit an effect in large hospitals. 3.9.1 Strength This analysis was able to incorporated all three levels of clustering while additionally controlling for many covariates, both categorical (nurse gender and ward type) and continuous (nurse age and experience in years). Also heterogeneity was accounted for in terms of the interventions’s effect at various hospitals. This would NOT be possible via any ANOVA type anlysis. 3.9.2 Weakness The approach presented by Hox, Moerbeek, and Van de Schoot (2017) and shown above involved mean-centering categorical variables. This would only be appropriate for a factor with more than two levels if its effect on the outcome was linear. Also, as the mean-centered variables are treated as continuous variables, post hoc tests are increasingly difficult. 3.10 Reproduction of Table 2.5 Hox, Moerbeek, and Van de Schoot (2017) presents a table on page 23 comparing various models. Note, that table includes models only fit via maximum likelihood, not REML. Also, the model \\(M_3\\): with cross-level interaction is slightly different for an unknown reason. # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(nurse_lmer_0_ml, nurse_lmer_1d_ml, nurse_lmer_2_ml, nurse_lmer_3_ml), custom.model.names = c(&quot;M0&quot;, &quot;M1&quot;, &quot;M2&quot;, &quot;M3&quot;), caption = &quot;Hox Table 2.5: Models for stress in hospitals and wards&quot;, caption.above = TRUE, single.row = TRUE) Hox Table 2.5: Models for stress in hospitals and wards M0 M1 M2 M3 (Intercept) 5.00 (0.11)*** 5.40 (0.12)*** 5.39 (0.11)*** 5.39 (0.11)*** expconNG -0.70 (0.12)*** -0.70 (0.18)*** -0.72 (0.11)*** age 0.02 (0.00)*** 0.02 (0.00)*** 0.02 (0.00)*** gender -0.45 (0.03)*** -0.46 (0.03)*** -0.46 (0.03)*** experien -0.06 (0.00)*** -0.06 (0.00)*** -0.06 (0.00)*** wardtypespecial care 0.05 (0.12) 0.05 (0.07) 0.05 (0.07) hospsizeNG 0.46 (0.12)*** 0.46 (0.12)*** 0.46 (0.12)*** expconNG:hospsizeNG 1.00 (0.16)*** AIC 1950.36 1624.36 1597.48 1576.07 BIC 1969.99 1673.44 1651.47 1634.96 Log Likelihood -971.18 -802.18 -787.74 -776.03 Num. obs. 1000 1000 1000 1000 Num. groups: ward:hospital 100 100 100 100 Num. groups: hospital 25 25 25 25 Var: ward:hospital (Intercept) 0.49 0.33 Var: hospital (Intercept) 0.16 0.10 0.15 0.15 Var: Residual 0.30 0.22 0.22 0.22 Var: ward.hospital (Intercept) 0.11 0.11 Var: hospital.1 expconNG 0.66 0.18 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 Deviance: c(deviance(nurse_lmer_0_ml), deviance(nurse_lmer_1d_ml), deviance(nurse_lmer_2_ml), deviance(nurse_lmer_3_ml)) %&gt;% round(1) [1] 1942.4 1604.4 1575.5 1552.1 "],
["intro-centering-and-scaling-example-reading-achievement-2-levels-only.html", "4 Intro Centering and Scaling Example: Reading Achievement (2-levels only) 4.1 Background 4.2 Exploratory Data Analysis 4.3 Single-Level Regression 4.4 MLM - Step 1: Null Model, only fixed and random intercepts 4.5 MLM - Step 2: Add Lower-level explanatory variables, fixed, ML 4.6 MLM - Step 3: Higher-level explanatory variables, fixed, ML 4.7 MLM - Step 4: Explanatory variables predict Slopes, random, REML 4.8 MLM - Step 5: Cross-Level interactions between explanatory variables - fixed, ML 4.9 Centering Predictors: Change Center 4.10 Rescaling Predictors: Change Units or Standardize 4.11 Final Model", " 4 Intro Centering and Scaling Example: Reading Achievement (2-levels only) library(tidyverse) library(haven) # read in SPSS dataset library(furniture) # nice table1() descriptives library(stargazer) # display nice tables: summary &amp; regression library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(psych) # contains some useful functions, like headTail library(lme4) # Linear, generalized linear, &amp; nonlinear mixed models library(sjstats) # ICC calculations library(effects) # Effects for regression models library(optimx) # Different optimizers to solve mlm&#39;s 4.1 Background The following example was included in the text “Multilevel Modeling in R” by Finch, Bolin, and Kelley (2016). The datasets for this textbook may be downloaded from the website: http://www.mlminr.com/data-sets/. I was unable to find any documentation on this dataset in the book or online, so I contacted the authors. There were unable to provide much either, but based on visual inspection designated the class of factor to thoes vairables that seem to represent categorical quantities. The labels for gender and class size are relative to the frequencies in the journal article the authors did point me to (although the samples sizes do not match up). FOR THIS CHAPTER WE WILL IGNORE ALL LEVELS EXCEPT FOR STUDNETS BEING NESTED WITHIN SCHOOLS. Read the SPSS data in with the haven package . data_raw &lt;- haven::read_sav(&quot;http://www.mlminr.com/data-sets/Achieve.sav?attredirects=0&quot;) Declare all categorical variables to be factors and apply labels where meaningful. Student-specific * gender = Male or Female * age = Age, in months * gevocab = Vocabulary Score * geread = Reading Score Class-specific * classsize = category of class’s size School-specific * senroll = school enrollment * ses = school’s SES level data_achieve &lt;- data_raw %&gt;% dplyr::mutate_at(vars(id, region, corp, school, class), factor) %&gt;% dplyr::mutate(gender = gender %&gt;% factor(labels = c(&quot;Female&quot;, &quot;Male&quot;))) %&gt;% dplyr::mutate(classize = classize %&gt;% factor(labels = c(&quot;12-17&quot;, &quot;18-21&quot;, &quot;22-26&quot;, &quot;&gt;26&quot;))) %&gt;% dplyr::select(id, region, corp, school, class, # Identifiers gender, age, geread, gevocab, # Pupil-level vars classize, # Class-Level vars senroll, ses) # School-level vars 4.1.1 Sample Structure It is obvious that the sample is hiarchical in nature. The nesting starts with students (level 1) nested within class (level 2), which are further nested within school (level 3), corp (level 4), and finally region (level 5). For this chapter we will only focus on TWO levels: students (level 1) are the units on which the outcome is measured and schools (level 2) are the units in which they are nested. The number of regions = 9: num_regions &lt;- data_achieve %&gt;% dplyr::group_by(region) %&gt;% dplyr::tally() %&gt;% nrow() num_regions [1] 9 The number of corps = 60: num_corps &lt;- data_achieve %&gt;% dplyr::group_by(region, corp) %&gt;% dplyr::tally() %&gt;% nrow() num_corps [1] 60 The number of schools = 160 num_schools &lt;- data_achieve %&gt;% dplyr::group_by(region, corp, school) %&gt;% dplyr::tally() %&gt;% nrow() num_schools [1] 160 The number of classes = 568 num_classes &lt;- data_achieve %&gt;% dplyr::group_by(region, corp, school, class) %&gt;% dplyr::tally() %&gt;% nrow() num_classes [1] 568 The number of students = 10320 num_subjects &lt;- data_achieve %&gt;% nrow num_subjects [1] 10320 4.2 Exploratory Data Analysis 4.2.1 Summarize Descriptive Statistics 4.2.1.1 The stargazer package Most posters, journal articles, and reports start with a table of descriptive statistics. Since it tends to come first, this type of table is often refered to as Table 1. The stargazer() function can be used to create such a table, but only for the entire dataset. I haven’t been able to find a way to get it to summarize subsamples and compare them in the standard format. Also, it only summarises continuous, not categorical variables. # Knit to Website: type = &quot;html&quot; # Knit to PDF: type = &quot;latex&quot; # View on Screen: type = &quot;text&quot; data_achieve %&gt;% dplyr::select(classize, gender, geread, gevocab, age, ses, senroll) %&gt;% data.frame() %&gt;% stargazer::stargazer(header = FALSE, title = &quot;Summary of the numeric variables with `stargazer`&quot;, type = &quot;html&quot;) Summary of the numeric variables with stargazer Statistic N Mean St. Dev. Min Pctl(25) Pctl(75) Max geread 10,320 4.341 2.332 0.000 2.800 4.900 12.000 gevocab 10,320 4.494 2.368 0.000 2.900 5.200 11.200 age 10,320 107.529 5.060 82 104 111 135 ses 10,320 72.849 21.982 0.000 66.300 87.800 100.000 senroll 10,320 533.415 154.797 115 438 644 916 4.2.1.2 The furniture package Tyson Barrett’s furniture package includes the extremely useful function table1() which simplifies the common task of creating a stratified, comparative table of descriptive statistics. Full documentation can be accessed by executing ?furniture::table1. # Knit to Website: output = &quot;html&quot; # Knit to PDF: output = &quot;latex2&quot; # View on Screen: output = &quot;&quot;text&quot;, or &quot;markdown&quot;, &quot;html&quot; data_achieve %&gt;% furniture:: table1(&quot;Reading Score&quot; = geread, &quot;Vocabulary Score&quot; = gevocab, &quot;Age (in months)&quot; = age, &quot;School SES&quot; = ses, &quot;School&#39;s Enrollment&quot; = senroll, splitby = ~ gender, # var to divide sample by test = TRUE, # test groups different? caption = &quot;Summary of the numeric variables with `table1`&quot;, output = &quot;html&quot;) Table 4.1: Summary of the numeric variables with table1 Female Male P-Value n = 5143 n = 5177 Reading Score 0.218 4.4 (2.3) 4.3 (2.3) Vocabulary Score &lt;.001 4.6 (2.4) 4.4 (2.3) Age (in months) &lt;.001 107.1 (5.0) 107.9 (5.1) School SES 0.155 72.5 (22.3) 73.2 (21.7) School’s Enrollment 0.483 532.3 (154.5) 534.5 (155.1) 4.2.2 Visualization of Raw Data 4.2.2.1 Level One Plots: Disaggregate or ignore higher levels For a first look, its useful to plot all the data points on a single scatterplot as displayed in the previous plot. Due to the large sample size, many points end up being plotted on top of or very near each other (overplotted). When this is the case, it can be useful to use geom_binhex() rather than geom_point() so the color saturation of the hexigons convey the number of points at that location, as seen in Figure . Note: I had to manually install the package hexbin for the geom_hex() to run. data_achieve %&gt;% ggplot() + aes(x = gevocab, y = geread) + stat_binhex(colour = &quot;grey85&quot;, na.rm = TRUE) + # outlines scale_fill_gradientn(colors = c(&quot;grey80&quot;,&quot;navyblue&quot;), # fill color extremes name = &quot;Frequency&quot;, # legend title na.value = NA) + # color for count = 0 theme_bw() Figure 4.1: Raw Data: Density, Vocab vs. Reading 4.2.2.2 Multilevel plots: illustrate two nested levels Up to this point, all investigation of this dataset has been only at the pupil level and any nesting or clustering within schools has been ignored. Plotting is a good was to start to get an idea of the school-to-school variability. This figure displays four handpicked school to illustrate the degreen of school-to-school variability in the association between vocab and reading scores. data_achieve %&gt;% dplyr::filter(school %in% c(1321, 6181, 6197, 6823)) %&gt;% # choose school numbers ggplot(aes(x = gevocab, y = geread))+ geom_count() + # creates points, size by overplotted number geom_smooth(method = &quot;lm&quot;) + # linear model (OLS) facet_wrap(~ school) + # panels by school theme_bw() Figure 4.2: Raw Data: Independent Single-Level Regression within each school, a few illustrative cases Another way to explore the school-to-school variability is to plot the linear model fit independently to each of the schools. This next figure displays only the smooth lines without the standard error bands or the raw data in the form of points or hexagons. data_achieve %&gt;% ggplot(aes(x = gevocab, y = geread)) + geom_smooth(aes(group = school), method = &quot;lm&quot;, se = FALSE, # do NOT want the SE bands size = 0.3) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;red&quot;, # do NOT want the SE bands size = 2) + # make the lines thinner theme_bw() Figure 4.3: Raw Data: Independent Single-Level Regression within each school, all schools shown together Due to the high number of schools, the figure with all the school’s independent linear regression lines resembles a hairball and is hard to deduce much about individual schools. By using the facet_grid() layer, we can seperate the schools out so better see school-to-school variability. It also allows investigation of higher level predictors, such as the school’s SES (median split with ntile(var, 2)) and class size. data_achieve %&gt;% dplyr::mutate(ses2 = ntile(ses, 2) %&gt;% # median split factor(labels = c(&quot;SES: Lower Half&quot;, &quot;SES: Upper Half&quot;))) %&gt;% dplyr::mutate(senroll = ntile(senroll, 3) %&gt;% factor(labels = c(&quot;Enroll: Smallest Third&quot;, &quot;Enroll: Middle Third&quot;, &quot;Enroll: Largest Third&quot;))) %&gt;% ggplot(aes(x = gevocab, y = geread, group = school)) + # sepearates students into schools geom_smooth(method = &quot;lm&quot;, se = FALSE, size = 0.3, color = &quot;black&quot;, alpha = .2) + theme_bw() + facet_grid(senroll ~ ses2) # makes seperate panels (rows ~ columns) Figure 4.4: Raw Data: Independent Single-Level Regression within each school, sepearated by school size and school SES 4.3 Single-Level Regression 4.3.1 Fit Nested Models Ignoring the fact that students are nested or clustered within schools, is called dissagregating. This treats all students as independent units. # linear model - ignores school (for reference only) fit_read_lm_0 &lt;- lm(formula = geread ~ 1, # intercept only data = data_achieve) fit_read_lm_1 &lt;- lm(formula = geread ~ gevocab , # one predictor data = data_achieve) fit_read_lm_2 &lt;- lm(formula = geread ~ gevocab + age, # two predictors data = data_achieve) fit_read_lm_3 &lt;- lm(formula = geread ~ gevocab*age, # interation+main effects data = data_achieve) Now compare the models: # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(fit_read_lm_0, fit_read_lm_1, fit_read_lm_2, fit_read_lm_3), custom.model.names = c(&quot;Null&quot;, &quot;1 IV&quot;, &quot;2 IV&quot;, &quot;Interaction&quot;), caption = &quot;OLS: Investigate Fixed, Pupil-level Predictors&quot;, caption.above = TRUE, single.row = TRUE) OLS: Investigate Fixed, Pupil-level Predictors Null 1 IV 2 IV Interaction (Intercept) 4.34 (0.02)*** 1.96 (0.04)*** 3.19 (0.42)*** 5.28 (0.87)*** gevocab 0.53 (0.01)*** 0.53 (0.01)*** 0.01 (0.19) age -0.01 (0.00)** -0.03 (0.01)*** gevocab:age 0.00 (0.00)** R2 0.00 0.29 0.29 0.29 Adj. R2 0.00 0.29 0.29 0.29 Num. obs. 10320 10320 10320 10320 RMSE 2.33 1.97 1.97 1.96 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 Assess the significance of terms in the last ‘best’ model summary(fit_read_lm_3) Call: lm(formula = geread ~ gevocab * age, data = data_achieve) Residuals: Min 1Q Median 3Q Max -6.2069 -1.1250 -0.4362 0.6041 8.6476 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.282607 0.869769 6.074 1.3e-09 *** gevocab 0.009154 0.189113 0.048 0.961394 age -0.030814 0.008066 -3.820 0.000134 *** gevocab:age 0.004830 0.001759 2.746 0.006039 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.965 on 10316 degrees of freedom Multiple R-squared: 0.2902, Adjusted R-squared: 0.29 F-statistic: 1406 on 3 and 10316 DF, p-value: &lt; 2.2e-16 sjstats::r2(fit_read_lm_3) R-Squared for Generalized Linear Mixed Model R-squared: 0.290 adjusted R-squared: 0.290 anova(fit_read_lm_3) Analysis of Variance Table Response: geread Df Sum Sq Mean Sq F value Pr(&gt;F) gevocab 1 16224 16223.9 4202.2783 &lt; 2.2e-16 *** age 1 34 33.7 8.7356 0.003128 ** gevocab:age 1 29 29.1 7.5419 0.006039 ** Residuals 10316 39827 3.9 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 4.3.2 Visualize the Interaction effects::Effect(focal.predictors = c(&quot;gevocab&quot;, &quot;age&quot;), # chooses defaul values for mod = fit_read_lm_3) # continuous vars gevocab*age effect age gevocab 82 95 110 120 140 0 2.755830 2.355243 1.893028 1.584884 0.9685969 3 3.971571 3.759370 3.514523 3.351291 3.0248284 6 5.187313 5.163497 5.136018 5.117699 5.0810599 8 5.997807 6.099582 6.217015 6.295304 6.4518809 10 6.808301 7.035667 7.298012 7.472909 7.8227019 effects::Effect(focal.predictors = c(&quot;gevocab&quot;, &quot;age&quot;), # chooses defaul values for mod = fit_read_lm_3) %&gt;% # continuous vars data.frame() %&gt;% mutate(age = factor(age)) %&gt;% # must make a factor to seperate lines ggplot(aes(x = gevocab, y = fit, color = age)) + geom_point() + geom_line() Here is a better version of the plot. Age is in months, so we want multiples of 12 for good visualization summary(data_achieve$age)/12 # divide by 12 to change months to years Min. 1st Qu. Median Mean 3rd Qu. Max. 6.833 8.667 8.917 8.961 9.250 11.250 A good set set of illustrative ages could be: 7, 9, and 11: c(7, 9, 11) * 12 # times by 12 to change years to months [1] 84 108 132 effects::Effect(focal.predictors = c(&quot;gevocab&quot;, &quot;age&quot;), mod = fit_read_lm_3, xlevels = list(age = c(84, 108, 132))) %&gt;% # age is in months data.frame() %&gt;% mutate(age_yr = factor(age/12)) %&gt;% # it would be nice to plot age in years ggplot(aes(x = gevocab, y = fit, color = age_yr, linetype = age_yr)) + geom_line(size = 1.25) + theme_bw() + labs(title = &quot;Best Linear Model - Disaggregated Data (OLS)&quot;, x = &quot;Vocabulary Score&quot;, y = &quot;Reading Score&quot;, linetype = &quot;Age (yrs)&quot;, color = &quot;Age (yrs)&quot;) + theme(legend.position = c(0.85, 0.2), legend.key.width = unit(2, &quot;cm&quot;), legend.background = element_rect(color = &quot;black&quot;)) + scale_linetype_manual(values = c(&quot;solid&quot;, &quot;longdash&quot;, &quot;dotted&quot;)) + scale_x_continuous(breaks = seq(from = 0, to = 11, by = 2)) + scale_y_continuous(breaks = seq(from = 0, to = 11, by = 1)) 4.4 MLM - Step 1: Null Model, only fixed and random intercepts A so called Empty Model only includes random intercepts. No independent variables are involved, other the grouping or clustering variable that designates how level 1 units are nested within level 2 units. For a cross-sectional study design this would be the grouping variables, where as for longitudinal or repeated measures designs this would be the subject identifier. This nested structure variable should be set to have class factor. 4.4.1 Fit the Model fit_read_0ml &lt;- lme4::lmer(geread ~ 1 + (1|school), data = data_achieve, REML = FALSE) # fit via ML (not the default) fit_read_0re &lt;- lme4::lmer(geread ~ 1 + (1|school) , data = data_achieve, REML = TRUE) # fit = REML (the default) Compare the two models to OLS: # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(fit_read_lm_0, fit_read_0ml, fit_read_0re), custom.model.names = c(&quot;OLS&quot;, &quot;MLM-ML&quot;, &quot;MLM-REML&quot;), caption = &quot;MLM: NULL Model,two estimation methods&quot;, caption.above = TRUE, single.row = TRUE) MLM: NULL Model,two estimation methods OLS MLM-ML MLM-REML (Intercept) 4.34 (0.02)*** 4.31 (0.05)*** 4.31 (0.05)*** R2 0.00 Adj. R2 0.00 Num. obs. 10320 10320 10320 RMSE 2.33 AIC 46270.34 46274.31 BIC 46292.06 46296.03 Log Likelihood -23132.17 -23134.15 Num. groups: school 160 160 Var: school (Intercept) 0.39 0.39 Var: Residual 5.05 5.05 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 Notice that the estimate for the intercept is nearly the same in the linear regression and intercept only models, but the standard errors are quite different. When there is clustering in sample, the result of ignoring it is under estimation of the standard errors and over stating the significance of associations. This table was made with the screenreg() function in the self named package. I tend to prefer this display over stargazer(). 4.4.2 Estimate the ICC First, ask for the variance compenents: lme4::VarCorr(fit_read_0re) %&gt;% print(comp = c(&quot;Variance&quot;, &quot;Std.Dev&quot;), digits = 4) Groups Name Variance Std.Dev. school (Intercept) 0.3915 0.6257 Residual 5.0450 2.2461 sjstats::re_var(fit_read_0re) Within-group-variance: 5.045 Between-group-variance: 0.392 (school) \\[ \\begin{align*} \\text{schools} \\rightarrow \\; &amp; \\sigma^2_{u0} = 0.6257^2 = 0.392 \\\\ \\text{students within schools} \\rightarrow \\; &amp; \\sigma^2_{e} = 2.2461^2 = 5.045 \\\\ \\end{align*} \\] Intraclass Correlation (ICC) Formula \\[ \\overbrace{\\rho}^{\\text{ICC}} = \\frac{\\overbrace{\\sigma^2_{u0}}^{\\text{Random Intercept}\\atop\\text{Variance}}} {\\underbrace{\\sigma^2_{u0}+\\sigma^2_{e}}_{\\text{Total}\\atop\\text{Variance}}} \\tag{Hox 2.9} \\] Then you can manually caluclate the ICC. 0.392 / (0.392 + 5.045) [1] 0.07209858 Or you can use the icc() function in the sjstats package. sjstats::icc(fit_read_0re) Linear mixed model Family : gaussian (identity) Formula: geread ~ 1 + (1 | school) ICC (school): 0.0720 Note: On page 45 (Finch, Bolin, and Kelley 2016), the authors substituted standard deviations into the formula, rather than variances. The mistake is listed on their webpage errata (http://www.mlminr.com/errata) and is repeated through the text. 4.5 MLM - Step 2: Add Lower-level explanatory variables, fixed, ML Variance Component models (steps 2 and 3) - decompose the INTERCEPT variance into different variance compondents for each level. The regression intercepts are assumed to varry ACROSS the groups, while the slopes are assumed fixed (no random effects). Fixed effects selection should come prior to random effects. You should use Maximum Likelihood (ML) estimation when fitting these models. IF: only level 1 predictors and random intercepts are incorporated Then: MLM \\(\\approx\\) ANCOVA . 4.5.1 Add pupil’s vocab score as a fixed effects predictor fit_read_1ml &lt;- lme4::lmer(geread ~ gevocab + (1|school), data = data_achieve, REML = FALSE) # to compare fixed var sig fit_read_1re &lt;- lme4::lmer(geread ~ gevocab + (1|school), data = data_achieve, REML = TRUE) # for R-sq calcs # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(fit_read_0ml, fit_read_1ml), custom.model.names = c(&quot;Null&quot;, &quot;w Pred&quot;), caption = &quot;MLM: Investigate a Fixed Pupil-level Predictor&quot;, caption.above = TRUE, doctype = FALSE, digits = 4) MLM: Investigate a Fixed Pupil-level Predictor Null w Pred (Intercept) 4.3068*** 2.0231*** (0.0548) (0.0492) gevocab 0.5130*** (0.0084) AIC 46270.3388 43132.4318 BIC 46292.0643 43161.3991 Log Likelihood -23132.1694 -21562.2159 Num. obs. 10320 10320 Num. groups: school 160 160 Var: school (Intercept) 0.3885 0.0987 Var: Residual 5.0450 3.7661 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 4.5.1.1 Assess Significance of Effects Likelihood Ratio Test (LRT) Since models 0 and 1 are nested models, only differing by the the inclusion or exclusion of the fixed effects predictor gevocab, AND both models were fit via Maximum Likelihood, we can compare the model fit may be compared via the Likilihood-Ratio Test (LRT). The Likelihood Ratio value (L. Ratio) is found by subtracting the two model’s -2 * logLik or deviance values. Significance is judged by the Chi Squared distribution, using the difference in the number of parameters fit as the degrees of freedom. anova(fit_read_0ml, fit_read_1ml) Data: data_achieve Models: fit_read_0ml: geread ~ 1 + (1 | school) fit_read_1ml: geread ~ gevocab + (1 | school) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) fit_read_0ml 3 46270 46292 -23132 46264 fit_read_1ml 4 43132 43161 -21562 43124 3139.9 1 &lt; 2.2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 What does the model look like? effects::Effect(focal.predictors = c(&quot;gevocab&quot;), mod = fit_read_1ml) %&gt;% data.frame() %&gt;% ggplot(aes(x = gevocab, y = fit)) + geom_ribbon(aes(ymin = lower, ymax = upper), alpha = .3) + geom_line() + theme_bw() 4.5.1.2 Proportion of Variance Explained Extract the variance-covariance estimates: BL = BAseline: The Null Model (fit via REML) sjstats::re_var(fit_read_0re) Within-group-variance: 5.045 Between-group-variance: 0.392 (school) \\[ \\sigma^2_{u0-BL} = 0.392 \\\\ \\sigma^2_{e-BL} = 5.045 \\] MC = Model to Compare: Model with Predictor (fit via REML) sjstats::re_var(fit_read_1re) Within-group-variance: 3.766 Between-group-variance: 0.100 (school) \\[ \\sigma^2_{u0-MC} = 0.100 \\\\ \\sigma^2_{e-MC} = 3.766 \\] Level 1 \\(R^2\\) - Snijders and Bosker Found on page 47 (Finch, Bolin, and Kelley 2016), theproportion of variance in the outcome explained by predictor on level one is given by: Snijders and Bosker Formula - Level 1 \\[ R^2_1 = 1 - \\frac{\\sigma^2_{e-MC} + \\sigma^2_{u0-MC}} {\\sigma^2_{e-BL} + \\sigma^2_{u0-BL}} \\] Note: This formula also apprears in the Finch errata. The subscripts in the denominator of the fraction should be for model 0, not model 1. The formula is given correctly here. They did substitute in the correct values. Calculate the value by hand: 1 - (0.100 + 3.766)/(0.392 + 5.045) [1] 0.2889461 Or use the sjstats package to help out: 1 - sum(sjstats::re_var(fit_read_1re)) / sum(sjstats::re_var(fit_read_0re)) [1] 0.288838 This means nearly 30% of the variance in reading scores, above and beyond that accounted for by school membership (i.e. school makeup or school-to-school variation), is attributable to vocabulary scores. Level 1 \\(R^2\\) - Raudenbush and Bryk Hox, Moerbeek, and Van de Schoot (2017) presents this formula on page 58 of chapter 2 Raudenbush and Bryk Approximate Formula - Level 1 \\[ approx\\; R^2_1 = \\frac{\\sigma^2_{e-BL} - \\sigma^2_{e-MC}} {\\sigma^2_{e-BL} } \\tag{Hox 4.8} \\] Calculate the value by hand: (5.045 - 3.766) / 5.045 [1] 0.2535183 Although slightly different in value and meaning, this value also conveys that vcabulary scores are highly associated with reading scores. Level 2 \\(R^2\\) - Snijders and Bosker Formula Extended Snijders and Bosker Formula Extended - Level 2 \\[ R^2_2 = 1 - \\frac{\\frac{\\sigma^2_{e-MC}}{B} + \\sigma^2_{u0-MC}} {\\frac{\\sigma^2_{e-BL}}{B} + \\sigma^2_{u0-BL}} \\] \\(B\\) is the average size of the Level 2 units (schools). Technically, you should use the harmonic mean, but unless the clusters differ greatly in size, it doesn’t make a huge difference. Average sample cluster size num_subjects / num_schools [1] 64.5 Calculate by hand: 1 - ((3.766 / 64.5) + 0.100)/ ((5.045 / 64.5) + 0.391) [1] 0.6624428 This means that over two-thrids in school mean reading levelsmay be explained by their student’s vocabulary scores. Level 2 \\(R^2\\) - Raudenbush and Bryk Raudenbush and Bryk Approximate Formula - Level 2 \\[ R^2_1 = \\frac{\\sigma^2_{u0-BL} - \\sigma^2_{u0-MC}} {\\sigma^2_{u0-BL} } \\tag{Hox 4.9} \\] (0.392 - 0.100)/(0.392) [1] 0.744898 Remeber that these ‘variance accounted for’ estimations are not as straight forwards as we would like. 4.5.2 Investigate More Level 1 Predictors Part of investigating lower level explanatory variables, is checking for interactions between these variables. The interaction between fixed effects is also considered to be a fixed effect, so we need to employ Maximum Likelihood estimation to compare nested models. fit_read_2ml &lt;- lmer(geread ~ gevocab + age + (1 | school), # add main effect of age data = data_achieve, REML = FALSE) fit_read_3ml &lt;- lmer(geread ~ gevocab*age + (1 | school), # add interaction between vocab and age data = data_achieve, REML = FALSE) # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(fit_read_1ml, fit_read_2ml, fit_read_3ml), custom.model.names = c(&quot;Only Vocab&quot;, &quot;Both Main Effects&quot;, &quot;Interaction&quot;), caption = &quot;MLM: Investigate Other Fixed Pupil-level Predictors&quot;, caption.above = TRUE, doctype = FALSE, digits = 4) MLM: Investigate Other Fixed Pupil-level Predictors Only Vocab Both Main Effects Interaction (Intercept) 2.0231*** 3.0049*** 5.1874*** (0.0492) (0.4172) (0.8666) gevocab 0.5130*** 0.5121*** -0.0279 (0.0084) (0.0084) (0.1881) age -0.0091* -0.0294*** (0.0038) (0.0080) gevocab:age 0.0050** (0.0017) AIC 43132.4318 43128.8201 43122.5687 BIC 43161.3991 43165.0293 43166.0198 Log Likelihood -21562.2159 -21559.4100 -21555.2844 Num. obs. 10320 10320 10320 Num. groups: school 160 160 160 Var: school (Intercept) 0.0987 0.0973 0.0977 Var: Residual 3.7661 3.7646 3.7614 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 4.5.2.1 Assess Significance of Effects Likelihood Ratio Test (LRT) anova(fit_read_1ml, fit_read_2ml, fit_read_3ml) Data: data_achieve Models: fit_read_1ml: geread ~ gevocab + (1 | school) fit_read_2ml: geread ~ gevocab + age + (1 | school) fit_read_3ml: geread ~ gevocab * age + (1 | school) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) fit_read_1ml 4 43132 43161 -21562 43124 fit_read_2ml 5 43129 43165 -21559 43119 5.6117 1 0.017841 * fit_read_3ml 6 43123 43166 -21555 43111 8.2514 1 0.004072 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Not only is student’s age predictive of their reading level (I could have guessed that), but that age moderated the relationship between vocabulary and reading. 4.5.2.2 Visulaize the Interation Visulaizations are extremely helpful to interpred interactions. summary(data_achieve$age) Min. 1st Qu. Median Mean 3rd Qu. Max. 82.0 104.0 107.0 107.5 111.0 135.0 effects::Effect(focal.predictors = c(&quot;gevocab&quot;, &quot;age&quot;), # variables involved in the interaction mod = fit_read_3ml, xlevels = list(age = c(84, 108, 132))) %&gt;% # age is in months data.frame() %&gt;% mutate(age_yr = factor(age/12)) %&gt;% # it would be nice to plot age in years ggplot(aes(x = gevocab, y = fit, color = age_yr)) + geom_line() + theme_bw() There is a positive association between vocabulary and reading, but it is strongest for older childred. Among younger children, reading scores are more stable across vocabulary differences. 4.6 MLM - Step 3: Higher-level explanatory variables, fixed, ML School enrollment (senroll) applies to each school as a whole. When a variable is measured at a higher level, all units in the same group have the same value. In this case, all student in the same school have the same value for senroll. fit_read_4ml &lt;- lme4::lmer(geread ~ gevocab*age + senroll + (1 | school), data = data_achieve, REML = FALSE) # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(fit_read_0ml, fit_read_3ml, fit_read_4ml), custom.model.names = c(&quot;Null&quot;, &quot;Level 1 only&quot;, &quot;Level 2 Pred&quot;), caption = &quot;MLM: Investigate a Fixed School-Level Predictor&quot;, caption.above = TRUE, single.row = TRUE) MLM: Investigate a Fixed School-Level Predictor Null Level 1 only Level 2 Pred (Intercept) 4.31 (0.05)*** 5.19 (0.87)*** 5.24 (0.87)*** gevocab -0.03 (0.19) -0.03 (0.19) age -0.03 (0.01)*** -0.03 (0.01)*** gevocab:age 0.01 (0.00)** 0.01 (0.00)** senroll -0.00 (0.00) AIC 46270.34 43122.57 43124.31 BIC 46292.06 43166.02 43175.01 Log Likelihood -23132.17 -21555.28 -21555.16 Num. obs. 10320 10320 10320 Num. groups: school 160 160 160 Var: school (Intercept) 0.39 0.10 0.10 Var: Residual 5.05 3.76 3.76 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 4.6.1 Assess Significance of Effects Likelihood Ratio Test (LRT) anova(fit_read_0ml, fit_read_3ml, fit_read_4ml) Data: data_achieve Models: fit_read_0ml: geread ~ 1 + (1 | school) fit_read_3ml: geread ~ gevocab * age + (1 | school) fit_read_4ml: geread ~ gevocab * age + senroll + (1 | school) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) fit_read_0ml 3 46270 46292 -23132 46264 fit_read_3ml 6 43123 43166 -21555 43111 3153.7701 3 &lt;2e-16 fit_read_4ml 7 43124 43175 -21555 43110 0.2548 1 0.6137 fit_read_0ml fit_read_3ml *** fit_read_4ml --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 School enrollment (or size) does not seem be related to reading scores. 4.7 MLM - Step 4: Explanatory variables predict Slopes, random, REML Random Coefficient models - decompose the SLOPE variance BETWEEN groups. The fixed effect of the predictor captures the overall association it has with the outcome (intercept), while the random effect of the predictor captures the group-to-group variation in the association (slope). Note: A variable can be fit as BOTH a fixed and random effect. fit_read_3re &lt;- lme4::lmer(geread ~ gevocab*age + (1 | school), # refit the previous &#39;best&#39; model via REML data = data_achieve, REML = TRUE) #fit_read_5re &lt;- lmer(geread ~ gevocab + (gevocab | school), # data = achieve, # REML = TRUE) # failed to converge :( fit_read_5re &lt;- lme4::lmer(geread ~ gevocab*age + (gevocab | school), data = data_achieve, REML = TRUE, control = lmerControl(optimizer = &quot;optimx&quot;, # get it to converge calc.derivs = FALSE, optCtrl = list(method = &quot;nlminb&quot;, starttests = FALSE, kkt = FALSE))) # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(fit_read_3re, fit_read_5re), custom.model.names = c(&quot;Rand Int&quot;, &quot;Rand Int and Slopes&quot;), caption = &quot;MLM: Investigate Random Effects&quot;, caption.above = TRUE, single.row = TRUE) MLM: Investigate Random Effects Rand Int Rand Int and Slopes (Intercept) 5.19 (0.87)*** 5.61 (0.87)*** gevocab -0.03 (0.19) -0.14 (0.19) age -0.03 (0.01)*** -0.03 (0.01)*** gevocab:age 0.01 (0.00)** 0.01 (0.00)*** AIC 43155.49 43011.65 BIC 43198.95 43069.58 Log Likelihood -21571.75 -21497.82 Num. obs. 10320 10320 Num. groups: school 160 160 Var: school (Intercept) 0.10 0.28 Var: Residual 3.76 3.66 Var: school gevocab 0.02 Cov: school (Intercept) gevocab -0.06 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 4.7.0.1 Assess Significance of Effect Likelihood Ratio Test (LRT) for Random Effects You can use the Chi-squared LRT test based on deviances even though we fit our modesl with REML, since the models only differ in terms of including/exclusing of a random effects; they have same fixed effects. Just make sure to include the refit = FALSE option. anova(fit_read_3re, fit_read_5re, refit = FALSE) Data: data_achieve Models: fit_read_3re: geread ~ gevocab * age + (1 | school) fit_read_5re: geread ~ gevocab * age + (gevocab | school) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) fit_read_3re 6 43155 43199 -21572 43143 fit_read_5re 8 43012 43070 -21498 42996 147.84 2 &lt; 2.2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 There is evidence the effect child vocabulary has on reading varies across schools. 4.7.0.2 Visualize the Model What does the model look like? effects::Effect(focal.predictors = c(&quot;gevocab&quot;, &quot;age&quot;), mod = fit_read_5re, # just different model xlevels = list(age = c(84, 108, 132))) %&gt;% data.frame() %&gt;% dplyr::mutate(age_yr = factor(age/12)) %&gt;% ggplot(aes(x = gevocab, y = fit, color = age_yr)) + geom_line() + theme_bw() We are seeming much the same trends, but perhaps more separation between the lines. 4.8 MLM - Step 5: Cross-Level interactions between explanatory variables - fixed, ML Cross-level interacitons involve variables at different levels. Here we will investigate the school-level enrollment moderating vocabulary’s effect since we say that vocab’s effect differs across schools (step 4). Remember that an interaction beween fixed effects is also fixed. fit_read_5ml &lt;- lme4::lmer(geread ~ gevocab*age + (gevocab | school), data = data_achieve, REML = FALSE, control = lmerControl(optimizer = &quot;optimx&quot;, calc.derivs = FALSE, optCtrl = list(method = &quot;nlminb&quot;, starttests = FALSE, kkt = FALSE))) fit_read_6ml &lt;- lme4::lmer(geread ~ gevocab*age + senroll + (gevocab | school), data = data_achieve, REML = FALSE) fit_read_7ml &lt;- lme4::lmer(geread ~ gevocab*age + gevocab*senroll + (gevocab | school), data = data_achieve, REML = FALSE) fit_read_8ml &lt;- lme4::lmer(geread ~ gevocab*age*senroll + (gevocab | school), data = data_achieve, REML = FALSE) If you get thelmer() message: Some predictor variables are on very different scales: consider rescaling, you can trust your results, but you really should try re-scaling your variables. We are getting this message since gevoab is on mostly a single digit scale,0 to 11.2, and age (in months) ranges in the low thripe-digits, 82 through 135, while school enrollment is in the mid-hundreds, 112-916. When we compute the interactions we get much, much larger values. Having variables on such widely different ranges of values can cause estimation problems. data_achieve %&gt;% dplyr::mutate(gevocab_x_age = gevocab*age, gevocab_x_senroll = gevocab*senroll, gevocab_x_age_x_senroll = gevocab*senroll*age) %&gt;% dplyr::select(gevocab, age, senroll, gevocab_x_age, gevocab_x_senroll, gevocab_x_age_x_senroll) %&gt;% summary() gevocab age senroll gevocab_x_age Min. : 0.000 Min. : 82.0 Min. :115.0 Min. : 0.0 1st Qu.: 2.900 1st Qu.:104.0 1st Qu.:438.0 1st Qu.: 310.3 Median : 3.800 Median :107.0 Median :519.0 Median : 408.0 Mean : 4.494 Mean :107.5 Mean :533.4 Mean : 482.5 3rd Qu.: 5.200 3rd Qu.:111.0 3rd Qu.:644.0 3rd Qu.: 560.0 Max. :11.200 Max. :135.0 Max. :916.0 Max. :1355.2 gevocab_x_senroll gevocab_x_age_x_senroll Min. : 0 Min. : 0 1st Qu.: 1378 1st Qu.: 148054 Median : 1998 Median : 214379 Mean : 2401 Mean : 257821 3rd Qu.: 2970 3rd Qu.: 319752 Max. :10259 Max. :1149030 For now, let us look at the results. # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(fit_read_3ml, fit_read_6ml, fit_read_7ml, fit_read_8ml), custom.model.names = c(&quot;Level 1 only&quot;, &quot;Both Levels&quot;, &quot;Cross-Level&quot;, &quot;3-way&quot;), caption = &quot;MLM: Investigate a Fixed Cross-level Interaction&quot;, caption.above = TRUE, single.row = TRUE) MLM: Investigate a Fixed Cross-level Interaction Level 1 only Both Levels Cross-Level 3-way (Intercept) 5.19 (0.87)*** 5.60 (0.88)*** 5.52 (0.89)*** 5.50 (3.11) gevocab -0.03 (0.19) -0.14 (0.19) -0.11 (0.20) 0.53 (0.66) age -0.03 (0.01)*** -0.03 (0.01)*** -0.03 (0.01)*** -0.03 (0.03) gevocab:age 0.01 (0.00)** 0.01 (0.00)*** 0.01 (0.00)*** 0.00 (0.01) senroll 0.00 (0.00) 0.00 (0.00) 0.00 (0.01) gevocab:senroll -0.00 (0.00) -0.00 (0.00) age:senroll -0.00 (0.00) gevocab:age:senroll 0.00 (0.00) AIC 43122.57 42981.70 42983.44 42982.85 BIC 43166.02 43046.87 43055.86 43069.75 Log Likelihood -21555.28 -21481.85 -21481.72 -21479.43 Num. obs. 10320 10320 10320 10320 Num. groups: school 160 160 160 160 Var: school (Intercept) 0.10 0.27 0.27 0.27 Var: Residual 3.76 3.66 3.66 3.66 Var: school gevocab 0.02 0.02 0.02 Cov: school (Intercept) gevocab -0.06 -0.06 -0.06 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 There is no evidence school enrollment moderates either of age or vocabulary’s effects. 4.8.0.1 Assess Significance of Effects Likelihood Ratio Test (LRT) When you have a list of sequentially nested models, you can test them in order with one call to the anova() funtion. anova(fit_read_3ml, fit_read_6ml, fit_read_7ml, fit_read_8ml) Data: data_achieve Models: fit_read_3ml: geread ~ gevocab * age + (1 | school) fit_read_6ml: geread ~ gevocab * age + senroll + (gevocab | school) fit_read_7ml: geread ~ gevocab * age + gevocab * senroll + (gevocab | school) fit_read_8ml: geread ~ gevocab * age * senroll + (gevocab | school) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) fit_read_3ml 6 43123 43166 -21555 43111 fit_read_6ml 9 42982 43047 -21482 42964 146.8710 3 &lt;2e-16 *** fit_read_7ml 10 42983 43056 -21482 42963 0.2534 1 0.6147 fit_read_8ml 12 42983 43070 -21479 42959 4.5932 2 0.1006 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 4.9 Centering Predictors: Change Center Centering variables measured on the lowest level only involves subtacting the mean from every value. The spread or standard deviation is not changed. Although there are functions to automatically center and standardize variables, it is beneficial to manually create these variables, as it is more transparent and facilitates un-centering them later. 4.9.1 Compute the Grand Means # Knit to Website: output = &quot;html&quot; # Knit to PDF: output = &quot;latex2&quot; # View on Screen: output = &quot;&quot;text&quot;, or &quot;markdown&quot;, &quot;html&quot; data_achieve %&gt;% furniture::table1(gevocab, age, senroll, output = &quot;html&quot;, digits = 4) Mean/Count (SD/%) n = 10320 gevocab 4.4938 (2.3679) age 107.5290 (5.0596) senroll 533.4148 (154.7968) 4.9.1.1 Subtract the Grand Mean Subract the grand-mean from each observation’s vale to create grand-mean centered variables. data_achieve_center &lt;- data_achieve %&gt;% dplyr::mutate(gevocab_c = gevocab - 4.4938) %&gt;% dplyr::mutate(age_c = age - 107.5290) %&gt;% dplyr::mutate(senroll_c = senroll - 533.4148) 4.9.1.2 Compare the centered and uncentered measures View the first and last few observations: data_achieve_center %&gt;% dplyr::select(id, school, gevocab, gevocab_c, age, age_c, senroll, senroll_c) %&gt;% headTail() id school gevocab gevocab_c age age_c senroll senroll_c 1 1 767 3.1 -1.39 104 -3.53 463 -70.41 2 2 767 2.8 -1.69 106 -1.53 463 -70.41 3 3 767 1.7 -2.79 112 4.47 463 -70.41 4 4 767 2.1 -2.39 109 1.47 463 -70.41 5 &lt;NA&gt; &lt;NA&gt; ... ... ... ... ... ... 6 7893 9085 4.7 0.21 108 0.47 603 69.59 7 7894 9085 3.1 -1.39 108 0.47 603 69.59 8 7895 9085 3.8 -0.69 114 6.47 603 69.59 9 7896 9085 3 -1.49 106 -1.53 603 69.59 Compare the summary statistcs: # Knit to Website: output = &quot;html&quot; # Knit to PDF: output = &quot;latex2&quot; # View on Screen: output = &quot;&quot;text&quot;, or &quot;markdown&quot;, &quot;html&quot; data_achieve_center %&gt;% furniture::table1(gevocab, gevocab_c, age, age_c, senroll, senroll_c, output = &quot;html&quot;, digits = 4) Mean/Count (SD/%) n = 10320 gevocab 4.4938 (2.3679) gevocab_c 0.0000 (2.3679) age 107.5290 (5.0596) age_c -0.0000 (5.0596) senroll 533.4148 (154.7968) senroll_c 0.0000 (154.7968) 4.9.2 Use Centered Variables NOTE: The models with CENTERED variables are able to be fit with the default optimizer settings and do not return the error: “unable to evaluate scaled gradientModel failed to converge: degenerate Hessian with 1 negative eigenvalues” fit_read_9ml &lt;- lme4::lmer(geread ~ gevocab + age + (gevocab | school), data = data_achieve_center, REML = FALSE, control = lmerControl(optimizer = &quot;optimx&quot;, calc.derivs = FALSE, optCtrl = list(method = &quot;nlminb&quot;, starttests = FALSE, kkt = FALSE))) fit_read_10ml &lt;- lme4::lmer(geread ~ gevocab*age + (gevocab | school), data = data_achieve_center, REML = FALSE, control = lmerControl(optimizer = &quot;optimx&quot;, calc.derivs = FALSE, optCtrl = list(method = &quot;nlminb&quot;, starttests = FALSE, kkt = FALSE))) fit_read_9ml_c &lt;- lme4::lmer(geread ~ gevocab_c + age_c + (gevocab_c | school), data = data_achieve_center, REML = FALSE) fit_read_10ml_c &lt;- lme4::lmer(geread ~ gevocab_c*age_c + (gevocab_c | school), data = data_achieve_center, REML = FALSE) 4.9.2.1 Compare the Models # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(fit_read_9ml, fit_read_10ml, fit_read_9ml_c, fit_read_10ml_c), custom.model.names = c(&quot;Main Effects&quot;, &quot;Interaction&quot;, &quot;Main Effects&quot;, &quot;Interaction&quot;), groups = list(&quot;Raw Scale&quot; = 2:4, &quot;Mean Centered&quot; = 5:7), caption = &quot;MLM: Investigate Centering Variables Involved in an Interaction&quot;, caption.above = TRUE, single.row = TRUE) MLM: Investigate Centering Variables Involved in an Interaction Main Effects Interaction Main Effects Interaction (Intercept) 2.98 (0.41)*** 5.61 (0.87)*** 4.34 (0.03)*** 4.35 (0.03)*** Raw Scale gevocab 0.52 (0.01)*** -0.14 (0.19) age -0.01 (0.00)* -0.03 (0.01)*** gevocab:age 0.01 (0.00)*** Mean Centered gevocab_c 0.52 (0.01)*** 0.52 (0.01)*** age_c -0.01 (0.00)* -0.01 (0.00) gevocab_c:age_c 0.01 (0.00)*** AIC 42989.44 42979.71 42989.44 42979.71 BIC 43040.13 43037.65 43040.13 43037.65 Log Likelihood -21487.72 -21481.86 -21487.72 -21481.86 Num. obs. 10320 10320 10320 10320 Num. groups: school 160 160 160 160 Var: school (Intercept) 0.27 0.27 0.10 0.10 Var: school gevocab 0.02 0.02 Cov: school (Intercept) gevocab -0.06 -0.06 Var: Residual 3.66 3.66 3.66 3.66 Var: school gevocab_c 0.02 0.02 Cov: school (Intercept) gevocab_c 0.02 0.02 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 Notice that the interactions yield the exact same parameter estimates and significances, but the main effects (including the interactions) are different. Model fit statistics include \\(-2LL\\) are exactly the same, too. anova(fit_read_9ml, fit_read_10ml) Data: data_achieve_center Models: fit_read_9ml: geread ~ gevocab + age + (gevocab | school) fit_read_10ml: geread ~ gevocab * age + (gevocab | school) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) fit_read_9ml 7 42989 43040 -21488 42975 fit_read_10ml 8 42980 43038 -21482 42964 11.726 1 0.0006162 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(fit_read_9ml_c, fit_read_10ml_c) Data: data_achieve_center Models: fit_read_9ml_c: geread ~ gevocab_c + age_c + (gevocab_c | school) fit_read_10ml_c: geread ~ gevocab_c * age_c + (gevocab_c | school) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) fit_read_9ml_c 7 42989 43040 -21488 42975 fit_read_10ml_c 8 42980 43038 -21482 42964 11.726 1 0.0006162 fit_read_9ml_c fit_read_10ml_c *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 4.9.2.2 Visualize the Model What does the model look like? First plot the model fit to the centered variables with all defaut settings. effects::Effect(focal.predictors = c(&quot;gevocab_c&quot;, &quot;age_c&quot;), mod = fit_read_10ml_c) %&gt;% data.frame() %&gt;% dplyr::mutate(age_c = factor(age_c)) %&gt;% ggplot(aes(x = gevocab_c, y = fit, color = age_c)) + geom_line() + theme_bw() Notice that the vocab and ages are centered at zero. This makes it hard to read the plot, especially if we are unsure what the means of the variables were. Plot the model again, but this time UN-centering the variables by adding back the grand-mean we subtracted prior to fitting the model. effects::Effect(focal.predictors = c(&quot;gevocab_c&quot;, &quot;age_c&quot;), mod = fit_read_10ml_c, xlevels = list(age_c = c(84, 108, 132) - 107.5290)) %&gt;% # add back the mean of age data.frame() %&gt;% dplyr::mutate(age_yrs = factor((age_c + 107.5290)/12)) %&gt;% # add back the mean of age dplyr::mutate(gevocab = gevocab_c + 4.4938) %&gt;% # add back the mean of vocab ggplot(aes(x = gevocab, y = fit, color = age_yrs)) + geom_line() + theme_bw() Now the plot is back on to the origial variables. 4.10 Rescaling Predictors: Change Units or Standardize Where centering variables involved subtracting a set value, scalling a varaibles involves dividing by a set amount. When we both center to the mean and divide by the standard deviation, the new resulting varaible is said to be standardized (not to be confusing with normalizing, which is does not do). To retain meaningful units, you can multiply or divide all the measured values of a variable by a set amount, like a multiple of 10. This retains the meaning behind the units while still bringing them into line with other variables in the model and can avoid some convergence issues. 4.10.1 Scale Varaibles 4.10.1.1 Divide by a Meaningful Value data_achieve_center %&gt;% dplyr::select(senroll_c, ses) %&gt;% summary() senroll_c ses Min. :-418.41 Min. : 0.00 1st Qu.: -95.41 1st Qu.: 66.30 Median : -14.41 Median : 81.70 Mean : 0.00 Mean : 72.85 3rd Qu.: 110.59 3rd Qu.: 87.80 Max. : 382.59 Max. :100.00 For this situation, lets both center at the mean and scale (by 100) the school enrollemnt variable. For SES, lets only divide by ten. data_achieve_center_scale &lt;- data_achieve_center %&gt;% dplyr::mutate(senroll_ch = senroll_c / 100) %&gt;% # centered AND divided by one hundred dplyr::mutate(ses_t = ses / 10) # JUST divide by ten 4.10.1.2 Compare the scaled and unscaled measures View the first and last few observations: data_achieve_center_scale %&gt;% dplyr::select(id, school, age_c, gevocab_c, senroll, senroll_c, senroll_ch, ses, ses_t) %&gt;% headTail() id school age_c gevocab_c senroll senroll_c senroll_ch ses ses_t 1 1 767 -3.53 -1.39 463 -70.41 -0.7 80.4 8.04 2 2 767 -1.53 -1.69 463 -70.41 -0.7 80.4 8.04 3 3 767 4.47 -2.79 463 -70.41 -0.7 80.4 8.04 4 4 767 1.47 -2.39 463 -70.41 -0.7 80.4 8.04 5 &lt;NA&gt; &lt;NA&gt; ... ... ... ... ... ... ... 6 7893 9085 0.47 0.21 603 69.59 0.7 84.4 8.44 7 7894 9085 0.47 -1.39 603 69.59 0.7 84.4 8.44 8 7895 9085 6.47 -0.69 603 69.59 0.7 84.4 8.44 9 7896 9085 -1.53 -1.49 603 69.59 0.7 84.4 8.44 Compare the summary statistcs: # Knit to Website: output = &quot;html&quot; # Knit to PDF: output = &quot;latex2&quot; # View on Screen: output = &quot;&quot;text&quot;, or &quot;markdown&quot;, &quot;html&quot; data_achieve_center_scale %&gt;% furniture::table1(senroll, senroll_c, senroll_ch, ses, ses_t, output = &quot;html&quot;, digits = 4) Mean/Count (SD/%) n = 10320 senroll 533.4148 (154.7968) senroll_c 0.0000 (154.7968) senroll_ch 0.0000 (1.5480) ses 72.8490 (21.9817) ses_t 7.2849 (2.1982) 4.10.2 Use Scaled Variables Using the new versions of our variables, investigate is SES has an effect, either in 2-way or 3-way interactions with age and vocabulary. fit_read_10ml_s &lt;- lme4::lmer(geread ~ gevocab_c*age_c + # no SES (gevocab_c | school), data = data_achieve_center_scale, REML = FALSE) fit_read_11ml_s &lt;- lme4::lmer(geread ~ gevocab_c*age_c + gevocab_c*ses_t + # 2 2-way interactions (gevocab_c | school), data = data_achieve_center_scale, REML = FALSE) fit_read_12ml_s &lt;- lme4::lmer(geread ~ gevocab_c*age_c*ses_t + # 3-way interaction (gevocab_c | school), data = data_achieve_center_scale, REML = FALSE) # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(fit_read_10ml_s, fit_read_11ml_s, fit_read_12ml_s), custom.model.names = c(&quot;no SES&quot;, &quot;2-way&quot;, &quot;3-way&quot;), caption = &quot;MLM: Investigate More Complex Fixed Interactions&quot;, caption.above = TRUE, single.row = TRUE) MLM: Investigate More Complex Fixed Interactions no SES 2-way 3-way (Intercept) 4.35 (0.03)*** 3.94 (0.11)*** 3.94 (0.11)*** gevocab_c 0.52 (0.01)*** 0.67 (0.05)*** 0.67 (0.05)*** age_c -0.01 (0.00) -0.00 (0.00) -0.01 (0.01) gevocab_c:age_c 0.01 (0.00)*** 0.01 (0.00)** 0.00 (0.01) ses_t 0.06 (0.01)*** 0.06 (0.01)*** gevocab_c:ses_t -0.02 (0.01)** -0.02 (0.01)** age_c:ses_t 0.00 (0.00) gevocab_c:age_c:ses_t 0.00 (0.00) AIC 42979.71 42928.81 42932.05 BIC 43037.65 43001.23 43018.95 Log Likelihood -21481.86 -21454.41 -21454.02 Num. obs. 10320 10320 10320 Num. groups: school 160 160 160 Var: school (Intercept) 0.10 0.08 0.08 Var: school gevocab_c 0.02 0.02 0.02 Cov: school (Intercept) gevocab_c 0.02 0.03 0.03 Var: Residual 3.66 3.66 3.66 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 anova(fit_read_10ml_s, fit_read_11ml_s, fit_read_12ml_s) Data: data_achieve_center_scale Models: fit_read_10ml_s: geread ~ gevocab_c * age_c + (gevocab_c | school) fit_read_11ml_s: geread ~ gevocab_c * age_c + gevocab_c * ses_t + (gevocab_c | fit_read_11ml_s: school) fit_read_12ml_s: geread ~ gevocab_c * age_c * ses_t + (gevocab_c | school) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) fit_read_10ml_s 8 42980 43038 -21482 42964 fit_read_11ml_s 10 42929 43001 -21454 42909 54.8974 2 1.2e-12 fit_read_12ml_s 12 42932 43019 -21454 42908 0.7674 2 0.6813 fit_read_10ml_s fit_read_11ml_s *** fit_read_12ml_s --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 There is evidence that SES moderates the main effect of vocabualry, after accounting for the interaction between age and vocabulary. 4.11 Final Model Always refit the final model via REML. fit_read_11re_s &lt;- lme4::lmer(geread ~ gevocab_c*age_c + gevocab_c*ses_t + # 2 2-way interactions (gevocab_c | school), data = data_achieve_center_scale, REML = TRUE) 4.11.1 Table # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(fit_read_11re_s), caption = &quot;MLM: Final Model&quot;, caption.above = TRUE, single.row = TRUE) MLM: Final Model Model 1 (Intercept) 3.94 (0.11)*** gevocab_c 0.67 (0.05)*** age_c -0.00 (0.00) ses_t 0.06 (0.01)*** gevocab_c:age_c 0.01 (0.00)** gevocab_c:ses_t -0.02 (0.01)** AIC 42976.51 BIC 43048.93 Log Likelihood -21478.26 Num. obs. 10320 Num. groups: school 160 Var: school (Intercept) 0.08 Var: school gevocab_c 0.02 Cov: school (Intercept) gevocab_c 0.03 Var: Residual 3.66 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 4.11.1.1 Visualize the Model Recall the scales that the revised variables are now on: data_achieve_center_scale %&gt;% dplyr::select(gevocab_c, age_c, ses_t) %&gt;% summary() gevocab_c age_c ses_t Min. :-4.493800 Min. :-25.529000 Min. : 0.000 1st Qu.:-1.593800 1st Qu.: -3.529000 1st Qu.: 6.630 Median :-0.693800 Median : -0.529000 Median : 8.170 Mean : 0.000044 Mean : -0.000027 Mean : 7.285 3rd Qu.: 0.706200 3rd Qu.: 3.471000 3rd Qu.: 8.780 Max. : 6.706200 Max. : 27.471000 Max. :10.000 effects::Effect(focal.predictors = c(&quot;gevocab_c&quot;, &quot;age_c&quot;, &quot;ses_t&quot;), mod = fit_read_11re_s, xlevels = list(age_c = c(84, 108, 132) - 107.5290, ses_t = c(0, 5, 10))) %&gt;% data.frame() %&gt;% dplyr::mutate(age_yrs = factor((age_c + 107.5290)/12)) %&gt;% dplyr::mutate(gevocab = gevocab_c + 4.4938) %&gt;% dplyr::mutate(ses = factor(ses_t * 10)) %&gt;% ggplot(aes(x = gevocab, y = fit, color = age_yrs)) + geom_line() + theme_bw() + facet_grid(.~ ses, labeller = &quot;label_both&quot;) + labs(x = &quot;Vocabulary Score&quot;, y = &quot;Reading Score\\nEstimated Marginal Mean&quot;, color = &quot;Student Age&quot;) There is evidence that higher vocabulary scores correlate with higher reading scores. This relationship os strongest in low SES schools and among older students. This relationship is especially weaker in younger students attending high SES schools. See: Nakagawa and Schielzeth (2013) for more regarding model \\(R^2\\) From the sjstats::r2() documentation, for mixed models: marginal r-squared considers only the variance of the fixed effects conditional r-squared takes both the fixed and random effects into account sjstats::r2(fit_read_11re_s) R-Squared for Generalized Linear Mixed Model Family : gaussian (identity) Formula: geread ~ gevocab_c * age_c + gevocab_c * ses_t + (gevocab_c | school) Marginal R2: 0.289 Conditional R2: 0.322 Helpful links: http://maths-people.anu.edu.au/~johnm/r-book/xtras/mlm-ohp.pdf http://ase.tufts.edu/gsc/gradresources/guidetomixedmodelsinr/mixed%20model%20guide.html http://web.stanford.edu/class/psych252/section_2015/Section_week9.html https://www.r-bloggers.com/visualizing-generalized-linear-mixed-effects-models-with-ggplot-rstats-lme4/ https://www.r-bloggers.com/visualizing-generalized-linear-mixed-effects-models-part-2-rstats-lme4/ http://www.strengejacke.de/sjPlot/sjp.lmer/ "],
["centering-and-standardizing-explanitory-variables.html", "5 Centering and Standardizing Explanitory Variables 5.1 Background 5.2 Grand-Mean-Centering and Standardizing Variables 5.3 RI = ONLY Random Intercepts 5.4 RIAS = Random Intercepts AND Slopes", " 5 Centering and Standardizing Explanitory Variables library(tidyverse) library(haven) # read in SPSS dataset library(furniture) # nice table1() descriptives library(stargazer) # display nice tables: summary &amp; regression library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(RColorBrewer) # nice color palettes for plots library(gridExtra) # place ggplots together as one plot library(psych) # contains some useful functions, like headTail library(car) # Companion to Applied Regression library(nlme) # non-linear mixed-effects models library(lme4) # Linear, generalized linear, &amp; nonlinear mixed models library(lmerTest) # Tests on lmer objects library(HLMdiag) # Diagnostic Tools for for nlme &amp; lmer4 library(sjstats) # ICC calculations library(optimx) # Different optimizers to solve mlm&#39;s 5.1 Background The text “Multilevel Analysis: Techniques and Applications, Third Edition” (Hox, Moerbeek, and Van de Schoot 2017) has a companion website which includes links to all the data files used throughout the book (housed on the book’s GitHub repository). The following example is used through out Hox, Moerbeek, and Van de Schoot (2017)’s chapater 2. From Appendix E: The popularity data in popular2.sav are simulated data for 2000 pupils in 100 schools. The purpose is to offer a very simple example for multilevel regression analysis. The main outcome variable is the pupil popularity, a popularity rating on a scale of 1-10 derived by a sociometric procedure. Typically, a sociometric procedure asks all pupils in a class to rate all the other pupils, and then assigns the average received popularity rating to each pupil. Because of the sociometric procedure, group effects as apparent from higher level variance components are rather strong. There is a second outcome variable: pupil popularity as rated by their teacher, on a scale from 1-10. The explanatory variables are pupil gender (boy=0, girl=1), pupil extraversion (10-point scale) and teacher experience in years. The popularity data have been generated to be a ‘nice’ well-behaved data set: the sample sizes at both levels are sufficient, the residuals have a normal distribution, and the multilevel effects are strong. data_raw &lt;- haven::read_sav(&quot;https://github.com/MultiLevelAnalysis/Datasets-third-edition-Multilevel-book/raw/master/chapter%202/popularity/SPSS/popular2.sav&quot;) %&gt;% haven::as_factor() # retain the labels from SPSS --&gt; factor data_pop &lt;- data_raw %&gt;% dplyr::mutate(id = paste(class, pupil, sep = &quot;_&quot;) %&gt;% # create a unique id for each student (char) factor()) %&gt;% # declare id is a factor dplyr::select(id, pupil:popteach) # reduce the variables included tibble::glimpse(data_pop) Observations: 2,000 Variables: 8 $ id &lt;fct&gt; 1_1, 1_2, 1_3, 1_4, 1_5, 1_6, 1_7, 1_8, 1_9, 1_10, 1_... $ pupil &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16... $ class &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... $ extrav &lt;dbl&gt; 5, 7, 4, 3, 5, 4, 5, 4, 5, 5, 5, 5, 5, 5, 5, 6, 4, 4,... $ sex &lt;fct&gt; girl, boy, girl, girl, girl, boy, boy, boy, boy, boy,... $ texp &lt;dbl&gt; 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 2... $ popular &lt;dbl&gt; 6.3, 4.9, 5.3, 4.7, 6.0, 4.7, 5.9, 4.2, 5.2, 3.9, 5.7... $ popteach &lt;dbl&gt; 6, 5, 6, 5, 6, 5, 5, 5, 5, 3, 5, 5, 5, 6, 5, 5, 2, 3,... data_pop %&gt;% ggplot() + aes(x = extrav, y = popular, group = class) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;black&quot;, size = .2) + theme_bw() + geom_vline(xintercept = 0, color = &quot;red&quot;) + labs(title = &quot;OLS: Single Level Regression&quot;, subtitle = &quot;Thin black lines are OLS regression ran independently on each class&quot;, x = &quot;Student&#39;s Extroversion, as rated by their teacher&quot;, y = &quot;Student&#39;s Populartity, mean rating by their peers&quot;) + coord_cartesian(xlim = c(0, 10), ylim = c(0, 10)) + scale_x_continuous(breaks = seq(from = 0, to = 10, by = 2)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) 5.2 Grand-Mean-Centering and Standardizing Variables It is best to manually determine the variable’s mean (mean()) and standard deviation (sd()). mean(data_pop$extrav) [1] 5.215 sd(data_pop$extrav) [1] 1.262368 5.2.1 Grand-Mean-Centering \\[ VAR_G = VAR - mean(VAR) \\] 5.2.2 Standardizing \\[ VAR_Z = \\frac{VAR - mean(VAR)}{sd(VAR)} \\] data_pop &lt;- data_pop %&gt;% dplyr::mutate(extravG = extrav - 5.215) %&gt;% dplyr::mutate(extravZ = (extrav - 5.215) / 1.262368) data_pop %&gt;% dplyr::select(extrav, extravG, extravZ) %&gt;% data.frame() %&gt;% stargazer::stargazer(title = &quot;Descriptive statistics: Three versions of Extraversion&quot;, header = FALSE, type = &quot;html&quot;) Descriptive statistics: Three versions of Extraversion Statistic N Mean St. Dev. Min Pctl(25) Pctl(75) Max extrav 2,000 5.215 1.262 1 4 6 10 extravG 2,000 0.000 1.262 -4.215 -1.215 0.785 4.785 extravZ 2,000 0.000 1.000 -3.339 -0.962 0.622 3.790 5.3 RI = ONLY Random Intercepts 5.3.1 Fit MLM with all 3 versions of the predictor pop_lmer_1_raw &lt;- lme4::lmer(popular ~ extrav + (1|class), data = data_pop, REML = FALSE) pop_lmer_1_cen &lt;- lme4::lmer(popular ~ extravG + (1|class), data = data_pop, REML = FALSE) pop_lmer_1_std &lt;- lme4::lmer(popular ~ extravZ + (1|class), data = data_pop, REML = FALSE) # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(pop_lmer_1_raw, pop_lmer_1_cen, pop_lmer_1_std), custom.model.names = c(&quot;Raw&quot;, &quot;Centered&quot;, &quot;Standardized&quot;), caption = &quot;MLM - RI: Effect of Grand-Mean Centering and Standardizing&quot;, caption.above = TRUE, single.row = TRUE) MLM - RI: Effect of Grand-Mean Centering and Standardizing Raw Centered Standardized (Intercept) 2.54 (0.14)*** 5.08 (0.09)*** 5.08 (0.09)*** extrav 0.49 (0.02)*** extravG 0.49 (0.02)*** extravZ 0.61 (0.03)*** AIC 5831.78 5831.78 5831.78 BIC 5854.18 5854.18 5854.18 Log Likelihood -2911.89 -2911.89 -2911.89 Num. obs. 2000 2000 2000 Num. groups: class 100 100 100 Var: class (Intercept) 0.83 0.83 0.83 Var: Residual 0.93 0.93 0.93 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 ** MLM - Random Intercepts ONLY** Grand-Mean Centering a Predictor Different than when using the Raw Predictor: fixed intercept Same as when using the Raw Predictor: fixed estimates or slopes for all predictors (main effects and interactions) random estimates, i.e. variance and covariance components, includin the residual variance model fit statistics, including AIC, BIC, and the Log Loikelihood (-2LL or deviance) Standardize a Predictor Different than when using the Raw Predictor: fixed intercept (same as if using the grand-mean centered predictor) fixed estimate (slope) for that variable Stays the SAME: random estimates, i.e. variance and covariance components, includin the residual variance model fit statistics, including AIC, BIC, and the Log Loikelihood (-2LL or deviance) 5.3.2 Investigating a MLM-RI Model pop_lmer_1_raw %&gt;% broom::tidy() # A tibble: 4 x 5 term estimate std.error statistic group &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 (Intercept) 2.54 0.141 18.1 fixed 2 extrav 0.486 0.0201 24.1 fixed 3 sd_(Intercept).class 0.912 NA NA class 4 sd_Observation.Residual 0.964 NA NA Residual pop_lmer_1_raw %&gt;% broom::glance() # A tibble: 1 x 6 sigma logLik AIC BIC deviance df.residual &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; 1 0.964 -2912. 5832. 5854. 5824. 1996 pop_lmer_1_raw %&gt;% broom::augment() %&gt;% head() # onle line per observation (2000 students) popular extrav class .fitted .resid .cooksd .fixed .mu 1 6.3 5 1 5.138703 1.16129692 2.712828e-05 4.973704 5.138703 2 4.9 7 1 6.111103 -1.21110349 1.752270e-03 5.946104 6.111103 3 5.3 4 1 4.652503 0.64749713 8.856823e-05 4.487503 4.652503 4 4.7 3 1 4.166303 0.53369734 2.626814e-04 4.001303 4.166303 5 6.0 5 1 5.138703 0.86129692 1.492250e-05 4.973704 5.138703 6 4.7 4 1 4.652503 0.04749713 4.765813e-07 4.487503 4.652503 .offset .sqrtXwt .sqrtrwt .weights .wtres 1 0 1 1 1 1.16129692 2 0 1 1 1 -1.21110349 3 0 1 1 1 0.64749713 4 0 1 1 1 0.53369734 5 0 1 1 1 0.86129692 6 0 1 1 1 0.04749713 5.3.2.1 Fixed Effects: intercept and slope There is only ONE fixed intercept and ONE fixed slope. The fixef() function extracts the estimates of the fixed effects. fixef(pop_lmer_1_raw) (Intercept) extrav 2.5427027 0.4862002 fixef(pop_lmer_1_raw)[[&quot;(Intercept)&quot;]] [1] 2.542703 fixef(pop_lmer_1_raw)[[&quot;extrav&quot;]] [1] 0.4862002 data_pop %&gt;% ggplot() + aes(x = extrav, y = popular, group = class) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;black&quot;, size = .2) + geom_abline(intercept = fixef(pop_lmer_1_raw)[[&quot;(Intercept)&quot;]], slope = fixef(pop_lmer_1_raw)[[&quot;extrav&quot;]], color = &quot;hot pink&quot;, size = 2) + theme_bw() + geom_vline(xintercept = 0, color = &quot;red&quot;) + labs(title = &quot;MLM-RI: Extroversion = raw score&quot;, subtitle = &quot;Thin black lines are OLS regression ran independently on each class&quot;, x = &quot;Student&#39;s Extroversion, as rated by their teacher&quot;, y = &quot;Student&#39;s Populartity, mean rating by their peers&quot;) + coord_cartesian(xlim = c(0, 10), ylim = c(0, 10)) + scale_x_continuous(breaks = seq(from = 0, to = 10, by = 2)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) 5.3.2.2 Random Effects: intercepts There is a different random intercept for EACH CLASS. These tell how far each class’s average is off of the grand average. The ranef() function extracts the random effects from a fitted model object ranef(pop_lmer_1_raw) %&gt;% str() List of 1 $ class:&#39;data.frame&#39;: 100 obs. of 1 variable: ..$ (Intercept): num [1:100] 0.165 -0.7536 -0.3646 0.5405 -0.0994 ... - attr(*, &quot;class&quot;)= chr &quot;ranef.mer&quot; ranef(pop_lmer_1_raw)$class %&gt;% head() # onle line per group (100 classes) (Intercept) 1 0.16499938 2 -0.75362983 3 -0.36464658 4 0.54049206 5 -0.09943663 6 -0.60487822 ranef(pop_lmer_1_raw)$class %&gt;% dplyr::rename(Random_Intercepts = &quot;(Intercept)&quot;) %&gt;% ggplot(aes(Random_Intercepts)) + geom_histogram(binwidth = .25) 5.3.2.3 Predictions predict(pop_lmer_1_raw) %&gt;% str() Named num [1:2000] 5.14 6.11 4.65 4.17 5.14 ... - attr(*, &quot;names&quot;)= chr [1:2000] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... predict(pop_lmer_1_raw) %&gt;% head() # onle value per observation (2000 students) 1 2 3 4 5 6 5.138703 6.111103 4.652503 4.166303 5.138703 4.652503 data_pop %&gt;% dplyr::mutate(pred = predict(pop_lmer_1_raw)) %&gt;% ggplot(aes(x = extrav, y = pred, group = class)) + geom_line(size = .2) + geom_abline(intercept = fixef(pop_lmer_1_raw)[[&quot;(Intercept)&quot;]], slope = fixef(pop_lmer_1_raw)[[&quot;extrav&quot;]], color = &quot;hot pink&quot;, size = 2) + theme_bw() + geom_vline(xintercept = 0, color = &quot;red&quot;) + geom_vline(xintercept = 5.215, color = &quot;blue&quot;) + labs(title = &quot;MLM-RI: Extroversion = raw score&quot;, subtitle = &quot;Thin black lines are group-wise predictions, one per class&quot;, x = &quot;Student&#39;s Extroversion, as rated by their teacher&quot;, y = &quot;Predicted\\nStudent&#39;s Populartity, mean rating by their peers&quot;) + coord_cartesian(xlim = c(0, 10), ylim = c(0, 10)) + scale_x_continuous(breaks = seq(from = 0, to = 10, by = 2)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) 5.3.2.4 Combined Effects The coef() function computes the sum of the random and fixed effects coefficients for each explanatory variable for each level of each grouping factor. coef(pop_lmer_1_raw) %&gt;% str() List of 1 $ class:&#39;data.frame&#39;: 100 obs. of 2 variables: ..$ (Intercept): num [1:100] 2.71 1.79 2.18 3.08 2.44 ... ..$ extrav : num [1:100] 0.486 0.486 0.486 0.486 0.486 ... - attr(*, &quot;class&quot;)= chr &quot;coef.mer&quot; coef(pop_lmer_1_raw)$class %&gt;% head() # onle line per group (100 classes) (Intercept) extrav 1 2.707702 0.4862002 2 1.789073 0.4862002 3 2.178056 0.4862002 4 3.083195 0.4862002 5 2.443266 0.4862002 6 1.937824 0.4862002 data_pop %&gt;% dplyr::mutate(pred = predict(pop_lmer_1_raw)) %&gt;% ggplot() + aes(x = extrav, y = pred, group = class) + geom_rect(aes(xmin = 0 - 0.25, xmax = 0 + 0.25, ymin = fixef(pop_lmer_1_raw)[[&quot;(Intercept)&quot;]]- 2.5, ymax = fixef(pop_lmer_1_raw)[[&quot;(Intercept)&quot;]]+ 2.5), fill = &quot;yellow&quot;, alpha = 0.05) + geom_abline(data = coef(pop_lmer_1_raw)$class %&gt;% dplyr::rename(Intercept = &quot;(Intercept)&quot;), aes(intercept = Intercept, slope = extrav), color = &quot;gray&quot;, size = .1) + geom_line(size = .2) + geom_abline(intercept = fixef(pop_lmer_1_raw)[[&quot;(Intercept)&quot;]], slope = fixef(pop_lmer_1_raw)[[&quot;extrav&quot;]], color = &quot;hot pink&quot;, size = 2) + theme_bw() + geom_vline(xintercept = 0, color = &quot;red&quot;) + geom_vline(xintercept = 5.215, color = &quot;blue&quot;) + labs(title = &quot;MLM-RI: Extroversion = raw score&quot;, subtitle = &quot;Thin black lines are group-wise predictions, one per class EXTRAPOLATED OUT&quot;, x = &quot;Student&#39;s Extroversion, as rated by their teacher&quot;, y = &quot;Predicted\\nStudent&#39;s Populartity, mean rating by their peers&quot;) + coord_cartesian(xlim = c(0, 10), ylim = c(0, 10)) + scale_x_continuous(breaks = seq(from = 0, to = 10, by = 2)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) 5.3.3 Comapre the Centered Version data_pop %&gt;% dplyr::mutate(pred = predict(pop_lmer_1_cen)) %&gt;% ggplot() + aes(x = extravG, y = pred, group = class) + geom_rect(aes(xmin = -5.215 - 0.25, xmax = -5.215 + 0.25, ymin = fixef(pop_lmer_1_raw)[[&quot;(Intercept)&quot;]]- 2.5, ymax = fixef(pop_lmer_1_raw)[[&quot;(Intercept)&quot;]]+ 2.5), fill = &quot;yellow&quot;, alpha = 0.05) + geom_rect(aes(xmin = 0 - 0.25, xmax = 0 + 0.25, ymin = fixef(pop_lmer_1_cen)[[&quot;(Intercept)&quot;]]- 2.5, ymax = fixef(pop_lmer_1_cen)[[&quot;(Intercept)&quot;]]+ 2.5), fill = &quot;yellow&quot;, alpha = 0.05) + geom_abline(data = coef(pop_lmer_1_cen)$class %&gt;% dplyr::rename(Intercept = &quot;(Intercept)&quot;), aes(intercept = Intercept, slope = extravG), color = &quot;gray&quot;, size = .1) + geom_line(size = .2) + geom_abline(intercept = fixef(pop_lmer_1_cen)[[&quot;(Intercept)&quot;]], slope = fixef(pop_lmer_1_cen)[[&quot;extravG&quot;]], color = &quot;hot pink&quot;, size = 2) + theme_bw() + geom_vline(xintercept = -5.215, color = &quot;blue&quot;) + geom_vline(xintercept = 0, color = &quot;red&quot;) + labs(title = &quot;MLM-RI: Extroversion = grand-mean centered&quot;, subtitle = &quot;Thin black lines are group-wise predictions, one per class EXTRAPOLATED OUT&quot;, x = &quot;GRAND-MEAN CENTERED Student&#39;s Extroversion, as rated by their teacher&quot;, y = &quot;Predicted\\nStudent&#39;s Populartity, mean rating by their peers&quot;) + coord_cartesian(xlim = c(-5, 5), ylim = c(0, 10)) + scale_x_continuous(breaks = seq(from = -4, to = 4, by = 2)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) 5.3.4 Comapre the Standardized Version data_pop %&gt;% dplyr::mutate(pred = predict(pop_lmer_1_std)) %&gt;% ggplot() + aes(x = extravZ, y = pred, group = class) + geom_rect(aes(xmin = 0 - 0.25, xmax = 0 + 0.25, ymin = fixef(pop_lmer_1_cen)[[&quot;(Intercept)&quot;]]- 2.5, ymax = fixef(pop_lmer_1_cen)[[&quot;(Intercept)&quot;]]+ 2.5), fill = &quot;yellow&quot;, alpha = 0.05) + geom_abline(data = coef(pop_lmer_1_std)$class %&gt;% dplyr::rename(Intercept = &quot;(Intercept)&quot;), aes(intercept = Intercept, slope = extravZ), color = &quot;gray&quot;, size = .1) + geom_line(size = .2) + geom_abline(intercept = fixef(pop_lmer_1_std)[[&quot;(Intercept)&quot;]], slope = fixef(pop_lmer_1_std)[[&quot;extravZ&quot;]], color = &quot;hot pink&quot;, size = 2) + theme_bw() + geom_vline(xintercept = 0, color = &quot;red&quot;) + geom_vline(xintercept = -5.215, color = &quot;blue&quot;) + labs(title = &quot;MLM-RI: Extroversion = standardized&quot;, subtitle = &quot;Thin black lines are group-wise predictions, one per class EXTRAPOLATED OUT&quot;, x = &quot;STANDARDIZED Student&#39;s Extroversion, as rated by their teacher&quot;, y = &quot;Predicted\\nStudent&#39;s Populartity, mean rating by their peers&quot;) + coord_cartesian(xlim = c(-5, 5), ylim = c(0, 10)) + scale_x_continuous(breaks = seq(from = -4, to = 4, by = 2)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) 5.4 RIAS = Random Intercepts AND Slopes 5.4.1 Fit MLM with all 3 versions of the predictor pop_lmer_2_raw &lt;- lme4::lmer(popular ~ extrav + (extrav|class), data = data_pop, REML = FALSE) pop_lmer_2_cen &lt;- lme4::lmer(popular ~ extravG + (extravG|class), data = data_pop, REML = FALSE) pop_lmer_2_std &lt;- lme4::lmer(popular ~ extravZ + (extravZ|class), data = data_pop, REML = FALSE) # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(pop_lmer_2_raw, pop_lmer_2_cen, pop_lmer_2_std), custom.model.names = c(&quot;Raw&quot;, &quot;Centered&quot;, &quot;Standardized&quot;), caption = &quot;MLM - RIAS: Effect of Grand-Mean Centering and Standardizing&quot;, caption.above = TRUE, single.row = TRUE) MLM - RIAS: Effect of Grand-Mean Centering and Standardizing Raw Centered Standardized (Intercept) 2.46 (0.20)*** 5.03 (0.10)*** 5.03 (0.10)*** extrav 0.49 (0.03)*** extravG 0.49 (0.03)*** extravZ 0.62 (0.03)*** AIC 5782.69 5782.69 5782.69 BIC 5816.29 5816.29 5816.29 Log Likelihood -2885.34 -2885.34 -2885.34 Num. obs. 2000 2000 2000 Num. groups: class 100 100 100 Var: class (Intercept) 2.95 0.88 0.88 Var: class extrav 0.03 Cov: class (Intercept) extrav -0.26 Var: Residual 0.90 0.90 0.90 Var: class extravG 0.03 Cov: class (Intercept) extravG -0.13 Var: class extravZ 0.04 Cov: class (Intercept) extravZ -0.17 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 ** MLM - Random Intercepts AND Slopes** Grand-Mean Centering a Predictor Different than when using the Raw Predictor: fixed intercept random estimates, i.e. variance and covariance components, includin the residual variance Same as when using the Raw Predictor: fixed estimates or slopes for all predictors (main effects and interactions) model fit statistics, including AIC, BIC, and the Log Loikelihood (-2LL or deviance) Standardize a Predictor Different than when using the Raw Predictor: fixed intercept (same as if using the grand-mean centered predictor) fixed estimate (slope) for that variable random estimates, i.e. variance and covariance components, includin the residual variance Stays the SAME: model fit statistics, including AIC, BIC, and the Log Loikelihood (-2LL or deviance) 5.4.2 Investigating a MLM-RI Model pop_lmer_2_raw %&gt;% broom::tidy() # A tibble: 6 x 5 term estimate std.error statistic group &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 (Intercept) 2.46 0.202 12.2 fixed 2 extrav 0.493 0.0253 19.5 fixed 3 sd_(Intercept).class 1.72 NA NA class 4 sd_extrav.class 0.159 NA NA class 5 cor_(Intercept).extrav.class -0.969 NA NA class 6 sd_Observation.Residual 0.946 NA NA Residual pop_lmer_2_raw %&gt;% broom::glance() # A tibble: 1 x 6 sigma logLik AIC BIC deviance df.residual &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; 1 0.946 -2885. 5783. 5816. 5771. 1994 pop_lmer_2_raw %&gt;% broom::augment() %&gt;% head() # onle line per observation (2000 students) popular extrav class .fitted .resid .cooksd .fixed .mu 1 6.3 5 1 5.131082 1.16891841 3.872950e-05 4.925861 5.131082 2 4.9 7 1 6.062463 -1.16246304 1.546709e-03 5.911665 6.062463 3 5.3 4 1 4.665391 0.63460913 5.582607e-05 4.432959 4.665391 4 4.7 3 1 4.199700 0.50029985 1.873118e-04 3.940057 4.199700 5 6.0 5 1 5.131082 0.86891841 2.140087e-05 4.925861 5.131082 6 4.7 4 1 4.665391 0.03460913 1.660373e-07 4.432959 4.665391 .offset .sqrtXwt .sqrtrwt .weights .wtres 1 0 1 1 1 1.16891841 2 0 1 1 1 -1.16246304 3 0 1 1 1 0.63460913 4 0 1 1 1 0.50029985 5 0 1 1 1 0.86891841 6 0 1 1 1 0.03460913 5.4.2.1 Fixed Effects: intercept and slope There is only ONE fixed intercept and ONE fixed slope. The fixef() function extracts the estimates of the fixed effects. fixef(pop_lmer_2_raw) (Intercept) extrav 2.461351 0.492902 fixef(pop_lmer_2_raw)[[&quot;(Intercept)&quot;]] [1] 2.461351 fixef(pop_lmer_2_raw)[[&quot;extrav&quot;]] [1] 0.492902 data_pop %&gt;% ggplot() + aes(x = extrav, y = popular, group = class) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;black&quot;, size = .2) + geom_abline(intercept = fixef(pop_lmer_2_raw)[[&quot;(Intercept)&quot;]], slope = fixef(pop_lmer_2_raw)[[&quot;extrav&quot;]], color = &quot;hot pink&quot;, size = 2) + theme_bw() + geom_vline(xintercept = 0, color = &quot;red&quot;) + geom_vline(xintercept = 5.215, color = &quot;blue&quot;) + labs(title = &quot;MLM-RIAS: Extroversion = raw score&quot;, subtitle = &quot;Thin black lines are OLS regression ran independently on each class&quot;, x = &quot;Student&#39;s Extroversion, as rated by their teacher&quot;, y = &quot;Student&#39;s Populartity, mean rating by their peers&quot;) + coord_cartesian(xlim = c(0, 10), ylim = c(0, 10)) + scale_x_continuous(breaks = seq(from = 0, to = 10, by = 2)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) 5.4.2.2 Random Effects: intercepts There is a different random intercept AND random slope for EACH CLASS. These tell how far each class’s average is off of the grand average AND how far each class’s slope is off of the grand average sope. The ranef() function extracts the random effects from a fitted model object ranef(pop_lmer_2_raw) %&gt;% str() List of 1 $ class:&#39;data.frame&#39;: 100 obs. of 2 variables: ..$ (Intercept): num [1:100] 0.341 -1.18 -0.627 1.079 -0.191 ... ..$ extrav : num [1:100] -0.0272 0.0974 0.0565 -0.0988 0.0208 ... - attr(*, &quot;class&quot;)= chr &quot;ranef.mer&quot; ranef(pop_lmer_2_raw)$class %&gt;% head() # onle line per group (100 classes) (Intercept) extrav 1 0.3412771 -0.02721131 2 -1.1800352 0.09737305 3 -0.6268933 0.05654048 4 1.0791097 -0.09876733 5 -0.1910488 0.02083082 6 -0.9833296 0.08369927 ranef(pop_lmer_2_raw)$class %&gt;% dplyr::rename(Random_Intercepts = &quot;(Intercept)&quot;) %&gt;% ggplot(aes(Random_Intercepts)) + geom_histogram() ranef(pop_lmer_2_raw)$class %&gt;% ggplot(aes(extrav)) + geom_histogram() 5.4.2.3 Predictions predict(pop_lmer_2_raw) %&gt;% str() Named num [1:2000] 5.13 6.06 4.67 4.2 5.13 ... - attr(*, &quot;names&quot;)= chr [1:2000] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... predict(pop_lmer_2_raw) %&gt;% head() # onle value per observation (2000 students) 1 2 3 4 5 6 5.131082 6.062463 4.665391 4.199700 5.131082 4.665391 data_pop %&gt;% dplyr::mutate(pred = predict(pop_lmer_2_raw)) %&gt;% ggplot(aes(x = extrav, y = pred, group = class)) + geom_line(size = .2) + geom_abline(intercept = fixef(pop_lmer_2_raw)[[&quot;(Intercept)&quot;]], slope = fixef(pop_lmer_2_raw)[[&quot;extrav&quot;]], color = &quot;hot pink&quot;, size = 2) + theme_bw() + geom_vline(xintercept = 0, color = &quot;red&quot;) + geom_vline(xintercept = 5.215, color = &quot;blue&quot;) + labs(title = &quot;MLM-RIAS: Extroversion = raw score&quot;, subtitle = &quot;Thin black lines are group-wise predictions, one per class&quot;, x = &quot;Student&#39;s Extroversion, as rated by their teacher&quot;, y = &quot;Predicted\\nStudent&#39;s Populartity, mean rating by their peers&quot;) + coord_cartesian(xlim = c(0, 10), ylim = c(0, 10)) + scale_x_continuous(breaks = seq(from = 0, to = 10, by = 2)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) 5.4.2.4 Combined Effects The coef() function computes the sum of the random and fixed effects coefficients for each explanatory variable for each level of each grouping factor. coef(pop_lmer_1_raw) %&gt;% str() List of 1 $ class:&#39;data.frame&#39;: 100 obs. of 2 variables: ..$ (Intercept): num [1:100] 2.71 1.79 2.18 3.08 2.44 ... ..$ extrav : num [1:100] 0.486 0.486 0.486 0.486 0.486 ... - attr(*, &quot;class&quot;)= chr &quot;coef.mer&quot; coef(pop_lmer_1_raw)$class %&gt;% head() # onle line per group (100 classes) (Intercept) extrav 1 2.707702 0.4862002 2 1.789073 0.4862002 3 2.178056 0.4862002 4 3.083195 0.4862002 5 2.443266 0.4862002 6 1.937824 0.4862002 data_pop %&gt;% dplyr::mutate(pred = predict(pop_lmer_2_raw)) %&gt;% ggplot() + aes(x = extrav, y = pred, group = class) + geom_rect(aes(xmin = 0 - 0.25, xmax = 0 + 0.25, ymin = fixef(pop_lmer_2_raw)[[&quot;(Intercept)&quot;]]- 4.5, ymax = fixef(pop_lmer_2_raw)[[&quot;(Intercept)&quot;]]+ 4.5), fill = &quot;yellow&quot;, alpha = 0.05) + geom_abline(data = coef(pop_lmer_2_raw)$class %&gt;% dplyr::rename(Intercept = &quot;(Intercept)&quot;), aes(intercept = Intercept, slope = extrav), color = &quot;gray&quot;, size = .1) + geom_line(size = .2) + geom_abline(intercept = fixef(pop_lmer_2_raw)[[&quot;(Intercept)&quot;]], slope = fixef(pop_lmer_2_raw)[[&quot;extrav&quot;]], color = &quot;hot pink&quot;, size = 2) + theme_bw() + geom_vline(xintercept = 0, color = &quot;red&quot;) + geom_vline(xintercept = 5.215, color = &quot;blue&quot;) + labs(title = &quot;MLM-RIAS: Extroversion = raw score&quot;, subtitle = &quot;Thin black lines are group-wise predictions, one per class EXTRAPOLATED OUT&quot;, x = &quot;Student&#39;s Extroversion, as rated by their teacher&quot;, y = &quot;Predicted\\nStudent&#39;s Populartity, mean rating by their peers&quot;) + coord_cartesian(xlim = c(0, 10), ylim = c(0, 10)) + scale_x_continuous(breaks = seq(from = 0, to = 10, by = 2)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) 5.4.3 Comapre the Centered Version data_pop %&gt;% dplyr::mutate(pred = predict(pop_lmer_2_cen)) %&gt;% ggplot() + aes(x = extravG, y = pred, group = class) + geom_rect(aes(xmin = -5.215 - 0.25, xmax = -5.215 + 0.25, ymin = fixef(pop_lmer_2_raw)[[&quot;(Intercept)&quot;]]- 4.5, ymax = fixef(pop_lmer_2_raw)[[&quot;(Intercept)&quot;]]+ 4.5), fill = &quot;yellow&quot;, alpha = 0.05) + geom_rect(aes(xmin = 0 - 0.25, xmax = 0 + 0.25, ymin = fixef(pop_lmer_2_cen)[[&quot;(Intercept)&quot;]]- 2.5, ymax = fixef(pop_lmer_2_cen)[[&quot;(Intercept)&quot;]]+ 2.5), fill = &quot;yellow&quot;, alpha = 0.05) + geom_abline(data = coef(pop_lmer_2_cen)$class %&gt;% dplyr::rename(Intercept = &quot;(Intercept)&quot;), aes(intercept = Intercept, slope = extravG), color = &quot;gray&quot;, size = .1) + geom_line(size = .2) + geom_abline(intercept = fixef(pop_lmer_2_cen)[[&quot;(Intercept)&quot;]], slope = fixef(pop_lmer_2_cen)[[&quot;extravG&quot;]], color = &quot;hot pink&quot;, size = 2) + theme_bw() + geom_vline(xintercept = -5.215, color = &quot;blue&quot;) + geom_vline(xintercept = 0, color = &quot;red&quot;) + labs(title = &quot;MLM-RIAS: Extroversion = grand-mean centered&quot;, subtitle = &quot;Thin black lines are group-wise predictions, one per class EXTRAPOLATED OUT&quot;, x = &quot;GRAND-MEAN CENTERED Student&#39;s Extroversion, as rated by their teacher&quot;, y = &quot;Predicted\\nStudent&#39;s Populartity, mean rating by their peers&quot;) + coord_cartesian(xlim = c(-5, 5), ylim = c(0, 10)) + scale_x_continuous(breaks = seq(from = -4, to = 4, by = 2)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) 5.4.4 Comapre the Standardized Version data_pop %&gt;% dplyr::mutate(pred = predict(pop_lmer_2_std)) %&gt;% ggplot() + aes(x = extravZ, y = pred, group = class) + geom_rect(aes(xmin = 0 - 0.25, xmax = 0 + 0.25, ymin = fixef(pop_lmer_2_cen)[[&quot;(Intercept)&quot;]]- 2.5, ymax = fixef(pop_lmer_2_cen)[[&quot;(Intercept)&quot;]]+ 2.5), fill = &quot;yellow&quot;, alpha = 0.05) + geom_abline(data = coef(pop_lmer_2_std)$class %&gt;% dplyr::rename(Intercept = &quot;(Intercept)&quot;), aes(intercept = Intercept, slope = extravZ), color = &quot;gray&quot;, size = .1) + geom_line(size = .2) + geom_abline(intercept = fixef(pop_lmer_2_std)[[&quot;(Intercept)&quot;]], slope = fixef(pop_lmer_2_std)[[&quot;extravZ&quot;]], color = &quot;hot pink&quot;, size = 2) + theme_bw() + geom_vline(xintercept = 0, color = &quot;red&quot;) + geom_vline(xintercept = -5.215, color = &quot;blue&quot;) + labs(title = &quot;MLM-RIAS: Extroversion = standardized&quot;, subtitle = &quot;Thin black lines are group-wise predictions, one per class EXTRAPOLATED OUT&quot;, x = &quot;STANDARDIZED Student&#39;s Extroversion, as rated by their teacher&quot;, y = &quot;Predicted\\nStudent&#39;s Populartity, mean rating by their peers&quot;) + coord_cartesian(xlim = c(-5, 5), ylim = c(0, 10)) + scale_x_continuous(breaks = seq(from = -4, to = 4, by = 2)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) "],
["variance-explained.html", "6 Variance Explained 6.1 For Generalized MLMs", " 6 Variance Explained Nakagawa and Schielzeth (2013) https://ecologyforacrowdedplanet.wordpress.com/2013/08/27/r-squared-in-mixed-models-the-easy-way/ 6.1 For Generalized MLMs https://stats.stackexchange.com/questions/111150/calculating-r2-in-mixed-models-using-nakagawa-schielzeths-2013-r2glmm-me I am answering by pasting Douglas Bates’s reply in the R-Sig-ME mailing list, on 17 Dec 2014 on the question of how to calculate an \\(R^2\\) statistic for generalized linear mixed models, which I believe is required reading for anyone interested in such a thing. Bates is the original author of the lme4 package for \\(R\\) and co-author of nlme, as well as co-author of a well-known book on mixed models, and CV will benefit from having the text in an answer, rather than just a link to it. I must admit to getting a little twitchy when people speak of the “\\(R^2\\) for GLMMs”. \\(R^2\\) for a linear model is well-defined and has many desirable properties. For other models one can define different quantities that reflect some but not all of these properties. But this is not calculating an \\(R^2\\) in the sense of obtaining a number having all the properties that the \\(R^2\\) for linear models does. Usually there are several different ways that such a quantity could be defined. Especially for GLMs and GLMMs before you can define “proportion of response variance explained” you first need to define what you mean by “response variance”. The whole point of GLMs and GLMMs is that a simple sum of squares of deviations does not meaningfully reflect the variability in the response because the variance of an individual response depends on its mean. Confusion about what constitutes \\(R^2\\) or degrees of freedom of any of the other quantities associated with linear models as applied to other models comes from confusing the formula with the concept. Although formulas are derived from models the derivation often involves quite sophisticated mathematics. To avoid a potentially confusing derivation and just “cut to the chase” it is easier to present the formulas. But the formula is not the concept. Generalizing a formula is not equivalent to generalizing the concept. And those formulas are almost never used in practice, especially for generalized linear models, analysis of variance and random effects. I have a “meta-theorem” that the only quantity actually calculated according to the formulas given in introductory texts is the sample mean. It may seem that I am being a grumpy old man about this, and perhaps I am, but the danger is that people expect an “\\(R^2\\)-like” quantity to have all the properties of an \\(R^2\\) for linear models. It can’t. There is no way to generalize all the properties to a much more complicated model like a GLMM. I was once on the committee reviewing a thesis proposal for Ph.D. candidacy. The proposal was to examine I think 9 different formulas that could be considered ways of computing an \\(R^2\\) for a nonlinear regression model to decide which one was “best”. Of course, this would be done through a simulation study with only a couple of different models and only a few different sets of parameter values for each. My suggestion that this was an entirely meaningless exercise was not greeted warmly. "],
["sjplot-package.html", "7 sjPlot Package 7.1 Plotting Coefficients 7.2 Plotting Marginal Effects 7.3 Model Diagnostics", " 7 sjPlot Package Daniel Lüdecke is German researcher that has put together several GREAT packages, including sjPlot which we will detail here. Documentation can be found at: http://www.strengejacke.de/sjPlot/index.html library(tidyverse) library(haven) # read in SPSS dataset library(furniture) # nice table1() descriptives library(stargazer) # display nice tables: summary &amp; regression library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(psych) # contains some useful functions, like headTail library(lme4) # Linear, generalized linear, &amp; nonlinear mixed models library(sjstats) # ICC calculations library(effects) # Effects for regression models library(optimx) # Different optimizers to solve mlm&#39;s Read the SPSS data in with the haven package and prepare it (see previous chapter). data_raw &lt;- haven::read_sav(&quot;http://www.mlminr.com/data-sets/Achieve.sav?attredirects=0&quot;) data_achieve_center_scale &lt;- data_raw %&gt;% dplyr::mutate_at(vars(id, region, corp, school, class), factor) %&gt;% dplyr::mutate(gender = gender %&gt;% factor(labels = c(&quot;Female&quot;, &quot;Male&quot;))) %&gt;% dplyr::mutate(classize = classize %&gt;% factor(labels = c(&quot;12-17&quot;, &quot;18-21&quot;, &quot;22-26&quot;, &quot;&gt;26&quot;))) %&gt;% dplyr::select(id, region, corp, school, class, # Identifiers gender, age, geread, gevocab, # Pupil-level vars classize, # Class-Level vars senroll, ses) %&gt;% # School-level vars dplyr::mutate(gevocab_c = gevocab - 4.4938) %&gt;% dplyr::mutate(age_c = age - 107.5290) %&gt;% dplyr::mutate(senroll_c = senroll - 533.4148) %&gt;% dplyr::mutate(senroll_ch = senroll_c / 100) %&gt;% # centered AND divided by one hundred dplyr::mutate(ses_t = ses / 10) # JUST divide by ten Fit the final model (see previous chapter) fit_read_11re_s &lt;- lme4::lmer(geread ~ gevocab_c*age_c + gevocab_c*ses_t + # 2 2-way interactions (gevocab_c | school), data = data_achieve_center_scale, REML = TRUE) # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(fit_read_11re_s), custom.model.names = c(&quot;Final&quot;), caption = &quot;MLM: Final Model&quot;, caption.above = TRUE, single.row = TRUE) MLM: Final Model Final (Intercept) 3.94 (0.11)*** gevocab_c 0.67 (0.05)*** age_c -0.00 (0.00) ses_t 0.06 (0.01)*** gevocab_c:age_c 0.01 (0.00)** gevocab_c:ses_t -0.02 (0.01)** AIC 42976.51 BIC 43048.93 Log Likelihood -21478.26 Num. obs. 10320 Num. groups: school 160 Var: school (Intercept) 0.08 Var: school gevocab_c 0.02 Cov: school (Intercept) gevocab_c 0.03 Var: Residual 3.66 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 Now we will show some of the things the sjPlot package can do! 7.1 Plotting Coefficients Select terms that should be plotted. All other term are removed from the output. Note that the term names must match the names of the model’s coefficients. For factors, this means that the variable name is suffixed with the related factor level, and each category counts as one term. E.g. rm.terms = &quot;t_name [2,3]&quot; would remove the terms t_name2 and t_name3 (assuming that the variable t_name is categorical and has at least the factor levels 2 and 3). Another example for the iris-dataset: terms = &quot;Species&quot; would not work, instead you would write terms = &quot;Species [versicolor, virginica]&quot; to remove these two levels, or terms = &quot;Speciesversicolor&quot; if you just want to remove the level versicolor from the plot. 7.1.1 Fixed Effects sjPlot::plot_model(fit_read_11re_s, type = &quot;est&quot;) sjPlot::plot_model(fit_read_11re_s, type = &quot;est&quot;, show.values = TRUE) # Logical, whether values should be plotted or not. sjPlot::plot_model(fit_read_11re_s, type = &quot;std&quot;) Determines in which way estimates are sorted in the plot with the option: sort.est = If NULL (default), no sorting is done and estimates are sorted in the same order as they appear in the model formula. If TRUE, estimates are sorted in descending order, with highest estimate at the top. If sort.est = &quot;sort.all&quot;, estimates are re-sorted for each coefficient (only applies if type = &quot;re&quot; and grid = FALSE), i.e. the estimates of the random effects for each predictor are sorted and plotted to an own plot. If type = &quot;re&quot;, specify a predictor’s / coefficient’s name to sort estimates according to this random effect. sjPlot::plot_model(fit_read_11re_s, type = &quot;std&quot;, sort.est = TRUE) sjPlot::plot_model(fit_read_11re_s, type = &quot;std&quot;, sort.est = TRUE, show.values = TRUE) # Logical, whether values should be plotted or not. Plots standardized beta values, however, standardization follows Gelman (2008) suggestion, rescaling the estimates by dividing them by two standard deviations instead of just one. Resulting coefficients are then directly comparable for untransformed binary predictors. sjPlot::plot_model(fit_read_11re_s, type = &quot;std2&quot;) 7.1.2 Random Effects sjPlot::plot_model(fit_read_11re_s, type = &quot;re&quot;) sjPlot::plot_model(fit_read_11re_s, type = &quot;re&quot;, grid = FALSE, sort.est = TRUE) [[1]] [[2]] 7.2 Plotting Marginal Effects Here terms indicates for which terms marginal effects should be displayed. At least one term is required to calculate effects, maximum length is three terms, where the second and third term indicate the groups, i.e. predictions of first term are grouped by the levels of the second (and third) term. terms may also indicate higher order terms (e.g. interaction terms). Indicating levels in square brackets allows for selecting only specific groups. Term name and levels in brackets must be separated by a whitespace character, e.g. terms = c(&quot;age&quot;, &quot;education [1,3]&quot;). It is also possible to specify a range of numeric values for the predictions with a colon, for instance terms = c(&quot;education [1,3]&quot;, &quot;age [30:50]&quot;). Furthermore, it is possible to specify a function name. Values for predictions will then be transformed, e.g. terms = &quot;income [exp]&quot;. This is useful when model predictors were transformed for fitting the model and should be back-transformed to the original scale for predictions. Finally, using pretty for numeric variables (e.g. terms = &quot;age [pretty]&quot;) calculates a pretty range of values for the term, roughly of proportional length to the term’s value range. For more details, see the documentation for the ggpredict package. 7.2.1 Predicted Values Based on (i.e. is a wrapper for): ggeffects::ggpredict() sjPlot::plot_model(fit_read_11re_s, type = &quot;pred&quot;, terms = c(&quot;gevocab_c&quot;, &quot;ses_t&quot;, &quot;age_c&quot;)) The pred.type = option only applies for Marginal Effects plots with mixed effects models. Indicates whether predicted values should be conditioned on random effects (pred.type = &quot;re&quot;) or fixed effects only (pred.type = &quot;fe&quot;, the default). For details, see documentation of the type-argument in ggpredict() function. sjPlot::plot_model(fit_read_11re_s, type = &quot;pred&quot;, pred.type = &quot;re&quot;, terms = c(&quot;gevocab_c&quot;, &quot;ses_t&quot;, &quot;age_c&quot;)) 7.2.2 Effect Plots Based on (i.e. is a wrapper for): ggeffects::ggeffect() Similar to type = &quot;pred&quot;, however, discrete predictors are held constant at their proportions (not reference level). See the ggeffect package documentation for details. sjPlot::plot_model(fit_read_11re_s, type = &quot;eff&quot;, terms = c(&quot;gevocab_c&quot;, &quot;ses_t&quot;, &quot;age_c&quot;)) 7.2.3 Interaction Plots A shortcut for marginal effects plots, where interaction terms are automatically detected and used as terms-argument. Furthermore, if the moderator variable (the second - and third - term in an interaction) is continuous, type = &quot;int&quot; automatically chooses useful values based on the mdrt.values-argument, which are passed to terms. Then, ggpredict is called. type = &quot;int&quot; plots the interaction term that appears: first in the formula along the x-axis, while the second (and possibly third) variable in an interaction is used as grouping factor(s) (moderating variable). Use type = &quot;pred&quot; or type = &quot;eff&quot; and specify a certain order in the terms-argument to indicate which variable(s) should be used as moderator. sjPlot::plot_model(fit_read_11re_s, type = &quot;int&quot;) [[1]] [[2]] The mdrt.values = option indicates which values of the moderator variable should be used when plotting interaction terms (i.e. type = &quot;int&quot;). minmax (default) minimum and maximum values (lower and upper bounds) of the moderator are used to plot the interaction between independent variable and moderator(s). meansd uses the mean value of the moderator as well as one standard deviation below and above mean value to plot the effect of the moderator on the independent variable (following the convention suggested by Cohen and Cohen and popularized by Aiken and West (1991), i.e. using the mean, the value one standard deviation above, and the value one standard deviation below the mean as values of the moderator, see Grace-Martin K: 3 Tips to Make Interpreting Moderation Effects Easier). zeromax is similar to the minmax option, however, \\(0\\) is always used as minimum value for the moderator. This may be useful for predictors that don’t have an empirical zero-value, but absence of moderation should be simulated by using \\(0\\) as minimum. quart calculates and uses the quartiles (lower, median and upper) of the moderator value. all uses all values of the moderator variable. sjPlot::plot_model(fit_read_11re_s, type = &quot;int&quot;, mdrt.values = &quot;meansd&quot;) [[1]] [[2]] 7.3 Model Diagnostics Note: For mixed models, the diagnostic plots like linear relationship or check for Homoscedasticity, do not take the uncertainty of random effects into account, but is only based on the fixed effects part of the model. 7.3.1 Slope of Coefficentes Slope of coefficients for each single predictor, against the response (linear relationship between each model term and response). sjPlot::plot_model(fit_read_11re_s, type = &quot;slope&quot;) 7.3.2 Residuals Slope of coefficients for each single predictor, against the residuals (linear relationship between each model term and residuals). sjPlot::plot_model(fit_read_11re_s, type = &quot;resid&quot;) 7.3.3 Diagnostics Check model assumptions. sjPlot::plot_model(fit_read_11re_s, type = &quot;diag&quot;) [[1]] [[2]] [[2]]$school [[3]] [[4]] "],
["intro-to-longitudinal-autism-example.html", "8 Intro to Longitudinal: Autism Example 8.1 The dataset", " 8 Intro to Longitudinal: Autism Example library(tidyverse) library(haven) # read in SPSS dataset library(furniture) # nice table1() descriptives library(stargazer) # display nice tables: summary &amp; regression library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(RColorBrewer) # nice color palettes for plots library(gridExtra) # place ggplots together as one plot library(psych) # contains some useful functions, like headTail library(car) # Companion to Applied Regression library(nlme) # non-linear mixed-effects models library(lme4) # Linear, generalized linear, &amp; nonlinear mixed models library(lmerTest) # Tests on lmer objects library(HLMdiag) # Diagnostic Tools for for nlme &amp; lmer4 library(sjstats) # ICC calculations library(optimx) # Different optimizers to solve mlm&#39;s library(effects) 8.1 The dataset data_raw &lt;- read.table(&quot;https://raw.githubusercontent.com/CEHS-research/eBook_multilevel/master/data/autism.txt?token=AScXBXahjPlTjqc6ytn9y6vwkwC6D09nks5bvSeiwA%3D%3D&quot;, header = TRUE) tibble::glimpse(data_raw) Observations: 612 Variables: 4 $ age &lt;int&gt; 2, 3, 5, 9, 13, 2, 3, 5, 9, 13, 2, 3, 5, 9, 2, 3, 9, 1... $ vsae &lt;int&gt; 6, 7, 18, 25, 27, 17, 18, 12, 18, 24, 12, 14, 38, 114,... $ sicdegp &lt;int&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ... $ childid &lt;int&gt; 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 4, 4, 4, 4, 19, 19, 19, ... "],
["rct-longitudinal-exercise-and-diet-example.html", "9 RCT Longitudinal: Exercise and Diet Example 9.1 The dataset 9.2 Exploratory Data Analysis 9.3 Multilevel Modeling", " 9 RCT Longitudinal: Exercise and Diet Example library(tidyverse) library(haven) # read in SPSS dataset library(furniture) # nice table1() descriptives library(stargazer) # display nice tables: summary &amp; regression library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(RColorBrewer) # nice color palettes for plots library(gridExtra) # place ggplots together as one plot library(psych) # contains some useful functions, like headTail library(car) # Companion to Applied Regression library(nlme) # non-linear mixed-effects models library(lme4) # Linear, generalized linear, &amp; nonlinear mixed models library(lmerTest) # Tests on lmer objects library(HLMdiag) # Diagnostic Tools for for nlme &amp; lmer4 library(sjstats) # ICC calculations library(optimx) # Different optimizers to solve mlm&#39;s library(effects) 9.1 The dataset This comes from a Randomized Controled Trial. data_raw &lt;- read.table(&quot;https://raw.githubusercontent.com/CEHS-research/eBook_multilevel/master/data/exercise_diet.txt?token=AScXBcNdbT5roh2XgMae-6vm1fruDo8Bks5bvQJIwA%3D%3D&quot;, header = TRUE, sep = &quot;,&quot;) tibble::glimpse(data_raw) Observations: 120 Variables: 5 $ id &lt;int&gt; 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5,... $ exertype &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... $ diet &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... $ pulse &lt;int&gt; 90, 92, 93, 93, 90, 92, 93, 93, 97, 97, 94, 94, 80, 8... $ time &lt;int&gt; 0, 228, 296, 639, 0, 56, 434, 538, 0, 150, 295, 541, ... data_long &lt;- data_raw %&gt;% dplyr::mutate(id = id %&gt;% factor) %&gt;% dplyr::mutate(exertype = exertype %&gt;% factor(levels = 1:3, labels = c(&quot;At Rest&quot;, &quot;Leisurely Walking&quot;, &quot;Moderate Running&quot;))) %&gt;% dplyr::mutate(diet = diet %&gt;% factor(levels = 1:2, labels = c(&quot;low-fat&quot;, &quot;non-fat&quot;))) %&gt;% dplyr::mutate(time_min = time / 60) data_long %&gt;% psych::headTail(top = 10, bottom = 10) %&gt;% pander::pander(caption = &quot;Raw Data&quot;) Raw Data id exertype diet pulse time time_min 1 1 At Rest low-fat 90 0 0 2 1 At Rest low-fat 92 228 3.8 3 1 At Rest low-fat 93 296 4.93 4 1 At Rest low-fat 93 639 10.65 5 2 At Rest low-fat 90 0 0 6 2 At Rest low-fat 92 56 0.93 7 2 At Rest low-fat 93 434 7.23 8 2 At Rest low-fat 93 538 8.97 9 3 At Rest low-fat 97 0 0 10 3 At Rest low-fat 97 150 2.5 … NA NA NA … … … 111 28 Moderate Running non-fat 140 263 4.38 112 28 Moderate Running non-fat 143 588 9.8 113 29 Moderate Running non-fat 94 0 0 114 29 Moderate Running non-fat 135 164 2.73 115 29 Moderate Running non-fat 130 353 5.88 116 29 Moderate Running non-fat 137 560 9.33 117 30 Moderate Running non-fat 99 0 0 118 30 Moderate Running non-fat 111 114 1.9 119 30 Moderate Running non-fat 140 362 6.03 120 30 Moderate Running non-fat 148 501 8.35 9.2 Exploratory Data Analysis 9.2.1 Demographic Summary data_long %&gt;% dplyr::filter(time == 0) %&gt;% furniture::table1(diet, pulse, splitby = ~ exertype, caption = &quot;Baseline Descriptives&quot;, output = &quot;html&quot;, test = TRUE) Table 9.1: Baseline Descriptives At Rest Leisurely Walking Moderate Running P-Value n = 10 n = 10 n = 10 diet 1 low-fat 5 (50%) 5 (50%) 5 (50%) non-fat 5 (50%) 5 (50%) 5 (50%) pulse 0.129 90.7 (6.3) 93.1 (6.3) 96.1 (4.5) 9.2.2 Baseline Summary data_long %&gt;% dplyr::filter(time == 0) %&gt;% dplyr::group_by(exertype, diet) %&gt;% dplyr::summarise(mean = mean(pulse)) %&gt;% tidyr::spread(key = diet, value = mean) %&gt;% pander::pander(caption = &quot;Baseline Pulse, Means&quot;) Baseline Pulse, Means exertype low-fat non-fat At Rest 89.6 91.8 Leisurely Walking 90.6 95.6 Moderate Running 94 98.2 9.2.3 Raw Trajectories - Person Profile Plot 9.2.3.1 Connect the dots data_long %&gt;% ggplot(aes(x = time_min, y = pulse)) + geom_point() + geom_line(aes(group = id)) + facet_grid(diet ~ exertype) + theme_bw() 9.2.3.2 Loess - Moving Average Smoothers data_long %&gt;% ggplot(aes(x = time_min, y = pulse, color = diet)) + geom_line(aes(group = id)) + facet_grid(~ exertype) + theme_bw() + geom_smooth(method = &quot;loess&quot;, se = FALSE, size = 2, span = 5) + theme(legend.position = c(0.08, 0.85), legend.background = element_rect(color = &quot;black&quot;)) + labs(title = &quot;Raw Pulse Trajectories&quot;, subtitle = &quot;By Exercise and Diet Groupings&quot;, x = &quot;Time (Minutes Post-Baseline)&quot;, y = &quot;Pulse (Beats per Minute)&quot;, color = &quot;Diet Plan&quot;) 9.3 Multilevel Modeling 9.3.1 Null Model fit_lmer_0re &lt;- lme4::lmer(pulse ~ 1 + (1 | id), data = data_long) texreg::htmlreg(fit_lmer_0re) Statistical models Model 1 (Intercept) 102.13*** (2.54) AIC 963.89 BIC 972.25 Log Likelihood -478.95 Num. obs. 120 Num. groups: id 30 Var: id (Intercept) 165.84 Var: Residual 109.39 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 9.3.2 ICC sjstats::icc(fit_lmer_0re) Linear mixed model Family : gaussian (identity) Formula: pulse ~ 1 + (1 | id) ICC (id): 0.6025 9.3.3 Add fixed effects: level specific 9.3.3.1 Fit nested models # Null Model (random intercept only) fit_lmer_0ml &lt;- lme4::lmer(pulse ~ 1 + (1 | id), data = data_long, REML = FALSE) # Add quadratic time fit_lmer_1ml &lt;- lme4::lmer(pulse ~ time_min + I(time_min^2) + (1 | id), data = data_long, REML = FALSE) # Add main effects for 2 interventions (person-specific, i.e. level-2 factors) fit_lmer_2ml &lt;- lme4::lmer(pulse ~ diet + exertype + time_min + I(time_min^2) + (1 | id), data = data_long, REML = FALSE) # Add interaction between level-2 factors fit_lmer_3ml &lt;- lme4::lmer(pulse ~ diet*exertype + time_min + I(time_min^2) + (1 | id), data = data_long, REML = FALSE) # Add exercise interacting with [time &amp; time-squared] fit_lmer_4ml &lt;- lme4::lmer(pulse ~ diet*exertype + exertype*time_min + exertype*I(time_min^2) + (1 | id), data = data_long, REML = FALSE) # Add diet interacting with [time &amp; time-squared] fit_lmer_5ml &lt;- lme4::lmer(pulse ~ diet*exertype*time_min + diet*exertype*I(time_min^2) + (1 | id), data = data_long, REML = FALSE) texreg::htmlreg(list(fit_lmer_1ml, fit_lmer_2ml, fit_lmer_3ml, fit_lmer_4ml, fit_lmer_5ml)) Statistical models Model 1 Model 2 Model 3 Model 4 Model 5 (Intercept) 94.05*** 79.30*** 82.15*** 89.89*** 89.81*** (2.71) (2.46) (2.64) (2.69) (2.78) time_min 3.57*** 3.58*** 3.44*** 0.24 0.37 (0.65) (0.64) (0.64) (0.62) (0.87) I(time_min^2) -0.21*** -0.21*** -0.20*** -0.01 -0.03 (0.06) (0.06) (0.06) (0.05) (0.09) dietnon-fat 8.36*** 2.89 1.99 2.11 (2.21) (3.36) (3.45) (3.89) exertypeLeisurely Walking 5.20 3.81 0.84 1.40 (2.70) (3.34) (3.78) (3.92) exertypeModerate Running 26.43*** 19.71*** 0.53 5.71 (2.70) (3.34) (3.77) (3.92) dietnon-fat:exertypeLeisurely Walking 2.83 3.70 2.53 (4.73) (4.86) (5.50) dietnon-fat:exertypeModerate Running 13.47** 14.02** 3.99 (4.74) (4.86) (5.50) exertypeLeisurely Walking:time_min 1.17 1.09 (0.87) (1.17) exertypeModerate Running:time_min 8.19*** 5.77*** (0.90) (1.20) exertypeLeisurely Walking:I(time_min^2) -0.07 -0.08 (0.08) (0.11) exertypeModerate Running:I(time_min^2) -0.48*** -0.33** (0.08) (0.11) dietnon-fat:time_min -0.17 (1.14) dietnon-fat:I(time_min^2) 0.02 (0.10) dietnon-fat:exertypeLeisurely Walking:time_min 0.21 (1.56) dietnon-fat:exertypeModerate Running:time_min 4.42** (1.61) dietnon-fat:exertypeLeisurely Walking:I(time_min^2) 0.01 (0.14) dietnon-fat:exertypeModerate Running:I(time_min^2) -0.27 (0.15) AIC 927.70 884.96 881.11 785.34 769.23 BIC 941.64 907.26 908.99 824.36 824.98 Log Likelihood -458.85 -434.48 -430.56 -378.67 -364.62 Num. obs. 120 120 120 120 120 Num. groups: id 30 30 30 30 30 Var: id (Intercept) 167.58 19.46 11.03 24.13 25.64 Var: Residual 67.47 67.47 67.52 20.95 15.32 p &lt; 0.001, p &lt; 0.01, p &lt; 0.05 9.3.3.2 Evaluate Model Fit, i.e. variable significance anova(fit_lmer_1ml, fit_lmer_2ml, fit_lmer_3ml, fit_lmer_4ml, fit_lmer_5ml) Data: data_long Models: fit_lmer_1ml: pulse ~ time_min + I(time_min^2) + (1 | id) fit_lmer_2ml: pulse ~ diet + exertype + time_min + I(time_min^2) + (1 | id) fit_lmer_3ml: pulse ~ diet * exertype + time_min + I(time_min^2) + (1 | id) fit_lmer_4ml: pulse ~ diet * exertype + exertype * time_min + exertype * I(time_min^2) + fit_lmer_4ml: (1 | id) fit_lmer_5ml: pulse ~ diet * exertype * time_min + diet * exertype * I(time_min^2) + fit_lmer_5ml: (1 | id) Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) fit_lmer_1ml 5 927.70 941.64 -458.85 917.70 fit_lmer_2ml 8 884.96 907.26 -434.48 868.96 48.742 3 1.480e-10 fit_lmer_3ml 10 881.11 908.99 -430.56 861.11 7.847 2 0.01977 fit_lmer_4ml 14 785.34 824.36 -378.67 757.34 103.776 4 &lt; 2.2e-16 fit_lmer_5ml 20 769.23 824.98 -364.62 729.23 28.108 6 8.968e-05 fit_lmer_1ml fit_lmer_2ml *** fit_lmer_3ml * fit_lmer_4ml *** fit_lmer_5ml *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 9.3.4 Final Model Refit via REML fit_lmer_5re &lt;- lme4::lmer(pulse ~ diet*exertype*time_min + diet*exertype*I(time_min^2) + (1 | id), data = data_long, REML = TRUE) 9.3.4.1 Visualize sjPlot::plot_model(fit_lmer_5re, type = &quot;pred&quot;, terms = c(&quot;time_min&quot;, &quot;diet&quot;, &quot;exertype&quot;)) effects::Effect(focal.predictors = c(&quot;diet&quot;, &quot;exertype&quot;, &quot;time_min&quot;), mod = fit_lmer_5re) %&gt;% data.frame %&gt;% ggplot(aes(x = time_min, y = fit, fill = diet, color = diet)) + geom_line(size = 1.5) + theme_bw() + facet_grid(~ exertype) + theme(legend.position = c(0.08, 0.85), legend.background = element_rect(color = &quot;black&quot;)) + labs(title = &quot;Raw Pulse Trajectories&quot;, subtitle = &quot;By Exercise and Diet Groupings&quot;, x = &quot;Time (Minutes Post-Baseline)&quot;, y = &quot;Estimated Marginal Mean\\nPulse (Beats per Minute)&quot;, fill = &quot;Diet Plan&quot;, color = &quot;Diet Plan&quot;) effects::Effect(focal.predictors = c(&quot;diet&quot;, &quot;exertype&quot;, &quot;time_min&quot;), mod = fit_lmer_5re, xlevels = list(&quot;time_min&quot; = seq(from = 0, to = 12, by = 0.5))) %&gt;% data.frame %&gt;% dplyr::mutate(diet = fct_rev(diet)) %&gt;% # reverse the order of the levels ggplot(aes(x = time_min, y = fit)) + geom_ribbon(aes(ymin = fit - se, ymax = fit + se, fill = diet), alpha = 0.3) + geom_line(aes(color = diet, linetype = diet), size = 1) + theme_bw() + facet_grid(~ exertype) + theme(legend.position = c(0.12, 0.85), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(2, &quot;cm&quot;)) + labs(title = &quot;Raw Pulse Trajectories&quot;, subtitle = &quot;By Exercise and Diet Groupings&quot;, x = &quot;Time (Minutes Post-Baseline)&quot;, y = &quot;Estimated Marginal Mean\\nPulse (Beats per Minute)&quot;, fill = &quot;Diet Plan&quot;, color = &quot;Diet Plan&quot;, linetype = &quot;Diet Plan&quot;) + scale_color_manual(values = c(&quot;gray50&quot;, &quot;black&quot;)) + scale_fill_manual(values = c(&quot;gray50&quot;, &quot;black&quot;)) + scale_x_continuous(breaks = seq(from = 0, to = 14, by = 5)) "],
["references.html", "References", " References "]
]
