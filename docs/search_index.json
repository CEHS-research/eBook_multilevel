[["index.html", "R Notebook Welcome The Author Blocked Notes Code and Output", " R Notebook Author: Sarah Schwartz, PhD Last updated: 2022-12-02 Welcome Backgroup and links to other volumes of this encyclopedia may be found at the Encyclopedia’s Home Website. The Author left right www.SarahSchwartzStats.com eMail: Sarah.Schwartz@usu.edu Blocked Notes Thoughout all the eBooks in this encyclopedia, several small sections will be blocked out in the following ways: These blocks denote an area UNDER CONSTRUCTION, so check back often. This massive undertaking started during the summer of 2018 and is far from complete. The outline of seven volumes is given above despite any one being complete. Feedback is welcome via either author’s email. These blocks denote something EXTREMELY IMPORTANT. Do NOT skip these notes as they will be used very sparingly. These blocks denote something to DOWNLOAD. This may include software installations, example datasets, or notebook code files. These blocks denote a CITATION. These should be use for writting manuscripts. These blocks denote something INTERESTING. These point out information we found of interest or added value. These blocks denote LINKS to other websites. This may include instructional video clips, articles, or blog posts. We are all about NOT re-creating the wheel. If somebody else has described or illustrated a topic well, we celebrate it! Code and Output This is how \\(R\\) code is shown: 1 + 1 This is what the output of the \\(R\\) code above will look: ## [1] 2 Why choose R ? Check it out: an article from Fall 2016… No more excuses: R is better than SPSS for psychology undergrads, and students agree FYI This entire encyclopedia is written in \\(R Markdown\\), using \\(R Studio\\) as the text editor and the bookdown package to turn a collection of markdown documents into a coherent whole. The book’s source code is hosted on GitHub. If you notice typos or other issues, feel free to email author. This work is licensed under the Attribution-NonCommercial-NoDerivatives 4.0 International License. "],["what-are-mlm-and-why-do-i-care.html", "1 What are MLM and Why do I care? 1.1 GEE", " 1 What are MLM and Why do I care? This chapter is UNDER CONSTRUCTION, so check back often. 1.0.1 John Nezlek John Nezlek is currently a professor at SWPS University of Social Sciences and Humanities in Poznań Poland and the Department of Psychological Sciences, College of William and Mary (Emeritus). He divides his time between Poland and the US. John does research in Personality Psychology, Psychometrics, and Social Psychology. Much of his research concerns daily experience. He recently received a grant to study vegetarianism as a social identity Nezlek, J. B. (2012). Multilevel modeling for psychologists. In H. Cooper, P. M. Camic, D. L. Long, A. T. Panter, D. Rindskopf, &amp; K. J. Sher (Eds.), APA handbook of research methods in psychology, Vol. 3. Data analysis and research publication (pp. 219–241). American Psychological Association. DOI: 10.1037/13621-011 Full PDF online 1.1 GEE The GEE method is an extension of the Generalized Linear Model (GLM) of Nelder and Wedderburn (1972) to enable correlated data be analyzed appropriately. Repeated measures on an individual in a longitudinal study are likely to be correlated because of the continuity of the measurement over time (Burger et al., 2000). In analyzing longitudinal data using the GEE method, a working correlation structure is formulated to incorporate the possible correlations among the response observations measured at different occasions during follow-up for each individual. Formulation of the working correlation structure depends on the degree of correlation perceived among an individual’s response observations, resulting in the use of such common covariance matrix functions as the so-called independent, exchangeable, autoregressive, stationary, non stationary, and general unstructured model. Correctness of the specification of the working correlation structure is not so important in a GEE analysis because the resulting regression coefficient estimators are still consistent even when the working correlation structure is mis-specified to some extent (Liang and Zeger, 1986). "],["citations-for-writeup.html", "2 Citations for Writeup 2.1 MLM: Multilevel Modeling, aka Mixed Effects Regression 2.2 GLMM: Generalized Linear Multilevel Models or Generalized Mixed Effects Regression 2.3 GEE: Generalized Estimating Equations", " 2 Citations for Writeup This chapter is UNDER CONSTRUCTION, so check back often. 2.1 MLM: Multilevel Modeling, aka Mixed Effects Regression 2.1.1 General MLM Fitzmaurice, G. M., Laird, N. M., Ware, J. H. (2004). Applied Longitudinal Analysis. Hoboken, NJ: John Wiley &amp; Sons. Hox, J. J., Moerbeek, M., &amp; Van de Schoot, R. (2017). Multilevel analysis: Techniques and applications. Routledge. Gelman, A., &amp; Hill, J. (2006). Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press. Raudenbush, S. W., &amp; Bryk, A. S. (2002). Hierarchical Linear Models: Applications and Data Analysis Methods (Vol. 1). Sage. Singer, J. D., Willett, J. B., &amp; Willett, J. B. (2003). Applied Longitudinal Data Analysis: Modeling Change and Event Occurrence. Oxford University Press. Snijders, T. A. B., &amp; Bosker, R. J. (2012). Multilevel analysis: An introduction to basic and advanced multilevel modeling. Los Angeles; London, SAGE. 2.1.2 Pairwise Differenes Effect Size (Standardized Mean Difference, SMD) There are a number of ways to calculate standardized mean difference (SMD) effect sizes and there really is not a universally accepted “right” way. Using residual SD and the square root of the sum of variance components from a mixed model for standardization are NOT traditional Cohen’s d statistics, so they should be referred to as standardized effect sizes. The important thing is to detail which method you are using and make sure to label it appropriately. Probably the effect size calculation with the most academic support is taking the square root of the sum of all variance components in the model. This method was first discussed by Westfall, Judd, and Kenny (2014) and later included in a statistical tutorial on Effect Sizes and Power Analysis in Mixed Effects Models by Marc Brysbaert and Michael Stevens (2018). We can add interpretations to these SMD effects. There have been different propositions regarding the qualitative interpretation of effect sizes’. The original interpretation given by Cohen suggests: Small: &lt; 0.5 Moderate: 0.5 - 0.7 Large: &gt; 0.8 More recently Hopkins suggested a revised scale with threshold values at: 0.2 small 0.6 moderate 1.2 large 2.0 very large 4.0 nearly perfect Brysbaert, M., &amp; Stevens, M. (2018). Power analysis and effect size in mixed effects models: A tutorial. Journal of cognition, 1(1). https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6646942/ Links to an external site. Luke, S. G. (2017). Evaluating significance in linear mixed-effects models in R. Behavior research methods, 49(4), 1494-1502. https://link.springer.com/article/10.3758/s13428-016-0809-y#Sec3 Links to an external site. Westfall, J., Kenny, D. A., &amp; Judd, C. M. (2014). Statistical power and optimal design in experiments in which samples of participants respond to samples of stimuli. Journal of Experimental Psychology: General, 143(5), 2020. https://psycnet.apa.org/doiLanding?doi=10.1037%2Fxge0000014 2.1.2.1 Example Write-up for Methods-Analysis section: Follow-up pairwise comparisons utilizing Kenward-Rodger degrees of freedom (Luke, 2017) were made without/with?? controlling for multiple comparisons. Corresponding effect sizes give Cohen’s d-like standardized mean differences (SMD) utilizing the standard deviation based on the sum of the MLM variance components (Brysbaert &amp; Stevens, 2018). 2.2 GLMM: Generalized Linear Multilevel Models or Generalized Mixed Effects Regression 2.3 GEE: Generalized Estimating Equations Fitzmaurice, G. M., Laird, N. M., Ware, J. H. (2004). Applied Longitudinal Analysis. Hoboken, NJ: John Wiley &amp; Sons. Hardin, J. W., Hilbe, J. M. (2003). Generalized Estimating Equations. Boca Raton: Chapman &amp; Hall/CRC. Liang, K.-Y., Zeger, S. L. (1986). Longitudinal data analysis using generalized linear models. Biometrika 73:13–22. McCullagh, P., Nelder, J. A. (1989). Generalized Linear Models. 2nd ed. London: Chapman &amp; Hall. Nelder, J. A., Wedderburn, R. W. M. (1972). Generalized linear models. J. Roy. Statist. Soc.,Ser. A 135:370–384. Burger, H. G., Dudley, E. C., Cui, J., Dennerstein, L., Hopper, J. L. (2000). A prospective longitudinal study of serum testosterone levels during and after the menopause transition. J. Clin. Endocrinol. Metabolism 85:2832–2838. 2.3.1 QIC: Comparing GEE Models The QIC involves using the quasi-likelihood constructed under the working independence model and the naive and robust covariance estimates of estimated regression coefficients. QIC-U is asymptotic, not appropriate for selected an optimal correlation structure Cui, J., &amp; Qian, G. (2007). Selection of working correlation structure and best model in GEE analyses of longitudinal data. Communications in statistics—Simulation and computation®, 36(5), 987-996. Download PDF We propose to choose the best correlation structure first and then choose the best subsets of covariates. The best correlation structure was selected based on the full model including all covariates. However, if we use only a subset of covariates to select the best correlation structure, we may end up with a different correlation structure. This is a dilemma as to choose the best correlation structure first or the best subset of covariates first. It seems that the full model contains most information to predict the outcome variable and is reasonable to use it to select the best correlation structure first (Hardin and Hilbe, 2003). Burnham, K. P. and D. R. Anderson. 2002. Model selection and multimodel inference: a practical information-theoretic approach. Second edition. Springer Science and Business Media, Inc., New York. Download PDF Pan, W. 2001. Akaike’s information criterion in generalized estimating equations. Biometrics 57:120-125. https://onlinelibrary.wiley.com/doi/abs/10.1111/j.0006-341X.2001.00120.x Download PDF Wedderburn, R. W. M. (1974). Quasi-likelihood functions, generalized liner models, and the Gauss-newton method. Biometrika 61:439–447. "],["formula-warehouse.html", "3 Formula Warehouse 3.1 Data Notation 3.2 Single-level Regression Analysis 3.3 Multi-level Regression Analysis 3.4 Intraclass Correlation (ICC) 3.5 Proporion of Variance Explianed 3.6 Using \\(\\LaTeX\\) for Equation Typesetting", " 3 Formula Warehouse This is the home for notation and formulas used thorugh this eBook. Most important equations will be located here. 3.1 Data Notation Sample Sizes: \\(n_j\\) = number of pupils in class \\(j\\) \\(N\\) = number of classes Indicators: \\(i \\in (1, 2, \\dots, n_j)\\) = index for pupil number \\(j \\in (1, 2, \\dots, N)\\) = index for class number Level Type of Variable Symbol pupil \\(i\\) in class \\(j\\) 1 Outcome (Dependent) \\(Y\\) \\(Y_{ij}\\) 1 Predictor (Independent) \\(X_1\\) \\(X_{1ij}\\) 1 Predictor (Independent) \\(X_2\\) \\(X_{2ij}\\) 2 Predictor (Independent) \\(Z\\) \\(Z_j\\) 3.2 Single-level Regression Analysis 3.2.1 The Only Equation Since we are don’t have or are ignoring clustering, there is only one level. Single-Level Regression Equation \\[ \\overbrace{Y_{ij}}^{Outcome} = \\underbrace{\\beta_{0}}_{\\text{Fixed}\\atop\\text{intercept}} + \\underbrace{\\beta_{1}}_{\\text{Fixed}\\atop\\text{slope } X_1} \\overbrace{X_{1ij}}^{\\text{Predictor 1}} + \\underbrace{\\beta_{2}}_{\\text{Fixed}\\atop\\text{slope } X_2} \\overbrace{X_{2ij}}^{\\text{Predictor 2}} + \\underbrace{e_{ij}}_{\\text{Random}\\atop\\text{residuals}} \\tag{Hox 2.1} \\] 3.2.2 Parameters Type Parameter of Interest Estimates This Fixed Intercept \\(\\beta_{0}\\) Fixed Slope or main effect of \\(X_1\\) \\(\\beta_{1}\\) Fixed Slope or main effect of \\(X_2\\) \\(\\beta_{2}\\) Random Residual Variance \\(var[e_{ij}]\\) \\(\\sigma^2_{e}\\) 3.2.3 Assumptions to Check The \\(e_{ij}\\)’s follow a normal distribution with a mean of \\(0\\) The \\(e_{ij}\\)’s have a constant variance (homoscedasticity) 3.3 Multi-level Regression Analysis Continue taking into account fixed slopes for two Level 1 variables, \\(X_1\\) and \\(X_2\\). 3.3.1 Level 1 Regression Equation* \\[ \\overbrace{Y_{ij}}^{\\text{Level 1}\\atop\\text{Outcome}} = \\underbrace{\\beta_{0j}}_{\\text{Level 2}\\atop\\text{intercepts}} + \\underbrace{\\beta_{1j}}_{\\text{Level 2}\\atop\\text{slopes}} \\overbrace{X_{1ij}}^{\\text{Level 1}\\atop\\text{Predictor 1}} + \\underbrace{\\beta_{2j}}_{\\text{Level 2}\\atop\\text{slopes}} \\overbrace{X_{2ij}}^{\\text{Level 1}\\atop\\text{Predictor 2}} + \\underbrace{e_{ij}}_{\\text{Random}\\atop\\text{residuals}} \\tag{Hox 2.1} \\] Now we take clustering into account and include random intercepts (\\(\\beta_{0j}\\)) and slopes (\\(\\beta_{1j}, \\beta_{2j}\\)), as well as including a single Level 2 variable, \\(Z\\) that interacts with both Level 1 variables. 3.3.2 Level 2 Regression Equations 3.3.2.1 Random Intercepts: \\[ \\overbrace{\\beta_{0j}}^{\\text{Level 2}\\atop\\text{intercepts}} = \\underbrace{\\gamma_{00}}_{\\text{Fixed}\\atop\\text{intercept}} + \\underbrace{\\gamma_{01}}_{\\text{Fixed}\\atop\\text{slope } Z} \\overbrace{Z_{j}}^{\\text{Level 2}\\atop\\text{Predictor 3}} + \\underbrace{u_{0j}}_{\\text{Intercept}\\atop\\text{residual}} \\tag{Hox 2.3} \\] 3.3.2.2 Random Slopes For the first predictor, \\(X_1\\): \\[ \\overbrace{\\beta_{1j}}^{\\text{Level 2}\\atop\\text{slopes}} = \\underbrace{\\gamma_{10}}_{\\text{Fixed}\\atop\\text{Slope } X_1} + \\underbrace{\\gamma_{11}}_{\\text{Fixed}\\atop X_1 \\times Z} \\overbrace{Z_{j}}^{\\text{Level 2}\\atop\\text{Predictor 3}} + \\underbrace{u_{1j}}_{\\text{Slope } X_1\\atop\\text{residual}} \\tag{Hox 2.4a} \\] For the second predictor, \\(X_2\\): \\[ \\overbrace{\\beta_{2j}}^{\\text{Level 2}\\atop\\text{slopes}} = \\underbrace{\\gamma_{20}}_{\\text{Fixed}\\atop\\text{Slope } X_2} + \\underbrace{\\gamma_{21}}_{\\text{Fixed}\\atop X_2 \\times Z} \\overbrace{Z_{j}}^{\\text{Level 2}\\atop\\text{Predictor 3}} + \\underbrace{u_{2j}}_{\\text{Slope } X_2\\atop\\text{residual}} \\tag{Hox 2.4a} \\] 3.3.2.3 Merging the Equations Starting with Level 1 equation (2.1) and allow the \\(\\beta\\)’s to be varry for each class and plug in the level 2 equations (2.3 and 2.4) into the level 1 equation (2.1) to make the combined equation. \\[ Y_{ij} = \\overbrace{(\\gamma_{00} + \\gamma_{01} Z_{j} + u_{0j})}^{\\beta_{0j}} + \\overbrace{(\\gamma_{10} + \\gamma_{11} Z_{j} + u_{1j})}^{\\beta_{1j}} X_{1ij} + \\overbrace{(\\gamma_{20} + \\gamma_{21} Z_{j} + u_{2j})}^{\\beta_{2j}} X_{2ij} + e_{ij} \\] Use the distributive property of multiplication to get rid of the parentheses. \\[ Y_{ij} = \\overbrace{\\gamma_{00} + \\gamma_{01} Z_{j} + u_{0j}}^{\\beta_{0j}} + \\overbrace{\\gamma_{10} X_{1ij} + \\gamma_{11} Z_{j} X_{1ij} + u_{1j} X_{1ij}}^{\\beta_{1j} \\times X_{1ij}} + \\overbrace{\\gamma_{20} X_{2ij} + \\gamma_{21} Z_{j} X_{2ij} + u_{2j} X_{2ij}}^{\\beta_{2j} \\times X_{2ij}} + e_{ij} \\] 3.3.3 Combinded, Multilevel Regression Equation Collect ‘like-terms’ (i.e. get the \\(\\gamma\\)’s together and the \\(u\\)’s together) Combinded, Multilevel Regression Equation - Generic \\[ Y_{ij} = \\overbrace{\\gamma_{00} + \\gamma_{10} X_{1ij} + \\gamma_{20} X_{2ij} + \\gamma_{01} Z_{j} + \\gamma_{11} Z_{j} X_{1ij} + \\gamma_{21} Z_{j} X_{2ij}}^{\\text{Fixed part}\\atop\\text{Deterministic}} + \\\\ \\underbrace{u_{0j} + u_{1j} X_{1ij} + u_{2j} X_{2ij} + e_{ij} }_{\\text{Random part}\\atop\\text{Stochastic}} \\tag{Hox 2.5} \\] 3.3.4 Parameters Type Parameter of Interest Estimates This Fixed Intercept \\(\\gamma_{00}\\) Fixed Main Effect of \\(X_1\\) \\(\\gamma_{10}\\) Fixed Main Effect of \\(X_2\\) \\(\\gamma_{20}\\) Fixed Main Effect of \\(Z\\) \\(\\gamma_{01}\\) Fixed Cross-Level interaction between \\(X_1\\) and \\(Z\\) \\(\\gamma_{11}\\) Fixed Cross-Level interaction between \\(X_2\\) and \\(Z\\) \\(\\gamma_{21}\\) Random Variance in random intercepts, \\(var[u_{0j}]\\) \\(\\sigma^2_{u0}\\) Random Variance in random slope of \\(X_1\\), \\(var[u_{1j}]\\) \\(\\sigma^2_{u1}\\) Random Variance in random slope of \\(X_2\\), \\(var[u_{2j}]\\) \\(\\sigma^2_{u2}\\) Random Covariance between random intercepts and random slope of \\(X_1\\), \\(cov[u_{0j}, u_{1j}]\\) \\(\\sigma^2_{u01}\\) Random Covariance between random intercepts and random slope of \\(X_2\\), \\(cov[u_{0j}, u_{2j}]\\) \\(\\sigma^2_{u02}\\) Random Covariance between random slopes of \\(X_1\\) and \\(X_2\\), \\(cov[u_{1j}, u_{2j}]\\) \\(\\sigma^2_{u12}\\) Random Residual Variance \\(var[e_{ij}]\\) \\(\\sigma^2_{e}\\) The \\(u_{1j}\\) and \\(u_{2j}\\) terms allow for heteroscedasticity by fitting different error terms for different values of \\(X_1\\) and \\(X_2\\). The HOV assumption is that AFTER accounting for this, the remaining residuals are HOV. 3.4 Intraclass Correlation (ICC) 3.4.1 Two Level Models Combined, Multilevel Model Equation - Null Model, 2 levels \\[ \\overbrace{Y_{ij}}^{Outcome} = \\underbrace{\\gamma_{00}}_{\\text{Fixed}\\atop\\text{intercept}} + \\underbrace{u_{0j}}_{\\text{Random}\\atop\\text{intercepts}} + \\underbrace{e_{ij}}_{\\text{Random}\\atop\\text{residuals}} \\tag{Hox 2.8} \\] Although the Null model above does not explain any variance in the dependent variable, since there are no independent variables, it does decompose (i.e. divide up) the variance into two pieces. We can compute the amount of total variance in the outcome that is attribute to the clustering of Level 1 untis (micro-units) into clusters of Level 2 units (macro-units) verses the total variance. Intraclass Correlation (ICC) Formula, 2 level model \\[ \\overbrace{\\rho}^{\\text{ICC}} = \\frac{\\overbrace{\\sigma^2_{u0}}^{\\text{Random Intercept}\\atop\\text{Variance}}} {\\underbrace{\\sigma^2_{u0}+\\sigma^2_{e}}_{\\text{Total}\\atop\\text{Variance}}} \\tag{Hox 2.9} \\] 3.4.2 Three Level Models Indicators: \\(i\\) = index for units in the lowest level (Level 1) \\(j\\) = index for units in the middle level (Level 2) \\(k\\) = index for units in the highest level (Level 3) Combined, Multilevel Model Equation - Null Model, 3 levels \\[ \\overbrace{Y_{ijk}}^{Outcome} = \\underbrace{\\gamma_{000}}_{\\text{Fixed}\\atop\\text{intercept}} + \\underbrace{v_{0k }}_{\\text{Random Intercepts}\\atop\\text{Level 3}} + \\underbrace{u_{0jk}}_{\\text{Random Intercepts}\\atop\\text{Level 2}} + \\underbrace{e_{ijk}}_{\\text{Random}\\atop\\text{residuals}} \\tag{Hox 2.15} \\] If you are interested in teh decomposition of variance across all levels, use the Davis and Scott method: Intraclass Correlation (ICC) Formula, 3 level model - Davis and Scott Method \\[ \\overbrace{\\rho_{mid}}^{\\text{ICC}\\atop\\text{at level 2}} = \\frac{\\overbrace{\\sigma^2_{u0}}^{\\text{Random Intercept}\\atop\\text{Variance Level 2}}} {\\underbrace{\\sigma^2_{v0}+\\sigma^2_{u0}+\\sigma^2_{e}}_{\\text{Total}\\atop\\text{Variance}}} \\tag{Hox 2.16} \\] \\[ \\overbrace{\\rho_{top}}^{\\text{ICC}\\atop\\text{ at level 3}} = \\frac{\\overbrace{\\sigma^2_{u0}}^{\\text{Random Intercept}\\atop\\text{Variance Level 3}}} {\\underbrace{\\sigma^2_{v0}+\\sigma^2_{u0}+\\sigma^2_{e}}_{\\text{Total}\\atop\\text{Variance}}} \\tag{Hox 2.17} \\] If you would like to estimate the expected (population) correlation between two randomly chosen elements of the same group: Intraclass Correlation (ICC) Formula, 3 level model - Siddiqui Method \\[ \\overbrace{\\rho_{mid}}^{\\text{ICC}\\atop\\text{at level 2}} = \\frac{\\overbrace{\\sigma^2_{v0}+\\sigma^2_{u0}}^{\\text{Random variance}\\atop\\text{at levels 2 &amp;amp; 3}}} {\\underbrace{\\sigma^2_{v0}+\\sigma^2_{u0}+\\sigma^2_{e}}_{\\text{Total}\\atop\\text{Variance}}} \\tag{Hox 2.18} \\] \\[ \\overbrace{\\rho_{top}}^{\\text{ICC}\\atop\\text{ at level 3}} = \\frac{\\overbrace{\\sigma^2_{u0}}^{\\text{Random variance}\\atop\\text{at only level 3}}} {\\underbrace{\\sigma^2_{v0}+\\sigma^2_{u0}+\\sigma^2_{e}}_{\\text{Total}\\atop\\text{Variance}}} \\tag{Hox 2.19} \\] 3.5 Proporion of Variance Explianed See pages 61-63 of Hox, Moerbeek, and Van de Schoot (2017) http://journals.sagepub.com/doi/10.1177/1094428114541701 Analogous to multiple \\(R^2\\) - done seperately by level \\(BL\\) = Baseline model (Null) \\(MC\\) = Model to Compare to 3.5.1 Level 1 Variance Explained MODELS SHOULD NOT INCLUDE ANY RANOM EFFECTS, OTHER THAN RANDOM INTERCEPTS. Different approaches differ in values and meaning. 3.5.1.1 Snijders and Bosker Explained variance is a proportion of the total variance, because in principle first-level variables can explain all variation, including the variation at the second level. Correction removes the spurious increase in \\(R^2\\) when random slopes are added to a model Snijders and Bosker Formula - Level 1 Random Intercepts Models Only, address potential negative \\(R^2\\) issue \\[ R^2_1 = 1 - \\frac{\\sigma^2_{e-MC} + \\sigma^2_{u0-MC}} {\\sigma^2_{e-BL} + \\sigma^2_{u0-BL}} \\] 3.5.1.2 Raudenbush and Bryk Explained variance is a proportion of first-level variance only A good option when the multilevel sampling process is is close to two-stage simple random sampling Raudenbush and Bryk Approximate Formula - Level 1 approximate \\[ approx \\;R^2_1 = \\frac{\\sigma^2_{e-BL} - \\sigma^2_{e-MC}} {\\sigma^2_{e-BL} } \\tag{Hox 4.8} \\] 3.5.2 Level 2 Variance Explined 3.5.2.1 Snijders and Bosker Snijders and Bosker Formula Extended - Level 2 \\[ R^2_2 = 1 - \\frac{\\frac{\\sigma^2_{e-MC}}{B} + \\sigma^2_{u0-MC}} {\\frac{\\sigma^2_{e-BL}}{B} + \\sigma^2_{u0-BL}} \\] \\(B\\) is the average size of the Level 2 units (schools). Technically, you should use the harmonic mean, but unless the clusters differ greatly in size, it doesn’t make a huge difference. 3.5.2.2 Raudenbush and Bryk Raudenbush and Bryk Approximate Formula - Level 2 \\[ approx \\; R^2_s = \\frac{\\sigma^2_{u0-BL} - \\sigma^2_{u0-MC}} {\\sigma^2_{u0-BL} } \\tag{Hox 4.9} \\] 3.6 Using \\(\\LaTeX\\) for Equation Typesetting R markdown is a user friendly, simplified language that allows for more complex formating utilizing standard \\(\\LaTeX\\) code. A great resource for learning how to many common tasks in \\(\\LaTeX\\) is the Sharewebsite. Specific mathematical equation documentation may be found on the Mathematical Expressions subpage. There are also many websites that offer Point-n-click interfaces to build \\(\\LaTeX\\) equations, including: Host Math, Code Cogs, LaTeX 4 Technics, and Sci-Weavers "],["example-articles.html", "4 Example Articles 4.1 Cross-sectional: clustered or hierarchical 4.2 Repeated Measures: longitudinal (change over time) or conditional (no time component) 4.3 Both: Hierarchical longitudinal", " 4 Example Articles 4.1 Cross-sectional: clustered or hierarchical 4.1.1 Non-randomized Intervention Efficacy Study of Zearn Math in a Large Urban School District Public link USU-full text Note: The schools chose where to implement the ‘treatment’ (n = 15) or not (n = 20). Concept Article Details Terms hierarchical linear modeling (HLM) Samples Level 2: schools (n = 35) Level 1: students (n = 8718) Missing none noted Centering none noted Components descriptives, HLM, followup subgroup analysis (seems link only random intercepts) Results No plots, only tables presenting the main effect (treatment) and excluding covariates and variance components, effect sizes looks like the are Cohen’s d? Software Stata 4.1.2 Dyadic Design Parent couples’ participation in speech-language therapy for school-age children with autism spectrum disorder in the United States Public link USU-full text link Concept Article Details Terms ICC, multilevel models Samples Level 2: child (n = 40) Level 1: parent, mother and father (n = 80) Missing Only a small proportion of missing data, so composite variables were imputed using the expectation–maximization (EM) algorithm Centering Binary variables were entered uncentered Continuous variables were grand mean centered Components Null for ICC, Random intercepts (no random slopes), residual diagnostics Results Table with 3 MLM models, discussed moderation Software HLM (Version 7.01) using restricted maximum likelihood 4.1.3 Binary Outcome County-level social factors and schizophrenia: A multilevel study of 1.9 million Chinese adults Public link with PDF download Concept Article Details Terms multilevel logistic regression Samples Level 2: county (n = 734) Level 1: person (n = 1,909,205) Note: further nesting in 771,797 households, 2980 towns (streets) and 5964 communities (villages) from 31 province was not modeled Missing -not mentioned- Centering -not mentioned- Components ICC, adjusted odds ratios from GzLMM, subgroup analysis by gender Results Tables of un-adjusted and adjusted odds ratios Software Stata version 13.0 for Windows 4.2 Repeated Measures: longitudinal (change over time) or conditional (no time component) 4.2.1 Repeated Measures - linear growth One-to-one iPad echnology in the Middle School Mathematics and Science Classrooms Public link with PDF download Concept Article Details Terms hierarchical linear modeling (HLM) Samples Level 2: students (n = 112) Level 1: up to 6 observations across 2 years (n = 8718) Note: further nesting among 10 teachers in 3 schools was not modeled Time Unclear, but assume time is treated as 6 equally spaced intervals (t = numeric: 0, 1, 2, 3, 4, 5) Missing Students were not eliminated if they did not have six scores since HLM allows for missing data at the first level (i.e. complete case analysis) Centering grand mean centering for the MAP test scores Components Single-level OLS, Null model HLM, RIAS (random slope for time), add covars Results Table showing design, nested equations, several ‘final’ model tables of results Software SPSS 4.2.2 Cohort sequential or accelerated longitudinal design Examining the Links Between Received Network Support and Marital Quality Among Mothers of Children with ASD: A Longitudinal Mediation Analysis Public link USU-full text link PDF download Concept Article Details Terms multilevel modeling (MLM), conditional growth model, longitudinal multiple mediation models Samples Level 2 = mother/child (n = 96) Level 1 = time point/age (n = 3 x 96 ? ) Time Unclear, assume time is measured in years at study years 5, 7, and 9 (t = numeric age at each observation) Missing Assumed data was missing at random, so complete-case analysis Centering Time-varying predictors and mediators were disaggregated into their constituent within and between-person effects. To assess within-person effects, Level 1 predictors were created by person-mean centering each predictor or mediator (i.e., subtracting each mother’s cross-time mean score on a predictor from her actual score on that measure). Level 2 predictors were created by first computing a cross-time mean score on a predictor for each mother and then grand-mean centering that score. Finally, baseline child problem behavior severity was grand-mean centered with a mean of zero. Components Bivariate correlation matrix at baseline, Null for ICC, add fixed effects, mediation Results MLM only reported in text. All tables and figures apply to the mediation Software SPSS 25.0 with MLmed, a macro which tests for mediation and moderated mediation in multilevel data, Restricted maximum likelihood, 95% confidence intervals (CIs) based on Monte Carlo bootstrapping estimates 4.2.3 SEM Framework Disability multilevel modelling in first episodes of psychosis at 3-year follow-up Public link with PDF download Concept Article Details Terms multilevel modeling statistical approach to repeated measures data, growth model Samples Level 2: participants (n = 449) Level 1: observations = baseline, 1 year, and 3 years later (n = 3 x 449 = 1347) Time Very unclear, but it does include “linear time” in the results Missing Only patients providing data for all the study variables during follow-up, and those who were assessed from the beginning of the study to the 1-year and 3-year follow-up were finally analyzed. Centering -not mentioned- Components group comparisons with Cohen’s d effect sizes, and decrease in Bayesian Information Criterion adjusted to sample size (SABIC) used to assess significance when comparing models, RIAS: Random intercepts and slope of time, assumption checking Results Table comparing nested models Software Mplus 6.11 4.3 Both: Hierarchical longitudinal Concept Article Details Terms Samples Level 2: child (n = 40) Level 1: parent, mother and father (n = 80) Missing Centering Components Results Software "],["mlm-2-levels-pupil-popularity.html", "5 MLM, 2-levels: Pupil Popularity 5.1 Background 5.2 Exploratory Data Analysis 5.3 Single-level Regression Analysis 5.4 Multi-level Regression Analysis", " 5 MLM, 2-levels: Pupil Popularity Download/install a new GitHub package install.packages(&quot;devtools&quot;) devtools::install_github(&quot;goodekat/redres&quot;) Load/activate these packages library(tidyverse) library(haven) # read in SPSS dataset library(furniture) # nice table1() descriptives library(stargazer) # display nice tables: summary &amp; regression library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(gridExtra) # place ggplots together as one plot library(psych) # contains some useful functions, like headTail library(car) # Companion to Applied Regression library(nlme) # non-linear mixed-effects models library(lme4) # Linear, generalized linear, &amp; nonlinear mixed models library(lmerTest) # Tests on lmer objects library(optimx) # Different optimizers to solve mlm&#39;s library(performance) # icc and r-squared functions **NEWER** library(interactions) # interaction plots **NEWER** library(HLMdiag) # Diagnostic Tools for for MLM library(sjstats) # ICC calculations 5.1 Background The text “Multilevel Analysis: Techniques and Applications, Third Edition” (Hox, Moerbeek, and Van de Schoot 2017) has a companion website which includes links to all the data files used throughout the book (housed on the book’s GitHub repository). The following example is used through out (Hox, Moerbeek, and Van de Schoot 2017)’s chapater 2. From Appendix E: The popularity data in popular2.sav are simulated data for 2000 pupils (pupil) in 100 schools (class). The purpose is to offer a very simple example for multilevel regression analysis. The main OUTCOME or DEPENDENT VARIABLE (DV) is the pupil popularity, a popularity rating on a scale of 1-10 derived by a sociometric procedure. Typically, a sociometric procedure asks all pupils in a class to rate all the other pupils, and then assigns the average received popularity rating to each pupil (pupular). Because of the sociometric procedure, group effects as apparent from higher level variance components are rather strong. There is a second outcome variable, pupil popularity as rated by their teacher (popteach), on a scale from 1-10. The PREDICTORS or INDEPENDENT VARIABLES (IVs) are: pupil gender (sex) only two levels: boy = 0, girl = 1 pupil extroversion (extrav) 10-point scale (whole number) as rated by the teacher, higher values correspond to more extroverted teacher experience (texp) in years, reported as a whole number The popularity data have been generated to be a ‘nice’ well-behaved data set: the sample sizes at both levels are sufficient, the residuals have a normal distribution, and the multilevel effects are strong. Note: We will ignore the centered and standardized variables, which start with a capital Z or C. data_raw &lt;- haven::read_sav(&quot;https://github.com/MultiLevelAnalysis/Datasets-third-edition-Multilevel-book/raw/master/chapter%202/popularity/SPSS/popular2.sav&quot;) %&gt;% haven::as_factor() # retain the labels from SPSS --&gt; factor tibble::glimpse(data_raw) Rows: 2,000 Columns: 15 $ pupil &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1… $ class &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … $ extrav &lt;dbl&gt; 5, 7, 4, 3, 5, 4, 5, 4, 5, 5, 5, 5, 5, 5, 5, 6, 4, 4, 7, 4, … $ sex &lt;fct&gt; girl, boy, girl, girl, girl, boy, boy, boy, boy, boy, girl, … $ texp &lt;dbl&gt; 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, … $ popular &lt;dbl&gt; 6.3, 4.9, 5.3, 4.7, 6.0, 4.7, 5.9, 4.2, 5.2, 3.9, 5.7, 4.8, … $ popteach &lt;dbl&gt; 6, 5, 6, 5, 6, 5, 5, 5, 5, 3, 5, 5, 5, 6, 5, 5, 2, 3, 7, 4, … $ Zextrav &lt;dbl&gt; -0.1703149, 1.4140098, -0.9624772, -1.7546396, -0.1703149, -… $ Zsex &lt;dbl&gt; 0.9888125, -1.0108084, 0.9888125, 0.9888125, 0.9888125, -1.0… $ Ztexp &lt;dbl&gt; 1.48615283, 1.48615283, 1.48615283, 1.48615283, 1.48615283, … $ Zpopular &lt;dbl&gt; 0.88501327, -0.12762911, 0.16169729, -0.27229230, 0.66801848… $ Zpopteach &lt;dbl&gt; 0.66905609, -0.04308451, 0.66905609, -0.04308451, 0.66905609… $ Cextrav &lt;dbl&gt; -0.215, 1.785, -1.215, -2.215, -0.215, -1.215, -0.215, -1.21… $ Ctexp &lt;dbl&gt; 9.737, 9.737, 9.737, 9.737, 9.737, 9.737, 9.737, 9.737, 9.73… $ Csex &lt;dbl&gt; 0.5, -0.5, 0.5, 0.5, 0.5, -0.5, -0.5, -0.5, -0.5, -0.5, 0.5,… 5.1.1 Unique Identifiers We will restrict ourselves to a few of the variables and create a unique identifier variable for each student. data_pop &lt;- data_raw %&gt;% dplyr::mutate(id = paste(class, pupil, sep = &quot;_&quot;) %&gt;% # create a unique id for each student (char) factor()) %&gt;% # declare id is a factor dplyr::select(id, pupil:popteach) # reduce the variables included tibble::glimpse(data_pop) Rows: 2,000 Columns: 8 $ id &lt;fct&gt; 1_1, 1_2, 1_3, 1_4, 1_5, 1_6, 1_7, 1_8, 1_9, 1_10, 1_11, 1_12… $ pupil &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18… $ class &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2… $ extrav &lt;dbl&gt; 5, 7, 4, 3, 5, 4, 5, 4, 5, 5, 5, 5, 5, 5, 5, 6, 4, 4, 7, 4, 8… $ sex &lt;fct&gt; girl, boy, girl, girl, girl, boy, boy, boy, boy, boy, girl, g… $ texp &lt;dbl&gt; 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 2… $ popular &lt;dbl&gt; 6.3, 4.9, 5.3, 4.7, 6.0, 4.7, 5.9, 4.2, 5.2, 3.9, 5.7, 4.8, 5… $ popteach &lt;dbl&gt; 6, 5, 6, 5, 6, 5, 5, 5, 5, 3, 5, 5, 5, 6, 5, 5, 2, 3, 7, 4, 6… 5.1.2 Structure and variables Its a good idea to visually inspect the first few lines in the datast to get a sense of how it is organized. data_pop %&gt;% psych::headTail(top = 25, bottom = 5) %&gt;% pander::pander() id pupil class extrav sex texp popular popteach 1_1 1 1 5 girl 24 6.3 6 1_2 2 1 7 boy 24 4.9 5 1_3 3 1 4 girl 24 5.3 6 1_4 4 1 3 girl 24 4.7 5 1_5 5 1 5 girl 24 6 6 1_6 6 1 4 boy 24 4.7 5 1_7 7 1 5 boy 24 5.9 5 1_8 8 1 4 boy 24 4.2 5 1_9 9 1 5 boy 24 5.2 5 1_10 10 1 5 boy 24 3.9 3 1_11 11 1 5 girl 24 5.7 5 1_12 12 1 5 girl 24 4.8 5 1_13 13 1 5 boy 24 5 5 1_14 14 1 5 girl 24 5.5 6 1_15 15 1 5 girl 24 6 5 1_16 16 1 6 girl 24 5.7 5 1_17 17 1 4 boy 24 3.2 2 1_18 18 1 4 boy 24 3.1 3 1_19 19 1 7 girl 24 6.6 7 1_20 20 1 4 boy 24 4.8 4 2_1 1 2 8 girl 14 6.4 6 2_2 2 2 4 boy 14 2.4 3 2_3 3 2 6 boy 14 3.7 4 2_4 4 2 5 girl 14 4.4 4 2_5 5 2 5 girl 14 4.3 4 NA … … … NA … … … 100_16 16 100 4 girl 7 4.3 5 100_17 17 100 4 boy 7 2.6 2 100_18 18 100 8 girl 7 6.7 7 100_19 19 100 5 boy 7 2.9 3 100_20 20 100 9 boy 7 5.3 5 Visual inspection reveals that most of the variables are measurements at level 1 and apply to specific pupils (extrav, sex, popular, and popteach), while the teacher’s years of experiene is a level 2 variable since it applies to the entire class. Notice how the texp variable is identical for all pupils in the same class. This is call Disaggregated data. 5.2 Exploratory Data Analysis 5.2.1 Summarize Descriptive Statistics 5.2.1.1 The stargazer package Most posters, journal articles, and reports start with a table of descriptive statistics. Since it tends to come first, this type of table is often refered to as Table 1. The stargazer() function can be used to create such a table, but only for the entire dataset (Hlavac 2022). I haven’t been able to find a way to get it to summarize subsamples and compare them in the standard format. It is nice that this table includes the minimum and maximum, as well as the quartiles. # Knit to Website: type = &quot;html&quot; # Knit to PDF: type = &quot;latex&quot; # View on Screen: type = &quot;text&quot; data_pop %&gt;% dplyr::select(extrav, texp, popular) %&gt;% data.frame() %&gt;% stargazer::stargazer(title = &quot;Descriptive statistics, aggregate over entire sample&quot;, header = FALSE, type = &quot;text&quot;) Descriptive statistics, aggregate over entire sample Statistic N Mean St. Dev. Min Max extrav 2,000 5.215 1.262 1 10 texp 2,000 14.263 6.552 2 25 popular 2,000 5.076 1.383 0.000 9.500 5.2.1.2 The furniture package Tyson Barrett’s furniture package includes the extremely useful function table1() which simplifies the common task of creating a stratified, comparative table of descriptive statistics. Full documentation can be accessed by executing ?furniture::table1. # Knit to Website: output = &quot;html&quot; # Knit to PDF: output = &quot;latex2&quot; # View on Screen: output = &quot;&quot;text&quot;, or &quot;markdown&quot;, &quot;html&quot; data_pop %&gt;% furniture::table1(&quot;Pupil&#39;s extroversion (10 pt)&quot; = extrav, &quot;Teacher&#39;s Experience (years)&quot; = texp, &quot;Popularity, Sociometric Score&quot; = popular, &quot;Popularity, Teacher Evaluated&quot; = popteach, splitby = ~ sex, # divide sample into columns by... total = TRUE, # include a total column test = TRUE, # test groups different? digits = 2, # 2 decimal places output = &quot;markdown&quot;, # output for on screen/knitting caption = &quot;Compare genders on four main variables&quot;) # title Table 5.1: Compare genders on four main variables Total boy girl P-Value n = 2000 n = 989 n = 1011 Pupil’s extroversion (10 pt) &lt;.001 5.21 (1.26) 5.10 (1.17) 5.33 (1.34) Teacher’s Experience (years) 0.001 14.26 (6.55) 13.78 (6.28) 14.73 (6.78) Popularity, Sociometric Score &lt;.001 5.08 (1.38) 4.28 (1.14) 5.85 (1.14) Popularity, Teacher Evaluated &lt;.001 5.06 (1.40) 4.29 (1.18) 5.82 (1.18) 5.2.2 Visualizations of Raw Data 5.2.2.1 Ignore Clustering 5.2.2.1.1 Scatterplots For a first look, its useful to plot all the data points on a single scatterplot as displayed in Figure 5.1. Due to ganularity in the rating scale, many points end up being plotted on top of each other (overplotted), so its a good idea to use geom_count() rather than geom_point() so the size of the dot can convey the number of points at that location (Wickham et al. 2022). # Disaggregate: pupil (level 1) only, ignore level 2&#39;s existance # extroversion treated: continuous measure data_pop %&gt;% ggplot() + aes(x = extrav, # x-axis variable y = popular) + # y-axis variable geom_count() + # POINTS w/ SIZE = COUNT geom_smooth(method = &quot;lm&quot;) + # linear regression line theme_bw() + # white background labs(x = &quot;extroversion (10 pt scale)&quot;, # x-axis label y = &quot;Popularity, Sociometric Score&quot;, # y-axis label size = &quot;Count&quot;) + # legend key&#39;s title theme(legend.position = c(0.9, 0.2), # key at legend.background = element_rect(color = &quot;black&quot;)) + # key box scale_x_continuous(breaks = seq(from = 0, to = 10, by = 1)) + # x-ticks scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) # y-ticks Figure 5.1: Disaggregate: pupil level only with extroversion treated as an continuous measure. 5.2.2.1.2 Density Plots When the degree of overplotting as high as it is in Figure 5.1, it can be useful to represent the data with density contours as seen in Figure 5.2. I’ve chosen to leave the points displayed in this redition, but color them much lighter so that they are present, but do not detract from the pattern of association. data_pop %&gt;% ggplot() + aes(x = extrav, # x-axis variable y = popular) + # y-axis variable geom_count(color = &quot;gray&quot;) + # POINTS w/ SIZE = COUNT geom_density2d() + # DENSITY CURVES geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;) + # linear regression line theme_bw() + # white background labs(x = &quot;Extroversion (10 pt scale)&quot;, # x-axis label y = &quot;Popularity, Sociometric Score&quot;) + # y-axis label guides(size = FALSE) + # don&#39;t include a legend scale_x_continuous(breaks = seq(from = 0, to = 10, by = 1)) + # x-ticks scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) # y-ticks Figure 5.2: Disaggregate: pupil level only with extroversion treated as an continuous measure. 5.2.2.1.3 Histograms, stacked The argument could be made that the extroversion score should be treated as an ordinal factor instead of as a truly continuous scale since the only valid values are the whole number 1 through 10 and there is no assurance that these category assignments represent a true ratio measurement scale. However, we must keep in mind that this was an observational study, ans as such, the number of pupils assignment each level of extroversion is not equal. # count the number of pupils in assigned each extroversion value, 1:10 table &lt;- data_pop %&gt;% group_by(extrav) %&gt;% summarise(count = n_distinct(id), percent = 100 * count / 2000) # Knit to Website: type = &quot;html&quot; # Knit to PDF: type = &quot;latex&quot; # View on Screen: type = &quot;text&quot; table %&gt;% stargazer::stargazer(summary = FALSE, rownames = FALSE, header = FALSE, type = &quot;text&quot;, title = &quot;Distribution of extroversion in pupils&quot;) Distribution of extroversion in pupils extrav count percent 1 3 0.15 2 13 0.65 3 119 5.95 4 423 21.15 5 688 34.4 6 478 23.9 7 194 9.7 8 58 2.9 9 18 0.9 10 6 0.3 data_pop %&gt;% ggplot(aes(popular)) + # y-axis variable geom_histogram() + theme_bw() + # white background labs(#y = &quot;Extroversion (10 pt scale)&quot;, # x-axis label x = &quot;Popularity, Sociometric Score&quot;) + # y-axis label guides(size = FALSE) + # don&#39;t include a legend facet_grid(extrav~.) Figure 5.3: Disaggregate: pupil level only with extroversion treated as an continuous measure. 5.2.2.1.4 Boxplots Figure 5.4 displays the same data as Figure 5.1, but uses boxplots for the distribution of scores at each level of extroversion. On one extreme, the lowest extroversion score possible was a value of “one”, but only 3 pupils or 0.15% of the 2000 pupils recieved this value. On the other extreme, the middle value of “five” was applied to 688 pupils or a wopping 34.4%. The option varwidth=TRUE in the geom_boxplot() function helps reflect such unbalanced sample sizes by allowing the width of the boxes to be proportional to the square-roots of the number of observations each box represents. # Disaggregate: pupil (level 1) only, ignore level 2&#39;s existance # extroversion treated: ordinal factor ggplot(data_pop, # dataset&#39;s name aes(x = factor(extrav), # x-axis values - make factor! y = popular, # y-axis values fill = factor(extrav))) + # makes seperate boxes geom_boxplot(varwidth = TRUE) + # draw boxplots instead of points theme_bw() + # white background guides(fill = FALSE) + # don&#39;t include a legend scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) + # y-ticks labs(x = &quot;extroversion (10 pt scale)&quot;, # x-axis label y = &quot;Popularity, Sociometric Score&quot;) + # y-axis label scale_fill_brewer(palette = &quot;Spectral&quot;, direction = 1) # select color Figure 5.4: Disaggregate: pupil level only with extroversion treated as an ordinal factor. The width of the boxes are proportional to the square-roots of the number of observations each box represents. 5.2.3 Consider Clustering 5.2.3.1 Scatterplots Up to this point, all investigation of this dataset has been only at the pupil level and any nesting or clustering within classes has been ignored. Plotting is a good was to start to get an idea of the class-to-class variability. # compare the first 9 classrooms becuase all of there are too many at once data_pop %&gt;% dplyr::filter(class &lt;= 9) %&gt;% # select ONLY NINE classes ggplot(aes(x = extrav, # x-axis values y = popular)) + # y-axis values geom_count() + # POINTS w/ SIZE = COUNT geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;) + # linear regression line theme_bw() + # white background labs(x = &quot;extroversion (10 pt scale)&quot;, # x-axis label y = &quot;Popularity, Sociometric Score&quot;, # y-axis label size = &quot;Count&quot;) + # legend key&#39;s title guides(size = FALSE) + # don&#39;t include a legend scale_x_continuous(breaks = seq(from = 0, to = 10, by = 3)) + # x-ticks scale_y_continuous(breaks = seq(from = 0, to = 10, by = 3)) + # y-ticks facet_wrap(~ class, labeller = label_both) + theme(strip.background = element_rect(colour = NA, fill = NA)) Figure 5.5: Illustration of the degree of class level variability in the association between extroversion and popularity. Each panel represents a class and each point a pupil in that class. First nice classes shown. # select specific classes by number for illustration purposes data_pop %&gt;% dplyr::filter(class %in% c(15, 25, 33, 35, 51, 64, 76, 94, 100)) %&gt;% ggplot(aes(x = extrav, # x-axis values y = popular)) + # y-axis values geom_count() + # POINTS w/ SIZE = COUNT geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;) + # linear regression line theme_bw() + # white background labs(x = &quot;extroversion (10 pt scale)&quot;, # x-axis label y = &quot;Popularity, Sociometric Score&quot;, # y-axis label size = &quot;Count&quot;) + # legend key&#39;s title guides(size = FALSE) + # don&#39;t include a legend scale_x_continuous(breaks = seq(from = 0, to = 10, by = 3)) + # x-ticks scale_y_continuous(breaks = seq(from = 0, to = 10, by = 3)) + # y-ticks facet_wrap(~ class) + theme(strip.background = element_blank(), strip.text = element_blank()) Figure 5.6: Illustration of the degree of class level variability in the association between extroversion and popularity. Each panel represents a class and each point a pupil in that class. A set of nine classes was chosen to show a sampling of variability. The facet labels are not shown as the identification number probably would not be advisable for a general publication. 5.2.3.2 Cluster-wise Regression # compare all 100 classrooms via linear model for each data_pop %&gt;% ggplot(aes(x = extrav, # x-axis values y = popular, # y-axis values group = class)) + # GROUPs for LINES geom_smooth(method = &quot;lm&quot;, # linear regression line color = &quot;gray40&quot;, size = 0.4, se = FALSE) + theme_bw() + # white background labs(x = &quot;extroversion (10 pt scale)&quot;, # x-axis label y = &quot;Popularity, Sociometric Score&quot;) + # y-axis label scale_x_continuous(breaks = seq(from = 0, to = 10, by = 2)) + # x-ticks scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) # y-ticks Figure 5.7: Spaghetti plot of seperate, independent linear models for each of the 100 classes. A helpful resource for choosing colors to use in plots: R color cheatsheet # compare all 100 classrooms via independent linear models data_pop %&gt;% dplyr::mutate(texp3 = cut(texp, breaks = c(0, 10, 18, 30)) %&gt;% factor(labels = c(&quot;&lt; 10 yrs&quot;, &quot;10 - 18 yrs&quot;, &quot;&gt; 18 yrs&quot;))) %&gt;% ggplot(aes(x = extrav, # x-axis values y = popular, # y-axis values group = class)) + # GROUPs for LINES geom_smooth(aes(color = sex), size = 0.3, method = &quot;lm&quot;, # linear regression line se = FALSE) + theme_bw() + # white background labs(x = &quot;extroversion (10 pt scale)&quot;, # x-axis label y = &quot;Popularity, Sociometric Score&quot;) + # y-axis label guides(color = FALSE) + # don&#39;t include a legend scale_x_continuous(breaks = seq(from = 0, to = 10, by = 3)) + # x-ticks scale_y_continuous(breaks = seq(from = 0, to = 10, by = 3)) + # y-ticks scale_color_manual(values = c(&quot;dodgerblue&quot;, &quot;maroon1&quot;)) + facet_grid(texp3 ~ sex) Figure 5.8: Spaghetti plot of seperate, independent linear models for each of the 100 classes. Seperate panels are used to untangle the ‘hairball’ in the previous figure. The columns are seperated by the pupils’ gender and the rows by the teacher’s experince in years. 5.3 Single-level Regression Analysis 5.3.1 Null Model In a Null, intercept-only, or Empty model, no predictors are included. 5.3.1.1 Equations Single-Level Regression Equation - Null Model \\[ \\overbrace{POP_{ij}}^{Outcome} = \\underbrace{\\beta_{0}}_{\\text{Fixed}\\atop\\text{intercept}} + \\underbrace{e_{ij}}_{\\text{Random}\\atop\\text{residuals}} \\] 5.3.1.2 Parameters Type Parameter of Interest Estimates This Fixed Intercept \\(\\beta_{0}\\) Random Residual Variance \\(var[e_{ij}]\\) \\(\\sigma^2_{e}\\) 5.3.1.3 Fit the Model pop_lm_0 &lt;- lm(popular ~ 1, # The 1 represents the intercept data = data_pop) summary(pop_lm_0) Call: lm(formula = popular ~ 1, data = data_pop) Residuals: Min 1Q Median 3Q Max -5.0765 -0.9765 0.0235 0.9236 4.4235 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.07645 0.03091 164.2 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.383 on 1999 degrees of freedom \\(\\hat{\\beta_0}\\) = 5.08 is the grand mean pop_glm_0 &lt;- glm(popular ~ 1, # The 1 represents the intercept data = data_pop) summary(pop_glm_0) Call: glm(formula = popular ~ 1, data = data_pop) Deviance Residuals: Min 1Q Median 3Q Max -5.0764 -0.9764 0.0236 0.9236 4.4236 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.07645 0.03091 164.2 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for gaussian family taken to be 1.911366) Null deviance: 3820.8 on 1999 degrees of freedom Residual deviance: 3820.8 on 1999 degrees of freedom AIC: 6974.4 Number of Fisher Scoring iterations: 2 5.3.1.4 Model Fit performance::performance(pop_lm_0) # Indices of model performance AIC | BIC | R2 | R2 (adj.) | RMSE | Sigma ------------------------------------------------------- 6974.390 | 6985.592 | 0.000 | 0.000 | 1.382 | 1.383 performance::performance(pop_glm_0) # Indices of model performance AIC | BIC | R2 | RMSE | Sigma ------------------------------------------- 6974.390 | 6985.592 | 0.000 | 1.382 | 1.383 performance::compare_performance(pop_lm_0, pop_glm_0) # Comparison of Model Performance Indices Name | Model | AIC (weights) | BIC (weights) | R2 | RMSE | Sigma | R2 (adj.) --------------------------------------------------------------------------------------- pop_lm_0 | lm | 6974.4 (0.500) | 6985.6 (0.500) | 0.000 | 1.382 | 1.383 | 0.000 pop_glm_0 | glm | 6974.4 (0.500) | 6985.6 (0.500) | 0.000 | 1.382 | 1.383 | Residual variance: sigma(pop_lm_0) # standard deviation of the residuals [1] 1.382522 sigma(pop_lm_0)^2 # variance of the residuals [1] 1.911366 \\(\\hat{\\sigma_e^2}\\) = 1.9114 is residual variance (RMSE is sigma = 1.3825) Variance Explained: summary(pop_lm_0)$r.squared [1] 0 \\(R^2\\) = 0 is the proportion of variance in popularity that is explained by the grand mean alone. Deviance: -2 * logLik(pop_lm_0) &#39;log Lik.&#39; 6970.39 (df=2) 5.3.1.5 Interpretation The grand average popularity of all pupils in all the classes is 5.08, and there is strong evidence that it is statistically significantly different than zero, \\(p&lt;.0001\\). The mean alone accounts for none of the variance in popularity. The residual variance is the same as the total variance in popularity, 1.9114. Just to make sure… mean(data_pop$popular) [1] 5.07645 var(data_pop$popular) [1] 1.911366 5.3.2 Add Predictors to the Model 5.3.2.1 Equations LEVEL 1: Student-specific predictors: \\(X_1 = GEN\\), pupils’s gender (girl vs. boy) \\(X_2 = EXT\\), pupil’s extroversion (scale: 1-10) Single-Level Regression Equation \\[ \\overbrace{POP_{ij}}^{Outcome} = \\underbrace{\\beta_{0}}_{\\text{Fixed}\\atop\\text{intercept}} + \\underbrace{\\beta_{1}}_{\\text{Fixed}\\atop\\text{slope}} \\overbrace{GEN_{ij}}^{\\text{Predictor 1}} + \\underbrace{\\beta_{2}}_{\\text{Fixed}\\atop\\text{slope}} \\overbrace{EXT_{ij}}^{\\text{Predictor 2}} + \\underbrace{e_{ij}}_{\\text{Random}\\atop\\text{residuals}} \\tag{Hox 2.1} \\] 5.3.2.2 Parameters Type Parameter of Interest Estimates This Fixed Intercept \\(\\beta_{0}\\) Fixed Slope or effect of sex \\(\\beta_{1}\\) Fixed Slope or effect of extrav \\(\\beta_{2}\\) Random Residual Variance \\(var[e_{ij}]\\) \\(\\sigma^2_{e}\\) 5.3.2.3 Fit the Model pop_lm_1 &lt;- lm(popular ~ sex + extrav, # implies: 1 + sex + extrav data = data_pop) summary(pop_lm_1) Call: lm(formula = popular ~ sex + extrav, data = data_pop) Residuals: Min 1Q Median 3Q Max -4.2527 -0.6652 -0.0454 0.7422 3.0473 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 2.78954 0.10355 26.94 &lt;2e-16 *** sexgirl 1.50508 0.04836 31.12 &lt;2e-16 *** extrav 0.29263 0.01916 15.28 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.077 on 1997 degrees of freedom Multiple R-squared: 0.3938, Adjusted R-squared: 0.3932 F-statistic: 648.6 on 2 and 1997 DF, p-value: &lt; 2.2e-16 pop_glm_1 &lt;- glm(popular ~ sex + extrav, # implies: 1 + sex + extrav data = data_pop) summary(pop_glm_1) Call: glm(formula = popular ~ sex + extrav, data = data_pop) Deviance Residuals: Min 1Q Median 3Q Max -4.2527 -0.6652 -0.0454 0.7422 3.0473 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 2.78954 0.10355 26.94 &lt;2e-16 *** sexgirl 1.50508 0.04836 31.12 &lt;2e-16 *** extrav 0.29263 0.01916 15.28 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for gaussian family taken to be 1.159898) Null deviance: 3820.8 on 1999 degrees of freedom Residual deviance: 2316.3 on 1997 degrees of freedom AIC: 5977.4 Number of Fisher Scoring iterations: 2 \\(\\hat{\\beta_0}\\) = 2.79 is the extrapolated mean for boys with an extroversion score of 0. \\(\\hat{\\beta_1}\\) = 1.51 is the mean difference between girls and boys with the same extroversion score. \\(\\hat{\\beta_2}\\) = 0.29 is the mean difference for pupils of the same gender that differ in extroversion by one point. 5.3.2.4 Model Fit Residual variance: sigma(pop_lm_1) # standard deviation of the residuals [1] 1.076985 sigma(pop_lm_1)^2 # variance of the residuals [1] 1.159898 \\(\\hat{\\sigma_e^2}\\) = 1.1599 is residual variance (RMSE is sigma) Variance Explained: summary(pop_lm_1)$r.squared [1] 0.393765 Deviance: -2 * logLik(pop_lm_1) &#39;log Lik.&#39; 5969.415 (df=4) \\(R^2\\) = 0.394 is the proportion of variance in popularity that is explained by tha pupils gender and extroversion score. performance::performance(pop_lm_1) # Indices of model performance AIC | BIC | R2 | R2 (adj.) | RMSE | Sigma ------------------------------------------------------- 5977.415 | 5999.819 | 0.394 | 0.393 | 1.076 | 1.077 Note”: BF = the Bayes factor performance::compare_performance(pop_lm_0, pop_lm_1, rank = TRUE) # Comparison of Model Performance Indices Name | Model | R2 | R2 (adj.) | RMSE | Sigma | AIC weights | BIC weights | Performance-Score ---------------------------------------------------------------------------------------------------- pop_lm_1 | lm | 0.394 | 0.393 | 1.076 | 1.077 | 1.00 | 1.00 | 100.00% pop_lm_0 | lm | 0.000 | 0.000 | 1.382 | 1.383 | 3.23e-217 | 8.75e-215 | 0.00% performance::compare_performance(pop_glm_0, pop_glm_1, rank = TRUE) # Comparison of Model Performance Indices Name | Model | R2 | RMSE | Sigma | AIC weights | BIC weights | Performance-Score ----------------------------------------------------------------------------------------- pop_glm_1 | glm | 0.394 | 1.076 | 1.077 | 1.00 | 1.00 | 100.00% pop_glm_0 | glm | 0.000 | 1.382 | 1.383 | 3.23e-217 | 8.75e-215 | 0.00% 5.3.2.5 Interpretation On average, girls were rated 1.51 points more popular than boys with the same extroversion score, \\(p&lt;.0001\\). One point higher extroversion scores were associated with 0.29 points higher popularity, within each gender, \\(p&lt;.0001\\). Together, these two factors account for 39.38% of the variance in populartiy. 5.3.3 Compare Fixed Effects 5.3.3.1 Compare Nested Models Create a table to compare the two nested models: texreg::knitreg(list(pop_glm_0, pop_glm_1), custom.model.names = c(&quot;Null Model&quot;, &quot;With Predictors&quot;), caption = &quot;Single Level Models: ML with the `glm()` function&quot;, caption.above = TRUE, single.row = TRUE, bold = TRUE, label = &quot;wow&quot;) Single Level Models: ML with the glm() function   Null Model With Predictors (Intercept) 5.08 (0.03)*** 2.79 (0.10)*** sexgirl   1.51 (0.05)*** extrav   0.29 (0.02)*** AIC 6974.39 5977.42 BIC 6985.59 5999.82 Log Likelihood -3485.20 -2984.71 Deviance 3820.82 2316.32 Num. obs. 2000 2000 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 texreg::knitreg(list(pop_glm_0, pop_glm_1), custom.model.names = c(&quot;Null Model&quot;, &quot;With Predictors&quot;), caption = &quot;Single Level Models: ML with the `glm()` function&quot;, caption.above = TRUE, single.row = TRUE) Single Level Models: ML with the glm() function   Null Model With Predictors (Intercept) 5.08 (0.03)*** 2.79 (0.10)*** sexgirl   1.51 (0.05)*** extrav   0.29 (0.02)*** AIC 6974.39 5977.42 BIC 6985.59 5999.82 Log Likelihood -3485.20 -2984.71 Deviance 3820.82 2316.32 Num. obs. 2000 2000 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 When comparing the fit of two single-level models fit via the lm() function, the anova() function runs an F-test where the test statistic is the difference in RSS. anova(pop_lm_0, pop_lm_1) Analysis of Variance Table Model 1: popular ~ 1 Model 2: popular ~ sex + extrav Res.Df RSS Df Sum of Sq F Pr(&gt;F) 1 1999 3820.8 2 1997 2316.3 2 1504.5 648.55 &lt; 2.2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(pop_glm_0, pop_glm_1) Analysis of Deviance Table Model 1: popular ~ 1 Model 2: popular ~ sex + extrav Resid. Df Resid. Dev Df Deviance 1 1999 3820.8 2 1997 2316.3 2 1504.5 Obviously the model with predictors fits better than the model with no predictors. 5.3.3.2 Terminology The following terminology applies to single-level models fit with ordinary least-squared estimation (the lm() function in \\(R\\)). Values are calculated below for the NULL model. Mean squared error (MSE) is the MEAN of the square of the residuals: mse &lt;- mean(residuals(pop_lm_0)^2) mse [1] 1.91041 Root mean squared error (RMSE) which is the SQUARE ROOT of MSE: rmse &lt;- sqrt(mse) rmse [1] 1.382176 Residual sum of squares (RSS) is the SUM of the squared residuals: rss &lt;- sum(residuals(pop_lm_0)^2) rss [1] 3820.821 Residual standard error (RSE) is the SQUARE ROOT of (RSS / degrees of freedom): rse &lt;- sqrt( sum(residuals(pop_lm_0)^2) / pop_lm_0$df.residual ) rse [1] 1.382522 The same calculation, may be simplified with the previously calculated RSS: sqrt(rss / pop_lm_0$df.residual) [1] 1.382522 When the ‘deviance()’ function is applied to a single-level model fit via ‘lm()’, the Residual sum of squares (RSS) is returned, not the deviance as defined as twice the negative log likelihood (-2LL). deviance(pop_lm_0) # returns the RSS, not deviance = -2LL [1] 3820.821 -2 * logLik(pop_lm_0) # this is how get deviance = -2LL &#39;log Lik.&#39; 6970.39 (df=2) 5.4 Multi-level Regression Analysis 5.4.1 Intercept-only or Null Model In a Null, intercept-only, or Empty model, no predictors are included. “The intercept-only model is useful as a null-model that serves as a benchmark with which other models are compared.” Hox, Moerbeek, and Van de Schoot (2017), page 13 5.4.1.1 Equations Level 1 Model Equation: \\[ \\overbrace{Y_{ij}}^{Outcome} = \\underbrace{\\beta_{0j}}_{\\text{Level 2}\\atop\\text{intercepts}} + \\underbrace{e_{ij}}_{\\text{Random}\\atop\\text{residuals}} \\tag{Hox 2.6} \\] Level 2 Model Equation: \\[ \\overbrace{\\beta_{0j}}^{\\text{Level 2}\\atop\\text{intercepts}} = \\underbrace{\\gamma_{00}}_{\\text{Fixed}\\atop\\text{intercept}} + \\underbrace{u_{0j}}_{\\text{Random}\\atop\\text{intercepts}} \\tag{Hox 2.7} \\] Substitute equation (2.7) into equation (2.6): Combined, Multilevel Model Equation - Null Model \\[ \\overbrace{Y_{ij}}^{Outcome} = \\underbrace{\\gamma_{00}}_{\\text{Fixed}\\atop\\text{intercept}} + \\underbrace{u_{0j}}_{\\text{Random}\\atop\\text{intercepts}} + \\underbrace{e_{ij}}_{\\text{Random}\\atop\\text{residuals}} \\tag{Hox 2.8} \\] 5.4.1.2 Parameters Type Parameter of Interest Estimates This Fixed Intercept \\(\\gamma_{00}\\) Random Variance in random intercepts, \\(var[u_{0j}]\\) \\(\\sigma^2_{u0}\\) Random Residual Variance \\(var[e_{ij}]\\) \\(\\sigma^2_{e}\\) (Hox, Moerbeek, and Van de Schoot 2017) labeled the Null model for this dataset “\\(M_0\\)” in chapter 2: Combined, Multilevel Model Equation - Popularity, Random Intercepts Only! \\[ \\overbrace{POP_{ij}}^{Outcome} = \\underbrace{\\gamma_{00}}_{\\text{Fixed}\\atop\\text{intercept}} + \\underbrace{u_{0j}}_{\\text{Random}\\atop\\text{intercepts}} + \\underbrace{e_{ij}}_{\\text{Random}\\atop\\text{residuals}} \\tag{M0: intercept only} \\] 5.4.1.3 Fit the Model Fit the model to the data. pop_lmer_0_re &lt;- lmerTest::lmer(popular ~ 1 + (1|class), # include a fixed and random intercept data = data_pop, REML = TRUE) # fit via REML (the default) for ICC calculations summary(pop_lmer_0_re) Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ lmerModLmerTest] Formula: popular ~ 1 + (1 | class) Data: data_pop REML criterion at convergence: 6330.5 Scaled residuals: Min 1Q Median 3Q Max -3.5655 -0.6975 0.0020 0.6758 3.3175 Random effects: Groups Name Variance Std.Dev. class (Intercept) 0.7021 0.8379 Residual 1.2218 1.1053 Number of obs: 2000, groups: class, 100 Fixed effects: Estimate Std. Error df t value Pr(&gt;|t|) (Intercept) 5.07786 0.08739 98.90973 58.1 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Estimation Methods Multilevel models may be fit by various methods. The most commonly used (and availabel in ‘lme4’) optimize various criterions: Maximum Likelihood (ML) -or- Restricted Maximum Likelihood (REML). Hox, Moerbeek, and Van de Schoot (2017) discusses these and other methods in chapter 3. At the end of chapter 2, the authors’ second note staes that the details of estimation methods are glossed over in the current example in an effort to simplfy the introductory. Here we follow these guidelines: Use ML for fitting: nested models that differ only by inclusion/exclusion of FIXED effects, to test parameter significance via a likelihood ratio test Use REML for fitting: the NULL model, on which to base ICC calculations nested models that differ only by inclusion/exclusion of RANDOM effects, to test parameter significance via a likelihood ratio test the FINAL model This often leads to refitting identical models via BOTH estimation methods. 5.4.1.4 Interpretation The grand average popularity of all students is 5.0779 and the class averages tend to vary by about 0.8379 points above or below that. 5.4.2 Intraclass Correlation (ICC) Although the Null model above does not explain any variance in the dependent variable (popularity), since there are no independent variables, it does decompose (i.e. divide up) the variance in the dependent variable into two pieces. We can compute the amount of total variance in popularity that is attribute to the clustering of students (i.e. class-to-class variance or between-class variance) in classes verses the residual variance (i.e. student-to-student variance or within-class variance). Intraclass Correlation (ICC) Formula \\[ \\overbrace{\\rho}^{\\text{ICC}} = \\frac{\\overbrace{\\sigma^2_{u0}}^{\\text{Random Intercept}\\atop\\text{Variance}}} {\\underbrace{\\sigma^2_{u0}+\\sigma^2_{e}}_{\\text{Total}\\atop\\text{Variance}}} \\tag{Hox 2.9} \\] The VarCorr() function in the lme4 package returns the standard deviations, not the variances (\\(var = SD^2\\)) for a model fit via the lme4::lmer() function. The summary() function reports both the variances and the stadard deviations. lme4::VarCorr(pop_lmer_0_re) %&gt;% # extract random compondent: varrainces and correlations print(comp = c(&quot;Variance&quot;, &quot;Std.Dev&quot;), digits = 3) Groups Name Variance Std.Dev. class (Intercept) 0.702 0.838 Residual 1.222 1.105 insight::get_variance(pop_lmer_0_re) $var.fixed [1] 0 $var.random [1] 0.7021047 $var.residual [1] 1.221793 $var.distribution [1] 1.221793 $var.dispersion [1] 0 $var.intercept class 0.7021047 Again, this partitions the amount of total variance in popularity that is attribute to the clustering of students (i.e. class-to-class variance or between-class variance) in classes verses the residual variance (i.e. student-to-student variance or within-class variance). \\[ \\begin{align*} \\text{between classes} \\rightarrow \\; &amp; \\sigma^2_{u0} = 0.83792^2 = 0.702\\\\ \\text{pupils within classes} \\rightarrow \\; &amp; \\sigma^2_{e} = 1.10535^2 = 1.222\\\\ \\end{align*} \\] 5.4.2.1 By Hand Calculate the ICC by hand: \\[ \\overbrace{\\rho}^{\\text{ICC}} = \\frac{\\overbrace{\\sigma^2_{u0}}^{\\text{Random Intercept}\\atop\\text{Variance}}} {\\underbrace{\\sigma^2_{u0}+\\sigma^2_{e}}_{\\text{Total}\\atop\\text{Variance}}} = \\frac{0.702} {0.702+1.222} = \\frac{0.702} {1.924} = 0.3648649 \\] 0.702 / (0.702 + 1.222) [1] 0.3648649 5.4.2.2 The performance package citation(&quot;performance&quot;) To cite package &#39;performance&#39; in publications use: Lüdecke et al., (2021). performance: An R Package for Assessment, Comparison and Testing of Statistical Models. Journal of Open Source Software, 6(60), 3139. https://doi.org/10.21105/joss.03139 A BibTeX entry for LaTeX users is @Article{, title = {{performance}: An {R} Package for Assessment, Comparison and Testing of Statistical Models}, author = {Daniel Lüdecke and Mattan S. Ben-Shachar and Indrajeet Patil and Philip Waggoner and Dominique Makowski}, year = {2021}, journal = {Journal of Open Source Software}, volume = {6}, number = {60}, pages = {3139}, doi = {10.21105/joss.03139}, } Calculate the ICC with the icc() function in the performance package: performance::icc(pop_lmer_0_re) # Intraclass Correlation Coefficient Adjusted ICC: 0.365 Unadjusted ICC: 0.365 5.4.2.3 Interpretation WOW! 36.5% of the variance of the popularity scores is at the group level, which is very high for social science data. The ICC should be based on a Null (intercept only) model fit via REML (restricted maximum likelihood) estimation. This is the default for the ‘lme4::lmer()’ function. In chapter 2, Hox, Moerbeek, and Van de Schoot (2017) presents the numbers based on fitting the model via ML (maximum likelihood) estimation and thus does not match the presentation above exactly (not just rounding error). This is because: (1) estimation methods (REML &amp; ML) are not discussed until chapter 3 and (2) due to the Null model also being used for model fit comparisons in Table 2.1 on the top of page 14. Here we will fit the empty model twice, above by ML and below by REML 5.4.2.4 Percent of variance explained The marginal \\(R^2\\) considers only the variance of the fixed effects, while the conditional \\(R^2\\) takes both the fixed and random effects into account. The random effect variances are actually the mean random effect variances, thus the \\(R^2\\) value is also appropriate for mixed models with random slopes or nested random effects (see Johnson 2014)*. Johnson, P. C. D. (2014). Extension of Nakagawa &amp; Schielzeth’s R2 GLMM to random slopes models. Methods in Ecology and Evolution, 5(9), 944–946. doi: 10.1111/2041-210X.12225 performance::r2(pop_lmer_0_re) # for MLM&#39;s it computes Nakagawa&#39;s R2 # R2 for Mixed Models Conditional R2: 0.365 Marginal R2: 0.000 performance::performance(pop_lmer_0_re) # Indices of model performance AIC | AICc | BIC | R2 (cond.) | R2 (marg.) | ICC | RMSE | Sigma -------------------------------------------------------------------------------- 6336.510 | 6336.522 | 6353.312 | 0.365 | 0.000 | 0.365 | 1.080 | 1.105 5.4.3 Add Predictors to the Model (Hox, Moerbeek, and Van de Schoot 2017) labeled this as “\\(M_1\\)” in chapter 2 for their Table 2.1 (page 14), but adjusted it for Tables 2.2 (page 15) and 2.3 (page 17). LEVEL 1: Student-specific predictors: \\(X_1 = GEN\\), pupils’s gender (girl vs. boy) \\(X_2 = EXT\\), pupil’s extroversion (scale: 1-10) LEVEL 2: Class-specific Predictors: \\(Z = YRS\\), teacher’s experience (range of 2-25 years) 5.4.3.1 Equations Level 1 Model Equation: Include main effects for sex and extrav \\[ \\overbrace{POP_{ij}}^{Outcome} = \\underbrace{\\beta_{0j}}_{\\text{Level 2}\\atop\\text{intercept}} + \\underbrace{\\beta_{1j}}_{\\text{Level 2}\\atop\\text{slopes}} \\overbrace{GEN_{ij}}^{\\text{Level 1}\\atop\\text{Predictor 1}} + \\underbrace{\\beta_{2j}}_{\\text{Level 2}\\atop\\text{slopes}} \\overbrace{EXT_{ij}}^{\\text{Level 1}\\atop\\text{Predictor 2}} + \\underbrace{e_{ij}}_{\\text{Random}\\atop\\text{residuals}} \\] Level 2 Model Equations: Include a random intercepts and random slopes for both for sex and extrav, but NO cross level interactions for now. We will assume this is due to some theoretical reasoning to be our starting point after the fitting of the null model. Random Intercepts: \\[ \\overbrace{\\beta_{0j}}^{\\text{Level 2}\\atop\\text{intercepts}} = \\underbrace{\\gamma_{00}}_{\\text{Fixed}\\atop\\text{intercept}} + \\underbrace{\\gamma_{01}}_{\\text{Fixed}\\atop\\text{slope } Z} \\overbrace{YRS_{j}}^{\\text{Level 2}\\atop\\text{Predictor 3}} + \\underbrace{u_{0j}}_{\\text{Intercept}\\atop\\text{residual}} \\] Random Slopes, for the first predictor, sex: \\[ \\overbrace{\\beta_{1j}}^{\\text{Level 2}\\atop\\text{slopes}} = \\underbrace{\\gamma_{10}}_{\\text{Fixed}\\atop\\text{Slope } X_1} + \\underbrace{u_{1j}}_{\\text{Slope } X_1\\atop\\text{residual}} \\] Random Slopes, for the second predictor, extrav: \\[ \\overbrace{\\beta_{2j}}^{\\text{Level 2}\\atop\\text{slopes}} = \\underbrace{\\gamma_{20}}_{\\text{Fixed}\\atop\\text{Slope } X_2} + \\underbrace{u_{2j}}_{\\text{Slope } X_2\\atop\\text{residual}} \\] Substitute the level 2 equations into the level 1 equation: Combined, Multilevel Model Equation - Popularity, Include Predictors (no cross-level interactions) \\[ \\overbrace{POP_{ij}}^{Outcome} = \\overbrace{\\gamma_{00} + \\gamma_{10} GEN_{ij} + \\gamma_{20} EXT_{ij} + \\gamma_{01} YRS_{j}}^{\\text{Fixed part}\\atop\\text{Deterministic}} + \\\\ \\underbrace{u_{0j} + u_{1j} GEN_{ij} + u_{2j} EXT_{ij} + e_{ij} }_{\\text{Random part}\\atop\\text{Stochastic}} \\tag{M1} \\] 5.4.3.2 Parameters Type Parameter of Interest Estimates This Fixed Intercept \\(\\gamma_{00}\\) Fixed Main Effect of sex \\(\\gamma_{10}\\) Fixed Main Effect of extrav \\(\\gamma_{20}\\) Fixed Main Effect of texp \\(\\gamma_{01}\\) Random Variance in random intercepts, \\(var[u_{0j}]\\) \\(\\sigma^2_{u0}\\) Random Variance in random slope of sex, \\(var[u_{1j}]\\) \\(\\sigma^2_{u1}\\) Random Variance in random slope of extrav, \\(var[u_{2j}]\\) \\(\\sigma^2_{u2}\\) Random Covariance between random intercepts and random slope of sex, \\(cov[u_{0j}, u_{1j}]\\) \\(\\sigma^2_{u01}\\) Random Covariance between random intercepts and random slope of extrav, \\(cov[u_{0j}, u_{2j}]\\) \\(\\sigma^2_{u02}\\) Random Covariance between random slopes of sex and extrav, \\(cov[u_{1j}, u_{2j}]\\) \\(\\sigma^2_{u12}\\) Random Residual Variance \\(var[e_{ij}]\\) \\(\\sigma^2_{e}\\) Troubleshooting ‘lme4’ Linear Mixed-Effects Models website. This website attempts to summarize some of the common problems with fitting lmer models and how to troubleshoot them. This is a helpful post on Stack Exchange regarding using differen t optimizers to get the ‘lme4::lmer()’ function to converge. Note: Convergence issues MAY signify problems in the model specification. 5.4.3.3 Fit the Model pop_lmer_0_ml &lt;- lmerTest::lmer(popular ~ 1 + (1|class), data = data_pop, REML = FALSE) # refit via ML to compare the model below to pop_lmer_1_ml &lt;- lmerTest::lmer(popular ~ sex + extrav + texp + (sex + extrav|class), data = data_pop, REML = FALSE, control = lmerControl(optimizer = &quot;Nelder_Mead&quot;)) #helps converge summary(pop_lmer_1_ml) Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s method [lmerModLmerTest] Formula: popular ~ sex + extrav + texp + (sex + extrav | class) Data: data_pop Control: lmerControl(optimizer = &quot;Nelder_Mead&quot;) AIC BIC logLik deviance df.resid 4833.3 4894.9 -2405.6 4811.3 1989 Scaled residuals: Min 1Q Median 3Q Max -3.1686 -0.6550 -0.0227 0.6728 2.9571 Random effects: Groups Name Variance Std.Dev. Corr class (Intercept) 1.319429 1.14866 sexgirl 0.002389 0.04888 -0.40 extrav 0.034115 0.18470 -0.88 -0.09 Residual 0.551144 0.74239 Number of obs: 2000, groups: class, 100 Fixed effects: Estimate Std. Error df t value Pr(&gt;|t|) (Intercept) 7.601e-01 1.959e-01 1.839e+02 3.879 0.000146 *** sexgirl 1.251e+00 3.692e-02 9.930e+02 33.884 &lt; 2e-16 *** extrav 4.529e-01 2.451e-02 9.715e+01 18.477 &lt; 2e-16 *** texp 8.942e-02 8.533e-03 1.034e+02 10.480 &lt; 2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Correlation of Fixed Effects: (Intr) sexgrl extrav sexgirl -0.063 extrav -0.720 -0.066 texp -0.683 -0.040 0.090 optimizer (Nelder_Mead) convergence code: 0 (OK) boundary (singular) fit: see help(&#39;isSingular&#39;) 5.4.3.4 Interpretation After accounting for the heiarchical nesting of students in classes, girls were rated 1.25 points more popular on average, than boys with the same extroversion score. One point higher extroversion scores were associated with 0.45 points higher popularity, within each gender. Reproduce Table 2.1 on the top of page 14 (Hox, Moerbeek, and Van de Schoot 2017) texreg::knitreg(list(pop_lm_0, pop_glm_0, pop_lmer_0_ml, pop_lmer_1_ml), custom.model.names = c(&quot;Single-level, OLS&quot;, &quot;Single-level, ML&quot;, &quot;M0: int only&quot;, &quot;M1: w pred&quot;), caption = &quot;Hox Table 2.1 on the top of page 14&quot;, caption.above = TRUE, single.row = TRUE) Hox Table 2.1 on the top of page 14   Single-level, OLS Single-level, ML M0: int only M1: w pred (Intercept) 5.08 (0.03)*** 5.08 (0.03)*** 5.08 (0.09)*** 0.76 (0.20)*** sexgirl       1.25 (0.04)*** extrav       0.45 (0.02)*** texp       0.09 (0.01)*** R2 0.00       Adj. R2 0.00       Num. obs. 2000 2000 2000 2000 AIC   6974.39 6333.47 4833.29 BIC   6985.59 6350.27 4894.90 Log Likelihood   -3485.20 -3163.73 -2405.64 Deviance   3820.82     Num. groups: class     100 100 Var: class (Intercept)     0.69 1.32 Var: Residual     1.22 0.55 Var: class sexgirl       0.00 Var: class extrav       0.03 Cov: class (Intercept) sexgirl       -0.02 Cov: class (Intercept) extrav       -0.19 Cov: class sexgirl extrav       -0.00 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 The regression tables from the texreg package include estimates of the covariances between random components. “These covarianes are rarely interpreted (for an exception see Chapter 5 and Chapter 16 where growth models are discussed), and for that reason they are often not included in the reported tables. However, as Table 2.2 demonstrates, they can be quite large adn significant, so as a rule they are always included in the model.” (Hox, Moerbeek, and Van de Schoot 2017), Chapter 2, pages 15-16 Comparing Model Fit Residual Variance in the Residuals In single-level regression, the Root Mean Squared Error (RMSE) is usually reported. It is the standard deviation of the residuals and is called “Residual standard error” in the R output of summary() function applied to an model fit via lm. In multi-level regression, residual variance is reported as \\(\\sigma_e^2\\). \\[ {\\text{RMSE}}^2 = MSE = \\sigma_e^2 \\] Deviance In single-level regression, the model is fit in such a way as to make the sum of the squared residuals as small as possible. Deviance is the sum of the squared residuals. In multi-level regression, the model is fit via a method called ‘Maximum Likelihood’. \\[ \\text{Deviance} = -2LL = -2 \\times log(likelihood) \\] 5.4.4 Testing Random Components In Hox’s table 2.1 (page 14) we see that the MLM with predictors (\\(M_0\\)) includes a random compondnt with virtually no variance. This is likely why the model didn’t easily converge (a different optimizer was employed). It makes sence to remove the random slope component for gender and refit the model. While we are at it, we will also fit a third model dropping the second random slope component for extroversion. 5.4.4.1 Fit Nested Models Since we are going to compare models that are nested on random effects (identical except for inclusing/exclusing of random components, we will specify the REML = TRUE option. pop_lmer_1_re &lt;- lmerTest::lmer(popular ~ sex + extrav + texp + (sex + extrav|class), data = data_pop, REML = TRUE, control = lmerControl(optimizer =&quot;Nelder_Mead&quot;)) #helps converge pop_lmer_1a_re &lt;- lmerTest::lmer(popular ~ sex + extrav + texp + (extrav|class), data = data_pop, REML = TRUE) pop_lmer_1b_re &lt;- lmerTest::lmer(popular ~ sex + extrav + texp + (1 |class), data = data_pop, REML = TRUE) Create a table to compare the three nested models: The middle column below reproduces Hox’s Table 2.2 found on the bottom of page 15 (Hox, Moerbeek, and Van de Schoot 2017), except the values differ slightly becuase here the model was fit via REML where as in the text, Hox used ML. texreg::knitreg(list(pop_lmer_1_re, pop_lmer_1a_re, pop_lmer_1b_re), custom.model.names = c(&quot;M1&quot;, &quot;M1a&quot;, &quot;M1b&quot;), caption = &quot;Assessing Significance of Random Slopes&quot;, caption.above = TRUE, single.row = TRUE) Assessing Significance of Random Slopes   M1 M1a M1b (Intercept) 0.76 (0.20)*** 0.74 (0.20)*** 0.81 (0.17)*** sexgirl 1.25 (0.04)*** 1.25 (0.04)*** 1.25 (0.04)*** extrav 0.45 (0.02)*** 0.45 (0.02)*** 0.45 (0.02)*** texp 0.09 (0.01)*** 0.09 (0.01)*** 0.09 (0.01)*** AIC 4855.26 4850.77 4897.02 BIC 4916.87 4895.58 4930.63 Log Likelihood -2416.63 -2417.38 -2442.51 Num. obs. 2000 2000 2000 Num. groups: class 100 100 100 Var: class (Intercept) 1.34 1.30 0.30 Var: class sexgirl 0.00     Var: class extrav 0.03 0.03   Cov: class (Intercept) sexgirl -0.02     Cov: class (Intercept) extrav -0.19 -0.19   Cov: class sexgirl extrav -0.00     Var: Residual 0.55 0.55 0.59 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 5.4.4.2 Compare Fit Likelihood Ratio Test (LRT) of Nested MLM Models When comparing the fit of two multi-level models fit via the lmer() function, the anova() function runs an Chi-squared test where the test statistic is the difference in -2LL (deviances). Likelihood Ratio Test (LRT) for Random Effects When using the ‘anova()’ function to conduct a LRT for RANDOM effects, make sure: the nested models have identical FIXED effects never test models that differ in fixed and random effects at the same time the models were fit with ‘REML = TRUE’ this results in the best variance/covariance component estimation add the ‘refit = FALSE’ option to the ‘anova()’ call without this \\(R\\) re-runs the models with ‘REML = FALSE’ for you Investigate dropping the random slope component for sex These two models are identical, except for the inclusing/exclusion of the random specification of the level 1 sex predictor. Note, both models were fit with REML. Although we are dropping only ONE variance component, we are also dropping TWO covariances (sex paired with both the random intercept and random slope for extrav). This results in a \\(\\chi^2\\) test with THREE degrees of freedom. anova(pop_lmer_1_re, pop_lmer_1a_re, refit = FALSE) # don&#39;t let it refit the models via LM Data: data_pop Models: pop_lmer_1a_re: popular ~ sex + extrav + texp + (extrav | class) pop_lmer_1_re: popular ~ sex + extrav + texp + (sex + extrav | class) npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) pop_lmer_1a_re 8 4850.8 4895.6 -2417.4 4834.8 pop_lmer_1_re 11 4855.3 4916.9 -2416.6 4833.3 1.5133 3 0.6792 The NON-significance likelihood ratio test (LRT: \\(\\chi^2(3) = 1.51\\), \\(p = .679\\)) conveys that the more complex model does NOT fit the data better. Thus the more SIMPLE model does just as good of a job. This is evidence for the EXCLUSION of sex as a random component. Investigate dropping the random slope component for extrav These two models are identical, except for the inclusing/exclusion of the random specification of the level 1 extrav predictor. Note, both models were fit with REML. Although we are dropping only ONE variance component, we are also dropping ONE covariances (extrav paired with the random intercept). This results in a \\(\\chi^2\\) test with TWO degrees of freedom. anova(pop_lmer_1a_re, pop_lmer_1b_re, refit = FALSE) # don&#39;t let it refit the models via LM Data: data_pop Models: pop_lmer_1b_re: popular ~ sex + extrav + texp + (1 | class) pop_lmer_1a_re: popular ~ sex + extrav + texp + (extrav | class) npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) pop_lmer_1b_re 6 4897.0 4930.6 -2442.5 4885.0 pop_lmer_1a_re 8 4850.8 4895.6 -2417.4 4834.8 50.256 2 1.222e-11 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The significance likelihood ratio test (LRT: \\(\\chi^2(2) = 50.26\\), \\(p &lt; .0001\\)) conveys that the more complex model DOES fit the data better. Thus the more COMPLEX model does just as good of a job. This is evidence for the INCLUSION of extrav as a random component. 5.4.5 Testing Cross-Level Interactions We have already seen formulas of this form for a NULL or emply models, as well as for intercept implied models of main effects: intercept only Y ~ 1 intercept implied Y ~ A = Y ~ 1 + A Y ~ A + B = Y ~ 1 + A + B Including Interactions in Formulas If we wish to include an interaction between the two predictors, we signify this with a colon (:) between the two predictor names. A shortcut may also be employed to signify the including of the main effects and the interaction at the same time by placing an astric (*) between the two variable names. Both of the following specify the outcome is being predicted by an intercept (implied), the main effects for 2 predictors, and the interaction between the two predictors Y ~ A + B + A:B Y ~ A*B Examples 2-way: A*B = A + B + A:B 3-way: A*B*C = A + B + C + A:B + A:C + B:C + A:B:C 4-way: A*B*C*D = A + B + C + D + A:B + A:C + A:D + B:C + B:D + A:B:C + A:B:D+ A:C:D + B:C:D + A:B:C:D 5.4.5.1 Fit Nested Models “Given the significant variance of the regression coefficient of pupil extroversion across the classes, it is attractive to attempt to predict its variation using class-level variables. We have one class-level variable: teacher experience.” (Hox, Moerbeek, and Van de Schoot 2017), Chapter 2, page 16 Now that we wish to compare nested that will differ only in terms of the inclusing/exclusion of a FIXED effect, the estimation method should be standard maximum likelihood (REML = FALSE). pop_lmer_1a_ml &lt;- lmerTest::lmer(popular ~ sex + extrav + texp + (extrav|class), # main effects only data = data_pop, REML = FALSE) pop_lmer_2_ml &lt;- lmerTest::lmer(popular ~ sex + extrav*texp + (extrav|class), # include cross-level interaction data = data_pop, REML = FALSE) pop_lmer_3_ml &lt;- lmerTest::lmer(popular ~ extrav*texp + sex*texp + sex*extrav + (extrav|class), data = data_pop, REML = FALSE) pop_lmer_4_ml &lt;- lmerTest::lmer(popular ~ extrav*texp*sex + (extrav|class), data = data_pop, REML = FALSE, control = lmerControl(optimizer =&quot;Nelder_Mead&quot;)) Create a table to compare the two nested models: texreg::knitreg(list(pop_lmer_1a_ml, pop_lmer_2_ml), custom.model.names = c(&quot;M1a: Main Effects&quot;, &quot;M2: With Interaction&quot;), caption = &quot;Hox Table 2.3 on page 17&quot;, caption.above = TRUE, single.row = TRUE) Hox Table 2.3 on page 17   M1a: Main Effects M2: With Interaction (Intercept) 0.74 (0.20)*** -1.21 (0.27)*** sexgirl 1.25 (0.04)*** 1.24 (0.04)*** extrav 0.45 (0.02)*** 0.80 (0.04)*** texp 0.09 (0.01)*** 0.23 (0.02)*** extrav:texp   -0.02 (0.00)*** AIC 4828.81 4765.62 BIC 4873.61 4816.03 Log Likelihood -2406.40 -2373.81 Num. obs. 2000 2000 Num. groups: class 100 100 Var: class (Intercept) 1.28 0.45 Var: class extrav 0.03 0.00 Cov: class (Intercept) extrav -0.18 -0.03 Var: Residual 0.55 0.55 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 Investigate further interactions, not shown in by (Hox, Moerbeek, and Van de Schoot 2017). texreg::knitreg(list(pop_lmer_1a_ml, pop_lmer_2_ml, pop_lmer_3_ml, pop_lmer_4_ml), custom.model.names = c(&quot;M1a: Main Effects&quot;, &quot;M2: With Interaction&quot;, &quot;Add 2-way Inter&quot;, &quot;Add 3-way Interaction&quot;), caption = &quot;Hox Table 2.3 on page 17&quot;, caption.above = TRUE, single.row = TRUE) Hox Table 2.3 on page 17   M1a: Main Effects M2: With Interaction Add 2-way Inter Add 3-way Interaction (Intercept) 0.74 (0.20)*** -1.21 (0.27)*** -1.09 (0.28)*** -0.94 (0.33)** sexgirl 1.25 (0.04)*** 1.24 (0.04)*** 0.96 (0.21)*** 0.66 (0.38) extrav 0.45 (0.02)*** 0.80 (0.04)*** 0.78 (0.04)*** 0.75 (0.05)*** texp 0.09 (0.01)*** 0.23 (0.02)*** 0.23 (0.02)*** 0.22 (0.02)*** extrav:texp   -0.02 (0.00)*** -0.02 (0.00)*** -0.02 (0.00)*** texp:sexgirl     0.00 (0.01) 0.02 (0.02) extrav:sexgirl     0.05 (0.03) 0.10 (0.06) extrav:texp:sexgirl       -0.00 (0.00) AIC 4828.81 4765.62 4767.17 4768.26 BIC 4873.61 4816.03 4828.78 4835.47 Log Likelihood -2406.40 -2373.81 -2372.58 -2372.13 Num. obs. 2000 2000 2000 2000 Num. groups: class 100 100 100 100 Var: class (Intercept) 1.28 0.45 0.49 0.49 Var: class extrav 0.03 0.00 0.01 0.01 Cov: class (Intercept) extrav -0.18 -0.03 -0.03 -0.03 Var: Residual 0.55 0.55 0.55 0.55 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 5.4.5.2 Compare Fit Since these two models only differ by the including/exclusing of a FIXED effect, they both employed ML estimation. Thus we do not need worry about the anova() function refitting the models prior to conduction the LRT. anova(pop_lmer_1a_ml, pop_lmer_2_ml) Data: data_pop Models: pop_lmer_1a_ml: popular ~ sex + extrav + texp + (extrav | class) pop_lmer_2_ml: popular ~ sex + extrav * texp + (extrav | class) npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) pop_lmer_1a_ml 8 4828.8 4873.6 -2406.4 4812.8 pop_lmer_2_ml 9 4765.6 4816.0 -2373.8 4747.6 65.183 1 6.827e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The significance likelihood ratio test (LRT: \\(\\chi^2(1) = 65.18\\), \\(p &lt; .0001\\)) conveys that the more complex model DOES fit the data better. Thus the more COMPLEX model does just as good of a job. This is evidence for the INCLUSION of cross-level interaction between extrav and texp as a fixed component. anova(pop_lmer_2_ml, pop_lmer_3_ml) Data: data_pop Models: pop_lmer_2_ml: popular ~ sex + extrav * texp + (extrav | class) pop_lmer_3_ml: popular ~ extrav * texp + sex * texp + sex * extrav + (extrav | class) npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) pop_lmer_2_ml 9 4765.6 4816.0 -2373.8 4747.6 pop_lmer_3_ml 11 4767.2 4828.8 -2372.6 4745.2 2.4552 2 0.293 The significance likelihood ratio test (LRT: \\(\\chi^2(2) = 2.46\\), \\(p=.293\\)) conveys that the more complex model does NOT fit the data better. Thus the more SIMPLE model does just as good of a job. This is evidence for the EXCLUSION of the additional 2-way interactions as a fixed components. anova(pop_lmer_2_ml, pop_lmer_4_ml) Data: data_pop Models: pop_lmer_2_ml: popular ~ sex + extrav * texp + (extrav | class) pop_lmer_4_ml: popular ~ extrav * texp * sex + (extrav | class) npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) pop_lmer_2_ml 9 4765.6 4816.0 -2373.8 4747.6 pop_lmer_4_ml 12 4768.3 4835.5 -2372.1 4744.3 3.3636 3 0.3389 The significance likelihood ratio test (LRT: \\(\\chi^2(3) = 3.36\\), \\(p=.339\\)) conveys that the more complex model does NOT fit the data better. Thus the more SIMPLE model does just as good of a job. This is evidence for the EXCLUSION of the additional 3-way interactions as a fixed components. performance::compare_performance(pop_lmer_1a_ml, pop_lmer_2_ml, pop_lmer_3_ml, pop_lmer_4_ml, rank = TRUE) # Comparison of Model Performance Indices Name | Model | R2 (cond.) | R2 (marg.) | ICC | RMSE | Sigma | AIC weights | AICc weights | BIC weights | Performance-Score ------------------------------------------------------------------------------------------------------------------------------------------------- pop_lmer_2_ml | lmerModLmerTest | 0.706 | 0.554 | 0.342 | 0.723 | 0.743 | 0.578 | 0.584 | 0.998 | 59.65% pop_lmer_3_ml | lmerModLmerTest | 0.708 | 0.555 | 0.343 | 0.722 | 0.743 | 0.267 | 0.264 | 0.002 | 46.75% pop_lmer_4_ml | lmerModLmerTest | 0.708 | 0.556 | 0.343 | 0.722 | 0.742 | 0.155 | 0.151 | 6.00e-05 | 46.68% pop_lmer_1a_ml | lmerModLmerTest | 0.695 | 0.513 | 0.375 | 0.715 | 0.743 | 1.10e-14 | 1.12e-14 | 3.13e-13 | 31.13% 5.4.6 Final Model 5.4.6.1 Refit with REML pop_lmer_2_re &lt;- lmerTest::lmer(popular ~ sex + extrav*texp + (extrav|class), data = data_pop, REML = TRUE) # re-fit the final model via REML 5.4.6.2 Parameter Summary Table texreg::knitreg(list(pop_lmer_2_re), custom.model.names = c(&quot;Final Model&quot;), caption = &quot;MLM for Popularity&quot;, caption.above = TRUE, single.row = TRUE) MLM for Popularity   Final Model (Intercept) -1.21 (0.27)*** sexgirl 1.24 (0.04)*** extrav 0.80 (0.04)*** texp 0.23 (0.02)*** extrav:texp -0.02 (0.00)*** AIC 4798.45 BIC 4848.86 Log Likelihood -2390.23 Num. obs. 2000 Num. groups: class 100 Var: class (Intercept) 0.48 Var: class extrav 0.01 Cov: class (Intercept) extrav -0.03 Var: Residual 0.55 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 5.4.6.3 Visualization - interactions package Predictors: involved in the interaction … * extrav 1 value per student, continuous, score with range 1-10 * texp 1 value per class, continuous, years with range 2-25 Fastest way: all defaults interactions::interact_plot(model = pop_lmer_2_re, # model name pred = extrav, # x-axis &#39;predictor&#39; independent variable name modx = texp, # &#39;moderator&#39; (x) independent variable name mod2 = sex) # 2nd moderator independent variable name (optional) interactions::sim_slopes(model = pop_lmer_2_re, pred = extrav, modx = texp) JOHNSON-NEYMAN INTERVAL When texp is OUTSIDE the interval [29.16, 37.41], the slope of extrav is p &lt; .05. Note: The range of observed values of texp is [2.00, 25.00] SIMPLE SLOPES ANALYSIS Slope of extrav when texp = 7.711184 (- 1 SD): Est. S.E. t val. p ------ ------ -------- ------ 0.61 0.02 25.59 0.00 Slope of extrav when texp = 14.263000 (Mean): Est. S.E. t val. p ------ ------ -------- ------ 0.45 0.02 25.89 0.00 Slope of extrav when texp = 20.814816 (+ 1 SD): Est. S.E. t val. p ------ ------ -------- ------ 0.29 0.02 11.86 0.00 For student’s who’s teach has average experience (M = 14.25 years), a 1 unit increase in extraversion is associated with a nearly half point increase in popularity, b = 0.45, SE = 0.02, p &lt; .01. When the teacher has more experience, this association is less distinct and when teachers have more experience, this relationship is more pronounced. Girls have higher popularities after controlling for their level of extroversion and their teacher’s experience, b = 1.24, SE = 0.04, p &lt; .001. interactions::sim_slopes(model = pop_lmer_2_re, pred = extrav, modx = texp, modx.values = c(5, 10, 20)) JOHNSON-NEYMAN INTERVAL When texp is OUTSIDE the interval [29.16, 37.41], the slope of extrav is p &lt; .05. Note: The range of observed values of texp is [2.00, 25.00] SIMPLE SLOPES ANALYSIS Slope of extrav when texp = 5.00: Est. S.E. t val. p ------ ------ -------- ------ 0.68 0.03 23.33 0.00 Slope of extrav when texp = 10.00: Est. S.E. t val. p ------ ------ -------- ------ 0.56 0.02 27.29 0.00 Slope of extrav when texp = 20.00: Est. S.E. t val. p ------ ------ -------- ------ 0.31 0.02 13.46 0.00 For publications, you can get fancier interactions::interact_plot(pop_lmer_2_re, # model name pred = extrav, # x-axis &#39;predictor&#39; variable name modx = texp, # &#39;moderator&#39; variable name modx.values = c(5, 15, 25), # values to pick for a continuous &quot;modx&quot; interval = TRUE, # adds CI bands for pop/marginal mean y.label = &quot;Estimated Marginal Mean\\nPupil Popularity, Mean Rating of Classroom Peers&quot;, x.label = &quot;Pupil&#39;s Extroversion, as Rated by Teacher&quot;, legend.main = &quot;Teacher&#39;s Experience&quot;, modx.labels = c(&quot;5 years&quot;, &quot;15 years&quot;, &quot;25 years&quot;), colors = c(&quot;black&quot;, &quot;black&quot;, &quot;black&quot;)) + # default is &quot;Blues&quot; for modx.values theme_bw() + theme(legend.key.width = unit(2, &quot;cm&quot;), legend.background = element_rect(color = &quot;Black&quot;), legend.position = c(1, 0), legend.justification = c(1.1, -0.1)) + scale_x_continuous(breaks = seq(from = 0, to = 10, by = 2)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 1)) 5.4.6.4 Visualization - effects &amp; ggplot2 packages Get Estimated Marginal Means - default ‘nice’ predictor values: Focal predictors: All combinations of… * sex categorical, both levels * extrav continuous 1-10, default: 1, 3, 6, 8, 10 * texp continuous, default: 2.0, 7.8, 14.0, 19.0, 25.0 Always followed by: * fit estimated marginal mean * se standard error for the marginal mean * lower lower end of the 95% confidence interval around the estimated marginal mean * upper upper end of the 95% confidence interval around the estimated marginal mean effects::Effect(focal.predictors = c(&quot;sex&quot;, &quot;extrav&quot;, &quot;texp&quot;), mod = pop_lmer_2_re) %&gt;% data.frame() %&gt;% head(n = 12) sex extrav texp fit se lower upper 1 boy 1 2.0 -0.003090894 0.2113412 -0.4175635 0.4113817 2 girl 1 2.0 1.237606960 0.2138275 0.8182583 1.6569556 3 boy 3 2.0 1.505152972 0.1580311 1.1952296 1.8150763 4 girl 3 2.0 2.745850827 0.1602411 2.4315933 3.0601083 5 boy 6 2.0 3.767518772 0.1201240 3.5319371 4.0031004 6 girl 6 2.0 5.008216627 0.1208411 4.7712286 5.2452046 7 boy 8 2.0 5.275762639 0.1416414 4.9979820 5.5535433 8 girl 8 2.0 6.516460494 0.1410013 6.2399352 6.7929857 9 boy 10 2.0 6.784006506 0.1892732 6.4128127 7.1552003 10 girl 10 2.0 8.024704360 0.1878555 7.6562909 8.3931179 11 boy 1 7.8 1.165430181 0.1405323 0.8898247 1.4410356 12 girl 1 7.8 2.406128036 0.1432174 2.1252567 2.6869993 Pick ‘nicer’ illustrative values for texp effects::Effect(focal.predictors = c(&quot;sex&quot;, &quot;extrav&quot;, &quot;texp&quot;), mod = pop_lmer_2_re, xlevels = list(texp = c(5, 15, 25))) %&gt;% data.frame() %&gt;% head(n = 12) sex extrav texp fit se lower upper 1 boy 1 5 0.6013166 0.17311622 0.261809 0.9408241 2 girl 1 5 1.8420144 0.17570895 1.497422 2.1866067 3 boy 3 5 1.9611918 0.12950715 1.707208 2.2151752 4 girl 3 5 3.2018897 0.13175620 2.943495 3.4602838 5 boy 6 5 4.0010047 0.09856839 3.807697 4.1943124 6 girl 6 5 5.2417025 0.09913677 5.047280 5.4361250 7 boy 8 5 5.3608799 0.11620822 5.132978 5.5887821 8 girl 8 5 6.6015778 0.11532522 6.375407 6.8277483 9 boy 10 5 6.7207552 0.15520075 6.416383 7.0251277 10 girl 10 5 7.9614530 0.15351226 7.660392 8.2625142 11 boy 1 15 2.6160081 0.09471133 2.430265 2.8017516 12 girl 1 15 3.8567059 0.09677625 3.666913 4.0464990 Basic, default plot Other than selecting three illustrative values for the teacher extroversion rating, most everything is left to default. effects::Effect(focal.predictors = c(&quot;sex&quot;, &quot;extrav&quot;, &quot;texp&quot;), mod = pop_lmer_2_re, xlevels = list(texp = c(5, 15, 25))) %&gt;% data.frame() %&gt;% dplyr::mutate(texp = factor(texp)) %&gt;% ggplot() + aes(x = extrav, y = fit, fill = texp, linetype = texp) + geom_ribbon(aes(ymin = lower, ymax = upper), alpha = .3) + geom_line(aes(color = texp)) + facet_grid(.~ sex) More Clean Plot There are many ways to clean up a plot, including labeling the axes. effects::Effect(focal.predictors = c(&quot;sex&quot;, &quot;extrav&quot;, &quot;texp&quot;), mod = pop_lmer_2_re, xlevels = list(texp = c(5, 15, 25))) %&gt;% data.frame() %&gt;% dplyr::mutate(texp = factor(texp)) %&gt;% dplyr::mutate(sex = sex %&gt;% forcats::fct_recode(&quot;Amoung Boys&quot; = &quot;boy&quot;, &quot;Among Girls&quot; = &quot;girl&quot;)) %&gt;% ggplot() + aes(x = extrav, y = fit, fill = texp, linetype = texp) + geom_ribbon(aes(ymin = lower, ymax = upper), alpha = .3) + geom_line(aes(color = texp)) + theme_bw() + facet_grid(.~ sex) + labs(x = &quot;Pupil&#39;s Extroversion, Rated by Teacher&quot;, y = &quot;Estimated Marginal Mean\\nPupil Popularity, Mean Rating of Classroom Peers&quot;, color = &quot;Teacher&#39;s Experience, Years&quot;, linetype = &quot;Teacher&#39;s Experience, Years&quot;, fill = &quot;Teacher&#39;s Experience, Years&quot;) + theme(legend.position = &quot;bottom&quot;) + scale_x_continuous(breaks = seq(from = 0, to = 10, by = 2)) Publishable Plot Since gender only exhibited a main effect and is not involved in any interactions, if would be a better use of space to not muddy the water with seperate panels. The Effect() function will estimate the marginal means using the reference category for categorical variables and the mean for continuous variables. effects::Effect(focal.predictors = c(&quot;extrav&quot;, &quot;texp&quot;), # choose not to investigate sex (the reference category will be used) mod = pop_lmer_2_re, xlevels = list(texp = c(5, 15, 25))) %&gt;% data.frame() %&gt;% dplyr::mutate(texp = factor(texp) %&gt;% forcats::fct_rev()) %&gt;% ggplot() + aes(x = extrav, y = fit, linetype = texp) + geom_ribbon(aes(ymin = lower, ymax = upper), fill = &quot;black&quot;, alpha = .3) + geom_line() + theme_bw() + labs(x = &quot;Pupil&#39;s Extroversion, Rated by Teacher&quot;, y = &quot;Estimated Marginal Mean\\nPupil Popularity, Mean Rating of Classroom Peers&quot;, color = &quot;Teacher&#39;s\\nExperience,\\nYears&quot;, linetype = &quot;Teacher&#39;s\\nExperience,\\nYears&quot;, alpha = &quot;Teacher&#39;s\\nExperience,\\nYears&quot;) + theme(legend.key.width = unit(2, &quot;cm&quot;), legend.background = element_rect(color = &quot;Black&quot;), legend.position = c(1, 0), legend.justification = c(1.1, -0.1)) + scale_linetype_manual(values = c(&quot;solid&quot;, &quot;dashed&quot;, &quot;dotted&quot;)) + scale_x_continuous(breaks = seq(from = 0, to = 10, by = 2)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 1)) 5.4.6.5 Interpretation After accounting for class-to-class variation and the effect of gender, a positive association was found between teacher rated extroversion and peer rated popularity. This relationship was more marked for less experienced teachers. 5.4.7 Residual Plots Form more infromation, see the vingette page for the redre package. sjPlot::plot_model(pop_lmer_2_re, type = &quot;diag&quot;) [[1]] [[2]] [[2]]$class [[3]] [[4]] Standardized residuals vs. fitted values You always want to use studentized, conditional residuals for MLM! As you look across the plot, left to right: GOOD = no pattern &amp; HOV BAD = any pattern or change in the spread This plot looks great! "],["mlm-3-levels-nurses-stress-intervention.html", "6 MLM, 3 levels: Nurse’s Stress Intervention 6.1 Background 6.2 Exploratory Data Analysis 6.3 MLM: Null Model 6.4 Estimate the ICC 6.5 MLM: Add Fixed Effects 6.6 MLM: Add Random Slope 6.7 MLM: Add Cross-Level Interaction 6.8 Final Model 6.9 Interpretation 6.10 Reproduction of Table 2.5", " 6 MLM, 3 levels: Nurse’s Stress Intervention library(tidyverse) # basically everything ;) library(haven) # read in SPSS dataset library(furniture) # nice table1() descriptives library(stargazer) # display nice tables: summary &amp; regression library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(psych) # contains some useful functions, like headTail library(car) # Companion to Applied Regression library(lme4) # Linear, generalized linear, &amp; nonlinear mixed models library(lmerTest) # Tests on lmer objects library(performance) # ICC calculations library(interactions) # plotting interactions library(effects) # Effects for regression models library(optimx) # Different optimizers to solve mlm&#39;s 6.1 Background The text “Multilevel Analysis: Techniques and Applications, Third Edition” (Hox, Moerbeek, and Van de Schoot 2017) has a companion website which includes links to all the data files used throughout the book (housed on the book’s GitHub repository). The following example is used through out Hox, Moerbeek, and Van de Schoot (2017)’s chapater 2. From Appendix E: The nurses.sav file contains three-level simulated data from a hypothetical study on stress in hospitals. The data are from nurses working in wards nested within hospitals. It is a cluster-randomized experiment. In each of 25 hospitals, four wards are selected and randomly assigned to an experimental and a control condition. In the experimental condition, a training program is offered to all nurses to cope with job-related stress. After the program is completed, a sample of about 10 nurses from each ward is given a test that measures job-related stress. Additional variables (covariates) are: nurse age (years), nurse experience (years), nurse gender (0=male, 1 = female), type of ward (0=general care, 1=special care), and hospital size (0=small, 1 = medium, 2=large). The data have been generated to illustrate three-level analysis with a random slope for the effect of the intervention. Here the data is read in and the SPSS variables with labels are converted to \\(R\\) factors. data_raw &lt;- haven::read_spss(&quot;https://github.com/MultiLevelAnalysis/Datasets-third-edition-Multilevel-book/raw/master/chapter%202/Nurses/SPSS/Nurses.sav&quot;) %&gt;% haven::as_factor() # retain the labels from SPSS --&gt; factor 6.1.1 Unique Identifiers All standardized (starts with “Z”) and mean centered (starts with “C”) variables will be remove so that their creation may be shown later. A new indicator varible for nurses with be created by combining the hospital, ward, and nurse indicators. Having a unique, distinct identifier variable for each of the units on lower (Level 1 and 2) levels is helpful for multilevel anlayses. data_nurse &lt;- data_raw %&gt;% dplyr::mutate(genderF = factor(gender, labels = c(&quot;Male&quot;, &quot;Female&quot;))) %&gt;% # apply factor labels dplyr::mutate(id = paste(hospital, ward, nurse, sep = &quot;_&quot;) %&gt;% # unique id for each nurse factor()) %&gt;% # declare id is a factor dplyr::mutate_at(vars(hospital, ward, wardid, nurse), factor) %&gt;% # declare to be factors dplyr::mutate(age = age %&gt;% as.character %&gt;% as.numeric) %&gt;% # declare to be numeric dplyr::select(id, wardid, nurse, ward, hospital, age, gender, genderF, experien, wardtype, hospsize, expcon, stress) # reduce variables included tibble::glimpse(data_nurse) Rows: 1,000 Columns: 13 $ id &lt;fct&gt; 1_1_1, 1_1_2, 1_1_3, 1_1_4, 1_1_5, 1_1_6, 1_1_7, 1_1_8, 1_1_9… $ wardid &lt;fct&gt; 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 1… $ nurse &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18… $ ward &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3… $ hospital &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… $ age &lt;dbl&gt; 36, 45, 32, 57, 46, 60, 23, 32, 60, 45, 57, 47, 32, 42, 42, 5… $ gender &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1… $ genderF &lt;fct&gt; Male, Male, Male, Female, Female, Female, Female, Female, Mal… $ experien &lt;dbl&gt; 11, 20, 7, 25, 22, 22, 13, 13, 17, 21, 24, 24, 14, 13, 17, 20… $ wardtype &lt;fct&gt; general care, general care, general care, general care, gener… $ hospsize &lt;fct&gt; large, large, large, large, large, large, large, large, large… $ expcon &lt;fct&gt; experiment, experiment, experiment, experiment, experiment, e… $ stress &lt;dbl&gt; 7, 7, 7, 6, 6, 6, 6, 7, 7, 6, 6, 6, 6, 6, 6, 5, 5, 6, 5, 5, 4… 6.1.2 Centering Variables When variables are involved in an interaction, it may be advantageous to center the variables. Hox, Moerbeek, and Van de Schoot (2017) covers this in chapter 4. To center categorical variables: 1. Convert then to integers, starting with zero: \\(0, 1, \\dots\\) 2. Subtract the mean data_nurse %&gt;% dplyr::mutate(expconN = as.numeric(expcon) - 1) %&gt;% # Numeric Version of experimental condition dplyr::mutate(hospsizeN = as.numeric(hospsize) - 1) %&gt;% # Numeric Version of hospital size dplyr::select(expcon, expconN, hospsize, hospsizeN) %&gt;% summary() expcon expconN hospsize hospsizeN control :496 Min. :0.000 small :374 Min. :0.000 experiment:504 1st Qu.:0.000 medium:476 1st Qu.:0.000 Median :1.000 large :150 Median :1.000 Mean :0.504 Mean :0.776 3rd Qu.:1.000 3rd Qu.:1.000 Max. :1.000 Max. :2.000 data_nurse &lt;- data_nurse %&gt;% dplyr::mutate(expconN = as.numeric(expcon) - 1) %&gt;% # Numeric Version of experimental condition dplyr::mutate(hospsizeN = as.numeric(hospsize) - 1) %&gt;% # Numeric Version of hospital size dplyr::mutate(expconNG = expconN - 0.504) %&gt;% # Grand-Mean Centered version of experimental condition dplyr::mutate(hospsizeNG = hospsizeN - 0.776) # Grand-Mean Centered version of ehospital size data_nurse %&gt;% dplyr::select(expcon, expconNG) %&gt;% table() expconNG expcon -0.504 0.496 control 496 0 experiment 0 504 data_nurse %&gt;% dplyr::select(hospsize, hospsizeNG) %&gt;% table() hospsizeNG hospsize -0.776 0.224 1.224 small 374 0 0 medium 0 476 0 large 0 0 150 6.2 Exploratory Data Analysis 6.2.1 Summarize Descriptive Statistics 6.2.1.1 The stargazer package Most posters, journal articles, and reports start with a table of descriptive statistics. Since it tends to come first, this type of table is often refered to as Table 1. The stargazer() function can be used to create such a table, but only for the entire dataset (Hlavac 2022). I haven’t been able to find a way to get it to summarize subsamples and compare them in the standard format. # Knit to Website: type = &quot;html&quot; # Knit to PDF: type = &quot;latex&quot; # View on Screen: type = &quot;text&quot; data_nurse %&gt;% data.frame() %&gt;% stargazer::stargazer(title = &quot;Descriptive statistics, aggregate over entire sample&quot;, header = FALSE, type = &quot;text&quot;) Descriptive statistics, aggregate over entire sample Statistic N Mean St. Dev. Min Max age 1,000 43.005 12.042 23 64 gender 1,000 0.735 0.442 0 1 experien 1,000 17.057 6.042 1 38 stress 1,000 4.977 0.980 1 7 expconN 1,000 0.504 0.500 0 1 hospsizeN 1,000 0.776 0.689 0 2 expconNG 1,000 -0.000 0.500 -0.504 0.496 hospsizeNG 1,000 -0.000 0.689 -0.776 1.224 6.2.1.2 The furniture package Tyson Barrett’s furniture package includes the extremely useful function table1() which simplifies the common task of creating a stratified, comparative table of descriptive statistics. Full documentation can be accessed by executing ?furniture::table1. # Knit to Website: output = &quot;html&quot; # Knit to PDF: output = &quot;latex2&quot; # View on Screen: output = &quot;&quot;text&quot;, or &quot;markdown&quot;, &quot;html&quot; data_nurse %&gt;% furniture::table1(age, genderF, experien, wardtype, hospsize, hospsizeN, hospsizeNG, splitby = ~ expcon, # var to divide sample by test = TRUE, # test groups different? type = &quot;full&quot;, # give the test statistic output = &quot;text&quot;, # output for html caption = &quot;Compare Intervention groups on five main variables&quot;) # title Table 6.1: Compare Intervention groups on five main variables control experiment Test P-Value n = 496 n = 504 age T-Test: 0.82 0.411 43.3 (11.6) 42.7 (12.5) genderF Chi Square: 0.19 0.661 Male 135 (27.2%) 130 (25.8%) Female 361 (72.8%) 374 (74.2%) experien T-Test: 0.69 0.492 17.2 (5.8) 16.9 (6.3) wardtype Chi Square: 0 1 general care 247 (49.8%) 252 (50%) special care 249 (50.2%) 252 (50%) hospsize Chi Square: 0.01 0.993 small 185 (37.3%) 189 (37.5%) medium 237 (47.8%) 239 (47.4%) large 74 (14.9%) 76 (15.1%) hospsizeN T-Test: 0.01 0.992 0.8 (0.7) 0.8 (0.7) hospsizeNG T-Test: 0.01 0.992 0.0 (0.7) -0.0 (0.7) The t-test performed by the furniture::table1() function will always assume indepent groups and that HOV is not violated. This may or may not be appropriate. 6.3 MLM: Null Model In a Null, intercept-only, or Empty model, no predictors are included. 6.3.0.1 Fit the Model Fit the model to the data, with both ML and REML. nurse_lmer_0_re &lt;- lmerTest::lmer(stress ~ 1 + # Fixed Intercept for all nurses (1|hospital/ward), # Random Intercepts for wards within hospitals data = data_nurse, REML = TRUE) # fit via REML (the default) for ICC calculations nurse_lmer_0_ml &lt;- lmerTest::lmer(stress ~ 1 + # Fixed Intercept for all nurses (1|hospital/ward), # Random Intercepts for wards within hospitals data = data_nurse, REML = FALSE) # fit via ML for comparing FIXED effects inclusion texreg::knitreg(list(nurse_lmer_0_ml, nurse_lmer_0_re), custom.model.names = c(&quot;M0: Null, ML&quot;, &quot;M0: Null, REML&quot;), caption = &quot;NULL Model: different estimation methods&quot;, caption.above = TRUE, single.row = TRUE) NULL Model: different estimation methods   M0: Null, ML M0: Null, REML (Intercept) 5.00 (0.11)*** 5.00 (0.11)*** AIC 1950.36 1952.95 BIC 1969.99 1972.58 Log Likelihood -971.18 -972.48 Num. obs. 1000 1000 Num. groups: ward:hospital 100 100 Num. groups: hospital 25 25 Var: ward:hospital (Intercept) 0.49 0.49 Var: hospital (Intercept) 0.16 0.17 Var: Residual 0.30 0.30 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 6.4 Estimate the ICC The ICC is calculated by dividing the between-group-variance (random intercept variance) by the total variance (i.e. sum of between-group-variance and within-group (residual) variance). lme4::VarCorr(nurse_lmer_0_re) Groups Name Std.Dev. ward:hospital (Intercept) 0.69916 hospital (Intercept) 0.41749 Residual 0.54887 lme4::VarCorr(nurse_lmer_0_re) %&gt;% print(comp = c(&quot;Variance&quot;, &quot;Std.Dev&quot;), digits = 3) Groups Name Variance Std.Dev. ward:hospital (Intercept) 0.489 0.699 hospital (Intercept) 0.174 0.417 Residual 0.301 0.549 vc &lt;- lme4::VarCorr(nurse_lmer_0_re) %&gt;% data.frame() pie(x = vc$vcov, labels = vc$grp) The performance package has a few really helpful funcitons: lme4::VarCorr(nurse_lmer_0_re) Groups Name Std.Dev. ward:hospital (Intercept) 0.69916 hospital (Intercept) 0.41749 Residual 0.54887 insight::get_variance(nurse_lmer_0_re) $var.fixed [1] 0 $var.random [1] 0.6631239 $var.residual [1] 0.3012569 $var.distribution [1] 0.3012569 $var.dispersion [1] 0 $var.intercept ward:hospital hospital 0.4888231 0.1743008 \\[ \\begin{align*} \\text{hospitals} \\rightarrow \\; &amp; \\sigma^2_{v0} = 0.417^2 = 0.174\\\\ \\text{wards within hospitals} \\rightarrow \\; &amp; \\sigma^2_{u0} = 0.699^2 = 0.489\\\\ \\text{nurses within wards within hospitals} \\rightarrow \\; &amp; \\sigma^2_{e} = 0.549^2 = 0.301\\\\ \\end{align*} \\] Intraclass Correlation (ICC) Formula, 3 level model - Davis and Scott Method \\[ \\overbrace{\\rho_{mid}}^{\\text{ICC}\\atop\\text{at level 2}} = \\frac{\\overbrace{\\sigma^2_{u0}}^{\\text{Random Intercept}\\atop\\text{Variance Level 2}}} {\\underbrace{\\sigma^2_{v0}+\\sigma^2_{u0}+\\sigma^2_{e}}_{\\text{Total}\\atop\\text{Variance}}} \\tag{Hox 2.16} \\] \\[ \\overbrace{\\rho_{top}}^{\\text{ICC}\\atop\\text{ at level 3}} = \\frac{\\overbrace{\\sigma^2_{v0}}^{\\text{Random Intercept}\\atop\\text{Variance Level 3}}} {\\underbrace{\\sigma^2_{v0}+\\sigma^2_{u0}+\\sigma^2_{e}}_{\\text{Total}\\atop\\text{Variance}}} \\tag{Hox 2.17} \\] 0.489 / (0.174 + 0.489 + 0.301) # middle level (wards) [1] 0.5072614 0.174 / (0.174 + 0.489 + 0.301) # top level (hospitals) [1] 0.1804979 For more than two levels, the ‘performance::icc()’ function computes ICC’s by the Davis and Scott method. performance::icc(nurse_lmer_0_re) # Intraclass Correlation Coefficient Adjusted ICC: 0.688 Unadjusted ICC: 0.688 performance::icc(nurse_lmer_0_re, by_group = TRUE) # ICC by Group Group | ICC --------------------- ward:hospital | 0.507 hospital | 0.181 The proportion of variance in nurse stress level is 0.51 at the ward level and 0.18 at the hospital level, for a total of 0.69. To test if the three level model is justified statistically, compare the null models with and without the nesting of wards in hospitals. nurse_lmer_0_re_2level &lt;- lmerTest::lmer(stress ~ 1 + (1|wardid), # each hospital contains several wards data = data_nurse, REML = TRUE) # fit via REML (the default) for ICC calculations texreg::knitreg(list(nurse_lmer_0_re_2level, nurse_lmer_0_re), custom.model.names = c(&quot;2 levels&quot;, &quot;3 levels&quot;), caption = &quot;MLM: Two or Three Levels?&quot;, caption.above = TRUE, single.row = TRUE) MLM: Two or Three Levels?   2 levels 3 levels (Intercept) 5.00 (0.08)*** 5.00 (0.11)*** AIC 1958.43 1952.95 BIC 1973.15 1972.58 Log Likelihood -976.21 -972.48 Num. obs. 1000 1000 Num. groups: wardid 100   Var: wardid (Intercept) 0.66   Var: Residual 0.30 0.30 Num. groups: ward:hospital   100 Num. groups: hospital   25 Var: ward:hospital (Intercept)   0.49 Var: hospital (Intercept)   0.17 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 The deviance test or likelihood-ratio test shows that the inclusion of the nesting of wards within hospitals better explains the variance in nurse stress levels. anova(nurse_lmer_0_re, nurse_lmer_0_re_2level, refit = FALSE) Data: data_nurse Models: nurse_lmer_0_re_2level: stress ~ 1 + (1 | wardid) nurse_lmer_0_re: stress ~ 1 + (1 | hospital/ward) npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) nurse_lmer_0_re_2level 3 1958.4 1973.2 -976.21 1952.4 nurse_lmer_0_re 4 1953.0 1972.6 -972.48 1945.0 7.4738 1 0.00626 nurse_lmer_0_re_2level nurse_lmer_0_re ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 6.5 MLM: Add Fixed Effects 6.5.1 Fit the Model Hox, Moerbeek, and Van de Schoot (2017), page 22: “In this example, the variable expcon is of main interest, and the other variables are covariates. Their funciton is to control for differences between the groups, which can occur even if randomization is used, especially with small samples, and to explain variance in the outcome variable stress. To the extent that these variables successfully explain the variance, the power of the test for the effect of expcon will be increased.” nurse_lmer_1_ml &lt;- lmerTest::lmer(stress ~ expcon + # experimental condition = CATEGORICAL FACTOR age + gender + experien + # level 1 covariates wardtype + # level 2 covariates hospsize + # level 3 covariates, hospital size = CATEGORICAL FACTOR (1|hospital/ward), # Random Intercepts for wards within hospitals data = data_nurse, REML = FALSE) # fit via ML for nested FIXED effects texreg::knitreg(list(nurse_lmer_0_ml, nurse_lmer_1_ml), custom.model.names = c(&quot;M0: null&quot;, &quot;M1: fixed pred&quot;), caption = &quot;Nested Models: Fixed effects via ML&quot;, caption.above = TRUE, single.row = TRUE) Nested Models: Fixed effects via ML   M0: null M1: fixed pred (Intercept) 5.00 (0.11)*** 5.38 (0.18)*** expconexperiment   -0.70 (0.12)*** age   0.02 (0.00)*** gender   -0.45 (0.03)*** experien   -0.06 (0.00)*** wardtypespecial care   0.05 (0.12) hospsizemedium   0.49 (0.19)** hospsizelarge   0.90 (0.26)*** AIC 1950.36 1626.32 BIC 1969.99 1680.30 Log Likelihood -971.18 -802.16 Num. obs. 1000 1000 Num. groups: ward:hospital 100 100 Num. groups: hospital 25 25 Var: ward:hospital (Intercept) 0.49 0.33 Var: hospital (Intercept) 0.16 0.10 Var: Residual 0.30 0.22 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 6.5.2 Assess Significance anova(nurse_lmer_0_ml, nurse_lmer_1_ml) Data: data_nurse Models: nurse_lmer_0_ml: stress ~ 1 + (1 | hospital/ward) nurse_lmer_1_ml: stress ~ expcon + age + gender + experien + wardtype + hospsize + (1 | hospital/ward) npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) nurse_lmer_0_ml 4 1950.4 1970.0 -971.18 1942.4 nurse_lmer_1_ml 11 1626.3 1680.3 -802.16 1604.3 338.04 7 &lt; 2.2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 It is clear that the inclusing of these fixed, main effects improves the model’s fit, but it is questionable that the type of ward is significant (Wald test is non-significant). Rather than test it directly, we will leave it in the model. This is common practice to show that an expected variable is not significant. 6.5.3 Centering Variables Because we will will find that the experimental condition is moderated by hospital size (in other words there is a significant interaction between expcon and hospsize), Hox, Moerbeek, and Van de Schoot (2017) presents the models fit with centered values for these two variables. Let us see how this changes the model. (1) Experimental Condition Experimental conditon is a BINARY or two-level factor. When it is alternatively coded as a numeric, continuous variable taking the values of zero (\\(0\\)) for the reference category and one (\\(1\\)) for the other category, the model estimates are exactly the same, including the paramters for the variables and the intercept, AND the model fit statistics. When the numeric, continuous variable is further grand-mean centered by additionally subtraction the MEAN of the numberic version, the value of the intercept is the only estimate that changes. nurse_lmer_1a_ml &lt;- lmerTest::lmer(stress ~ expconN + # experimental condition = CONTINUOUS CODED 0/1 age + gender + experien + wardtype + hospsize + # hospital size = CATEGORICAL FACTOR (1|hospital/ward), data = data_nurse, REML = FALSE) nurse_lmer_1b_ml &lt;- lmerTest::lmer(stress ~ expconNG + # experimental condition = CONTINUOUS GRAND-MEAN CENTERED age + gender + experien + wardtype + hospsize + # hospital size = CATEGORICAL FACTOR (1|hospital/ward), data = data_nurse, REML = FALSE) texreg::knitreg(list(nurse_lmer_1_ml, nurse_lmer_1a_ml, nurse_lmer_1b_ml), custom.model.names = c(&quot;Factor&quot;, &quot;0 vs 1&quot;, &quot;Centered&quot;), caption = &quot;MLM: Model 1 - Expereimental Condiditon Coding (2-levels)&quot;, caption.above = TRUE, single.row = TRUE) MLM: Model 1 - Expereimental Condiditon Coding (2-levels)   Factor 0 vs 1 Centered (Intercept) 5.38 (0.18)*** 5.38 (0.18)*** 5.03 (0.17)*** expconexperiment -0.70 (0.12)***     age 0.02 (0.00)*** 0.02 (0.00)*** 0.02 (0.00)*** gender -0.45 (0.03)*** -0.45 (0.03)*** -0.45 (0.03)*** experien -0.06 (0.00)*** -0.06 (0.00)*** -0.06 (0.00)*** wardtypespecial care 0.05 (0.12) 0.05 (0.12) 0.05 (0.12) hospsizemedium 0.49 (0.19)** 0.49 (0.19)** 0.49 (0.19)** hospsizelarge 0.90 (0.26)*** 0.90 (0.26)*** 0.90 (0.26)*** expconN   -0.70 (0.12)***   expconNG     -0.70 (0.12)*** AIC 1626.32 1626.32 1626.32 BIC 1680.30 1680.30 1680.30 Log Likelihood -802.16 -802.16 -802.16 Num. obs. 1000 1000 1000 Num. groups: ward:hospital 100 100 100 Num. groups: hospital 25 25 25 Var: ward:hospital (Intercept) 0.33 0.33 0.33 Var: hospital (Intercept) 0.10 0.10 0.10 Var: Residual 0.22 0.22 0.22 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 (2) Hospital Size Experimental conditon is a three-level factor. When it is alternatively coded as a numeric, continuous variables taking the values of whole numbers, starting with zero (\\(0, 1, 2, \\dots\\)), the model there will only be ONE parameter estimated instead of several (one less than the number of categories). This is becuase the levels are treated as being equally different from each other in terms of the outcome. This treats the effect of the categorical variable as if it is linear, which may or may not be appropriate. User beware! When the numeric, continuous variable is further grand-mean centered by additionally subtraction the MEAN of the numberic version, the value of the intercept is the only estimate that changes. nurse_lmer_1c_ml &lt;- lmerTest::lmer(stress ~ expconNG + # experimental condition = CONTINUOUS GRAND-MEAN CENTERED age + gender + experien + wardtype + hospsizeN + # hospital size = CONTINUOUS CODED 0/1 (1|hospital/ward), data = data_nurse, REML = FALSE) nurse_lmer_1d_ml &lt;- lmerTest::lmer(stress ~ expconNG + # experimental condition = CONTINUOUS GRAND-MEAN CENTERED age + gender + experien + wardtype + hospsizeNG + # hospital size = CONTINUOUS GRAND-MEAN CENTERED (1|hospital/ward), data = data_nurse, REML = FALSE) texreg::knitreg(list(nurse_lmer_1b_ml, nurse_lmer_1c_ml, nurse_lmer_1d_ml), custom.model.names = c(&quot;Factor&quot;, &quot;0 vs 1&quot;, &quot;Centered&quot;), caption = &quot;MLM: Model 1 - Hospital Coding (3-levels)&quot;, caption.above = TRUE, single.row = TRUE) MLM: Model 1 - Hospital Coding (3-levels)   Factor 0 vs 1 Centered (Intercept) 5.03 (0.17)*** 5.04 (0.16)*** 5.40 (0.12)*** expconNG -0.70 (0.12)*** -0.70 (0.12)*** -0.70 (0.12)*** age 0.02 (0.00)*** 0.02 (0.00)*** 0.02 (0.00)*** gender -0.45 (0.03)*** -0.45 (0.03)*** -0.45 (0.03)*** experien -0.06 (0.00)*** -0.06 (0.00)*** -0.06 (0.00)*** wardtypespecial care 0.05 (0.12) 0.05 (0.12) 0.05 (0.12) hospsizemedium 0.49 (0.19)**     hospsizelarge 0.90 (0.26)***     hospsizeN   0.46 (0.12)***   hospsizeNG     0.46 (0.12)*** AIC 1626.32 1624.36 1624.36 BIC 1680.30 1673.44 1673.44 Log Likelihood -802.16 -802.18 -802.18 Num. obs. 1000 1000 1000 Num. groups: ward:hospital 100 100 100 Num. groups: hospital 25 25 25 Var: ward:hospital (Intercept) 0.33 0.33 0.33 Var: hospital (Intercept) 0.10 0.10 0.10 Var: Residual 0.22 0.22 0.22 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 6.6 MLM: Add Random Slope Hox, Moerbeek, and Van de Schoot (2017), page 22: “Although logically we can test if explanatory variables at the first level have random coefficients (slopes) at the second or third level, and if explanatory variables at teh second level have random coefficients (slopes) at the third level, these possibilities are not pursued. We DO test a model with a random coefficient (slope) for expcon at the third level, where there turns out to be significant slope variation.” 6.6.1 Fit the Model nurse_lmer_1d_re &lt;- lmerTest::lmer(stress ~ expconNG + # experimental condition = CONTINUOUS GRAND-MEAN CENTERED age + gender + experien + # level 1 covariates wardtype + # level 2 covariate hospsizeNG + # level 3 covariate, hospital size = CONTINUOUS GRAND-MEAN CENTERED (1|hospital/ward), # Random Intercepts for wards within hospitals data = data_nurse, REML = TRUE) # fit via REML for nested Random Effects nurse_lmer_2_re &lt;- lmerTest::lmer(stress ~ expconNG + # experimental condition = CONTINUOUS GRAND-MEAN CENTERED age + gender + experien + # level 1 covariates wardtype + # level 2 covariate hospsizeNG + # level 3 covariate, hospital size = CONTINUOUS GRAND-MEAN CENTERED (1|hospital/ward) + # Random Intercepts for wards within hospitals (0 + expconNG|hospital), # RANDOM SLOPES for exp cond within hospital (does not vary witin a ward!) data = data_nurse, REML = TRUE) # fit via REML for nested Random Effects texreg::knitreg(list(nurse_lmer_1d_re, nurse_lmer_2_re), custom.model.names = c(&quot;M1: RI&quot;, &quot;M2: RIAS&quot;), caption = &quot;Nested Models: Random Slope via REML&quot;, caption.above = TRUE, single.row = TRUE) Nested Models: Random Slope via REML   M1: RI M2: RIAS (Intercept) 5.40 (0.12)*** 5.39 (0.11)*** expconNG -0.70 (0.12)*** -0.70 (0.18)*** age 0.02 (0.00)*** 0.02 (0.00)*** gender -0.45 (0.03)*** -0.45 (0.03)*** experien -0.06 (0.00)*** -0.06 (0.00)*** wardtypespecial care 0.05 (0.12) 0.05 (0.07) hospsizeNG 0.46 (0.13)*** 0.46 (0.13)*** AIC 1659.89 1633.18 BIC 1708.97 1687.17 Log Likelihood -819.94 -805.59 Num. obs. 1000 1000 Num. groups: ward:hospital 100 100 Num. groups: hospital 25 25 Var: ward:hospital (Intercept) 0.34   Var: hospital (Intercept) 0.11 0.17 Var: Residual 0.22 0.22 Var: ward.hospital (Intercept)   0.11 Var: hospital.1 expconNG   0.69 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 6.6.2 Assess Significance anova(nurse_lmer_1d_re, nurse_lmer_2_re, refit = FALSE) Data: data_nurse Models: nurse_lmer_1d_re: stress ~ expconNG + age + gender + experien + wardtype + hospsizeNG + (1 | hospital/ward) nurse_lmer_2_re: stress ~ expconNG + age + gender + experien + wardtype + hospsizeNG + (1 | hospital/ward) + (0 + expconNG | hospital) npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) nurse_lmer_1d_re 10 1659.9 1709.0 -819.94 1639.9 nurse_lmer_2_re 11 1633.2 1687.2 -805.59 1611.2 28.708 1 8.417e-08 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The inclusion of a random slope effect for the experimental condition expcon significantly improves the models’s fit, thus is should be retained. 6.7 MLM: Add Cross-Level Interaction Hox, Moerbeek, and Van de Schoot (2017), page 22: “The varying slope can be predicted by adding a cross-level interaction between the variables expcon and hospsize. In view of this interaction, the variables expcon and hospsize have been centered on tehir overal means.” 6.7.1 Fit the Model nurse_lmer_2_ml &lt;- lmerTest::lmer(stress ~ expconNG + # experimental condition = CONTINUOUS GRAND-MEAN CENTERED age + gender + experien + # level 1 covariates wardtype + # level 2 covariate hospsizeNG + # level 3 covariate, hospital size = CONTINUOUS GRAND-MEAN CENTERED (1|hospital/ward) + # Random Intercepts for wards within hospitals (0 + expconNG|hospital), # RANDOM SLOPES for exp cond within hospital (does not vary witin a ward!) data = data_nurse, REML = FALSE) # fit via ML for nested FIXED Effects nurse_lmer_3_ml &lt;- lmerTest::lmer(stress ~ expconNG + # experimental condition = CONTINUOUS GRAND-MEAN CENTERED age + gender + experien + # level 1 covariates wardtype + # level 2 covariate hospsizeNG + # level 3 covariate, hospital size = CONTINUOUS GRAND-MEAN CENTERED expconNG*hospsizeNG + # CROSS-LEVEL interaction (1|hospital/ward) + # Random Intercepts for wards within hospitals (0 + expconNG|hospital), # RANDOM SLOPES for exp cond within hospital (does not vary within a ward!) data = data_nurse, REML = FALSE) # fit via ML for nested FIXED Effects texreg::knitreg(list(nurse_lmer_2_ml, nurse_lmer_3_ml), custom.model.names = c(&quot;M2: RAIS&quot;, &quot;M3: Xlevel Int&quot;), caption = &quot;Nested Models: Fixed Cross-Level Interaction via ML&quot;, caption.above = TRUE, single.row = TRUE) Nested Models: Fixed Cross-Level Interaction via ML   M2: RAIS M3: Xlevel Int (Intercept) 5.39 (0.11)*** 5.39 (0.11)*** expconNG -0.70 (0.18)*** -0.72 (0.11)*** age 0.02 (0.00)*** 0.02 (0.00)*** gender -0.46 (0.03)*** -0.46 (0.03)*** experien -0.06 (0.00)*** -0.06 (0.00)*** wardtypespecial care 0.05 (0.07) 0.05 (0.07) hospsizeNG 0.46 (0.12)*** 0.46 (0.12)*** expconNG:hospsizeNG   1.00 (0.16)*** AIC 1597.48 1576.07 BIC 1651.47 1634.96 Log Likelihood -787.74 -776.03 Num. obs. 1000 1000 Num. groups: ward:hospital 100 100 Num. groups: hospital 25 25 Var: ward.hospital (Intercept) 0.11 0.11 Var: hospital (Intercept) 0.15 0.15 Var: hospital.1 expconNG 0.66 0.18 Var: Residual 0.22 0.22 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 6.7.2 Assess Significance anova(nurse_lmer_2_ml, nurse_lmer_3_ml) Data: data_nurse Models: nurse_lmer_2_ml: stress ~ expconNG + age + gender + experien + wardtype + hospsizeNG + (1 | hospital/ward) + (0 + expconNG | hospital) nurse_lmer_3_ml: stress ~ expconNG + age + gender + experien + wardtype + hospsizeNG + expconNG * hospsizeNG + (1 | hospital/ward) + (0 + expconNG | hospital) npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) nurse_lmer_2_ml 11 1597.5 1651.5 -787.74 1575.5 nurse_lmer_3_ml 12 1576.1 1635.0 -776.03 1552.1 23.413 1 1.307e-06 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 There is evidence that hospital size moderated the effect of the intervention. We will want to plot the estimated marginal means to interpret the meaning of this interaction. 6.8 Final Model 6.8.1 Fit the model The final model should be fit via REML. I’m going to revert back to the factor versions of hospital size and experimental condition, but grand-mean center age and experience data_nurse %&gt;% dplyr::select(age, experien) %&gt;% dplyr::summarise_all(mean) # A tibble: 1 × 2 age experien &lt;dbl&gt; &lt;dbl&gt; 1 43.0 17.1 nurse_lmer_final &lt;- lmerTest::lmer(stress ~ expcon + # experimental condition = CONTINUOUS GRAND-MEAN CENTERED I(age - 43.01) + gender + I(experien - 17.06) + # level 1 covariates wardtype + # level 2 covariate hospsize + # level 3 covariate, hospital size = CONTINUOUS GRAND-MEAN CENTERED expcon*hospsize + # CROSS-LEVEL interaction (1|hospital/ward) + # Random Intercepts for wards within hospitals (0 + expconNG|hospital), # RANDOM SLOPES for exp condition within hospital (does not vary within a ward!) data = data_nurse, REML = TRUE) # fit via REML for final model 6.8.2 Table of Estimated Parameters anova(nurse_lmer_final) Type III Analysis of Variance Table with Satterthwaite&#39;s method Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) expcon 3.532 3.532 1 22.04 16.2512 0.0005573 *** I(age - 43.01) 22.400 22.400 1 909.68 103.0680 &lt; 2.2e-16 *** gender 36.949 36.949 1 914.46 170.0117 &lt; 2.2e-16 *** I(experien - 17.06) 41.656 41.656 1 918.66 191.6692 &lt; 2.2e-16 *** wardtype 0.115 0.115 1 49.10 0.5278 0.4709777 hospsize 2.622 1.311 2 22.00 6.0321 0.0081539 ** expcon:hospsize 7.608 3.804 2 22.01 17.5040 2.822e-05 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 texreg::knitreg(list(nurse_lmer_final), custom.model.names = c(&quot;M3: Xlevel Int&quot;), caption = &quot;Final Model: with REML&quot;, caption.above = TRUE, single.row = TRUE) Final Model: with REML   M3: Xlevel Int (Intercept) 5.70 (0.19)*** expconexperiment -1.55 (0.20)*** age - 43.01 0.02 (0.00)*** gender -0.46 (0.03)*** experien - 17.06 -0.06 (0.00)*** wardtypespecial care 0.05 (0.07) hospsizemedium -0.07 (0.24) hospsizelarge -0.07 (0.33) expconexperiment:hospsizemedium 1.12 (0.26)*** expconexperiment:hospsizelarge 1.94 (0.35)*** AIC 1617.76 BIC 1686.47 Log Likelihood -794.88 Num. obs. 1000 Num. groups: ward:hospital 100 Num. groups: hospital 25 Var: ward.hospital (Intercept) 0.11 Var: hospital (Intercept) 0.18 Var: hospital.1 expconNG 0.21 Var: Residual 0.22 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 Interpretation: After accounting for the a nurses’ age, gender, and experience, as well as the ward type, the effect of the intervention is moderated by hospital size, b = 1.96, SE = 0.35, p &lt; .001, 6.8.3 Visualization: Estimated Marginal Means Plot Although there are many variables in this model, only two are involved in any interaction(s). For this reason, we will choose to display the estimated marginal means across only experimental condition and hospital size. For this illustration, all other continuous predictors are taken to be at their mean and categorical predictors at their reference category. 6.8.3.1 Using interactions::interact_plot() All Defaults: interactions::cat_plot(model = nurse_lmer_final, pred = hospsize, # main predictor/independent variable, for x-axis modx = expcon, # moderating independent variable, for different lines interval = TRUE) Look: We can make NASTY dynamite plots! DO NOT EVER USE THESE! From the help menu: “Many applied researchers advise against this type of plot because it does not represent the distribution of the observed data or the uncertainty of the predictions very well.” interactions::cat_plot(model = nurse_lmer_final, pred = hospsize, # main predictor/independent variable, for x-axis modx = expcon, # moderating independent variable, for different lines interval = TRUE, geom = &quot;bar&quot;) 6.8.4 Post Hoc Pairwise Tests When an interactions is between two categorical predictors, compare pairs of mean predictions. nurse_lmer_final %&gt;% emmeans::emmeans(~ expcon | hospsize) %&gt;% pairs() hospsize = small: contrast estimate SE df t.ratio p.value control - experiment 1.545 0.196 21.9 7.898 &lt;.0001 hospsize = medium: contrast estimate SE df t.ratio p.value control - experiment 0.427 0.170 22.0 2.518 0.0196 hospsize = large: contrast estimate SE df t.ratio p.value control - experiment -0.392 0.294 22.2 -1.332 0.1964 Results are averaged over the levels of: gender, wardtype Degrees-of-freedom method: kenward-roger Interpretation: For hospitals that are small in size, the intervention does substantially decrease stress levels by 1.55 points, SE = 0.20, p &lt;.001. For hospitals that are medium sizes, the intervention does decrease stress, but by a modest 0.43 points, SD = 0.17, p = .020. For large hospitals the intervention was not effective at reducing stress, p = .200. 6.8.5 Effect Sizes From the help page for emmeans::eff_size: For models having a single random effect, such as those fitted using lm; in that case, the stats::sigma() and stats::df.residual() functions may be useful for specifying sigma and edf.For models with **more than one random effect**,sigma` may be based on some combination of the random-effect variances. Specifying edf can be rather un-intuitive but is also relatively un-critical; but the smaller the value, the wider the confidence intervals for effect size. The value of sqrt(2/edf) can be interpreted as the relative accuracy of sigma; for example, with edf = 50, \\(\\sqrt{\\frac{2}{50}}\\) = 0.2, meaning that sigma is accurate to plus or minus 20 percent. Note in an example below, we tried two different edf values as kind of a bracketing/sensitivity-analysis strategy. A value of Inf is allowable, in which case you are assuming that sigma is known exactly. Obviously, this narrows the confidence intervals for the effect sizes – unrealistically if in fact sigma is unknown. 6.8.5.1 Compute Standardized Mean Differences FIRST: We need to compute the values to standardize by…“Sigma”. Option 1) Pooled SD &lt;– My favorite added all the model’s variance components took the standard deviation totalSD &lt;- VarCorr(nurse_lmer_final) %&gt;% data.frame() %&gt;% dplyr::summarise(tot_var = sum(vcov)) %&gt;% dplyr::pull(tot_var) %&gt;% sqrt() totalSD [1] 0.8465793 Option 2) SD of all stress levels for all participants data_nurse %&gt;% dplyr::summarise(sd(stress)) # A tibble: 1 × 1 `sd(stress)` &lt;dbl&gt; 1 0.980 Option 3) SD of stress levels for participants in the control group data_nurse %&gt;% dplyr::filter(expcon == &quot;control&quot;) %&gt;% dplyr::summarise(sd(stress)) # A tibble: 1 × 1 `sd(stress)` &lt;dbl&gt; 1 0.724 SECOND: We need to compute the numeric scalar that specifies the equivalent degrees of freedom for the sigma. This is a way of specifying the uncertainty in sigma, in that we regard our estimate of sigma^2 as being proportional to a chi-square random variable with edf degrees of freedom. (edf should not be confused with the df argument that may be passed via … to specify the degrees of freedom to use in t statistics and confidence intervals.) This value has an effect the SE and confidence intervals, which I suggest you do not report, so just throw any old number in there ;) nurse_lmer_final %&gt;% emmeans::emmeans(~ expcon | hospsize) %&gt;% emmeans::eff_size(sigma = totalSD, # which SD to divide by??? edf = 10) # df hospsize = small: contrast effect.size SE df lower.CL upper.CL control - experiment 1.826 0.469 37.5 0.875 2.776 hospsize = medium: contrast effect.size SE df lower.CL upper.CL control - experiment 0.505 0.230 37.7 0.039 0.971 hospsize = large: contrast effect.size SE df lower.CL upper.CL control - experiment -0.463 0.363 37.6 -1.198 0.272 Results are averaged over the levels of: gender, wardtype sigma used for effect sizes: 0.8466 Degrees-of-freedom method: inherited from kenward-roger when re-gridding Confidence level used: 0.95 nurse_lmer_final %&gt;% emmeans::emmeans(~ expcon | hospsize) %&gt;% emmeans::eff_size(sigma = totalSD, edf = 100) hospsize = small: contrast effect.size SE df lower.CL upper.CL control - experiment 1.826 0.265 37.5 1.2894 2.362 hospsize = medium: contrast effect.size SE df lower.CL upper.CL control - experiment 0.505 0.204 37.7 0.0925 0.917 hospsize = large: contrast effect.size SE df lower.CL upper.CL control - experiment -0.463 0.349 37.6 -1.1705 0.244 Results are averaged over the levels of: gender, wardtype sigma used for effect sizes: 0.8466 Degrees-of-freedom method: inherited from kenward-roger when re-gridding Confidence level used: 0.95 FULL INTERPRETATION: After accounting for the a nurses’ age, gender, and experience, as well as the ward type, the effect of the intervention is moderated by hospital size, F(2, 22) = 17.50, p &lt; .001. For hospitals that are small in size, the intervention does substantially decrease stress levels by 1.55 points, SE = 0.20, p &lt;.001, Cohen’s d = 1.83. For hospitals that are medium sizes, the intervention does decrease stress, but by a modest 0.43 points, SD = 0.17, p = .020, Cohen’s d = 0.51. For large hospitals the intervention was not effective at reducing stress, p = .200. Adding more options: interactions::cat_plot(nurse_lmer_final, pred = &quot;hospsize&quot;, modx = &quot;expcon&quot;, interval = TRUE, y.label = &quot;Estimated Marginal Mean Norse&#39;s Stress Score&quot;, x.label = &quot;Hospital Size&quot;, legend.main = &quot;Condition:&quot;, modx.labels = c(&quot;Control&quot;, &quot;Intervention&quot;), colors = c(&quot;gray40&quot;, &quot;black&quot;), point.shape = TRUE, pred.point.size = 5, dodge.width = .3, errorbar.width = .25, geom = &quot;line&quot;) + theme_bw() + theme(legend.key.width = unit(2, &quot;cm&quot;), legend.background = element_rect(color = &quot;Black&quot;), legend.position = c(1, 0), legend.justification = c(1.1, -0.1)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 1)) 6.8.5.2 Using sjPlot::plot_model() All Defaults: sjPlot::plot_model(nurse_lmer_final, type = &quot;pred&quot;, terms = c(&quot;hospsize&quot;, &quot;expcon&quot;)) Adding more options: sjPlot::plot_model(nurse_lmer_final, type = &quot;pred&quot;, terms = c(&quot;hospsize&quot;, &quot;expcon&quot;)) + labs(title = &quot;Multilevel Modeling of Hospital Nurse Stress Intervention&quot;, subtitle = &quot;Error bars display 95% Confidene Intervals&quot;, x = &quot;Hospital Size&quot;, y = &quot;Estimated Marginal Mean\\nNorse&#39;s Stress Score&quot;, color = &quot;Condition&quot;) + theme_bw() 6.8.5.3 Using effects::Effect() and ggplot Get Estimated Marginal Means - default ‘nice’ predictor values: Focal predictors: All combinations of… expcon categorical, both levels control and experiment hospsize categorical, all three levelssmall , medium, large Always followed by: fit estimated marginal mean se standard error for the marginal mean lower lower end of the 95% confidence interval around the estimated marginal mean upper upper end of the 95% confidence interval around the estimated marginal mean effects::Effect(focal.predictors = c(&quot;expcon&quot;, &quot;hospsize&quot;), mod = nurse_lmer_final) %&gt;% data.frame() expcon hospsize fit se lower upper 1 control small 5.394485 0.1814379 5.038438 5.750532 2 experiment small 3.849007 0.1808593 3.494096 4.203919 3 control medium 5.324360 0.1572391 5.015800 5.632920 4 experiment medium 4.897002 0.1567719 4.589358 5.204645 5 control large 5.326344 0.2726473 4.791311 5.861377 6 experiment large 5.718468 0.2717250 5.185245 6.251691 effects::Effect(focal.predictors = c(&quot;expcon&quot;, &quot;hospsize&quot;), mod = nurse_lmer_final) %&gt;% data.frame() expcon hospsize fit se lower upper 1 control small 5.394485 0.1814379 5.038438 5.750532 2 experiment small 3.849007 0.1808593 3.494096 4.203919 3 control medium 5.324360 0.1572391 5.015800 5.632920 4 experiment medium 4.897002 0.1567719 4.589358 5.204645 5 control large 5.326344 0.2726473 4.791311 5.861377 6 experiment large 5.718468 0.2717250 5.185245 6.251691 effects::Effect(focal.predictors = c(&quot;expcon&quot;, &quot;hospsize&quot;), mod = nurse_lmer_final) %&gt;% data.frame() %&gt;% ggplot() + aes(x = hospsize, y = fit, group = expcon, shape = expcon, color = expcon) + geom_errorbar(aes(ymin = fit - se, # mean plus/minus one Std Error ymax = fit + se), width = .4, position = position_dodge(width = .2)) + geom_errorbar(aes(ymin = lower, # 95% CIs ymax = upper), width = .2, position = position_dodge(width = .2)) + geom_line(aes(linetype = expcon), position = position_dodge(width = .2)) + geom_point(size = 4, position = position_dodge(width = .2)) + theme_bw() + labs(x = &quot;Hospital Size&quot;, y = &quot;Estimated Marginal Mean, Stress&quot;, shape = &quot;Condition&quot;, color = &quot;Condition&quot;, linetype = &quot;Condition&quot;) + theme(legend.key.width = unit(2, &quot;cm&quot;), legend.background = element_rect(color = &quot;black&quot;), legend.position = c(0, 0), legend.justification = c(-0.1, -0.1)) This plot illustrates the estimated marginal means among male (gender’s reference category) nurses at the overall mean age (43.01 years), with the mean level experience (17.06 years), since thoes variables were not included as focal.predictors in the effects::Effect() function. Different values for those predictors would yield the exact sample plot, shifted as a whole either up or down. 6.9 Interpretation There is evidence this intervention lowered stress among nurses working in small hospitals and to a smaller degree in medium sized hospitals. The intervention did not exhibit an effect in large hospitals. 6.9.1 Strength This analysis was able to incorporated all three levels of clustering while additionally controlling for many co0-variates, both categorical (nurse gender and ward type) and continuous (nurse age and experience in years). Also heterogeneity was accounted for in terms of the intervention’s effect at various hospitals. This would NOT be possible via any ANOVA type analysis. 6.9.2 Weakness The approach presented by Hox, Moerbeek, and Van de Schoot (2017) and shown above involved mean-centering categorical variables. This would only be appropriate for a factor with more than two levels if its effect on the outcome was linear. Also, as the mean-centered variables are treated as continuous variables, post hoc tests are increasingly difficult. That is why I did that in the final model. 6.10 Reproduction of Table 2.5 Hox, Moerbeek, and Van de Schoot (2017) presents a table on page 23 comparing various models. Note, that table includes models only fit via maximum likelihood, not REML. Also, the model \\(M_3\\): with cross-level interaction is slightly different for an unknown reason. texreg::knitreg(list(nurse_lmer_0_ml, nurse_lmer_1d_ml, nurse_lmer_2_ml, nurse_lmer_3_ml), custom.model.names = c(&quot;M0&quot;, &quot;M1&quot;, &quot;M2&quot;, &quot;M3&quot;), caption = &quot;Hox Table 2.5: Models for stress in hospitals and wards&quot;, caption.above = TRUE, single.row = TRUE) Hox Table 2.5: Models for stress in hospitals and wards   M0 M1 M2 M3 (Intercept) 5.00 (0.11)*** 5.40 (0.12)*** 5.39 (0.11)*** 5.39 (0.11)*** expconNG   -0.70 (0.12)*** -0.70 (0.18)*** -0.72 (0.11)*** age   0.02 (0.00)*** 0.02 (0.00)*** 0.02 (0.00)*** gender   -0.45 (0.03)*** -0.46 (0.03)*** -0.46 (0.03)*** experien   -0.06 (0.00)*** -0.06 (0.00)*** -0.06 (0.00)*** wardtypespecial care   0.05 (0.12) 0.05 (0.07) 0.05 (0.07) hospsizeNG   0.46 (0.12)*** 0.46 (0.12)*** 0.46 (0.12)*** expconNG:hospsizeNG       1.00 (0.16)*** AIC 1950.36 1624.36 1597.48 1576.07 BIC 1969.99 1673.44 1651.47 1634.96 Log Likelihood -971.18 -802.18 -787.74 -776.03 Num. obs. 1000 1000 1000 1000 Num. groups: ward:hospital 100 100 100 100 Num. groups: hospital 25 25 25 25 Var: ward:hospital (Intercept) 0.49 0.33     Var: hospital (Intercept) 0.16 0.10 0.15 0.15 Var: Residual 0.30 0.22 0.22 0.22 Var: ward.hospital (Intercept)     0.11 0.11 Var: hospital.1 expconNG     0.66 0.18 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 Deviance: c(deviance(nurse_lmer_0_ml), deviance(nurse_lmer_1d_ml), deviance(nurse_lmer_2_ml), deviance(nurse_lmer_3_ml)) %&gt;% round(1) [1] 1942.4 1604.4 1575.5 1552.1 "],["mlm-centeringscaling-reading-achievement-2-levels-only.html", "7 MLM, Centering/Scaling: Reading Achievement (2-levels only) 7.1 Background 7.2 Exploratory Data Analysis 7.3 Single-Level Regression 7.4 MLM - Step 1: Null Model, only fixed and random intercepts 7.5 MLM - Step 2: Add Lower-level explanatory variables, fixed, ML 7.6 MLM - Step 3: Higher-level explanatory variables, fixed, ML 7.7 MLM - Step 4: Explanatory variables predict Slopes, random, REML 7.8 MLM - Step 5: Cross-Level interactions between explanatory variables - fixed, ML 7.9 Centering Predictors: Change Center 7.10 Rescaling Predictors: Change Units or Standardize 7.11 Final Model", " 7 MLM, Centering/Scaling: Reading Achievement (2-levels only) library(tidyverse) # all things tidy library(haven) # read in SPSS dataset library(furniture) # nice table1() descriptives library(stargazer) # display nice tables: summary &amp; regression library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(psych) # contains some useful functions, like headTail library(performance) # ICC and R2 calculations library(effects) # Effects for regression models library(optimx) # Different optimizers to solve mlm&#39;s library(lme4) # non-linear mixed-effects models library(haven) # read in SPSS dataset 7.1 Background The following example was included in the text “Multilevel Modeling in R” by Finch, Bolin, and Kelley (2016). The datasets for this textbook may be downloaded from the website: http://www.mlminr.com/data-sets/. I was unable to find any documentation on this dataset in the book or online, so I contacted the authors. There were unable to provide much either, but based on visual inspection designated the class of factor to thoes vairables that seem to represent categorical quantities. The labels for gender and class size are relative to the frequencies in the journal article the authors did point me to (although the samples sizes do not match up). FOR THIS CHAPTER WE WILL IGNORE ALL LEVELS EXCEPT FOR STUDNETS BEING NESTED WITHIN SCHOOLS. Read the SPSS data in with the haven package . data_raw &lt;- haven::read_sav(&quot;http://www.mlminr.com/data-sets/Achieve.sav?attredirects=0&quot;) Declare all categorical variables to be factors and apply labels where meaningful. Student-specific * gender = Male or Female * age = Age, in months * gevocab = Vocabulary Score * geread = Reading Score Class-specific * classsize = category of class’s size School-specific * senroll = school enrollment * ses = school’s SES level data_achieve &lt;- data_raw %&gt;% dplyr::mutate_at(vars(id, region, corp, school, class), factor) %&gt;% dplyr::mutate(gender = gender %&gt;% factor(labels = c(&quot;Female&quot;, &quot;Male&quot;))) %&gt;% dplyr::mutate(classize = classize %&gt;% factor(labels = c(&quot;12-17&quot;, &quot;18-21&quot;, &quot;22-26&quot;, &quot;&gt;26&quot;))) %&gt;% dplyr::select(id, region, corp, school, class, # Identifiers gender, age, geread, gevocab, # Pupil-level vars classize, # Class-Level vars senroll, ses) # School-level vars 7.1.1 Sample Structure It is obvious that the sample is hiarchical in nature. The nesting starts with students (level 1) nested within class (level 2), which are further nested within school (level 3), corp (level 4), and finally region (level 5). For this chapter we will only focus on TWO levels: students (level 1) are the units on which the outcome is measured and schools (level 2) are the units in which they are nested. The number of regions = 9: num_regions &lt;- data_achieve %&gt;% dplyr::group_by(region) %&gt;% dplyr::tally() %&gt;% nrow() num_regions [1] 9 The number of corps = 60: num_corps &lt;- data_achieve %&gt;% dplyr::group_by(region, corp) %&gt;% dplyr::tally() %&gt;% nrow() num_corps [1] 60 The number of schools = 160 num_schools &lt;- data_achieve %&gt;% dplyr::group_by(region, corp, school) %&gt;% dplyr::tally() %&gt;% nrow() num_schools [1] 160 The number of classes = 568 num_classes &lt;- data_achieve %&gt;% dplyr::group_by(region, corp, school, class) %&gt;% dplyr::tally() %&gt;% nrow() num_classes [1] 568 The number of students = 10320 num_subjects &lt;- data_achieve %&gt;% nrow num_subjects [1] 10320 7.2 Exploratory Data Analysis 7.2.1 Summarize Descriptive Statistics 7.2.1.1 The stargazer package Most posters, journal articles, and reports start with a table of descriptive statistics. Since it tends to come first, this type of table is often refered to as Table 1. The stargazer() function can be used to create such a table, but only for the entire dataset. I haven’t been able to find a way to get it to summarize subsamples and compare them in the standard format. Also, it only summarises continuous, not categorical variables. # Knit to Website: type = &quot;html&quot; # Knit to PDF: type = &quot;latex&quot; # View on Screen: type = &quot;text&quot; data_achieve %&gt;% dplyr::select(classize, gender, geread, gevocab, age, ses, senroll) %&gt;% data.frame() %&gt;% stargazer::stargazer(header = FALSE, title = &quot;Summary of the numeric variables with `stargazer`&quot;, type = &quot;text&quot;) Summary of the numeric variables with stargazer Statistic N Mean St. Dev. Min Max geread 10,320 4.341 2.332 0.000 12.000 gevocab 10,320 4.494 2.368 0.000 11.200 age 10,320 107.529 5.060 82 135 ses 10,320 72.849 21.982 0.000 100.000 senroll 10,320 533.415 154.797 115 916 7.2.1.2 The furniture package Tyson Barrett’s furniture package includes the extremely useful function table1() which simplifies the common task of creating a stratified, comparative table of descriptive statistics. Full documentation can be accessed by executing ?furniture::table1. # Knit to Website: output = &quot;html&quot; # Knit to PDF: output = &quot;latex2&quot; # View on Screen: output = &quot;&quot;text&quot;, or &quot;markdown&quot;, &quot;html&quot; data_achieve %&gt;% furniture:: table1(&quot;Reading Score&quot; = geread, &quot;Vocabulary Score&quot; = gevocab, &quot;Age (in months)&quot; = age, &quot;School SES&quot; = ses, &quot;School&#39;s Enrollment&quot; = senroll, splitby = ~ gender, # var to divide sample by test = TRUE, # test groups different? caption = &quot;Summary of the numeric variables with `table1`&quot;, output = &quot;html&quot;) Table 7.1: Summary of the numeric variables with table1 Female Male P-Value n = 5143 n = 5177 Reading Score 0.218 4.4 (2.3) 4.3 (2.3) Vocabulary Score &lt;.001 4.6 (2.4) 4.4 (2.3) Age (in months) &lt;.001 107.1 (5.0) 107.9 (5.1) School SES 0.155 72.5 (22.3) 73.2 (21.7) School’s Enrollment 0.483 532.3 (154.5) 534.5 (155.1) 7.2.2 Visualization of Raw Data 7.2.2.1 Level One Plots: Disaggregate or ignore higher levels For a first look, its useful to plot all the data points on a single scatterplot as displayed in the previous plot. Due to the large sample size, many points end up being plotted on top of or very near each other (overplotted). When this is the case, it can be useful to use geom_binhex() rather than geom_point() so the color saturation of the hexigons convey the number of points at that location, as seen in Figure \\(\\ref{fig:hexbin}\\). Note: I had to manually install the package hexbin for the geom_hex() to run. data_achieve %&gt;% ggplot() + aes(x = gevocab, y = geread) + stat_binhex(colour = &quot;grey85&quot;, na.rm = TRUE) + # outlines scale_fill_gradientn(colors = c(&quot;grey80&quot;,&quot;navyblue&quot;), # fill color extremes name = &quot;Frequency&quot;, # legend title na.value = NA) + # color for count = 0 theme_bw() Figure 7.1: Raw Data: Density, Vocab vs. Reading 7.2.2.2 Multilevel plots: illustrate two nested levels Up to this point, all investigation of this dataset has been only at the pupil level and any nesting or clustering within schools has been ignored. Plotting is a good was to start to get an idea of the school-to-school variability. This figure displays four handpicked school to illustrate the degreen of school-to-school variability in the association between vocab and reading scores. data_achieve %&gt;% dplyr::filter(school %in% c(1321, 6181, 6197, 6823)) %&gt;% # choose school numbers ggplot(aes(x = gevocab, y = geread))+ geom_count() + # creates points, size by overplotted number geom_smooth(method = &quot;lm&quot;) + # linear model (OLS) facet_wrap(~ school) + # panels by school theme_bw() Figure 7.2: Raw Data: Independent Single-Level Regression within each school, a few illustrative cases Another way to explore the school-to-school variability is to plot the linear model fit independently to each of the schools. This next figure displays only the smooth lines without the standard error bands or the raw data in the form of points or hexagons. data_achieve %&gt;% ggplot(aes(x = gevocab, y = geread)) + geom_smooth(aes(group = school), method = &quot;lm&quot;, se = FALSE, # do NOT want the SE bands size = 0.3) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;red&quot;, # do NOT want the SE bands size = 2) + # make the lines thinner theme_bw() Figure 7.3: Raw Data: Independent Single-Level Regression within each school, all schools shown together Due to the high number of schools, the figure with all the school’s independent linear regression lines resembles a hairball and is hard to deduce much about individual schools. By using the facet_grid() layer, we can separate the schools out so better see school-to-school variability. It also allows investigation of higher level predictors, such as the school’s SES (median split with ntile(var, 2)) and class size. data_achieve %&gt;% dplyr::mutate(ses2 = ntile(ses, 2) %&gt;% # median split factor(labels = c(&quot;SES: Lower Half&quot;, &quot;SES: Upper Half&quot;))) %&gt;% dplyr::mutate(senroll = ntile(senroll, 3) %&gt;% factor(labels = c(&quot;Enroll: Smallest Third&quot;, &quot;Enroll: Middle Third&quot;, &quot;Enroll: Largest Third&quot;))) %&gt;% ggplot(aes(x = gevocab, y = geread, group = school)) + # separates students into schools geom_smooth(method = &quot;lm&quot;, se = FALSE, size = 0.3, color = &quot;black&quot;, alpha = .2) + theme_bw() + facet_grid(senroll ~ ses2) # makes separate panels (rows ~ columns) Figure 7.4: Raw Data: Independent Single-Level Regression within each school, sepearated by school size and school SES 7.3 Single-Level Regression 7.3.1 Fit Nested Models Ignoring the fact that students are nested or clustered within schools, is called dissagregating. This treats all students as independent units. # linear model - ignores school (for reference only) fit_read_lm_0 &lt;- lm(formula = geread ~ 1, # intercept only data = data_achieve) fit_read_lm_1 &lt;- lm(formula = geread ~ gevocab , # one predictor data = data_achieve) fit_read_lm_2 &lt;- lm(formula = geread ~ gevocab + age, # two predictors data = data_achieve) fit_read_lm_3 &lt;- lm(formula = geread ~ gevocab*age, # interation+main effects data = data_achieve) Now compare the models: texreg::knitreg(list(fit_read_lm_0, fit_read_lm_1, fit_read_lm_2, fit_read_lm_3), custom.model.names = c(&quot;Null&quot;, &quot;1 IV&quot;, &quot;2 IV&quot;, &quot;Interaction&quot;), caption = &quot;OLS: Investigate Fixed, Pupil-level Predictors&quot;, caption.above = TRUE, single.row = TRUE) OLS: Investigate Fixed, Pupil-level Predictors   Null 1 IV 2 IV Interaction (Intercept) 4.34 (0.02)*** 1.96 (0.04)*** 3.19 (0.42)*** 5.28 (0.87)*** gevocab   0.53 (0.01)*** 0.53 (0.01)*** 0.01 (0.19) age     -0.01 (0.00)** -0.03 (0.01)*** gevocab:age       0.00 (0.00)** R2 0.00 0.29 0.29 0.29 Adj. R2 0.00 0.29 0.29 0.29 Num. obs. 10320 10320 10320 10320 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 Assess the significance of terms in the last ‘best’ model summary(fit_read_lm_3) Call: lm(formula = geread ~ gevocab * age, data = data_achieve) Residuals: Min 1Q Median 3Q Max -6.2069 -1.1250 -0.4362 0.6041 8.6476 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.282607 0.869769 6.074 1.3e-09 *** gevocab 0.009154 0.189113 0.048 0.961394 age -0.030814 0.008066 -3.820 0.000134 *** gevocab:age 0.004830 0.001759 2.746 0.006039 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.965 on 10316 degrees of freedom Multiple R-squared: 0.2902, Adjusted R-squared: 0.29 F-statistic: 1406 on 3 and 10316 DF, p-value: &lt; 2.2e-16 performance::r2(fit_read_lm_3) # R2 for Linear Regression R2: 0.290 adj. R2: 0.290 anova(fit_read_lm_3) Analysis of Variance Table Response: geread Df Sum Sq Mean Sq F value Pr(&gt;F) gevocab 1 16224 16223.9 4202.2783 &lt; 2.2e-16 *** age 1 34 33.7 8.7356 0.003128 ** gevocab:age 1 29 29.1 7.5419 0.006039 ** Residuals 10316 39827 3.9 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 7.3.2 Visualize the Interaction effects::Effect(focal.predictors = c(&quot;gevocab&quot;, &quot;age&quot;), # chooses default values for mod = fit_read_lm_3) # continuous vars gevocab*age effect age gevocab 82 95 110 120 140 0 2.755830 2.355243 1.893028 1.584884 0.9685969 3 3.971571 3.759370 3.514523 3.351291 3.0248284 6 5.187313 5.163497 5.136018 5.117699 5.0810599 8 5.997807 6.099582 6.217015 6.295304 6.4518809 10 6.808301 7.035667 7.298012 7.472909 7.8227019 effects::Effect(focal.predictors = c(&quot;gevocab&quot;, &quot;age&quot;), # chooses default values for mod = fit_read_lm_3) %&gt;% # continuous vars data.frame() %&gt;% mutate(age = factor(age)) %&gt;% # must make a factor to separate lines ggplot(aes(x = gevocab, y = fit, color = age)) + geom_point() + geom_line() Here is a better version of the plot. Age is in months, so we want multiples of 12 for good visualization summary(data_achieve$age)/12 # divide by 12 to change months to years Min. 1st Qu. Median Mean 3rd Qu. Max. 6.833 8.667 8.917 8.961 9.250 11.250 A good set set of illustrative ages could be: 7, 9, and 11: c(7, 9, 11) * 12 # times by 12 to change years to months [1] 84 108 132 effects::Effect(focal.predictors = c(&quot;gevocab&quot;, &quot;age&quot;), mod = fit_read_lm_3, xlevels = list(age = c(84, 108, 132))) %&gt;% # age is in months data.frame() %&gt;% mutate(age_yr = factor(age/12)) %&gt;% # it would be nice to plot age in years ggplot(aes(x = gevocab, y = fit, color = age_yr, linetype = age_yr)) + geom_line(size = 1.25) + theme_bw() + labs(title = &quot;Best Linear Model - Disaggregated Data (OLS)&quot;, x = &quot;Vocabulary Score&quot;, y = &quot;Reading Score&quot;, linetype = &quot;Age (yrs)&quot;, color = &quot;Age (yrs)&quot;) + theme(legend.position = c(0.85, 0.2), legend.key.width = unit(2, &quot;cm&quot;), legend.background = element_rect(color = &quot;black&quot;)) + scale_linetype_manual(values = c(&quot;solid&quot;, &quot;longdash&quot;, &quot;dotted&quot;)) + scale_x_continuous(breaks = seq(from = 0, to = 11, by = 2)) + scale_y_continuous(breaks = seq(from = 0, to = 11, by = 1)) 7.4 MLM - Step 1: Null Model, only fixed and random intercepts A so called Empty Model only includes random intercepts. No independent variables are involved, other the grouping or clustering variable that designates how level 1 units are nested within level 2 units. For a cross-sectional study design this would be the grouping variables, where as for longitudinal or repeated measures designs this would be the subject identifier. This nested structure variable should be set to have class factor. 7.4.1 Fit the Model fit_read_0ml &lt;- lme4::lmer(geread ~ 1 + (1|school), data = data_achieve, REML = FALSE) # fit via ML (not the default) fit_read_0re &lt;- lme4::lmer(geread ~ 1 + (1|school) , data = data_achieve, REML = TRUE) # fit = REML (the default) Compare the two models to OLS: texreg::knitreg(list(fit_read_lm_0, fit_read_0ml, fit_read_0re), custom.model.names = c(&quot;OLS&quot;, &quot;MLM-ML&quot;, &quot;MLM-REML&quot;), caption = &quot;MLM: NULL Model,two estimation methods&quot;, caption.above = TRUE, single.row = TRUE) MLM: NULL Model,two estimation methods   OLS MLM-ML MLM-REML (Intercept) 4.34 (0.02)*** 4.31 (0.05)*** 4.31 (0.05)*** R2 0.00     Adj. R2 0.00     Num. obs. 10320 10320 10320 AIC   46270.34 46274.31 BIC   46292.06 46296.03 Log Likelihood   -23132.17 -23134.15 Num. groups: school   160 160 Var: school (Intercept)   0.39 0.39 Var: Residual   5.05 5.05 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 Notice that the estimate for the intercept is nearly the same in the linear regression and intercept only models, but the standard errors are quite different. When there is clustering in sample, the result of ignoring it is under estimation of the standard errors and over stating the significance of associations. This table was made with the screenreg() function in the self named package. I tend to prefer this display over stargazer(). 7.4.2 Estimate the ICC First, ask for the variance compenents: lme4::VarCorr(fit_read_0re) %&gt;% print(comp = c(&quot;Variance&quot;, &quot;Std.Dev&quot;), digits = 4) Groups Name Variance Std.Dev. school (Intercept) 0.3915 0.6257 Residual 5.0450 2.2461 insight::get_variance(fit_read_0re) $var.fixed [1] 0 $var.random [1] 0.3915154 $var.residual [1] 5.045008 $var.distribution [1] 5.045008 $var.dispersion [1] 0 $var.intercept school 0.3915154 \\[ \\begin{align*} \\text{schools} \\rightarrow \\; &amp; \\sigma^2_{u0} = 0.6257^2 = 0.392 \\\\ \\text{students within schools} \\rightarrow \\; &amp; \\sigma^2_{e} = 2.2461^2 = 5.045 \\\\ \\end{align*} \\] Intraclass Correlation (ICC) Formula \\[ \\overbrace{\\rho}^{\\text{ICC}} = \\frac{\\overbrace{\\sigma^2_{u0}}^{\\text{Random Intercept}\\atop\\text{Variance}}} {\\underbrace{\\sigma^2_{u0}+\\sigma^2_{e}}_{\\text{Total}\\atop\\text{Variance}}} \\tag{Hox 2.9} \\] Then you can manually calculate the ICC. 0.392 / (0.392 + 5.045) [1] 0.07209858 Or you can use the icc() function in the performance package. performance::icc(fit_read_0re) # Intraclass Correlation Coefficient Adjusted ICC: 0.072 Unadjusted ICC: 0.072 Note: On page 45 (Finch, Bolin, and Kelley 2016), the authors substituted standard deviations into the formula, rather than variances. The mistake is listed on their webpage errata (http://www.mlminr.com/errata) and is repeated through the text. 7.5 MLM - Step 2: Add Lower-level explanatory variables, fixed, ML Variance Component models (steps 2 and 3) - decompose the INTERCEPT variance into different variance compondents for each level. The regression intercepts are assumed to varry ACROSS the groups, while the slopes are assumed fixed (no random effects). Fixed effects selection should come prior to random effects. You should use Maximum Likelihood (ML) estimation when fitting these models. IF: only level 1 predictors and random intercepts are incorporated Then: MLM \\(\\approx\\) ANCOVA . 7.5.1 Add pupil’s vocab score as a fixed effects predictor fit_read_1ml &lt;- lme4::lmer(geread ~ gevocab + (1|school), data = data_achieve, REML = FALSE) # to compare fixed var sig fit_read_1re &lt;- lme4::lmer(geread ~ gevocab + (1|school), data = data_achieve, REML = TRUE) # for R-sq calcs texreg::knitreg(list(fit_read_0ml, fit_read_1ml), custom.model.names = c(&quot;Null&quot;, &quot;w Pred&quot;), caption = &quot;MLM: Investigate a Fixed Pupil-level Predictor&quot;, caption.above = TRUE, digits = 4) MLM: Investigate a Fixed Pupil-level Predictor   Null w Pred (Intercept) 4.3068*** 2.0231***   (0.0548) (0.0492) gevocab   0.5130***     (0.0084) AIC 46270.3388 43132.4318 BIC 46292.0643 43161.3991 Log Likelihood -23132.1694 -21562.2159 Num. obs. 10320 10320 Num. groups: school 160 160 Var: school (Intercept) 0.3885 0.0987 Var: Residual 5.0450 3.7661 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 7.5.1.1 Assess Significance of Effects Likelihood Ratio Test (LRT) Since models 0 and 1 are nested models, only differing by the the inclusion or exclusion of the fixed effects predictor gevocab, AND both models were fit via Maximum Likelihood, we can compare the model fit may be compared via the Likilihood-Ratio Test (LRT). The Likelihood Ratio value (L. Ratio) is found by subtracting the two model’s -2 * logLik or deviance values. Significance is judged by the Chi Squared distribution, using the difference in the number of parameters fit as the degrees of freedom. anova(fit_read_0ml, fit_read_1ml) Data: data_achieve Models: fit_read_0ml: geread ~ 1 + (1 | school) fit_read_1ml: geread ~ gevocab + (1 | school) npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) fit_read_0ml 3 46270 46292 -23132 46264 fit_read_1ml 4 43132 43161 -21562 43124 3139.9 1 &lt; 2.2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 What does the model look like? effects::Effect(focal.predictors = c(&quot;gevocab&quot;), mod = fit_read_1ml) %&gt;% data.frame() %&gt;% ggplot(aes(x = gevocab, y = fit)) + geom_ribbon(aes(ymin = lower, ymax = upper), alpha = .3) + geom_line() + theme_bw() 7.5.1.2 Proportion of Variance Explained Extract the variance-covariance estimates: BL = BAseline: The Null Model (fit via REML) lme4::VarCorr(fit_read_0re) %&gt;% print(comp = c(&quot;Variance&quot;, &quot;Std.Dev&quot;), digits = 4) Groups Name Variance Std.Dev. school (Intercept) 0.3915 0.6257 Residual 5.0450 2.2461 \\[ \\sigma^2_{u0-BL} = 0.392 \\\\ \\sigma^2_{e-BL} = 5.045 \\] MC = Model to Compare: Model with Predictor (fit via REML) lme4::VarCorr(fit_read_1re) %&gt;% print(comp = c(&quot;Variance&quot;, &quot;Std.Dev&quot;), digits = 4) Groups Name Variance Std.Dev. school (Intercept) 0.09978 0.3159 Residual 3.76647 1.9407 \\[ \\sigma^2_{u0-MC} = 0.100 \\\\ \\sigma^2_{e-MC} = 3.766 \\] Level 1 \\(R^2\\) - Snijders and Bosker Found on page 47 (Finch, Bolin, and Kelley 2016), the proportion of variance in the outcome explained by predictor on level one is given by: Snijders and Bosker Formula - Level 1 \\[ R^2_1 = 1 - \\frac{\\sigma^2_{e-MC} + \\sigma^2_{u0-MC}} {\\sigma^2_{e-BL} + \\sigma^2_{u0-BL}} \\] Note: This formula also apprears in the Finch errata. The subscripts in the denominator of the fraction should be for model 0, not model 1. The formula is given correctly here. They did substitute in the correct values. Calculate the value by hand: 1 - (0.100 + 3.766)/(0.392 + 5.045) [1] 0.2889461 Or use the performance package to help out: (Note it is using a difference method…) performance::r2(fit_read_1re) # R2 for Mixed Models Conditional R2: 0.295 Marginal R2: 0.276 This means nearly 30% of the variance in reading scores, above and beyond that accounted for by school membership (i.e. school makeup or school-to-school variation), is attributable to vocabulary scores. Level 1 \\(R^2\\) - Raudenbush and Bryk Hox, Moerbeek, and Van de Schoot (2017) presents this formula on page 58 of chapter 2 Raudenbush and Bryk Approximate Formula - Level 1 \\[ approx\\; R^2_1 = \\frac{\\sigma^2_{e-BL} - \\sigma^2_{e-MC}} {\\sigma^2_{e-BL} } \\tag{Hox 4.8} \\] Calculate the value by hand: (5.045 - 3.766) / 5.045 [1] 0.2535183 Although slightly different in value and meaning, this value also conveys that vocabulary scores are highly associated with reading scores. Level 2 \\(R^2\\) - Snijders and Bosker Formula Extended Snijders and Bosker Formula Extended - Level 2 \\[ R^2_2 = 1 - \\frac{\\frac{\\sigma^2_{e-MC}}{B} + \\sigma^2_{u0-MC}} {\\frac{\\sigma^2_{e-BL}}{B} + \\sigma^2_{u0-BL}} \\] \\(B\\) is the average size of the Level 2 units (schools). Technically, you should use the harmonic mean, but unless the clusters differ greatly in size, it doesn’t make a huge difference. Average sample cluster size num_subjects / num_schools [1] 64.5 Calculate by hand: 1 - ((3.766 / 64.5) + 0.100) / ((5.045 / 64.5) + 0.391) [1] 0.6624428 This means that over two-thirds in school mean reading levels may be explained by their student’s vocabulary scores. Level 2 \\(R^2\\) - Raudenbush and Bryk Raudenbush and Bryk Approximate Formula - Level 2 \\[ R^2_1 = \\frac{\\sigma^2_{u0-BL} - \\sigma^2_{u0-MC}} {\\sigma^2_{u0-BL} } \\tag{Hox 4.9} \\] (0.392 - 0.100)/(0.392) [1] 0.744898 Remeber that these ‘variance accounted for’ estimations are not as straight forwards as we would like. 7.5.2 Investigate More Level 1 Predictors Part of investigating lower level explanatory variables, is checking for interactions between these variables. The interaction between fixed effects is also considered to be a fixed effect, so we need to employ Maximum Likelihood estimation to compare nested models. fit_read_2ml &lt;- lmer(geread ~ gevocab + age + (1 | school), # add main effect of age data = data_achieve, REML = FALSE) fit_read_3ml &lt;- lmer(geread ~ gevocab*age + (1 | school), # add interaction between vocab and age data = data_achieve, REML = FALSE) texreg::knitreg(list(fit_read_1ml, fit_read_2ml, fit_read_3ml), custom.model.names = c(&quot;Only Vocab&quot;, &quot;Both Main Effects&quot;, &quot;Interaction&quot;), caption = &quot;MLM: Investigate Other Fixed Pupil-level Predictors&quot;, caption.above = TRUE, digits = 4) MLM: Investigate Other Fixed Pupil-level Predictors   Only Vocab Both Main Effects Interaction (Intercept) 2.0231*** 3.0049*** 5.1874***   (0.0492) (0.4172) (0.8666) gevocab 0.5130*** 0.5121*** -0.0279   (0.0084) (0.0084) (0.1881) age   -0.0091* -0.0294***     (0.0038) (0.0080) gevocab:age     0.0050**       (0.0017) AIC 43132.4318 43128.8201 43122.5687 BIC 43161.3991 43165.0293 43166.0198 Log Likelihood -21562.2159 -21559.4100 -21555.2844 Num. obs. 10320 10320 10320 Num. groups: school 160 160 160 Var: school (Intercept) 0.0987 0.0973 0.0977 Var: Residual 3.7661 3.7646 3.7614 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 7.5.2.1 Assess Significance of Effects Likelihood Ratio Test (LRT) anova(fit_read_1ml, fit_read_2ml, fit_read_3ml) Data: data_achieve Models: fit_read_1ml: geread ~ gevocab + (1 | school) fit_read_2ml: geread ~ gevocab + age + (1 | school) fit_read_3ml: geread ~ gevocab * age + (1 | school) npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) fit_read_1ml 4 43132 43161 -21562 43124 fit_read_2ml 5 43129 43165 -21559 43119 5.6117 1 0.017841 * fit_read_3ml 6 43123 43166 -21555 43111 8.2514 1 0.004072 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Not only is student’s age predictive of their reading level (I could have guessed that), but that age moderated the relationship between vocabulary and reading. 7.5.2.2 Visulaize the Interation Visulaizations are extremely helpful to interpred interactions. summary(data_achieve$age) Min. 1st Qu. Median Mean 3rd Qu. Max. 82.0 104.0 107.0 107.5 111.0 135.0 effects::Effect(focal.predictors = c(&quot;gevocab&quot;, &quot;age&quot;), # variables involved in the interaction mod = fit_read_3ml, xlevels = list(age = c(84, 108, 132))) %&gt;% # age is in months data.frame() %&gt;% mutate(age_yr = factor(age/12)) %&gt;% # it would be nice to plot age in years ggplot(aes(x = gevocab, y = fit, color = age_yr)) + geom_line() + theme_bw() There is a positive association between vocabulary and reading, but it is strongest for older childred. Among younger children, reading scores are more stable across vocabulary differences. 7.6 MLM - Step 3: Higher-level explanatory variables, fixed, ML School enrollment (senroll) applies to each school as a whole. When a variable is measured at a higher level, all units in the same group have the same value. In this case, all student in the same school have the same value for senroll. fit_read_4ml &lt;- lme4::lmer(geread ~ gevocab*age + senroll + (1 | school), data = data_achieve, REML = FALSE) texreg::knitreg(list(fit_read_0ml, fit_read_3ml, fit_read_4ml), custom.model.names = c(&quot;Null&quot;, &quot;Level 1 only&quot;, &quot;Level 2 Pred&quot;), caption = &quot;MLM: Investigate a Fixed School-Level Predictor&quot;, caption.above = TRUE, single.row = TRUE) MLM: Investigate a Fixed School-Level Predictor   Null Level 1 only Level 2 Pred (Intercept) 4.31 (0.05)*** 5.19 (0.87)*** 5.24 (0.87)*** gevocab   -0.03 (0.19) -0.03 (0.19) age   -0.03 (0.01)*** -0.03 (0.01)*** gevocab:age   0.01 (0.00)** 0.01 (0.00)** senroll     -0.00 (0.00) AIC 46270.34 43122.57 43124.31 BIC 46292.06 43166.02 43175.01 Log Likelihood -23132.17 -21555.28 -21555.16 Num. obs. 10320 10320 10320 Num. groups: school 160 160 160 Var: school (Intercept) 0.39 0.10 0.10 Var: Residual 5.05 3.76 3.76 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 7.6.1 Assess Significance of Effects Likelihood Ratio Test (LRT) anova(fit_read_0ml, fit_read_3ml, fit_read_4ml) Data: data_achieve Models: fit_read_0ml: geread ~ 1 + (1 | school) fit_read_3ml: geread ~ gevocab * age + (1 | school) fit_read_4ml: geread ~ gevocab * age + senroll + (1 | school) npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) fit_read_0ml 3 46270 46292 -23132 46264 fit_read_3ml 6 43123 43166 -21555 43111 3153.7701 3 &lt;2e-16 *** fit_read_4ml 7 43124 43175 -21555 43110 0.2548 1 0.6137 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 School enrollment (or size) does not seem be related to reading scores. 7.7 MLM - Step 4: Explanatory variables predict Slopes, random, REML Random Coefficient models - decompose the SLOPE variance BETWEEN groups. The fixed effect of the predictor captures the overall association it has with the outcome (intercept), while the random effect of the predictor captures the group-to-group variation in the association (slope). Note: A variable can be fit as BOTH a fixed and random effect. fit_read_3re &lt;- lme4::lmer(geread ~ gevocab*age + (1 | school), # refit the previous &#39;best&#39; model via REML data = data_achieve, REML = TRUE) #fit_read_5re &lt;- lmer(geread ~ gevocab + (gevocab | school), # data = achieve, # REML = TRUE) # failed to converge :( fit_read_5re &lt;- lme4::lmer(geread ~ gevocab*age + (gevocab | school), data = data_achieve, REML = TRUE, control = lmerControl(optimizer = &quot;optimx&quot;, # get it to converge calc.derivs = FALSE, optCtrl = list(method = &quot;nlminb&quot;, starttests = FALSE, kkt = FALSE))) texreg::knitreg(list(fit_read_3re, fit_read_5re), custom.model.names = c(&quot;Rand Int&quot;, &quot;Rand Int and Slopes&quot;), caption = &quot;MLM: Investigate Random Effects&quot;, caption.above = TRUE, single.row = TRUE) MLM: Investigate Random Effects   Rand Int Rand Int and Slopes (Intercept) 5.19 (0.87)*** 5.61 (0.87)*** gevocab -0.03 (0.19) -0.14 (0.19) age -0.03 (0.01)*** -0.03 (0.01)*** gevocab:age 0.01 (0.00)** 0.01 (0.00)*** AIC 43155.49 43011.65 BIC 43198.95 43069.58 Log Likelihood -21571.75 -21497.82 Num. obs. 10320 10320 Num. groups: school 160 160 Var: school (Intercept) 0.10 0.28 Var: Residual 3.76 3.66 Var: school gevocab   0.02 Cov: school (Intercept) gevocab   -0.06 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 7.7.0.1 Assess Significance of Effect Likelihood Ratio Test (LRT) for Random Effects You can use the Chi-squared LRT test based on deviances even though we fit our modesl with REML, since the models only differ in terms of including/exclusing of a random effects; they have same fixed effects. Just make sure to include the refit = FALSE option. anova(fit_read_3re, fit_read_5re, refit = FALSE) Data: data_achieve Models: fit_read_3re: geread ~ gevocab * age + (1 | school) fit_read_5re: geread ~ gevocab * age + (gevocab | school) npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) fit_read_3re 6 43155 43199 -21572 43143 fit_read_5re 8 43012 43070 -21498 42996 147.84 2 &lt; 2.2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 There is evidence the effect child vocabulary has on reading varies across schools. 7.7.0.2 Visualize the Model What does the model look like? effects::Effect(focal.predictors = c(&quot;gevocab&quot;, &quot;age&quot;), mod = fit_read_5re, # just different model xlevels = list(age = c(84, 108, 132))) %&gt;% data.frame() %&gt;% dplyr::mutate(age_yr = factor(age/12)) %&gt;% ggplot(aes(x = gevocab, y = fit, color = age_yr)) + geom_line() + theme_bw() We are seeming much the same trends, but perhaps more separation between the lines. 7.8 MLM - Step 5: Cross-Level interactions between explanatory variables - fixed, ML Cross-level interacitons involve variables at different levels. Here we will investigate the school-level enrollment moderating vocabulary’s effect since we say that vocab’s effect differs across schools (step 4). Remember that an interaction beween fixed effects is also fixed. fit_read_5ml &lt;- lme4::lmer(geread ~ gevocab*age + (gevocab | school), data = data_achieve, REML = FALSE, control = lmerControl(optimizer = &quot;optimx&quot;, calc.derivs = FALSE, optCtrl = list(method = &quot;nlminb&quot;, starttests = FALSE, kkt = FALSE))) fit_read_6ml &lt;- lme4::lmer(geread ~ gevocab*age + senroll + (gevocab | school), data = data_achieve, REML = FALSE, control = lmerControl(optimizer =&quot;Nelder_Mead&quot;)) fit_read_7ml &lt;- lme4::lmer(geread ~ gevocab*age + gevocab*senroll + (gevocab | school), data = data_achieve, REML = FALSE) fit_read_8ml &lt;- lme4::lmer(geread ~ gevocab*age*senroll + (gevocab | school), data = data_achieve, REML = FALSE) If you get thelmer() message: Some predictor variables are on very different scales: consider rescaling, you can trust your results, but you really should try re-scaling your variables. We are getting this message since gevoab is on mostly a single digit scale,0 to 11.2, and age (in months) ranges in the low thripe-digits, 82 through 135, while school enrollment is in the mid-hundreds, 112-916. When we compute the interactions we get much, much larger values. Having variables on such widely different ranges of values can cause estimation problems. data_achieve %&gt;% dplyr::select(gevocab, age, senroll) %&gt;% summary() gevocab age senroll Min. : 0.000 Min. : 82.0 Min. :115.0 1st Qu.: 2.900 1st Qu.:104.0 1st Qu.:438.0 Median : 3.800 Median :107.0 Median :519.0 Mean : 4.494 Mean :107.5 Mean :533.4 3rd Qu.: 5.200 3rd Qu.:111.0 3rd Qu.:644.0 Max. :11.200 Max. :135.0 Max. :916.0 For now, let us look at the results. texreg::knitreg(list(fit_read_5ml, fit_read_6ml, fit_read_7ml, fit_read_8ml), custom.model.names = c(&quot;Level 1 only&quot;, &quot;Both Levels&quot;, &quot;Cross-Level&quot;, &quot;3-way&quot;), caption = &quot;MLM: Investigate a Fixed Cross-level Interaction&quot;, caption.above = TRUE, digits = 3, bold = TRUE) MLM: Investigate a Fixed Cross-level Interaction   Level 1 only Both Levels Cross-Level 3-way (Intercept) 5.614*** 5.603*** 5.515*** 5.503   (0.872) (0.878) (0.894) (3.111) gevocab -0.138 -0.138 -0.114 0.530   (0.192) (0.192) (0.198) (0.661) age -0.033*** -0.033*** -0.033*** -0.033   (0.008) (0.008) (0.008) (0.029) gevocab:age 0.006*** 0.006*** 0.006*** 0.000   (0.002) (0.002) (0.002) (0.006) senroll   0.000 0.000 0.000     (0.000) (0.000) (0.006) gevocab:senroll     -0.000 -0.001       (0.000) (0.001) age:senroll       -0.000         (0.000) gevocab:age:senroll       0.000         (0.000) AIC 42979.711 42981.698 42983.452 42982.851 BIC 43037.646 43046.874 43055.870 43069.753 Log Likelihood -21481.855 -21481.849 -21481.726 -21479.426 Num. obs. 10320 10320 10320 10320 Num. groups: school 160 160 160 160 Var: school (Intercept) 0.272 0.271 0.274 0.269 Var: school gevocab 0.019 0.019 0.019 0.019 Cov: school (Intercept) gevocab -0.062 -0.062 -0.063 -0.061 Var: Residual 3.660 3.660 3.660 3.659 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 There is no evidence school enrollment moderates either of age or vocabulary’s effects. 7.8.0.1 Assess Significance of Effects Likelihood Ratio Test (LRT) When you have a list of sequentially nested models, you can test them in order with one call to the anova() funtion. anova(fit_read_5ml, fit_read_6ml) Data: data_achieve Models: fit_read_5ml: geread ~ gevocab * age + (gevocab | school) fit_read_6ml: geread ~ gevocab * age + senroll + (gevocab | school) npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) fit_read_5ml 8 42980 43038 -21482 42964 fit_read_6ml 9 42982 43047 -21482 42964 0.0132 1 0.9087 anova(fit_read_5ml, fit_read_7ml) Data: data_achieve Models: fit_read_5ml: geread ~ gevocab * age + (gevocab | school) fit_read_7ml: geread ~ gevocab * age + gevocab * senroll + (gevocab | school) npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) fit_read_5ml 8 42980 43038 -21482 42964 fit_read_7ml 10 42983 43056 -21482 42963 0.2593 2 0.8784 anova(fit_read_5ml, fit_read_8ml) Data: data_achieve Models: fit_read_5ml: geread ~ gevocab * age + (gevocab | school) fit_read_8ml: geread ~ gevocab * age * senroll + (gevocab | school) npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) fit_read_5ml 8 42980 43038 -21482 42964 fit_read_8ml 12 42983 43070 -21479 42959 4.8597 4 0.302 7.9 Centering Predictors: Change Center Centering variables measured on the lowest level only involves subtacting the mean from every value. The spread or standard deviation is not changed. Although there are functions to automatically center and standardize variables, it is beneficial to manually create these variables, as it is more transparent and facilitates un-centering them later. data_achieve %&gt;% dplyr::select(gevocab, age, senroll) %&gt;% summary() gevocab age senroll Min. : 0.000 Min. : 82.0 Min. :115.0 1st Qu.: 2.900 1st Qu.:104.0 1st Qu.:438.0 Median : 3.800 Median :107.0 Median :519.0 Mean : 4.494 Mean :107.5 Mean :533.4 3rd Qu.: 5.200 3rd Qu.:111.0 3rd Qu.:644.0 Max. :11.200 Max. :135.0 Max. :916.0 7.9.1 Use Centered Variables NOTE: The models with CENTERED variables are able to be fit with the default optimizer settings and do not return the error: \"unable to evaluate scaled gradientModel failed to converge: degenerate Hessian with 1 negative eigenvalues\" fit_read_5ml_c &lt;- lme4::lmer(geread ~ I(gevocab - 4.494) * I(age - 107.5) + (I(gevocab - 4.494) | school), data = data_achieve, REML = FALSE) 7.9.1.1 Compare the Models texreg::knitreg(list(fit_read_5ml, fit_read_5ml_c), custom.model.names = c(&quot;Raw Units&quot;, &quot;Grand Mean Centered&quot;), groups = list(&quot;Raw Scale&quot; = 2:4, &quot;Mean Centered&quot; = 5:7), caption = &quot;MLM: Investigate Centering Variables Involved in an Interaction&quot;, caption.above = TRUE, single.row = TRUE, digits = 3) MLM: Investigate Centering Variables Involved in an Interaction   Raw Units Grand Mean Centered (Intercept) 5.614 (0.872)*** 4.348 (0.033)*** Raw Scale          gevocab -0.138 (0.192)        age -0.033 (0.008)***        gevocab:age 0.006 (0.002)***   Mean Centered          gevocab - 4.494   0.519 (0.014)***      age - 107.5   -0.006 (0.004)      I(gevocab - 4.494):age - 107.5   0.006 (0.002)*** AIC 42979.711 42979.711 BIC 43037.646 43037.646 Log Likelihood -21481.855 -21481.855 Num. obs. 10320 10320 Num. groups: school 160 160 Var: school (Intercept) 0.272 0.100 Var: school gevocab 0.019   Cov: school (Intercept) gevocab -0.062   Var: Residual 3.660 3.660 Var: school I(gevocab - 4.494)   0.019 Cov: school (Intercept) I(gevocab - 4.494)   0.024 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 Notice that the interactions yield the exact same parameter estimates and significances, but the main effects (including the interactions) are different. Model fit statistics include \\(-2LL\\) are exactly the same, too. performance::compare_performance(fit_read_5ml, fit_read_5ml_c) # Comparison of Model Performance Indices Name | Model | AIC (weights) | AICc (weights) | BIC (weights) | R2 (cond.) | R2 (marg.) | ICC | RMSE | Sigma -------------------------------------------------------------------------------------------------------------------------------- fit_read_5ml | lmerMod | 42979.7 (0.500) | 42979.7 (0.500) | 43037.6 (0.500) | 0.321 | 0.282 | 0.054 | 1.897 | 1.913 fit_read_5ml_c | lmerMod | 42979.7 (0.500) | 42979.7 (0.500) | 43037.6 (0.500) | 0.321 | 0.282 | 0.054 | 1.897 | 1.913 7.9.1.2 Visualize the Model What does the model look like? First plot the model fit to the centered variables with all defaut settings. interactions::interact_plot(model = fit_read_5ml_c, pred = gevocab, modx = age) interactions::interact_plot(model = fit_read_5ml_c, pred = gevocab, modx = age, modx.values = c(80, 110, 150), interval = TRUE) 7.10 Rescaling Predictors: Change Units or Standardize Where centering variables involved subtracting a set value, scalling a varaibles involves dividing by a set amount. When we both center to the mean and divide by the standard deviation, the new resulting varaible is said to be standardized (not to be confusing with normalizing, which is does not do). To retain meaningful units, you can multiply or divide all the measured values of a variable by a set amount, like a multiple of 10. This retains the meaning behind the units while still bringing them into line with other variables in the model and can avoid some convergence issues. 7.10.1 Scale Varaibles 7.10.1.1 Divide by a Meaningful Value data_achieve %&gt;% dplyr::select(gevocab, age, ses) %&gt;% summary() gevocab age ses Min. : 0.000 Min. : 82.0 Min. : 0.00 1st Qu.: 2.900 1st Qu.:104.0 1st Qu.: 66.30 Median : 3.800 Median :107.0 Median : 81.70 Mean : 4.494 Mean :107.5 Mean : 72.85 3rd Qu.: 5.200 3rd Qu.:111.0 3rd Qu.: 87.80 Max. :11.200 Max. :135.0 Max. :100.00 For this situation, lets only divide SES by ten. 7.10.2 Use Scaled Variables Using the new versions of our variables, investigate is SES has an effect, either in 2-way or 3-way interactions with age and vocabulary. fit_read_5ml_s &lt;- lme4::lmer(geread ~ I(gevocab - 4.494) * I(age - 107.5) + (I(gevocab - 4.494) | school), data = data_achieve, REML = FALSE) fit_read_6ml_s &lt;- lme4::lmer(geread ~ I(gevocab - 4.494) * I(age - 107.5) + I(gevocab - 4.494) * I(ses/10) + (I(gevocab - 4.494) | school), data = data_achieve, REML = FALSE, control = lmerControl(optimizer =&quot;Nelder_Mead&quot;)) fit_read_7ml_s &lt;- lme4::lmer(geread ~ I(gevocab - 4.494) * I(age - 107.5) * I(ses/10) + (I(gevocab - 4.494) | school), data = data_achieve, REML = FALSE, control = lmerControl(optimizer =&quot;Nelder_Mead&quot;)) texreg::knitreg(list(fit_read_5ml_s, fit_read_6ml_s, fit_read_7ml_s), custom.model.names = c(&quot;no SES&quot;, &quot;2-way&quot;, &quot;3-way&quot;), caption = &quot;MLM: Investigate More Complex Fixed Interactions (with Scalling)&quot;, caption.above = TRUE, digits = 3) MLM: Investigate More Complex Fixed Interactions (with Scalling)   no SES 2-way 3-way (Intercept) 4.348*** 3.945*** 3.942***   (0.033) (0.107) (0.107) gevocab - 4.494 0.519*** 0.671*** 0.674***   (0.014) (0.051) (0.051) age - 107.5 -0.006 -0.004 -0.008   (0.004) (0.004) (0.014) I(gevocab - 4.494):age - 107.5 0.006*** 0.006** 0.000   (0.002) (0.002) (0.007) ses/10   0.057*** 0.058***     (0.014) (0.014) I(gevocab - 4.494):ses/10   -0.021** -0.022**     (0.007) (0.007) I(age - 107.5):ses/10     0.000       (0.002) I(gevocab - 4.494):I(age - 107.5):ses/10     0.001       (0.001) AIC 42979.711 42928.813 42932.046 BIC 43037.646 43001.232 43018.948 Log Likelihood -21481.855 -21454.407 -21454.023 Num. obs. 10320 10320 10320 Num. groups: school 160 160 160 Var: school (Intercept) 0.100 0.079 0.079 Var: school I(gevocab - 4.494) 0.019 0.017 0.017 Cov: school (Intercept) I(gevocab - 4.494) 0.024 0.031 0.031 Var: Residual 3.660 3.662 3.662 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 anova(fit_read_5ml_s, fit_read_6ml_s, fit_read_7ml_s) Data: data_achieve Models: fit_read_5ml_s: geread ~ I(gevocab - 4.494) * I(age - 107.5) + (I(gevocab - 4.494) | school) fit_read_6ml_s: geread ~ I(gevocab - 4.494) * I(age - 107.5) + I(gevocab - 4.494) * I(ses/10) + (I(gevocab - 4.494) | school) fit_read_7ml_s: geread ~ I(gevocab - 4.494) * I(age - 107.5) * I(ses/10) + (I(gevocab - 4.494) | school) npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) fit_read_5ml_s 8 42980 43038 -21482 42964 fit_read_6ml_s 10 42929 43001 -21454 42909 54.8974 2 1.2e-12 *** fit_read_7ml_s 12 42932 43019 -21454 42908 0.7674 2 0.6813 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 There is evidence that SES moderates the main effect of vocabulary, after accounting for the interaction between age and vocabulary. But there is NOT evidence of a three-way interaction between vobaculary, age, and SES. 7.11 Final Model Always refit the final model via REML. fit_read_6re_s &lt;- lme4::lmer(geread ~ I(gevocab - 4.494) * I(age - 107.5) + I(gevocab - 4.494) * I(ses/10) + (I(gevocab - 4.494) | school), data = data_achieve, REML = TRUE) 7.11.1 Table texreg::knitreg(fit_read_6re_s, caption = &quot;MLM: Final Model&quot;, caption.above = TRUE, single.row = TRUE, digits = 3) MLM: Final Model   Model 1 (Intercept) 3.945 (0.107)*** gevocab - 4.494 0.671 (0.051)*** age - 107.5 -0.004 (0.004) ses/10 0.057 (0.014)*** I(gevocab - 4.494):age - 107.5 0.006 (0.002)** I(gevocab - 4.494):ses/10 -0.021 (0.007)** AIC 42976.511 BIC 43048.929 Log Likelihood -21478.255 Num. obs. 10320 Num. groups: school 160 Var: school (Intercept) 0.081 Var: school I(gevocab - 4.494) 0.018 Cov: school (Intercept) I(gevocab - 4.494) 0.032 Var: Residual 3.662 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 7.11.1.1 Visualize the Model Recall the scales that the revised variables are now on: data_achieve %&gt;% dplyr::select(gevocab, age, ses) %&gt;% summary() gevocab age ses Min. : 0.000 Min. : 82.0 Min. : 0.00 1st Qu.: 2.900 1st Qu.:104.0 1st Qu.: 66.30 Median : 3.800 Median :107.0 Median : 81.70 Mean : 4.494 Mean :107.5 Mean : 72.85 3rd Qu.: 5.200 3rd Qu.:111.0 3rd Qu.: 87.80 Max. :11.200 Max. :135.0 Max. :100.00 interactions::interact_plot(model = fit_read_6ml_s, # model name pred = gevocab, # x-axis, main independent variable (continuous, ordinal) modx = age, # lines by moderator, another independent variable mod2 = ses) # panel by 2nd moderator, another indep var Define values for the moderator(s): interactions::interact_plot(model = fit_read_6ml_s, pred = gevocab, modx = age, mod2 = ses, modx.values = c(90, 110, 130), mod2.values = c(20, 55, 90), interval = TRUE) Swap the moderators: interactions::interact_plot(model = fit_read_6ml_s, pred = gevocab, mod2 = age, modx = ses, mod2.values = c(90, 110, 130), modx.values = c(20, 55, 90), interval = TRUE) 7.11.2 Interpretation of model There is strong evidence that higher vocabulary scores correlate with higher reading scores. This relationship is strongest in low SES schools and among older students. This relationship is also weaker in younger students and those attending high SES schools. See: Nakagawa and Schielzeth (2013) for more regarding model \\(R^2\\) From the performance::r2() documentation, for mixed models: marginal r-squared considers only the variance of the fixed effects conditional r-squared takes both the fixed and random effects into account performance::r2(fit_read_6re_s) # R2 for Mixed Models Conditional R2: 0.322 Marginal R2: 0.289 Over 32% of the variance in student reading is attributable vocab, ses, age, and school-to-school differences, R^2 = .322. Helpful links: http://maths-people.anu.edu.au/~johnm/r-book/xtras/mlm-ohp.pdf http://ase.tufts.edu/gsc/gradresources/guidetomixedmodelsinr/mixed%20model%20guide.html http://web.stanford.edu/class/psych252/section_2015/Section_week9.html https://www.r-bloggers.com/visualizing-generalized-linear-mixed-effects-models-with-ggplot-rstats-lme4/ https://www.r-bloggers.com/visualizing-generalized-linear-mixed-effects-models-part-2-rstats-lme4/ http://www.strengejacke.de/sjPlot/sjp.lmer/ "],["mlm-centeringscaling-student-popularity.html", "8 MLM, Centering/Scaling: Student Popularity 8.1 Background 8.2 Grand-Mean-Centering and Standardizing Variables 8.3 RI = ONLY Random Intercepts 8.4 RIAS = Random Intercepts AND Slopes", " 8 MLM, Centering/Scaling: Student Popularity library(tidyverse) # all things tidy library(broom) # converst stats objestcs to tidy tibbles library(haven) # read in SPSS dataset library(furniture) # nice table1() descriptives library(stargazer) # display nice tables: summary &amp; regression library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(psych) # contains some useful functions, like headTail library(sjstats) # ICC calculations library(lme4) # non-linear mixed-effects models library(haven) # read in SPSS dataset 8.1 Background The text “Multilevel Analysis: Techniques and Applications, Third Edition” (Hox, Moerbeek, and Van de Schoot 2017) has a companion website which includes links to all the data files used throughout the book (housed on the book’s GitHub repository). The following example is used through out Hox, Moerbeek, and Van de Schoot (2017)’s chapater 2. From Appendix E: The popularity data in popular2.sav are simulated data for 2000 pupils in 100 schools. The purpose is to offer a very simple example for multilevel regression analysis. The main outcome variable is the pupil popularity, a popularity rating on a scale of 1-10 derived by a sociometric procedure. Typically, a sociometric procedure asks all pupils in a class to rate all the other pupils, and then assigns the average received popularity rating to each pupil. Because of the sociometric procedure, group effects as apparent from higher level variance components are rather strong. There is a second outcome variable: pupil popularity as rated by their teacher, on a scale from 1-10. The explanatory variables are pupil gender (boy=0, girl=1), pupil extraversion (10-point scale) and teacher experience in years. The popularity data have been generated to be a ‘nice’ well-behaved data set: the sample sizes at both levels are sufficient, the residuals have a normal distribution, and the multilevel effects are strong. data_raw &lt;- haven::read_sav(&quot;https://github.com/MultiLevelAnalysis/Datasets-third-edition-Multilevel-book/raw/master/chapter%202/popularity/SPSS/popular2.sav&quot;) %&gt;% haven::as_factor() # retain the labels from SPSS --&gt; factor data_pop &lt;- data_raw %&gt;% dplyr::mutate(id = paste(class, pupil, sep = &quot;_&quot;) %&gt;% # create a unique id for each student (char) factor()) %&gt;% # declare id is a factor dplyr::select(id, pupil:popteach) # reduce the variables included tibble::glimpse(data_pop) Rows: 2,000 Columns: 8 $ id &lt;fct&gt; 1_1, 1_2, 1_3, 1_4, 1_5, 1_6, 1_7, 1_8, 1_9, 1_10, 1_11, 1_12… $ pupil &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18… $ class &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2… $ extrav &lt;dbl&gt; 5, 7, 4, 3, 5, 4, 5, 4, 5, 5, 5, 5, 5, 5, 5, 6, 4, 4, 7, 4, 8… $ sex &lt;fct&gt; girl, boy, girl, girl, girl, boy, boy, boy, boy, boy, girl, g… $ texp &lt;dbl&gt; 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 2… $ popular &lt;dbl&gt; 6.3, 4.9, 5.3, 4.7, 6.0, 4.7, 5.9, 4.2, 5.2, 3.9, 5.7, 4.8, 5… $ popteach &lt;dbl&gt; 6, 5, 6, 5, 6, 5, 5, 5, 5, 3, 5, 5, 5, 6, 5, 5, 2, 3, 7, 4, 6… data_pop %&gt;% ggplot() + aes(x = extrav, y = popular, group = class) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;black&quot;, size = .2) + theme_bw() + geom_vline(xintercept = 0, color = &quot;red&quot;) + labs(title = &quot;OLS: Single Level Regression&quot;, subtitle = &quot;Thin black lines are OLS regression ran independently on each class&quot;, x = &quot;Student&#39;s Extroversion, as rated by their teacher&quot;, y = &quot;Student&#39;s Populartity, mean rating by their peers&quot;) + coord_cartesian(xlim = c(0, 10), ylim = c(0, 10)) + scale_x_continuous(breaks = seq(from = 0, to = 10, by = 2)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) 8.2 Grand-Mean-Centering and Standardizing Variables It is best to manually determine the variable’s mean (mean()) and standard deviation (sd()). mean(data_pop$extrav) [1] 5.215 sd(data_pop$extrav) [1] 1.262368 8.2.1 Grand-Mean-Centering \\[ VAR_G = VAR - mean(VAR) \\] 8.2.2 Standardizing \\[ VAR_Z = \\frac{VAR - mean(VAR)}{sd(VAR)} \\] data_pop &lt;- data_pop %&gt;% dplyr::mutate(extravG = extrav - 5.215) %&gt;% dplyr::mutate(extravZ = (extrav - 5.215) / 1.262368) Descriptive statistics: Three versions of Extraversion Statistic N Mean St. Dev. Min Max extrav 2,000 5.215 1.262 1 10 extravG 2,000 0.000 1.262 -4.215 4.785 extravZ 2,000 0.000 1.000 -3.339 3.790 8.3 RI = ONLY Random Intercepts 8.3.1 Fit MLM with all 3 versions of the predictor pop_lmer_1_raw &lt;- lme4::lmer(popular ~ extrav + (1|class), data = data_pop, REML = FALSE) pop_lmer_1_cen &lt;- lme4::lmer(popular ~ extravG + (1|class), data = data_pop, REML = FALSE) pop_lmer_1_std &lt;- lme4::lmer(popular ~ extravZ + (1|class), data = data_pop, REML = FALSE) texreg::knitreg(list(pop_lmer_1_raw, pop_lmer_1_cen, pop_lmer_1_std), custom.model.names = c(&quot;Raw&quot;, &quot;Centered&quot;, &quot;Standardized&quot;), caption = &quot;MLM - RI: Effect of Grand-Mean Centering and Standardizing&quot;, caption.above = TRUE, single.row = TRUE) MLM - RI: Effect of Grand-Mean Centering and Standardizing   Raw Centered Standardized (Intercept) 2.54 (0.14)*** 5.08 (0.09)*** 5.08 (0.09)*** extrav 0.49 (0.02)***     extravG   0.49 (0.02)***   extravZ     0.61 (0.03)*** AIC 5831.78 5831.78 5831.78 BIC 5854.18 5854.18 5854.18 Log Likelihood -2911.89 -2911.89 -2911.89 Num. obs. 2000 2000 2000 Num. groups: class 100 100 100 Var: class (Intercept) 0.83 0.83 0.83 Var: Residual 0.93 0.93 0.93 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 ** MLM - Random Intercepts ONLY** Grand-Mean Centering a Predictor Different than when using the Raw Predictor: fixed intercept Same as when using the Raw Predictor: fixed estimates or slopes for all predictors (main effects and interactions) random estimates, i.e. variance and covariance components, includin the residual variance model fit statistics, including AIC, BIC, and the Log Loikelihood (-2LL or deviance) Standardize a Predictor Different than when using the Raw Predictor: fixed intercept (same as if using the grand-mean centered predictor) fixed estimate (slope) for that variable Stays the SAME: random estimates, i.e. variance and covariance components, includin the residual variance model fit statistics, including AIC, BIC, and the Log Loikelihood (-2LL or deviance) 8.3.1.1 Fixed Effects: intercept and slope There is only ONE fixed intercept and ONE fixed slope. The fixef() function extracts the estimates of the fixed effects. fixef(pop_lmer_1_raw) (Intercept) extrav 2.5427027 0.4862002 fixef(pop_lmer_1_raw)[[&quot;(Intercept)&quot;]] [1] 2.542703 fixef(pop_lmer_1_raw)[[&quot;extrav&quot;]] [1] 0.4862002 data_pop %&gt;% ggplot() + aes(x = extrav, y = popular, group = class) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;black&quot;, size = .2) + geom_abline(intercept = fixef(pop_lmer_1_raw)[[&quot;(Intercept)&quot;]], slope = fixef(pop_lmer_1_raw)[[&quot;extrav&quot;]], color = &quot;hot pink&quot;, size = 2) + theme_bw() + geom_vline(xintercept = 0, color = &quot;red&quot;) + labs(title = &quot;MLM-RI: Extroversion = raw score&quot;, subtitle = &quot;Thin black lines are OLS regression ran independently on each class&quot;, x = &quot;Student&#39;s Extroversion, as rated by their teacher&quot;, y = &quot;Student&#39;s Populartity, mean rating by their peers&quot;) + coord_cartesian(xlim = c(0, 10), ylim = c(0, 10)) + scale_x_continuous(breaks = seq(from = 0, to = 10, by = 2)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) 8.3.1.2 Random Effects: intercepts There is a different random intercept for EACH CLASS. These tell how far each class’s average is off of the grand average. The ranef() function extracts the random effects from a fitted model object ranef(pop_lmer_1_raw) %&gt;% str() List of 1 $ class:&#39;data.frame&#39;: 100 obs. of 1 variable: ..$ (Intercept): num [1:100] 0.165 -0.7536 -0.3646 0.5405 -0.0994 ... ..- attr(*, &quot;postVar&quot;)= num [1, 1, 1:100] 0.044 0.044 0.0486 0.0386 0.042 ... - attr(*, &quot;class&quot;)= chr &quot;ranef.mer&quot; ranef(pop_lmer_1_raw)$class %&gt;% head() # onle line per group (100 classes) (Intercept) 1 0.16499938 2 -0.75362983 3 -0.36464658 4 0.54049206 5 -0.09943663 6 -0.60487822 ranef(pop_lmer_1_raw)$class %&gt;% dplyr::rename(Random_Intercepts = &quot;(Intercept)&quot;) %&gt;% ggplot(aes(Random_Intercepts)) + geom_histogram(binwidth = .25) 8.3.1.3 Predictions predict(pop_lmer_1_raw) %&gt;% str() Named num [1:2000] 5.14 6.11 4.65 4.17 5.14 ... - attr(*, &quot;names&quot;)= chr [1:2000] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... predict(pop_lmer_1_raw) %&gt;% head() # onle value per observation (2000 students) 1 2 3 4 5 6 5.138703 6.111103 4.652503 4.166303 5.138703 4.652503 data_pop %&gt;% dplyr::mutate(pred = predict(pop_lmer_1_raw)) %&gt;% ggplot(aes(x = extrav, y = pred, group = class)) + geom_line(size = .2) + geom_abline(intercept = fixef(pop_lmer_1_raw)[[&quot;(Intercept)&quot;]], slope = fixef(pop_lmer_1_raw)[[&quot;extrav&quot;]], color = &quot;hot pink&quot;, size = 2) + theme_bw() + geom_vline(xintercept = 0, color = &quot;red&quot;) + geom_vline(xintercept = 5.215, color = &quot;blue&quot;) + labs(title = &quot;MLM-RI: Extroversion = raw score&quot;, subtitle = &quot;Thin black lines are group-wise predictions, one per class&quot;, x = &quot;Student&#39;s Extroversion, as rated by their teacher&quot;, y = &quot;Predicted\\nStudent&#39;s Populartity, mean rating by their peers&quot;) + coord_cartesian(xlim = c(0, 10), ylim = c(0, 10)) + scale_x_continuous(breaks = seq(from = 0, to = 10, by = 2)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) 8.3.1.4 Combined Effects The coef() function computes the sum of the random and fixed effects coefficients for each explanatory variable for each level of each grouping factor. coef(pop_lmer_1_raw) %&gt;% str() List of 1 $ class:&#39;data.frame&#39;: 100 obs. of 2 variables: ..$ (Intercept): num [1:100] 2.71 1.79 2.18 3.08 2.44 ... ..$ extrav : num [1:100] 0.486 0.486 0.486 0.486 0.486 ... - attr(*, &quot;class&quot;)= chr &quot;coef.mer&quot; coef(pop_lmer_1_raw)$class %&gt;% head() # onle line per group (100 classes) (Intercept) extrav 1 2.707702 0.4862002 2 1.789073 0.4862002 3 2.178056 0.4862002 4 3.083195 0.4862002 5 2.443266 0.4862002 6 1.937824 0.4862002 data_pop %&gt;% dplyr::mutate(pred = predict(pop_lmer_1_raw)) %&gt;% ggplot() + aes(x = extrav, y = pred, group = class) + geom_rect(aes(xmin = 0 - 0.25, xmax = 0 + 0.25, ymin = fixef(pop_lmer_1_raw)[[&quot;(Intercept)&quot;]]- 2.5, ymax = fixef(pop_lmer_1_raw)[[&quot;(Intercept)&quot;]]+ 2.5), fill = &quot;yellow&quot;, alpha = 0.05) + geom_abline(data = coef(pop_lmer_1_raw)$class %&gt;% dplyr::rename(Intercept = &quot;(Intercept)&quot;), aes(intercept = Intercept, slope = extrav), color = &quot;gray&quot;, size = .1) + geom_line(size = .2) + geom_abline(intercept = fixef(pop_lmer_1_raw)[[&quot;(Intercept)&quot;]], slope = fixef(pop_lmer_1_raw)[[&quot;extrav&quot;]], color = &quot;hot pink&quot;, size = 2) + theme_bw() + geom_vline(xintercept = 0, color = &quot;red&quot;) + geom_vline(xintercept = 5.215, color = &quot;blue&quot;) + labs(title = &quot;MLM-RI: Extroversion = raw score&quot;, subtitle = &quot;Thin black lines are group-wise predictions, one per class EXTRAPOLATED OUT&quot;, x = &quot;Student&#39;s Extroversion, as rated by their teacher&quot;, y = &quot;Predicted\\nStudent&#39;s Populartity, mean rating by their peers&quot;) + coord_cartesian(xlim = c(0, 10), ylim = c(0, 10)) + scale_x_continuous(breaks = seq(from = 0, to = 10, by = 2)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) 8.3.2 Comapre the Centered Version data_pop %&gt;% dplyr::mutate(pred = predict(pop_lmer_1_cen)) %&gt;% ggplot() + aes(x = extravG, y = pred, group = class) + geom_rect(aes(xmin = -5.215 - 0.25, xmax = -5.215 + 0.25, ymin = fixef(pop_lmer_1_raw)[[&quot;(Intercept)&quot;]]- 2.5, ymax = fixef(pop_lmer_1_raw)[[&quot;(Intercept)&quot;]]+ 2.5), fill = &quot;yellow&quot;, alpha = 0.05) + geom_rect(aes(xmin = 0 - 0.25, xmax = 0 + 0.25, ymin = fixef(pop_lmer_1_cen)[[&quot;(Intercept)&quot;]]- 2.5, ymax = fixef(pop_lmer_1_cen)[[&quot;(Intercept)&quot;]]+ 2.5), fill = &quot;yellow&quot;, alpha = 0.05) + geom_abline(data = coef(pop_lmer_1_cen)$class %&gt;% dplyr::rename(Intercept = &quot;(Intercept)&quot;), aes(intercept = Intercept, slope = extravG), color = &quot;gray&quot;, size = .1) + geom_line(size = .2) + geom_abline(intercept = fixef(pop_lmer_1_cen)[[&quot;(Intercept)&quot;]], slope = fixef(pop_lmer_1_cen)[[&quot;extravG&quot;]], color = &quot;hot pink&quot;, size = 2) + theme_bw() + geom_vline(xintercept = -5.215, color = &quot;blue&quot;) + geom_vline(xintercept = 0, color = &quot;red&quot;) + labs(title = &quot;MLM-RI: Extroversion = grand-mean centered&quot;, subtitle = &quot;Thin black lines are group-wise predictions, one per class EXTRAPOLATED OUT&quot;, x = &quot;GRAND-MEAN CENTERED Student&#39;s Extroversion, as rated by their teacher&quot;, y = &quot;Predicted\\nStudent&#39;s Populartity, mean rating by their peers&quot;) + coord_cartesian(xlim = c(-5, 5), ylim = c(0, 10)) + scale_x_continuous(breaks = seq(from = -4, to = 4, by = 2)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) 8.3.3 Comapre the Standardized Version data_pop %&gt;% dplyr::mutate(pred = predict(pop_lmer_1_std)) %&gt;% ggplot() + aes(x = extravZ, y = pred, group = class) + geom_rect(aes(xmin = 0 - 0.25, xmax = 0 + 0.25, ymin = fixef(pop_lmer_1_cen)[[&quot;(Intercept)&quot;]]- 2.5, ymax = fixef(pop_lmer_1_cen)[[&quot;(Intercept)&quot;]]+ 2.5), fill = &quot;yellow&quot;, alpha = 0.05) + geom_abline(data = coef(pop_lmer_1_std)$class %&gt;% dplyr::rename(Intercept = &quot;(Intercept)&quot;), aes(intercept = Intercept, slope = extravZ), color = &quot;gray&quot;, size = .1) + geom_line(size = .2) + geom_abline(intercept = fixef(pop_lmer_1_std)[[&quot;(Intercept)&quot;]], slope = fixef(pop_lmer_1_std)[[&quot;extravZ&quot;]], color = &quot;hot pink&quot;, size = 2) + theme_bw() + geom_vline(xintercept = 0, color = &quot;red&quot;) + geom_vline(xintercept = -5.215, color = &quot;blue&quot;) + labs(title = &quot;MLM-RI: Extroversion = standardized&quot;, subtitle = &quot;Thin black lines are group-wise predictions, one per class EXTRAPOLATED OUT&quot;, x = &quot;STANDARDIZED Student&#39;s Extroversion, as rated by their teacher&quot;, y = &quot;Predicted\\nStudent&#39;s Populartity, mean rating by their peers&quot;) + coord_cartesian(xlim = c(-5, 5), ylim = c(0, 10)) + scale_x_continuous(breaks = seq(from = -4, to = 4, by = 2)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) 8.4 RIAS = Random Intercepts AND Slopes 8.4.1 Fit MLM with all 3 versions of the predictor pop_lmer_2_raw &lt;- lme4::lmer(popular ~ extrav + (extrav|class), data = data_pop, REML = FALSE) pop_lmer_2_cen &lt;- lme4::lmer(popular ~ extravG + (extravG|class), data = data_pop, REML = FALSE) pop_lmer_2_std &lt;- lme4::lmer(popular ~ extravZ + (extravZ|class), data = data_pop, REML = FALSE) texreg::knitreg(list(pop_lmer_2_raw, pop_lmer_2_cen, pop_lmer_2_std), custom.model.names = c(&quot;Raw&quot;, &quot;Centered&quot;, &quot;Standardized&quot;), caption = &quot;MLM - RIAS: Effect of Grand-Mean Centering and Standardizing&quot;, caption.above = TRUE, single.row = TRUE) MLM - RIAS: Effect of Grand-Mean Centering and Standardizing   Raw Centered Standardized (Intercept) 2.46 (0.20)*** 5.03 (0.10)*** 5.03 (0.10)*** extrav 0.49 (0.03)***     extravG   0.49 (0.03)***   extravZ     0.62 (0.03)*** AIC 5782.69 5782.69 5782.69 BIC 5816.29 5816.29 5816.29 Log Likelihood -2885.34 -2885.34 -2885.34 Num. obs. 2000 2000 2000 Num. groups: class 100 100 100 Var: class (Intercept) 2.95 0.88 0.88 Var: class extrav 0.03     Cov: class (Intercept) extrav -0.26     Var: Residual 0.90 0.90 0.90 Var: class extravG   0.03   Cov: class (Intercept) extravG   -0.13   Var: class extravZ     0.04 Cov: class (Intercept) extravZ     -0.17 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 ** MLM - Random Intercepts AND Slopes** Grand-Mean Centering a Predictor Different than when using the Raw Predictor: fixed intercept random estimates, i.e. variance and covariance components, includin the residual variance Same as when using the Raw Predictor: fixed estimates or slopes for all predictors (main effects and interactions) model fit statistics, including AIC, BIC, and the Log Loikelihood (-2LL or deviance) Standardize a Predictor Different than when using the Raw Predictor: fixed intercept (same as if using the grand-mean centered predictor) fixed estimate (slope) for that variable random estimates, i.e. variance and covariance components, includin the residual variance Stays the SAME: model fit statistics, including AIC, BIC, and the Log Loikelihood (-2LL or deviance) 8.4.1.1 Fixed Effects: intercept and slope There is only ONE fixed intercept and ONE fixed slope. The fixef() function extracts the estimates of the fixed effects. fixef(pop_lmer_2_raw) (Intercept) extrav 2.4613520 0.4929015 fixef(pop_lmer_2_raw)[[&quot;(Intercept)&quot;]] [1] 2.461352 fixef(pop_lmer_2_raw)[[&quot;extrav&quot;]] [1] 0.4929015 data_pop %&gt;% ggplot() + aes(x = extrav, y = popular, group = class) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;black&quot;, size = .2) + geom_abline(intercept = fixef(pop_lmer_2_raw)[[&quot;(Intercept)&quot;]], slope = fixef(pop_lmer_2_raw)[[&quot;extrav&quot;]], color = &quot;hot pink&quot;, size = 2) + theme_bw() + geom_vline(xintercept = 0, color = &quot;red&quot;) + geom_vline(xintercept = 5.215, color = &quot;blue&quot;) + labs(title = &quot;MLM-RIAS: Extroversion = raw score&quot;, subtitle = &quot;Thin black lines are OLS regression ran independently on each class&quot;, x = &quot;Student&#39;s Extroversion, as rated by their teacher&quot;, y = &quot;Student&#39;s Populartity, mean rating by their peers&quot;) + coord_cartesian(xlim = c(0, 10), ylim = c(0, 10)) + scale_x_continuous(breaks = seq(from = 0, to = 10, by = 2)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) 8.4.1.2 Random Effects: intercepts There is a different random intercept AND random slope for EACH CLASS. These tell how far each class’s average is off of the grand average AND how far each class’s slope is off of the grand average sope. The ranef() function extracts the random effects from a fitted model object ranef(pop_lmer_2_raw) %&gt;% str() List of 1 $ class:&#39;data.frame&#39;: 100 obs. of 2 variables: ..$ (Intercept): num [1:100] 0.341 -1.18 -0.627 1.079 -0.191 ... ..$ extrav : num [1:100] -0.0272 0.0974 0.0565 -0.0988 0.0208 ... ..- attr(*, &quot;postVar&quot;)= num [1:2, 1:2, 1:100] 0.21332 -0.02975 -0.02975 0.00499 0.20574 ... - attr(*, &quot;class&quot;)= chr &quot;ranef.mer&quot; ranef(pop_lmer_2_raw)$class %&gt;% head() # onle line per group (100 classes) (Intercept) extrav 1 0.3412826 -0.02721226 2 -1.1800459 0.09737587 3 -0.6269008 0.05654230 4 1.0791254 -0.09877051 5 -0.1910515 0.02083157 6 -0.9833398 0.08370183 ranef(pop_lmer_2_raw)$class %&gt;% dplyr::rename(Random_Intercepts = &quot;(Intercept)&quot;) %&gt;% ggplot(aes(Random_Intercepts)) + geom_histogram() ranef(pop_lmer_2_raw)$class %&gt;% ggplot(aes(extrav)) + geom_histogram() 8.4.1.3 Predictions predict(pop_lmer_2_raw) %&gt;% str() Named num [1:2000] 5.13 6.06 4.67 4.2 5.13 ... - attr(*, &quot;names&quot;)= chr [1:2000] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... predict(pop_lmer_2_raw) %&gt;% head() # onle value per observation (2000 students) 1 2 3 4 5 6 5.131081 6.062459 4.665391 4.199702 5.131081 4.665391 data_pop %&gt;% dplyr::mutate(pred = predict(pop_lmer_2_raw)) %&gt;% ggplot(aes(x = extrav, y = pred, group = class)) + geom_line(size = .2) + geom_abline(intercept = fixef(pop_lmer_2_raw)[[&quot;(Intercept)&quot;]], slope = fixef(pop_lmer_2_raw)[[&quot;extrav&quot;]], color = &quot;hot pink&quot;, size = 2) + theme_bw() + geom_vline(xintercept = 0, color = &quot;red&quot;) + geom_vline(xintercept = 5.215, color = &quot;blue&quot;) + labs(title = &quot;MLM-RIAS: Extroversion = raw score&quot;, subtitle = &quot;Thin black lines are group-wise predictions, one per class&quot;, x = &quot;Student&#39;s Extroversion, as rated by their teacher&quot;, y = &quot;Predicted\\nStudent&#39;s Populartity, mean rating by their peers&quot;) + coord_cartesian(xlim = c(0, 10), ylim = c(0, 10)) + scale_x_continuous(breaks = seq(from = 0, to = 10, by = 2)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) 8.4.1.4 Combined Effects The coef() function computes the sum of the random and fixed effects coefficients for each explanatory variable for each level of each grouping factor. coef(pop_lmer_1_raw) %&gt;% str() List of 1 $ class:&#39;data.frame&#39;: 100 obs. of 2 variables: ..$ (Intercept): num [1:100] 2.71 1.79 2.18 3.08 2.44 ... ..$ extrav : num [1:100] 0.486 0.486 0.486 0.486 0.486 ... - attr(*, &quot;class&quot;)= chr &quot;coef.mer&quot; coef(pop_lmer_1_raw)$class %&gt;% head() # onle line per group (100 classes) (Intercept) extrav 1 2.707702 0.4862002 2 1.789073 0.4862002 3 2.178056 0.4862002 4 3.083195 0.4862002 5 2.443266 0.4862002 6 1.937824 0.4862002 data_pop %&gt;% dplyr::mutate(pred = predict(pop_lmer_2_raw)) %&gt;% ggplot() + aes(x = extrav, y = pred, group = class) + geom_rect(aes(xmin = 0 - 0.25, xmax = 0 + 0.25, ymin = fixef(pop_lmer_2_raw)[[&quot;(Intercept)&quot;]]- 4.5, ymax = fixef(pop_lmer_2_raw)[[&quot;(Intercept)&quot;]]+ 4.5), fill = &quot;yellow&quot;, alpha = 0.05) + geom_abline(data = coef(pop_lmer_2_raw)$class %&gt;% dplyr::rename(Intercept = &quot;(Intercept)&quot;), aes(intercept = Intercept, slope = extrav), color = &quot;gray&quot;, size = .1) + geom_line(size = .2) + geom_abline(intercept = fixef(pop_lmer_2_raw)[[&quot;(Intercept)&quot;]], slope = fixef(pop_lmer_2_raw)[[&quot;extrav&quot;]], color = &quot;hot pink&quot;, size = 2) + theme_bw() + geom_vline(xintercept = 0, color = &quot;red&quot;) + geom_vline(xintercept = 5.215, color = &quot;blue&quot;) + labs(title = &quot;MLM-RIAS: Extroversion = raw score&quot;, subtitle = &quot;Thin black lines are group-wise predictions, one per class EXTRAPOLATED OUT&quot;, x = &quot;Student&#39;s Extroversion, as rated by their teacher&quot;, y = &quot;Predicted\\nStudent&#39;s Populartity, mean rating by their peers&quot;) + coord_cartesian(xlim = c(0, 10), ylim = c(0, 10)) + scale_x_continuous(breaks = seq(from = 0, to = 10, by = 2)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) 8.4.2 Comapre the Centered Version data_pop %&gt;% dplyr::mutate(pred = predict(pop_lmer_2_cen)) %&gt;% ggplot() + aes(x = extravG, y = pred, group = class) + geom_rect(aes(xmin = -5.215 - 0.25, xmax = -5.215 + 0.25, ymin = fixef(pop_lmer_2_raw)[[&quot;(Intercept)&quot;]]- 4.5, ymax = fixef(pop_lmer_2_raw)[[&quot;(Intercept)&quot;]]+ 4.5), fill = &quot;yellow&quot;, alpha = 0.05) + geom_rect(aes(xmin = 0 - 0.25, xmax = 0 + 0.25, ymin = fixef(pop_lmer_2_cen)[[&quot;(Intercept)&quot;]]- 2.5, ymax = fixef(pop_lmer_2_cen)[[&quot;(Intercept)&quot;]]+ 2.5), fill = &quot;yellow&quot;, alpha = 0.05) + geom_abline(data = coef(pop_lmer_2_cen)$class %&gt;% dplyr::rename(Intercept = &quot;(Intercept)&quot;), aes(intercept = Intercept, slope = extravG), color = &quot;gray&quot;, size = .1) + geom_line(size = .2) + geom_abline(intercept = fixef(pop_lmer_2_cen)[[&quot;(Intercept)&quot;]], slope = fixef(pop_lmer_2_cen)[[&quot;extravG&quot;]], color = &quot;hot pink&quot;, size = 2) + theme_bw() + geom_vline(xintercept = -5.215, color = &quot;blue&quot;) + geom_vline(xintercept = 0, color = &quot;red&quot;) + labs(title = &quot;MLM-RIAS: Extroversion = grand-mean centered&quot;, subtitle = &quot;Thin black lines are group-wise predictions, one per class EXTRAPOLATED OUT&quot;, x = &quot;GRAND-MEAN CENTERED Student&#39;s Extroversion, as rated by their teacher&quot;, y = &quot;Predicted\\nStudent&#39;s Populartity, mean rating by their peers&quot;) + coord_cartesian(xlim = c(-5, 5), ylim = c(0, 10)) + scale_x_continuous(breaks = seq(from = -4, to = 4, by = 2)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) 8.4.3 Comapre the Standardized Version data_pop %&gt;% dplyr::mutate(pred = predict(pop_lmer_2_std)) %&gt;% ggplot() + aes(x = extravZ, y = pred, group = class) + geom_rect(aes(xmin = 0 - 0.25, xmax = 0 + 0.25, ymin = fixef(pop_lmer_2_cen)[[&quot;(Intercept)&quot;]]- 2.5, ymax = fixef(pop_lmer_2_cen)[[&quot;(Intercept)&quot;]]+ 2.5), fill = &quot;yellow&quot;, alpha = 0.05) + geom_abline(data = coef(pop_lmer_2_std)$class %&gt;% dplyr::rename(Intercept = &quot;(Intercept)&quot;), aes(intercept = Intercept, slope = extravZ), color = &quot;gray&quot;, size = .1) + geom_line(size = .2) + geom_abline(intercept = fixef(pop_lmer_2_std)[[&quot;(Intercept)&quot;]], slope = fixef(pop_lmer_2_std)[[&quot;extravZ&quot;]], color = &quot;hot pink&quot;, size = 2) + theme_bw() + geom_vline(xintercept = 0, color = &quot;red&quot;) + geom_vline(xintercept = -5.215, color = &quot;blue&quot;) + labs(title = &quot;MLM-RIAS: Extroversion = standardized&quot;, subtitle = &quot;Thin black lines are group-wise predictions, one per class EXTRAPOLATED OUT&quot;, x = &quot;STANDARDIZED Student&#39;s Extroversion, as rated by their teacher&quot;, y = &quot;Predicted\\nStudent&#39;s Populartity, mean rating by their peers&quot;) + coord_cartesian(xlim = c(-5, 5), ylim = c(0, 10)) + scale_x_continuous(breaks = seq(from = -4, to = 4, by = 2)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) "],["effect-size-variance-explained.html", "9 Effect Size: Variance Explained 9.1 MLM or LMM 9.2 GLMM", " 9 Effect Size: Variance Explained Note that these versions of \\(R^2\\) are becoming more common, but are not entirely agreed upon or standard. You will not be able to calculate them directly in standard software. Instead, you need to calculate the components and program the calculation. Importantly, if you choose to report one or both of them, you should not only identify which one you are using, but provide some brief interpretation and a citation of the article. The Analysis Factor, R-Squared for Mixed Effects Models, by Kim Love Marginal vs. Conditional \\(R^2\\): A general and simple method for obtaining R2 from generalized linear mixed‐effects models 2012: Journal Article Authors: Shinichi Nakagawa and Holger Schielzeth Somewhat technical, but good figures: Quantifying Explained Variance in Multilevel Models: An Integrative Framework for Defining R-Squared Measures 2019: manuscript, Psychological Methods Authors: Jason D. Rights and Sonya K. Sterba, Vanderbilt University 9.1 MLM or LMM Multilevel Model or Linear Multilevel Models Social and Behavioral Science Focus: R-squared measures in Multilevel Modeling: The undesirable property of negative R-squared values 2018: First Year Paper, Research Master in Social and Behavioral Science Student: Edoardo Costantini Supervisor: prof. dr. M.A.L.M. van Assen Education Focus: Effect size measures for multilevel models: definition, interpretation, and TIMSS example 2018: Manuscript, Large-sclae ASsessments in Educaiton Author: Julie Lorah Psychology Focus: Effect Size Measures for Multilevel Models in Clinical Child and Adolescent Research: New R-Squared Methods and Recommendations 2018: manuscript, Journal of Clinical Child &amp; Adolescent Psychology Authors: Jason D. Rights and David A. Cole Online Appendix: r2MLMcomp Software - R functions r2MLM computes and outputs R-squared measures and analytic decompositions of variance for multilevel models r2MLMcomp computes R-squared differences between pairs of multilevel models under comparison r2MLM3 computes all measures relevant for a three-level model and provides a barchart graphic 9.2 GLMM Generalized Linear Multilevel Models Ecology Focus: for MLM/LMM and GLMM R squared for mixed models 2013: Blogpost by Phil Martin R squared for mixed models – the easy way 2013: Blogplot by Phil Martin Nakagawa and Schielzeth (2013) https://stats.stackexchange.com/questions/111150/calculating-r2-in-mixed-models-using-nakagawa-schielzeths-2013-r2glmm-me I am answering by pasting Douglas Bates’s reply in the R-Sig-ME mailing list, on 17 Dec 2014 on the question of how to calculate an \\(R^2\\) statistic for generalized linear mixed models, which I believe is required reading for anyone interested in such a thing. Bates is the original author of the lme4 package for \\(R\\) and co-author of nlme, as well as co-author of a well-known book on mixed models, and CV will benefit from having the text in an answer, rather than just a link to it. I must admit to getting a little twitchy when people speak of the “\\(R^2\\) for GLMMs”. \\(R^2\\) for a linear model is well-defined and has many desirable properties. For other models one can define different quantities that reflect some but not all of these properties. But this is not calculating an \\(R^2\\) in the sense of obtaining a number having all the properties that the \\(R^2\\) for linear models does. Usually there are several different ways that such a quantity could be defined. Especially for GLMs and GLMMs before you can define “proportion of response variance explained” you first need to define what you mean by “response variance”. The whole point of GLMs and GLMMs is that a simple sum of squares of deviations does not meaningfully reflect the variability in the response because the variance of an individual response depends on its mean. Confusion about what constitutes \\(R^2\\) or degrees of freedom of any of the other quantities associated with linear models as applied to other models comes from confusing the formula with the concept. Although formulas are derived from models the derivation often involves quite sophisticated mathematics. To avoid a potentially confusing derivation and just “cut to the chase” it is easier to present the formulas. But the formula is not the concept. Generalizing a formula is not equivalent to generalizing the concept. And those formulas are almost never used in practice, especially for generalized linear models, analysis of variance and random effects. I have a “meta-theorem” that the only quantity actually calculated according to the formulas given in introductory texts is the sample mean. It may seem that I am being a grumpy old man about this, and perhaps I am, but the danger is that people expect an “\\(R^2\\)-like” quantity to have all the properties of an \\(R^2\\) for linear models. It can’t. There is no way to generalize all the properties to a much more complicated model like a GLMM. I was once on the committee reviewing a thesis proposal for Ph.D. candidacy. The proposal was to examine I think 9 different formulas that could be considered ways of computing an \\(R^2\\) for a nonlinear regression model to decide which one was “best”. Of course, this would be done through a simulation study with only a couple of different models and only a few different sets of parameter values for each. My suggestion that this was an entirely meaningless exercise was not greeted warmly. "],["sjplot-package.html", "10 sjPlot Package 10.1 Plotting Coefficients 10.2 Plotting Marginal Effects 10.3 Model Diagnostics", " 10 sjPlot Package Daniel Lüdecke is German researcher that has put together several GREAT packages, including sjPlot which we will detail here. Documentation can be found at: http://www.strengejacke.de/sjPlot/index.html library(tidyverse) # all things tidy library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(lme4) # Linear, generalized linear, &amp; nonlinear mixed modelsts models library(haven) # read in SPSS dataset library(sjPlot) # Quick predicitive and diagnostic plots Read the SPSS data in with the haven package and prepare it (see previous chapter). data_raw &lt;- haven::read_sav(&quot;http://www.mlminr.com/data-sets/Achieve.sav?attredirects=0&quot;) data_achieve_center_scale &lt;- data_raw %&gt;% dplyr::mutate_at(vars(id, region, corp, school, class), factor) %&gt;% dplyr::mutate(gender = gender %&gt;% factor(labels = c(&quot;Female&quot;, &quot;Male&quot;))) %&gt;% dplyr::mutate(classize = classize %&gt;% factor(labels = c(&quot;12-17&quot;, &quot;18-21&quot;, &quot;22-26&quot;, &quot;&gt;26&quot;))) %&gt;% dplyr::select(id, region, corp, school, class, # Identifiers gender, age, geread, gevocab, # Pupil-level vars classize, # Class-Level vars senroll, ses) %&gt;% # School-level vars dplyr::mutate(gevocab_c = gevocab - 4.4938) %&gt;% dplyr::mutate(age_c = age - 107.5290) %&gt;% dplyr::mutate(senroll_c = senroll - 533.4148) %&gt;% dplyr::mutate(senroll_ch = senroll_c / 100) %&gt;% # centered AND divided by one hundred dplyr::mutate(ses_t = ses / 10) # JUST divide by ten Fit the final model (see previous chapter) fit_read_11re_s &lt;- lme4::lmer(geread ~ gevocab_c*age_c + gevocab_c*ses_t + # 2 2-way interactions (gevocab_c | school), data = data_achieve_center_scale, REML = TRUE) # Knit to Website: texreg::htmlreg() # Knit to PDF: texreg::texreg() # View on Screen: texreg::screenreg() texreg::htmlreg(list(fit_read_11re_s), custom.model.names = c(&quot;Final&quot;), caption = &quot;MLM: Final Model&quot;, caption.above = TRUE, single.row = TRUE) MLM: Final Model   Final (Intercept) 3.94 (0.11)*** gevocab_c 0.67 (0.05)*** age_c -0.00 (0.00) ses_t 0.06 (0.01)*** gevocab_c:age_c 0.01 (0.00)** gevocab_c:ses_t -0.02 (0.01)** AIC 42976.51 BIC 43048.93 Log Likelihood -21478.26 Num. obs. 10320 Num. groups: school 160 Var: school (Intercept) 0.08 Var: school gevocab_c 0.02 Cov: school (Intercept) gevocab_c 0.03 Var: Residual 3.66 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 Now we will show some of the things the sjPlot package can do! 10.1 Plotting Coefficients Select terms that should be plotted. All other term are removed from the output. Note that the term names must match the names of the model’s coefficients. For factors, this means that the variable name is suffixed with the related factor level, and each category counts as one term. E.g. rm.terms = \"t_name [2,3]\" would remove the terms t_name2 and t_name3 (assuming that the variable t_name is categorical and has at least the factor levels 2 and 3). Another example for the iris-dataset: terms = \"Species\" would not work, instead you would write terms = \"Species [versicolor, virginica]\" to remove these two levels, or terms = \"Speciesversicolor\" if you just want to remove the level versicolor from the plot. 10.1.1 Fixed Effects sjPlot::plot_model(fit_read_11re_s, type = &quot;est&quot;) sjPlot::plot_model(fit_read_11re_s, type = &quot;est&quot;, show.values = TRUE) # Logical, whether values should be plotted or not. sjPlot::plot_model(fit_read_11re_s, type = &quot;std&quot;) Determines in which way estimates are sorted in the plot with the option: sort.est = If NULL (default), no sorting is done and estimates are sorted in the same order as they appear in the model formula. If TRUE, estimates are sorted in descending order, with highest estimate at the top. If sort.est = \"sort.all\", estimates are re-sorted for each coefficient (only applies if type = \"re\" and grid = FALSE), i.e. the estimates of the random effects for each predictor are sorted and plotted to an own plot. If type = \"re\", specify a predictor’s / coefficient’s name to sort estimates according to this random effect. sjPlot::plot_model(fit_read_11re_s, type = &quot;std&quot;, sort.est = TRUE) sjPlot::plot_model(fit_read_11re_s, type = &quot;std&quot;, sort.est = TRUE, show.values = TRUE) # Logical, whether values should be plotted or not. Plots standardized beta values, however, standardization follows Gelman (2008) suggestion, rescaling the estimates by dividing them by two standard deviations instead of just one. Resulting coefficients are then directly comparable for untransformed binary predictors. sjPlot::plot_model(fit_read_11re_s, type = &quot;std2&quot;) 10.1.2 Random Effects sjPlot::plot_model(fit_read_11re_s, type = &quot;re&quot;) sjPlot::plot_model(fit_read_11re_s, type = &quot;re&quot;, grid = FALSE, sort.est = TRUE) [[1]] [[2]] 10.2 Plotting Marginal Effects Here terms indicates for which terms marginal effects should be displayed. At least one term is required to calculate effects, maximum length is three terms, where the second and third term indicate the groups, i.e. predictions of first term are grouped by the levels of the second (and third) term. terms may also indicate higher order terms (e.g. interaction terms). Indicating levels in square brackets allows for selecting only specific groups. Term name and levels in brackets must be separated by a whitespace character, e.g. terms = c(\"age\", \"education [1,3]\"). It is also possible to specify a range of numeric values for the predictions with a colon, for instance terms = c(\"education [1,3]\", \"age [30:50]\"). Furthermore, it is possible to specify a function name. Values for predictions will then be transformed, e.g. terms = \"income [exp]\". This is useful when model predictors were transformed for fitting the model and should be back-transformed to the original scale for predictions. Finally, using pretty for numeric variables (e.g. terms = \"age [pretty]\") calculates a pretty range of values for the term, roughly of proportional length to the term’s value range. For more details, see the documentation for the ggpredict package. 10.2.1 Predicted Values Based on (i.e. is a wrapper for): ggeffects::ggpredict() sjPlot::plot_model(fit_read_11re_s, type = &quot;pred&quot;, terms = c(&quot;gevocab_c&quot;, &quot;ses_t&quot;, &quot;age_c&quot;)) The pred.type = option only applies for Marginal Effects plots with mixed effects models. Indicates whether predicted values should be conditioned on random effects (pred.type = \"re\") or fixed effects only (pred.type = \"fe\", the default). For details, see documentation of the type-argument in ggpredict() function. sjPlot::plot_model(fit_read_11re_s, type = &quot;pred&quot;, pred.type = &quot;re&quot;, terms = c(&quot;gevocab_c&quot;, &quot;ses_t&quot;, &quot;age_c&quot;)) 10.2.2 Effect Plots Based on (i.e. is a wrapper for): ggeffects::ggeffect() Similar to type = \"pred\", however, discrete predictors are held constant at their proportions (not reference level). See the ggeffect package documentation for details. sjPlot::plot_model(fit_read_11re_s, type = &quot;eff&quot;, terms = c(&quot;gevocab_c&quot;, &quot;ses_t&quot;, &quot;age_c&quot;)) 10.2.3 Interaction Plots A shortcut for marginal effects plots, where interaction terms are automatically detected and used as terms-argument. Furthermore, if the moderator variable (the second - and third - term in an interaction) is continuous, type = \"int\" automatically chooses useful values based on the mdrt.values-argument, which are passed to terms. Then, ggpredict is called. type = \"int\" plots the interaction term that appears: first in the formula along the x-axis, while the second (and possibly third) variable in an interaction is used as grouping factor(s) (moderating variable). Use type = \"pred\" or type = \"eff\" and specify a certain order in the terms-argument to indicate which variable(s) should be used as moderator. sjPlot::plot_model(fit_read_11re_s, type = &quot;int&quot;) [[1]] [[2]] The mdrt.values = option indicates which values of the moderator variable should be used when plotting interaction terms (i.e. type = \"int\"). minmax (default) minimum and maximum values (lower and upper bounds) of the moderator are used to plot the interaction between independent variable and moderator(s). meansd uses the mean value of the moderator as well as one standard deviation below and above mean value to plot the effect of the moderator on the independent variable (following the convention suggested by Cohen and Cohen and popularized by Aiken and West (1991), i.e. using the mean, the value one standard deviation above, and the value one standard deviation below the mean as values of the moderator, see Grace-Martin K: 3 Tips to Make Interpreting Moderation Effects Easier). zeromax is similar to the minmax option, however, \\(0\\) is always used as minimum value for the moderator. This may be useful for predictors that don’t have an empirical zero-value, but absence of moderation should be simulated by using \\(0\\) as minimum. quart calculates and uses the quartiles (lower, median and upper) of the moderator value. all uses all values of the moderator variable. sjPlot::plot_model(fit_read_11re_s, type = &quot;int&quot;, mdrt.values = &quot;meansd&quot;) [[1]] [[2]] 10.3 Model Diagnostics Note: For mixed models, the diagnostic plots like linear relationship or check for Homoscedasticity, do not take the uncertainty of random effects into account, but is only based on the fixed effects part of the model. 10.3.1 Slope of Coefficentes Slope of coefficients for each single predictor, against the response (linear relationship between each model term and response). sjPlot::plot_model(fit_read_11re_s, type = &quot;slope&quot;) 10.3.2 Residuals Slope of coefficients for each single predictor, against the residuals (linear relationship between each model term and residuals). sjPlot::plot_model(fit_read_11re_s, type = &quot;resid&quot;) 10.3.3 Diagnostics Check model assumptions. sjPlot::plot_model(fit_read_11re_s, type = &quot;diag&quot;) [[1]] [[2]] [[2]]$school [[3]] [[4]] "],["mlm-longitudinal-autism.html", "11 MLM, Longitudinal: Autism 11.1 Background 11.2 Exploratory Data Analysis 11.3 Model 1: Full model with ‘loaded’ mean structure 11.4 Model 2A: Drop Random Intercepts 11.5 Model 2B: Drop Random Quadratic Slope 11.6 Model 3: Drop Quadratic Time Fixed Effect 11.7 Final Model", " 11 MLM, Longitudinal: Autism library(tidyverse) # all things tidy library(pander) # nice looking general tabulations library(furniture) # nice table1() descriptive library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(psych) # contains some useful functions, like headTail library(performance) library(sjstats) # ICC calculations library(sjPlot) # Quick predictive and diagnostic plots library(effects) # Estimated Marginal Means library(VIM) # Visualization and Imputation of Missing Values library(naniar) # Summaries and Visualizations for Missing Data library(lme4) # Linear, generalized linear, &amp; nonlinear mixed models library(optimx) library(interactions) library(ggResidpanel) library(HLMdiag) # package with the dataset 11.1 Background The source: http://www-personal.umich.edu/~kwelch/ This data was collected by researchers at the University of Michigan (Anderson et al. 2009) as part of a prospective longitudinal study of 214 children. The children were divided into three diagnostic groups (bestest2) when they were 2 years old: Autism (autism), Pervasive Developmental Disorder (pdd), and non-spectrum children (none in this sample). The study was designed to collect information on each child at approximately 2, 3, 5, 9, and 13 years of age, although not all children were measured for each age. One of the study objectives was to assess the relative influence of the initial diagnostic category, language proficiency at age 2, and other covariates on the developmental trajectories of the socialization (vsae) of these children. Study participants were children who had had consecutive referrals to one of two autism clinics before the age of 3 years. Social development was assessed at each age using the Vineland Adaptive Behavior Interview survey form, a parent-reported measure of socialization. VSAE (Vineland Socialization Age Equivalent), was a combined score that included assessments of interpersonal relationships, play/leisure time activities, and coping skills. Initial language development was assessed using the Sequenced Inventory of Communication Development (SICD) scale; children were placed into one of three groups (sicdegp) based on their initial SICD scores on the expressive language subscale at age 2. childid Child’s identification number for this study sicdegp Sequenced Inventory of Communication Development group (an assessment of expressive language development) - a factor. Levels are low, med, and high age2 Age (in years) centered around age 2 (age at diagnosis) vsae Vineland Socialization Age Equivalent, Parent-reported measure of socialization, including measures of: interpersonal relationships play/leisure time activities coping skills gender Child’s gender - a factor. Levels are male and female race Child’s race - a factor. Levels are white and nonwhite bestest2 Diagnosis at age 2 - a factor. Levels are autism and pdd (pervasive developmental disorder) data(autism, package = &quot;HLMdiag&quot;) # make the dataset &#39;active&#39; from this package tibble::glimpse(autism) # first look at the dataset and its varaibles Rows: 604 Columns: 7 $ childid &lt;int&gt; 1, 1, 1, 1, 1, 10, 10, 10, 10, 100, 100, 100, 100, 101, 101, … $ sicdegp &lt;fct&gt; high, high, high, high, high, low, low, low, low, high, high,… $ age2 &lt;dbl&gt; 0, 1, 3, 7, 11, 0, 1, 7, 11, 0, 1, 3, 7, 0, 1, 7, 11, 0, 1, 3… $ vsae &lt;int&gt; 6, 7, 18, 25, 27, 9, 11, 18, 39, 15, 24, 37, 135, 8, 24, 75, … $ gender &lt;fct&gt; male, male, male, male, male, male, male, male, male, male, m… $ race &lt;fct&gt; white, white, white, white, white, white, white, white, white… $ bestest2 &lt;fct&gt; pdd, pdd, pdd, pdd, pdd, autism, autism, autism, autism, pdd,… 11.1.1 Long Format data_long &lt;- autism %&gt;% # save the dataset as a new name dplyr::mutate(childid = childid %&gt;% factor) %&gt;% # declare grouping var a factor dplyr::mutate(age = 2 + age2) %&gt;% # create the original age variable (unequally spaced) dplyr::mutate(obs = age %&gt;% factor %&gt;% as.numeric) %&gt;% # Observation Number = 1, 2, 3, 4, 5 (equally spaced) dplyr::select(childid, # choose variables and order to keep gender, race, bestest2, sicdegp, obs, age, age2, vsae) %&gt;% dplyr::arrange(childid, age2) # sort observations data_long %&gt;% psych::headTail(top = 11, bottom = 6) childid gender race bestest2 sicdegp obs age age2 vsae 1 1 male white pdd high 1 2 0 6 2 1 male white pdd high 2 3 1 7 3 1 male white pdd high 3 5 3 18 4 1 male white pdd high 4 9 7 25 5 1 male white pdd high 5 13 11 27 6 2 male white autism low 1 2 0 6 7 2 male white autism low 2 3 1 7 8 2 male white autism low 3 5 3 7 9 2 male white autism low 4 9 7 8 10 2 male white autism low 5 13 11 14 11 3 male nonwhite pdd high 1 2 0 17 ... &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ... ... ... ... 599 211 male nonwhite autism high 1 2 0 15 600 212 male white autism med 1 2 0 7 601 212 male white autism med 2 3 1 21 602 212 male white autism med 3 5 3 29 603 212 male white autism med 4 9 7 72 604 212 male white autism med 5 13 11 147 11.1.2 Wide Format data_wide &lt;- data_long %&gt;% # save the dataset as a new name dplyr::select(-age2, -obs) %&gt;% # delete (by deselecting) this variable tidyr::pivot_wider(names_from = age, # repeated indicator values_from = vsae, # variable repeated names_prefix = &quot;vsae_&quot;) %&gt;% # prefix in from of the dplyr::arrange(childid) # sort observations tibble::glimpse(data_wide) Rows: 155 Columns: 10 $ childid &lt;fct&gt; 1, 2, 3, 4, 6, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 21, … $ gender &lt;fct&gt; male, male, male, male, male, male, male, male, male, male, m… $ race &lt;fct&gt; white, white, nonwhite, nonwhite, white, nonwhite, white, whi… $ bestest2 &lt;fct&gt; pdd, autism, pdd, autism, autism, autism, pdd, autism, autism… $ sicdegp &lt;fct&gt; high, low, high, high, low, low, med, low, med, low, med, med… $ vsae_2 &lt;int&gt; 6, 6, 17, 12, 6, 9, 12, 9, 7, 6, 12, 13, 7, 5, 10, 17, 11, 11… $ vsae_3 &lt;int&gt; 7, 7, 18, 14, 12, 12, 21, 11, 10, 10, 19, 8, 6, 10, 6, 27, 21… $ vsae_5 &lt;int&gt; 18, 7, 12, 38, NA, 14, NA, NA, 8, 12, 14, 29, NA, 29, NA, NA,… $ vsae_9 &lt;int&gt; 25, 8, 18, 114, 12, NA, 66, 18, NA, NA, 28, 24, 39, 32, NA, 7… $ vsae_13 &lt;int&gt; 27, 14, 24, NA, 45, NA, 68, 39, NA, NA, 68, 44, 24, 67, NA, 1… Notice the missing values, displayed as NA. data_wide %&gt;% psych::headTail() childid gender race bestest2 sicdegp vsae_2 vsae_3 vsae_5 vsae_9 vsae_13 1 1 male white pdd high 6 7 18 25 27 2 2 male white autism low 6 7 7 8 14 3 3 male nonwhite pdd high 17 18 12 18 24 4 4 male nonwhite autism high 12 14 38 114 &lt;NA&gt; 5 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ... ... ... ... ... 6 209 male white autism med 2 4 &lt;NA&gt; 12 32 7 210 male white autism low 4 25 &lt;NA&gt; 130 &lt;NA&gt; 8 211 male nonwhite autism high 15 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 9 212 male white autism med 7 21 29 72 147 11.2 Exploratory Data Analysis 11.2.1 Demographic Summary 11.2.1.1 Using the WIDE formatted dataset Each person’s data is only stored on a single line data_wide %&gt;% dplyr::group_by(&quot;Initial Language Development&quot; = sicdegp) %&gt;% # how split into columns furniture::table1(&quot;Diagnosis&quot; = bestest2, &quot;Gender&quot; = gender, &quot;Race&quot; = race, digits = 2, na.rm = FALSE, total = TRUE, test = TRUE, # compare the groups output = &quot;markdown&quot;) Total low med high P-Value n = 155 n = 49 n = 65 n = 41 Diagnosis 0.018 autism 100 (64.5%) 38 (77.6%) 42 (64.6%) 20 (48.8%) pdd 55 (35.5%) 11 (22.4%) 23 (35.4%) 21 (51.2%) NA 0 (0%) 0 (0%) 0 (0%) 0 (0%) Gender 0.706 male 134 (86.5%) 44 (89.8%) 55 (84.6%) 35 (85.4%) female 21 (13.5%) 5 (10.2%) 10 (15.4%) 6 (14.6%) NA 0 (0%) 0 (0%) 0 (0%) 0 (0%) Race 0.944 white 101 (65.2%) 31 (63.3%) 43 (66.2%) 27 (65.9%) nonwhite 54 (34.8%) 18 (36.7%) 22 (33.8%) 14 (34.1%) NA 0 (0%) 0 (0%) 0 (0%) 0 (0%) 11.2.1.2 Using the LONG formatted dataset Each person’s data is stored on multiple lines, one for each time point. To ensure the summary table is correct, you must choose a single time point per person. # Note: One person is missing Age 2 data_long %&gt;% dplyr::filter(age == 2) %&gt;% # restrict to one-line per person dplyr::group_by(&quot;Initial Language Development&quot; = sicdegp) %&gt;% # how split into columns furniture::table1(&quot;Diagnosis&quot; = bestest2, &quot;Gender&quot; = gender, &quot;Race&quot; = race, digits = 2, na.rm = FALSE, total = TRUE, test = TRUE, # compare the groups output = &quot;markdown&quot;) Total low med high P-Value n = 154 n = 49 n = 65 n = 40 Diagnosis 0.025 autism 100 (64.9%) 38 (77.6%) 42 (64.6%) 20 (50%) pdd 54 (35.1%) 11 (22.4%) 23 (35.4%) 20 (50%) NA 0 (0%) 0 (0%) 0 (0%) 0 (0%) Gender 0.714 male 134 (87%) 44 (89.8%) 55 (84.6%) 35 (87.5%) female 20 (13%) 5 (10.2%) 10 (15.4%) 5 (12.5%) NA 0 (0%) 0 (0%) 0 (0%) 0 (0%) Race 0.95 white 100 (64.9%) 31 (63.3%) 43 (66.2%) 26 (65%) nonwhite 54 (35.1%) 18 (36.7%) 22 (33.8%) 14 (35%) NA 0 (0%) 0 (0%) 0 (0%) 0 (0%) 11.2.2 Baseline Summary 11.2.2.1 Using the WIDE formatted dataset Each person’s data is only stored on a single line data_wide %&gt;% dplyr::group_by(&quot;Initial Language Development&quot; = sicdegp) %&gt;% # how split into columns furniture::table1(vsae_2, digits = 2, na.rm = FALSE, total = TRUE, test = TRUE, # compare the groups output = &quot;markdown&quot;) Total low med high P-Value n = 155 n = 49 n = 65 n = 41 vsae_2 &lt;.001 9.16 (3.84) 7.06 (2.73) 8.74 (3.51) 12.40 (3.43) 11.2.2.2 Using the LONG formatted dataset Each person’s data is stored on multiple lines, one for each time point. To ensure the summary table is correct, you must choose a single time point per person. data_long %&gt;% dplyr::filter(age == 2) %&gt;% dplyr::group_by(&quot;Initial Language Development&quot; = sicdegp) %&gt;% # how split into columns furniture::table1(vsae, test = TRUE, output = &quot;markdown&quot;) low med high P-Value n = 49 n = 65 n = 40 vsae &lt;.001 7.1 (2.7) 8.7 (3.5) 12.4 (3.4) 11.2.3 Missing Data &amp; Attrition 11.2.3.1 VIM package Plot the amount of missing vlaues and the amount of each patter of missing values. data_wide %&gt;% VIM::aggr(numbers = TRUE, # shows the number to the far right prop = FALSE) # shows counts instead of proportions 11.2.3.2 naniar package data_wide %&gt;% naniar::vis_miss() data_wide %&gt;% naniar::gg_miss_var() data_wide %&gt;% naniar::gg_miss_var(show_pct = TRUE, # x-axis is PERCENT, not count facet = sicdegp) + # create seperate panels theme_bw() # add ggplot layers as normal data_wide %&gt;% naniar::gg_miss_upset() data_wide %&gt;% naniar::gg_miss_upset(sets = c(&quot;vsae_13_NA&quot;, &quot;vsae_9_NA&quot;, &quot;vsae_5_NA&quot;, &quot;vsae_3_NA&quot;, &quot;vsae_2_NA&quot;), keep.order = TRUE) 11.2.4 Means Across Time 11.2.4.1 Using the WIDE formatted dataset Default = COMPLETE CASES ONLY!!! Note - the sample size at each time point is the same, but subjects are only included if they have data at every time point (total n = 41) data_wide %&gt;% dplyr::group_by(&quot;Initial Language Development&quot; = sicdegp) %&gt;% # how split into columns dplyr::select(starts_with(&quot;vsae_&quot;)) %&gt;% furniture::table1(digits = 2, total = TRUE, test = TRUE, # compare the groups output = &quot;markdown&quot;) Total low med high P-Value n = 41 n = 10 n = 17 n = 14 vsae_2 0.003 9.80 (4.39) 6.60 (2.88) 9.47 (4.12) 12.50 (4.09) vsae_3 &lt;.001 14.54 (6.89) 8.00 (2.11) 14.29 (6.08) 19.50 (6.15) vsae_5 &lt;.001 23.00 (14.84) 12.50 (5.54) 20.18 (8.57) 33.93 (18.50) vsae_9 &lt;.001 36.68 (26.29) 12.30 (8.07) 34.41 (22.54) 56.86 (23.54) vsae_13 0.009 57.07 (47.83) 22.40 (24.45) 57.82 (50.33) 80.93 (44.38) Specify All data: note - that the smaple sizes will be different for each time point (total n = 155) data_wide %&gt;% dplyr::group_by(&quot;Initial Language Development&quot; = sicdegp) %&gt;% # how split into columns dplyr::select(starts_with(&quot;vsae_&quot;)) %&gt;% furniture::table1(digits = 2, na.rm = FALSE, total = TRUE, test = TRUE, # compare the groups output = &quot;markdown&quot;) Total low med high P-Value n = 155 n = 49 n = 65 n = 41 vsae_2 &lt;.001 9.16 (3.84) 7.06 (2.73) 8.74 (3.51) 12.40 (3.43) vsae_3 &lt;.001 15.12 (7.81) 12.02 (6.33) 13.68 (5.42) 21.24 (9.38) vsae_5 &lt;.001 21.48 (13.32) 15.03 (7.92) 17.69 (8.00) 33.92 (15.78) vsae_9 &lt;.001 39.55 (32.62) 25.56 (28.42) 32.12 (23.40) 64.14 (34.59) vsae_13 &lt;.001 59.49 (47.96) 37.11 (35.54) 56.17 (47.91) 88.69 (46.34) 11.2.4.2 Using the LONG formatted dataset Each person’s data is stored on multiple lines, one for each time point. FOR ALL DATA! data_sum_all &lt;- data_long %&gt;% dplyr::group_by(sicdegp, age2) %&gt;% # specify the groups dplyr::summarise(vsae_n = n(), # count of valid scores vsae_mean = mean(vsae), # mean score vsae_sd = sd(vsae), # standard deviation of scores vsae_sem = vsae_sd / sqrt(vsae_n)) # stadard error for the mean of scores data_sum_all # A tibble: 15 × 6 # Groups: sicdegp [3] sicdegp age2 vsae_n vsae_mean vsae_sd vsae_sem &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 low 0 49 7.06 2.73 0.389 2 low 1 46 12.0 6.33 0.934 3 low 3 29 15.0 7.92 1.47 4 low 7 36 25.6 28.4 4.74 5 low 11 28 37.1 35.5 6.72 6 med 0 65 8.74 3.51 0.436 7 med 1 62 13.7 5.42 0.688 8 med 3 36 17.7 8.00 1.33 9 med 7 48 32.1 23.4 3.38 10 med 11 40 56.2 47.9 7.58 11 high 0 40 12.4 3.43 0.542 12 high 1 38 21.2 9.38 1.52 13 high 3 26 33.9 15.8 3.09 14 high 7 35 64.1 34.6 5.85 15 high 11 26 88.7 46.3 9.09 FOR COMPLETE CASES ONLY!!! data_sum_cc &lt;- data_long %&gt;% dplyr::group_by(childid) %&gt;% # group-by child dplyr::mutate(child_vsae_n = n()) %&gt;% # count the number of valid VSAE scores dplyr::filter(child_vsae_n == 5) %&gt;% # restrict to only thoes children with 5 valid scores dplyr::group_by(sicdegp, age2) %&gt;% # specify the groups dplyr::summarise(vsae_n = n(), # count of valid scores vsae_mean = mean(vsae), # mean score vsae_sd = sd(vsae), # standard deviation of scores vsae_sem = vsae_sd / sqrt(vsae_n)) # stadard error for the mean of scores data_sum_cc # A tibble: 15 × 6 # Groups: sicdegp [3] sicdegp age2 vsae_n vsae_mean vsae_sd vsae_sem &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 low 0 10 6.6 2.88 0.909 2 low 1 10 8 2.11 0.667 3 low 3 10 12.5 5.54 1.75 4 low 7 10 12.3 8.07 2.55 5 low 11 10 22.4 24.4 7.73 6 med 0 17 9.47 4.12 1.00 7 med 1 17 14.3 6.08 1.47 8 med 3 17 20.2 8.57 2.08 9 med 7 17 34.4 22.5 5.47 10 med 11 17 57.8 50.3 12.2 11 high 0 14 12.5 4.09 1.09 12 high 1 14 19.5 6.15 1.64 13 high 3 14 33.9 18.5 4.94 14 high 7 14 56.9 23.5 6.29 15 high 11 14 80.9 44.4 11.9 11.2.5 Person Profile Plots Use the data in LONG format 11.2.5.1 Unequally Spaced data_long %&gt;% dplyr::mutate(sicdegp = fct_recode(sicdegp, &quot;Low Communication&quot; = &quot;low&quot;, &quot;Medium Communication&quot; = &quot;med&quot;, &quot;High Communication&quot; = &quot;high&quot;)) %&gt;% ggplot(aes(x = age, y = vsae)) + geom_point(size = 0.75) + geom_line(aes(group = childid), alpha = .5, size = 1) + facet_grid(. ~ sicdegp) + theme_bw() + scale_x_continuous(breaks = c(2, 3, 5, 9, 13)) + labs(x = &quot;Age of Child, Years&quot;, y = &quot;Vineland Socialization Age Equivalent&quot;, color = &quot;Sequenced Inventory of Communication Development&quot;) + geom_smooth(aes(color = &quot;Flexible&quot;), method = &quot;loess&quot;, se = FALSE,) + geom_smooth(aes(color = &quot;Linear&quot;), method = &quot;lm&quot;, se = FALSE) + scale_color_manual(name = &quot;Smoother Type: &quot;, values = c(&quot;Flexible&quot; = &quot;blue&quot;, &quot;Linear&quot; = &quot;red&quot;)) + theme(legend.position = &quot;bottom&quot;, legend.key.width = unit(2, &quot;cm&quot;)) 11.2.5.2 Equally Spaced data_long %&gt;% dplyr::mutate(sicdegp = fct_recode(sicdegp, &quot;Low Communication&quot; = &quot;low&quot;, &quot;Medium Communication&quot; = &quot;med&quot;, &quot;High Communication&quot; = &quot;high&quot;)) %&gt;% ggplot(aes(x = obs, y = vsae)) + geom_point(size = 0.75) + geom_line(aes(group = childid), alpha = .5, size = 1) + facet_grid(. ~ sicdegp) + theme_bw() + labs(x = &quot;Observation Number&quot;, y = &quot;Vineland Socialization Age Equivalent&quot;, color = &quot;Sequenced Inventory of Communication Development&quot;) + geom_smooth(aes(color = &quot;Flexible&quot;), method = &quot;loess&quot;, se = FALSE,) + geom_smooth(aes(color = &quot;Linear&quot;), method = &quot;lm&quot;, se = FALSE) + scale_color_manual(name = &quot;Smoother Type: &quot;, values = c(&quot;Flexible&quot; = &quot;blue&quot;, &quot;Linear&quot; = &quot;red&quot;)) + theme(legend.position = &quot;bottom&quot;, legend.key.width = unit(2, &quot;cm&quot;)) 11.2.6 Side-by-side Boxplots data_long %&gt;% ggplot(aes(x = sicdegp, y = vsae, fill = sicdegp)) + geom_boxplot() + theme_bw() + facet_grid(. ~ age, labeller = &quot;label_both&quot;) + theme(legend.position = &quot;none&quot;) + labs(x = &quot;Initial Language Development\\nSequenced Inventory of Communication Development (SICD) at Age 2&quot;, y = &quot;Parent-Reported Measure of Socialization\\nVineland Socialization Age Equivalent&quot;) 11.2.7 Means Plots 11.2.7.1 Default stat_summary It is nice that the stat_summary() layer computes the standard error for the mean for you using the data in LONG format data_long %&gt;% ggplot(aes(x = age, y = vsae, color = sicdegp)) + stat_summary() + # default: points at MEAN and extend vertically 1 standard error for the mean stat_summary(fun = &quot;mean&quot;, # plot the means geom = &quot;line&quot;) + # ...and connect with lines theme_bw() + scale_x_continuous(breaks = c(2, 3, 5, 9, 13)) data_long %&gt;% ggplot(aes(x = obs, y = vsae, color = sicdegp)) + stat_summary() + # default: points at MEAN and extend vertically 1 standard error for the mean stat_summary(fun = &quot;mean&quot;, # plot the means geom = &quot;line&quot;) + # ...and connect with lines theme_bw() 11.2.7.2 Manually Summarized data_sum_all %&gt;% dplyr::mutate(age = age2 + 2) %&gt;% ggplot() + aes(x = age, y = vsae_mean, color = sicdegp) + geom_errorbar(aes(ymin = vsae_mean - vsae_sem, # mean +/- one SE for the mean ymax = vsae_mean + vsae_sem), width = .25) + geom_point(aes(shape = sicdegp), size = 3) + geom_line(aes(group = sicdegp)) + theme_bw() + scale_x_continuous(breaks = c(2, 3, 5, 9, 13)) + labs(x = &quot;Age of Child, Years&quot;, y = &quot;Vineland Socialization Age Equivalent&quot;, color = &quot;Sequenced Inventory of Communication Development:&quot;, shape = &quot;Sequenced Inventory of Communication Development:&quot;, linetype = &quot;Sequenced Inventory of Communication Development:&quot;) + theme(legend.position = &quot;bottom&quot;, legend.key.width = unit(2, &quot;cm&quot;)) 11.3 Model 1: Full model with ‘loaded’ mean structure Take top-down approach: Quadratic regression model, describing vsae as a function of age2 Each child has a unique parabolic trajectory over time, with coefficients that vary randomly around fixed-effects defining a mean growth curve for each SICD group. Since there is no age = 0 in our data, we will use the age2 variables, which is age -2, so that intercept has meaning (mean at baseline age). I() denotes an internal calculated variable Fixed-effects I(age-2) age I((age-2)^2) quadratic age or age-squared, sicdegp SICD group (reference group = low) SICD group x age/age-squared interactions Random-effects intercept age and age-squared 11.3.1 Fit the Model fit_lmer_1_re &lt;- lmerTest::lmer(vsae ~ I(age-2)*sicdegp + I(I(age-2)^2)*sicdegp + (I(age-2) + I((age-2)^2) | childid), data = data_long, REML = TRUE, control = lmerControl(optimizer = &quot;optimx&quot;, # get it to converge calc.derivs = FALSE, optCtrl = list(method = &quot;nlminb&quot;, starttests = FALSE, kkt = FALSE))) 11.3.2 Table of Prameter Estimates texreg::knitreg(fit_lmer_1_re, caption = &quot;MLM: Full Model&quot;, caption.above = TRUE, single.row = TRUE) MLM: Full Model   Model 1 (Intercept) 8.40 (0.75)*** age - 2 2.28 (0.74)** sicdegpmed 1.26 (0.99) sicdegphigh 5.39 (1.11)*** I(age - 2)^2 0.07 (0.08) age - 2:sicdegpmed 0.43 (0.98) age - 2:sicdegphigh 3.31 (1.08)** sicdegpmed:I(age - 2)^2 -0.00 (0.11) sicdegphigh:I(age - 2)^2 0.14 (0.12) AIC 4586.50 BIC 4656.96 Log Likelihood -2277.25 Num. obs. 604 Num. groups: childid 155 Var: childid (Intercept) 1.27 Var: childid I(age - 2) 14.00 Var: childid I((age - 2)^2) 0.16 Cov: childid (Intercept) I(age - 2) -0.13 Cov: childid (Intercept) I((age - 2)^2) 0.41 Cov: childid I(age - 2) I((age - 2)^2) -0.61 Var: Residual 37.20 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 11.3.3 Plot of the Estimated Marginal Means Note: the \\(x-axis\\) is the age variable back on its original scale interactions::interact_plot(model = fit_lmer_1_re, # model name pred = age, # x-axis variable (must be continuous) modx = sicdegp, # seperate lines interval = TRUE) + # shaded bands scale_x_continuous(breaks = c(2, 3, 5, 9, 13)) Figure 11.1: Model 1: Loaded Means Structure 11.4 Model 2A: Drop Random Intercepts Note: There seems to be relatively little variation in baseline measurements of VSAE across individuals in the same SICD group, so the variation at age 2 can be attributed to random error, rather than between-subject variation. This indicates we may want to try removing the random intercepts, while retaining the same fixed- and other random-effects. This new model implies that children have common initial VSAE value at age 2, given their SICD group. 11.4.1 Fit the Model fit_lmer_2a_re &lt;- lmerTest::lmer(vsae ~ I(age-2)*sicdegp + I((age-2)^2)*sicdegp + (0 + I(age-2) + I((age-2)^2) | childid), data = data_long, REML = TRUE) 11.4.2 Assess the Signifcance anova(fit_lmer_1_re, fit_lmer_2a_re, refit = FALSE) Data: data_long Models: fit_lmer_2a_re: vsae ~ I(age - 2) * sicdegp + I((age - 2)^2) * sicdegp + (0 + I(age - 2) + I((age - 2)^2) | childid) fit_lmer_1_re: vsae ~ I(age - 2) * sicdegp + I(I(age - 2)^2) * sicdegp + (I(age - 2) + I((age - 2)^2) | childid) npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) fit_lmer_2a_re 13 4587.0 4644.2 -2280.5 4561.0 fit_lmer_1_re 16 4586.5 4657.0 -2277.2 4554.5 6.4643 3 0.09108 . --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The more complicated model (including random intercepts) does NOT fit better, thus the random intercepts may be removed from the model. Model 2a is better than Model 1 11.5 Model 2B: Drop Random Quadratic Slope We should formally test the necessity of quadratic age random-effect. Comparison of nested models with REML-based LRT using a 50:50 mixture χ2-distribution with 1 and 2 df Difference of 2 covariance parameters 11.5.1 Fit the Model fit_lmer_2b_re &lt;- lmerTest::lmer(vsae ~ I(age-2)*sicdegp + I((age-2)^2)*sicdegp + (0 + I(age-2) | childid), REML = TRUE, data = data_long) 11.5.2 Assess the Signifcance anova(fit_lmer_2a_re, fit_lmer_2b_re, refit = FALSE) Data: data_long Models: fit_lmer_2b_re: vsae ~ I(age - 2) * sicdegp + I((age - 2)^2) * sicdegp + (0 + I(age - 2) | childid) fit_lmer_2a_re: vsae ~ I(age - 2) * sicdegp + I((age - 2)^2) * sicdegp + (0 + I(age - 2) + I((age - 2)^2) | childid) npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) fit_lmer_2b_re 11 4669.3 4717.7 -2323.7 4647.3 fit_lmer_2a_re 13 4587.0 4644.2 -2280.5 4561.0 86.34 2 &lt; 2.2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The more complicated model (including random intercepts) DOES fit better, thus the random slopes for both the linear AND the quadratic effect of age should be retained in the model. Model 2a is better than model 2b 11.6 Model 3: Drop Quadratic Time Fixed Effect Fit the previous ‘best’ model via ML, not REML to compare nested model that differe in terms of fixed effects only 11.6.1 Fit the Models fit_lmer_2a_ml &lt;- lmerTest::lmer(vsae ~ I(age-2)*sicdegp + I((age-2)^2)*sicdegp + (0 + I(age-2) + I((age-2)^2) | childid), data = data_long, REML = FALSE) fit_lmer_3_ml &lt;- lmerTest::lmer(vsae ~ I(age-2)*sicdegp + (0 + I(age-2) + I((age-2)^2) | childid), data = data_long, REML = FALSE) 11.6.2 Assess the Signifcance anova(fit_lmer_2a_ml, fit_lmer_3_ml) Data: data_long Models: fit_lmer_3_ml: vsae ~ I(age - 2) * sicdegp + (0 + I(age - 2) + I((age - 2)^2) | childid) fit_lmer_2a_ml: vsae ~ I(age - 2) * sicdegp + I((age - 2)^2) * sicdegp + (0 + I(age - 2) + I((age - 2)^2) | childid) npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) fit_lmer_3_ml 10 4584.4 4628.5 -2282.2 4564.4 fit_lmer_2a_ml 13 4582.1 4639.3 -2278.0 4556.1 8.3704 3 0.03895 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The more complicated model (including fixed interaction between quadratic time and SICD group) DOES fit better, thus the higher level interaction should be retained in the model. Model 2a is better than model 3. 11.7 Final Model 11.7.1 Table of Parameter Esitmates texreg::knitreg(fit_lmer_2a_re, caption = &quot;MLM: Final Model&quot;, caption.above = TRUE, single.row = TRUE, digits = 4) MLM: Final Model   Model 1 (Intercept) 8.4085 (0.7370)*** age - 2 2.2694 (0.7399)** sicdegpmed 1.2644 (0.9741) sicdegphigh 5.3646 (1.0907)*** (age - 2)^2 0.0721 (0.0790) age - 2:sicdegpmed 0.4290 (0.9808) age - 2:sicdegphigh 3.3259 (1.0760)** sicdegpmed:(age - 2)^2 0.0007 (0.1038) sicdegphigh:(age - 2)^2 0.1335 (0.1138) AIC 4586.9689 BIC 4644.2154 Log Likelihood -2280.4845 Num. obs. 604 Num. groups: childid 155 Var: childid I(age - 2) 13.9915 Var: childid I((age - 2)^2) 0.1338 Cov: childid I(age - 2) I((age - 2)^2) -0.4436 Var: Residual 37.9869 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 11.7.2 Interpretation of Fixed Effects 11.7.2.1 Reference Group: low SICD group \\(\\gamma_{0}\\) = 8.408 is the estimated marginal mean VSAE score for children in the low SICD, at 2 years of age \\(\\gamma_{a}\\) = 2.269 and \\(\\gamma_{a^2}\\) = 0.072 are the fixed effects for age and age-squared on VSAE for children in the low SICD group (change over time) Thus the equation for the estimated marginal mean VASE trajectory for the low SICD group is: \\[ \\begin{align*} VASE =&amp; \\gamma_{0} + \\gamma_{a} (AGE - 2) + \\gamma_{a^2} (AGE - 2)^2 \\\\ =&amp; 8.408 + 2.269 (AGE - 2) + 0.072 (AGE - 2)^2 \\\\ \\end{align*} \\] 11.7.2.2 First Comparison Group: medium SICD group \\(\\gamma_{med}\\) = 1.264 is the DIFFERENCE in the estimated marginal mean VSAE score for children in the medium vs. the low SICD, at 2 years of age \\(\\gamma_{med:\\;a}\\) = 0.429 and \\(\\gamma_{med:\\;a^2}\\) = 0.001 are the DIFFERENCE in the fixed effects for age and age-squared on VSAE for children in the medium vs. the low SICD group Thus the equation for the estimated marginal mean VASE trajectory for the medium SICD group is: \\[ \\begin{align*} VASE =&amp; (\\gamma_{0} + \\gamma_{med}) + (\\gamma_{a} + \\gamma_{med:\\;a}) (AGE - 2) + (\\gamma_{a^2} + \\gamma_{med:\\;a^2})(AGE - 2)^2 \\\\ =&amp; (8.408 + 1.264) + (2.269 + 0.429) (AGE - 2) + (0.072 + 0.001)(AGE - 2)^2 \\\\ =&amp; 9.673 + 2.698 (AGE - 2) + 0.073 (AGE - 2)^2 \\\\ \\end{align*} \\] 11.7.2.3 Second Comparison Group: high SICD group \\(\\gamma_{hi}\\) = 5.365 is the DIFFERENCE in the estimated marginal mean VSAE score for children in the high vs. the low SICD, at 2 years of age \\(\\gamma_{hi:\\;a}\\) = 3.326 and \\(\\gamma_{hi:\\;a^2}\\) = 0.133 are the DIFFERENCE in the fixed effects for age and age-squared on VSAE for children in the high vs. the low SICD group Thus the equation for the estimated marginal mean VASE trajectory for the high SICD group is: \\[ \\begin{align*} VASE =&amp; (\\gamma_{0} + \\gamma_{hi}) + (\\gamma_{a} + \\gamma_{hi:\\;a}) (AGE - 2) + (\\gamma_{a^2} + \\gamma_{hi:\\;a^2})(AGE - 2)^2 \\\\ =&amp; (8.408 + 5.365) + (2.269 + 3.326) (AGE - 2) + (0.072 + 0.133)(AGE - 2)^2 \\\\ =&amp; 13.773 + 5.595 (AGE - 2) + 0.206 (AGE - 2)^2 \\\\ \\end{align*} \\] 11.7.3 Interpretation of Random Effects lme4::VarCorr(fit_lmer_2a_re)%&gt;% print(comp = c(&quot;Variance&quot;, &quot;Std.Dev&quot;), digits = 3) Groups Name Variance Std.Dev. Corr childid I(age - 2) 13.991 3.741 I((age - 2)^2) 0.134 0.366 -0.32 Residual 37.987 6.163 Here a group of observations = a CHILD 11.7.3.1 Residual Varaince Within-child-variance \\(e_{ti}\\) the residuals associated with observation at time \\(t\\) on child \\(i\\) \\[ \\sigma^2 = \\sigma^2_e = VAR[e_{ti}] = 37.987 \\] 11.7.3.2 2 Variance Components Between-children slope variances Random LINEAR effect of age variance \\(u_{1i}\\) the DIFFERENCE between child \\(i\\)’s specific linear component for age and the fixed linear component for age, given their SICD group \\[ \\tau_{11} = \\sigma^2_{u1} = VAR[u_{1i}] = 13.99 \\] Random QUADRATIC effect of age variance \\(u_{2i}\\) the DIFFERENCE between child \\(i\\)’s specific quadratic component for age and the fixed quadratic component for age, given their SICD group \\[ \\tau_{22} = \\sigma^2_{u2} = VAR[u_{2i}] = 0.13 \\] 11.7.3.3 1 Covariance (or correlation) Components Slope-slope covariance Random LINEAR and Quadratic effect of age covariance: \\[ \\tau_{12} = \\sigma^2_{u12} = COV[u_{1i}, u_{2i}] = -0.44 \\] \\[ Correlation(u_{1i}, u_{2i}) = -0.324 \\] 11.7.4 Assumption Checking The residuals are: Assumed to be normally, independently, and identically distributed (conditional on other random-effects) Assumed independent of random-effects \\[ e_{ti} \\sim N(0, \\sigma^2) \\] 11.7.4.1 The ggResidpanel package ggResidpanel::resid_panel(fit_lmer_2a_re) # default = pearson residuals 11.7.4.2 Manually with HLMdiag and ggplot2 fit_lmer_2a_re %&gt;% HLMdiag::hlm_augment(level = 1, include.ls = FALSE, standardized = TRUE) %&gt;% ggplot(aes(x = .resid)) + geom_histogram(bins = 40, color = &quot;gray20&quot;, alpha = .2) + theme_bw() + labs(main = &quot;Histogram&quot;) fit_lmer_2a_re %&gt;% ranef() %&gt;% data.frame() %&gt;% ggplot(aes(sample = condval)) + geom_qq() + geom_qq_line() + facet_wrap(~ term, scale = &quot;free_y&quot;)+ theme_bw() + labs(main = &quot;Random Slopes&quot;) fit_lmer_2a_re %&gt;% HLMdiag::hlm_augment(level = 1, include.ls = FALSE, standardized = TRUE) %&gt;% ggplot(aes(x = .fitted, y = .resid)) + geom_hline(yintercept = 0, color = &quot;red&quot;) + geom_point() + geom_smooth() + theme_bw() + labs(main = &quot;Residual Plot&quot;) fit_lmer_2a_re %&gt;% HLMdiag::hlm_augment(level = 1, include.ls = FALSE, standardized = TRUE) %&gt;% ggplot(aes(sample = .resid)) + geom_qq()+ stat_qq_line() + theme_bw() 11.7.4.3 The sjPlot package sjPlot::plot_model(fit_lmer_2a_re, type = &quot;diag&quot;) [[1]] [[2]] [[2]]$childid [[3]] [[4]] 11.7.5 Plot of the Estimated Marginal Means 11.7.5.1 Quick and Default Note: the \\(x-axis\\) is the age variable, back on its original scale interactions::interact_plot(model = fit_lmer_2a_re, # model name pred = age, # x-axis variable (must be continuous) modx = sicdegp, # seperate lines interval = TRUE) + # shaded bands scale_x_continuous(breaks = c(2, 3, 5, 9, 13)) Figure 11.2: Final Model (2a) interactions::interact_plot(model = fit_lmer_2a_re, # model name pred = age, # x-axis variable (must be continuous) modx = sicdegp, # separate lines interval = TRUE, x.label = &quot;Age of Child, in years&quot;, y.label = &quot;Estimated Marginal Mean\\nVineland Socialization Age Equivalent&quot;, legend.main = &quot;Initial Communication (SICD)&quot;, modx.labels = c(&quot;Low&quot;, &quot;Medium&quot;, &quot;High&quot;), colors = c(&quot;black&quot;, &quot;black&quot;, &quot;black&quot;)) + scale_x_continuous(breaks = c(2, 3, 5, 9, 13)) + scale_y_continuous(breaks = seq(from = 0, to = 120, by = 20)) + theme_bw() + theme(legend.position = c(0, 1), legend.justification = c(-0.1, 1.1), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(2, &quot;cm&quot;)) 11.7.5.2 More Customized - Color This version would look better on a poster or in a slide presentation. effects::Effect(focal.predictors = c(&quot;age&quot;,&quot;sicdegp&quot;), # variables involved in interactions mod = fit_lmer_2a_re, xlevels = list(age2 = seq(from = 2, to = 13, by = .1))) %&gt;% # add more values to smooth out the prediction lines and ribbons data.frame() %&gt;% dplyr::mutate(sicdegp = factor(sicdegp, levels = c(&quot;high&quot;, &quot;med&quot;, &quot;low&quot;), labels = c(&quot;High&quot;, &quot;Medium&quot;, &quot;Low&quot;))) %&gt;% ggplot(aes(x = age, y = fit, group = sicdegp)) + geom_ribbon(aes(ymin = lower, # 95% Confidence Intervals ymax = upper, fill = sicdegp), alpha = .3) + geom_line(aes(linetype = sicdegp, color = sicdegp), size = 1) + scale_x_continuous(breaks = c(2, 3, 5, 9, 13)) + # mark values that were actually measured scale_y_continuous(breaks = seq(from = 0, to = 120, by = 20)) + scale_linetype_manual(values = c(&quot;solid&quot;, &quot;longdash&quot;, &quot;dotted&quot;)) + theme_bw() + theme(legend.position = c(0, 1), legend.justification = c(-0.1, 1.1), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(2.5, &quot;cm&quot;)) + labs(x = &quot;Age of Child, in years&quot;, y = &quot;Estimated Marginal Mean\\nVineland Socialization Age Equivalent&quot;, linetype = &quot;Initial Communication (SICD)&quot;, fill = &quot;Initial Communication (SICD)&quot;, color = &quot;Initial Communication (SICD)&quot;) 11.7.5.3 More Customized - Black and White This version would be better for a publication. effects::Effect(focal.predictors = c(&quot;age&quot;,&quot;sicdegp&quot;), mod = fit_lmer_2a_re, xlevels = list(age2 = seq(from = 2, to = 13, by = .1))) %&gt;% data.frame() %&gt;% dplyr::mutate(sicdegp = factor(sicdegp, levels = c(&quot;high&quot;, &quot;med&quot;, &quot;low&quot;), labels = c(&quot;High&quot;, &quot;Medium&quot;, &quot;Low&quot;))) %&gt;% ggplot(aes(x = age, y = fit, group = sicdegp)) + geom_ribbon(aes(ymin = lower, ymax = upper, fill = sicdegp), alpha = .4) + geom_line(aes(linetype = sicdegp), size = .7) + scale_x_continuous(breaks = c(2, 3, 5, 9, 13)) + scale_y_continuous(breaks = seq(from = 0, to = 120, by = 20)) + scale_linetype_manual(values = c(&quot;solid&quot;, &quot;longdash&quot;, &quot;dotted&quot;)) + scale_fill_manual(values = c(&quot;gray10&quot;, &quot;gray40&quot;, &quot;gray60&quot;)) + theme_bw() + theme(legend.position = c(0, 1), legend.justification = c(-0.1, 1.1), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(2, &quot;cm&quot;)) + labs(x = &quot;Age of Child, in years&quot;, y = &quot;Estimated Marginal Mean\\nVineland Socialization Age Equivalent&quot;, linetype = &quot;Initial Communication (SICD)&quot;, fill = &quot;Initial Communication (SICD)&quot;, color = &quot;Initial Communication (SICD)&quot;) 11.7.6 Post Hoc Compairisons fit_lmer_2a_re %&gt;% emmeans::emmeans(pairwise ~ sicdegp, at = list(age = 13)) $emmeans sicdegp emmean SE df lower.CL upper.CL low 42.1 7.97 135 26.3 57.9 med 48.2 6.90 131 34.5 61.8 high 100.2 8.45 131 83.5 116.9 Degrees-of-freedom method: kenward-roger Confidence level used: 0.95 $contrasts contrast estimate SE df t.ratio p.value low - med -6.07 10.5 133 -0.575 0.8335 low - high -58.10 11.6 133 -5.002 &lt;.0001 med - high -52.03 10.9 131 -4.772 &lt;.0001 Degrees-of-freedom method: kenward-roger P value adjustment: tukey method for comparing a family of 3 estimates totalSD &lt;- VarCorr(fit_lmer_2a_re) %&gt;% data.frame() %&gt;% dplyr::summarise(tot_var = sum(vcov)) %&gt;% dplyr::pull(tot_var) %&gt;% sqrt() totalSD [1] 7.188084 fit_lmer_2a_re %&gt;% emmeans::emmeans(~ sicdegp, at = list(age = 13)) %&gt;% emmeans::eff_size(sigma = totalSD, # which SD to divide by??? edf = 50) # df contrast effect.size SE df lower.CL upper.CL low - med -0.844 1.47 131 -3.75 2.06 low - high -8.083 1.81 131 -11.66 -4.51 med - high -7.239 1.68 131 -10.56 -3.91 sigma used for effect sizes: 7.188 Degrees-of-freedom method: inherited from kenward-roger when re-gridding Confidence level used: 0.95 11.7.7 Blups vs. Fixed Effects BLUP = Best Linear Unbiased Predictor A BLUP is the specific prediction for an individual supject, showin by black lines below. This includes the fixed effects as well as the specific random effects for a given individual. Comparatively, the blue lines below display the predictions for fixed effects only. data_long %&gt;% dplyr::mutate(sicdegp = fct_recode(sicdegp, &quot;Low Communication&quot; = &quot;low&quot;, &quot;Medium Communication&quot; = &quot;med&quot;, &quot;High Communication&quot; = &quot;high&quot;)) %&gt;% dplyr::mutate(pred_fixed = predict(fit_lmer_2a_re, re.form = NA)) %&gt;% # fixed effects only dplyr::mutate(pred_wrand = predict(fit_lmer_2a_re)) %&gt;% # fixed and random effects together ggplot(aes(x = age, y = vsae)) + geom_line(aes(y = pred_wrand, # BLUP = fixed and random effects together group = childid, color = &quot;BLUP&quot;, size = &quot;BLUP&quot;)) + geom_line(aes(y = pred_fixed, # fixed effects only group = sicdegp, color = &quot;Fixed&quot;, size = &quot;Fixed&quot;)) + scale_color_manual(name = &quot;Model: &quot;, values = c(&quot;BLUP&quot; = &quot;black&quot;, &quot;Fixed&quot; = &quot;blue&quot;)) + scale_size_manual(name = &quot;Model: &quot;, values = c(&quot;BLUP&quot; = .5, &quot;Fixed&quot; = 1.5)) + facet_grid(. ~ sicdegp) + theme_bw() + scale_x_continuous(breaks = c(2, 3, 5, 9, 13)) + labs(x = &quot;Age, in years&quot;, y = &quot;Estimated Marginal Mean\\nVineland Socialization Age Equivalent&quot;) + theme(legend.position = &quot;bottom&quot;, legend.key.width = unit(1.5, &quot;cm&quot;)) data_long %&gt;% dplyr::mutate(pred_fixed = predict(fit_lmer_2a_re, re.form = NA)) %&gt;% dplyr::mutate(pred_wrand = predict(fit_lmer_2a_re)) %&gt;% dplyr::filter(childid %in% sample(levels(data_long$childid), 25)) %&gt;% # 25 randomly sampled children ggplot(aes(x = age, y = vsae)) + geom_point(aes(color = sicdegp), size = 3) + geom_line(aes(y = pred_wrand, linetype = &quot;BLUP&quot;, size = &quot;BLUP&quot;), color = &quot;black&quot;) + geom_line(aes(y = pred_fixed, color = sicdegp, linetype = &quot;Fixed&quot;, size = &quot;Fixed&quot;)) + scale_linetype_manual(name = &quot;Model: &quot;, values = c(&quot;BLUP&quot; = &quot;longdash&quot;, &quot;Fixed&quot; = &quot;solid&quot;)) + scale_size_manual(name = &quot;Model: &quot;, values = c(&quot;BLUP&quot; = .5, &quot;Fixed&quot; = 1)) + facet_wrap(. ~ childid, labeller = &quot;label_both&quot;) + theme_bw() + scale_x_continuous(breaks = c(2, 3, 5, 9, 13)) + theme(legend.position = &quot;bottom&quot;, legend.key.width = unit(1.5, &quot;cm&quot;)) + labs(x = &quot;Age, in years&quot;, y = &quot;Estimated Marginal Mean\\nVineland Socialization Age Equivalent&quot;, color = &quot;Communication:&quot;) "],["mlm-longitudinal-hedeker-gibbons---depression.html", "12 MLM, Longitudinal: Hedeker &amp; Gibbons - Depression 12.1 Background 12.2 Exploratory Data Analysis 12.3 Patterns in the Outcome Over Time 12.4 MLM - Null or Emptly Models 12.5 MLM: Add Random Slope for Time (i.e. Trend) 12.6 MLM: Coding of Time 12.7 MLM: Effect of DIagnosis on Time Trends (Fixed Interaction) 12.8 MLM: Quadratic Trend", " 12 MLM, Longitudinal: Hedeker &amp; Gibbons - Depression library(tidyverse) # all things tidy library(pander) # nice looking general tabulations library(furniture) # nice table1() descriptions library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(psych) # contains some useful functions, like headTail() library(sjstats) # ICC calculations library(effects) # Estimated Marginal Means library(performance) library(VIM) # Visualization and Imputation of Missing Values library(naniar) # Summaries and Visualizations for Missing Data library(corrplot) # Visualize correlation matrix library(lme4) # Linear, generalized linear, &amp; nonlinear mixed models library(lmerTest) library(optimx) 12.1 Background Starting in chapter 4, (Hedeker and Gibbons 2006) details analysis of a psychiatric study described by (Reisby et al. 1977). This study focuses on the relationship between Imipramine (IMI) and Desipramine (DMI) plasma levels and clinical response in 66 depressed inpatients (37 endogenous and 29 non-endogenous). Note: The IMI and DMI measures were only taken in the later weeks, but are not used here. hamd Hamilton Depression Scores (HD) Independent or predictor variables: endog Depression Diagnosis endogenous non-endogenous/reactive IMI (imipramine) drug-plasma levels (µg/l) antidepressant given 225 mg/day, weeks 3-6 DMI (desipramine) drug-plasma levels (µg/l) metabolite of imipramine data_raw &lt;- read.table(&quot;data/riesby.dat.txt&quot;) %&gt;% dplyr::rename(id = &quot;V1&quot;, hamd = &quot;V2&quot;, endog = &quot;V5&quot;, week = &quot;V4&quot;) %&gt;% dplyr::select(-V3, -V6) data_raw %&gt;% psych::headTail(top = 11, bottom = 8) id hamd week endog 1 101 26 0 0 2 101 22 1 0 3 101 18 2 0 4 101 7 3 0 5 101 4 4 0 6 101 3 5 0 7 103 33 0 0 8 103 24 1 0 9 103 15 2 0 10 103 24 3 0 11 103 15 4 0 ... ... &lt;NA&gt; ... ... 389 360 28 4 1 390 360 33 5 1 391 361 30 0 1 392 361 22 1 1 393 361 11 2 1 394 361 8 3 1 395 361 7 4 1 396 361 19 5 1 12.1.1 Long Format data_long &lt;- data_raw %&gt;% dplyr::filter(!is.na(hamd)) %&gt;% # remove NA or missing scores dplyr::mutate(id = factor(id)) %&gt;% # declare grouping var a factor dplyr::mutate(endog = factor(endog, # attach labels to a grouping variable levels = c(0, 1), # order of the levels should match levels labels = c(&quot;Reactive&quot;, # order matters! &quot;Endogenous&quot;))) %&gt;% dplyr::mutate(hamd = as.numeric(hamd)) %&gt;% dplyr::select(id, week, endog, hamd) %&gt;% # select the order of variables to include dplyr::arrange(id, week) # sort observations data_long %&gt;% psych::headTail(top = 11, bottom = 8) id week endog hamd 1 101 0 Reactive 26 2 101 1 Reactive 22 3 101 2 Reactive 18 4 101 3 Reactive 7 5 101 4 Reactive 4 6 101 5 Reactive 3 7 103 0 Reactive 33 8 103 1 Reactive 24 9 103 2 Reactive 15 10 103 3 Reactive 24 11 103 4 Reactive 15 ... &lt;NA&gt; ... &lt;NA&gt; ... 389 609 4 Endogenous 15 390 609 5 Endogenous 2 391 610 0 Endogenous 34 392 610 1 Endogenous &lt;NA&gt; 393 610 2 Endogenous 33 394 610 3 Endogenous 23 395 610 4 Endogenous &lt;NA&gt; 396 610 5 Endogenous 11 12.1.2 Wide Format data_wide &lt;- data_long %&gt;% # save the dataset as a new name tidyr::pivot_wider(names_from = week, names_prefix = &quot;hamd_&quot;, values_from = hamd) Notice the missing values, displayed as NA. data_wide %&gt;% psych::headTail() id endog hamd_0 hamd_1 hamd_2 hamd_3 hamd_4 hamd_5 1 101 Reactive 26 22 18 7 4 3 2 103 Reactive 33 24 15 24 15 13 3 104 Endogenous 29 22 18 13 19 0 4 105 Reactive 22 12 16 16 13 9 5 &lt;NA&gt; &lt;NA&gt; ... ... ... ... ... ... 6 607 Endogenous 30 39 30 27 20 4 7 608 Reactive 24 19 14 12 3 4 8 609 Endogenous &lt;NA&gt; 25 22 14 15 2 9 610 Endogenous 34 &lt;NA&gt; 33 23 &lt;NA&gt; 11 12.2 Exploratory Data Analysis 12.2.1 Diagnosis Group data_wide %&gt;% furniture::table1(&quot;Depression Type&quot; = endog, output = &quot;markdown&quot;) Mean/Count (SD/%) n = 66 Depression Type Reactive 29 (43.9%) Endogenous 37 (56.1%) 12.2.2 Missing Data &amp; Attrition Plot the amount of missing values and the amount of each patter of missing values. 12.2.2.1 VIM package data_wide %&gt;% VIM::aggr(numbers = TRUE, # shows the number to the far right prop = FALSE) # shows counts instead of proportions 12.2.2.2 naniar package data_wide %&gt;% naniar::gg_miss_upset(sets = c(&quot;hamd_5_NA&quot;, &quot;hamd_4_NA&quot;, &quot;hamd_3_NA&quot;, &quot;hamd_2_NA&quot;, &quot;hamd_1_NA&quot;, &quot;hamd_0_NA&quot;), keep.order = TRUE) 12.2.3 Depression Over Time, by Group 12.2.3.1 Table of Means Default = COMPLETE CASES ONLY!!! Note - the sample size at each time point is the same, but subjects are only included if they have data at every time point data_wide %&gt;% dplyr::group_by(endog) %&gt;% furniture::table1(&quot;Baseline&quot; = hamd_0, &quot;Week 1&quot; = hamd_1, &quot;Week 2&quot; = hamd_2, &quot;Week 3&quot; = hamd_3, &quot;Week 4&quot; = hamd_4, &quot;Week 5&quot; = hamd_5, total = TRUE, test = TRUE, na.rm = TRUE, # default: COMPLETE CASES ONLY!!!!! digits = 2, output = &quot;markdown&quot;, caption = &quot;Hamilton Depression Scores Across Time, by Depression Type for Participants with 6 Complete Weeks&quot;) Table 12.1: Hamilton Depression Scores Across Time, by Depression Type for Participants with 6 Complete Weeks Total Reactive Endogenous P-Value n = 46 n = 25 n = 21 Baseline 0.186 23.15 (4.41) 22.36 (3.90) 24.10 (4.87) Week 1 0.007 21.83 (4.92) 20.08 (3.68) 23.90 (5.47) Week 2 0.292 18.07 (5.17) 17.32 (4.34) 18.95 (6.01) Week 3 0.398 16.61 (6.31) 15.88 (5.84) 17.48 (6.86) Week 4 0.507 13.46 (6.78) 12.84 (6.68) 14.19 (6.98) Week 5 0.468 12.15 (7.57) 11.40 (6.54) 13.05 (8.73) Specify All data: note - that the smaple sizes will be different for each time point data_wide %&gt;% dplyr::group_by(endog) %&gt;% furniture::table1(&quot;Baseline&quot; = hamd_0, &quot;Week 1&quot; = hamd_1, &quot;Week 2&quot; = hamd_2, &quot;Week 3&quot; = hamd_3, &quot;Week 4&quot; = hamd_4, &quot;Week 5&quot; = hamd_5, total = TRUE, test = TRUE, na.rm = FALSE, # default: COMPLETE CASES ONLY!!!!! digits = 2, output = &quot;markdown&quot;, caption = &quot;Hamilton Depression Scores Across Time, by Depression Type for All Participants&quot;) Table 12.2: Hamilton Depression Scores Across Time, by Depression Type for All Participants Total Reactive Endogenous P-Value n = 66 n = 29 n = 37 Baseline 0.301 23.44 (4.53) 22.79 (4.12) 24.00 (4.85) Week 1 0.033 21.84 (4.70) 20.48 (3.83) 23.00 (5.10) Week 2 0.095 18.31 (5.49) 17.00 (4.35) 19.30 (6.08) Week 3 0.23 16.42 (6.42) 15.34 (6.17) 17.28 (6.56) Week 4 0.298 13.62 (6.97) 12.62 (6.72) 14.47 (7.17) Week 5 0.48 11.95 (7.22) 11.22 (6.34) 12.58 (7.96) 12.2.3.2 Using the LONG formatted dataset Each person’s data is stored on multiple lines, one for each time point. FOR ALL DATA! data_sum_all &lt;- data_long %&gt;% dplyr::group_by(endog, week) %&gt;% # specify the groups dplyr::filter(!is.na(hamd)) %&gt;% dplyr::summarise(hamd_n = n(), # count of valid scores hamd_mean = mean(hamd), # mean score hamd_sd = sd(hamd), # standard deviation of scores hamd_sem = hamd_sd / sqrt(hamd_n)) # stadard error for the mean of scores data_sum_all %&gt;% pander::pander() endog week hamd_n hamd_mean hamd_sd hamd_sem Reactive 0 28 23 4.1 0.78 Reactive 1 29 20 3.8 0.71 Reactive 2 28 17 4.3 0.82 Reactive 3 29 15 6.2 1.15 Reactive 4 29 13 6.7 1.25 Reactive 5 27 11 6.3 1.22 Endogenous 0 33 24 4.8 0.84 Endogenous 1 34 23 5.1 0.87 Endogenous 2 37 19 6.1 1.00 Endogenous 3 36 17 6.6 1.09 Endogenous 4 34 14 7.2 1.23 Endogenous 5 31 13 8.0 1.43 FOR COMPLETE CASES ONLY!!! data_sum_cc &lt;- data_long %&gt;% dplyr::group_by(id) %&gt;% # group-by participant dplyr::filter(!is.na(hamd)) %&gt;% dplyr::mutate(person_vsae_n = n()) %&gt;% # count the number of valid VSAE scores dplyr::filter(person_vsae_n == 6) %&gt;% # restrict to only thoes children with 5 valid scores dplyr::group_by(endog, week) %&gt;% # specify the groups dplyr::summarise(hamd_n = n(), # count of valid scores hamd_mean = mean(hamd), # mean score hamd_sd = sd(hamd), # standard deviation of scores hamd_sem = hamd_sd / sqrt(hamd_n)) # stadard error for the mean of scores data_sum_cc %&gt;% pander::pander() endog week hamd_n hamd_mean hamd_sd hamd_sem Reactive 0 25 22 3.9 0.78 Reactive 1 25 20 3.7 0.74 Reactive 2 25 17 4.3 0.87 Reactive 3 25 16 5.8 1.17 Reactive 4 25 13 6.7 1.34 Reactive 5 25 11 6.5 1.31 Endogenous 0 21 24 4.9 1.06 Endogenous 1 21 24 5.5 1.19 Endogenous 2 21 19 6.0 1.31 Endogenous 3 21 17 6.9 1.50 Endogenous 4 21 14 7.0 1.52 Endogenous 5 21 13 8.7 1.90 12.2.3.3 Person-Profile Plot or Spaghetti Plot Use the data in LONG format. A scatterplot of all observations of depression scores over time, joining the dots of each individual’s data. NOTE: Not all lines have a point for every week! data_long %&gt;% dplyr::filter(!is.na(hamd)) %&gt;% ggplot(aes(x = week, y = hamd)) + geom_point() + geom_line(aes(group = id)) + # join points that belong to the same &quot;id&quot; theme_bw() + labs(x = &quot;Weeks Since Baseline&quot;, y = &quot;Estimated Hamilton Depression Score (HD)&quot;) data_long %&gt;% dplyr::filter(!is.na(hamd)) %&gt;% ggplot(aes(x = week, y = hamd, color = endog)) + # color points and lines by the &quot;endog&quot; variable geom_line(aes(group = id)) + theme_bw() + labs(x = &quot;Weeks Since Baseline&quot;, y = &quot;Estimated Hamilton Depression Score (HD)&quot;) data_long %&gt;% dplyr::filter(!is.na(hamd)) %&gt;% ggplot(aes(x = week, y = hamd)) + geom_line(aes(group = id)) + facet_grid( ~ endog) + # side-by-side pandels by the &quot;endog&quot; variable theme_bw() + labs(x = &quot;Weeks Since Baseline&quot;, y = &quot;Estimated Hamilton Depression Score (HD)&quot;) data_long %&gt;% dplyr::filter(!is.na(hamd)) %&gt;% ggplot(aes(x = week %&gt;% factor(), y = hamd)) + geom_boxplot() + # compare center and spread facet_grid( ~ endog) + theme_bw() + labs(x = &quot;Weeks Since Baseline&quot;, y = &quot;Estimated Hamilton Depression Score (HD)&quot;) data_long %&gt;% dplyr::filter(!is.na(hamd)) %&gt;% ggplot(aes(x = week %&gt;% factor(), y = hamd)) + geom_violin(fill = &quot;gray75&quot;) + # similar to boxplots to show distribution stat_summary() + stat_summary(aes(group = &quot;endog&quot;), fun = mean, geom = &quot;line&quot;) + facet_grid( ~ endog) + theme_bw() + labs(x = &quot;Weeks Since Baseline&quot;, y = &quot;Estimated Hamilton Depression Score (HD)&quot;) data_long %&gt;% dplyr::filter(!is.na(hamd)) %&gt;% ggplot(aes(x = week, y = hamd, color = endog)) + stat_summary() + stat_summary(aes(group = endog, linetype = endog), fun = mean, geom = &quot;line&quot;) + scale_linetype_manual(values = c(&quot;solid&quot;, &quot;longdash&quot;)) + theme(legend.position = &quot;bottom&quot;, legend.key.width = unit(2, &quot;cm&quot;)) + theme_bw() + labs(x = &quot;Weeks Since Baseline&quot;, y = &quot;Estimated Hamilton Depression Score (HD)&quot;) data_long %&gt;% dplyr::filter(!is.na(hamd)) %&gt;% ggplot(aes(x = week, y = hamd)) + geom_line(aes(group = id)) + facet_grid( ~ endog) + geom_smooth() + # DEFAULTS: method = &quot;loess&quot;, se = TRUE, color = &quot;blue&quot; geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;hot pink&quot;)+ theme_bw() + labs(x = &quot;Weeks Since Baseline&quot;, y = &quot;Estimated Hamilton Depression Score (HD)&quot;) data_long %&gt;% dplyr::filter(!is.na(hamd)) %&gt;% ggplot(aes(x = week, y = hamd)) + facet_grid( ~ endog) + geom_smooth(aes(color = &quot;Flexible&quot;), method = &quot;loess&quot;, se = FALSE,) + geom_smooth(aes(color = &quot;Linear&quot;), method = &quot;lm&quot;, se = FALSE) + scale_color_manual(name = &quot;Smoother Type: &quot;, values = c(&quot;Flexible&quot; = &quot;blue&quot;, &quot;Linear&quot; = &quot;red&quot;)) + theme_bw() + theme(legend.position = &quot;bottom&quot;) + labs(x = &quot;Weeks Since Baseline&quot;, y = &quot;Estimated Hamilton Depression Score (HD)&quot;) data_long %&gt;% ggplot(aes(x = week, y = hamd, group = endog, linetype = endog)) + geom_smooth(method = &quot;loess&quot;, color = &quot;black&quot;, alpha = .25) + theme_bw() + scale_linetype_manual(values = c(&quot;solid&quot;, &quot;longdash&quot;)) + theme(legend.position = c(1, 1), legend.justification = c(1.1, 1.1), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(1.5, &quot;cm&quot;)) + labs(x = &quot;Weeks Since Baseline&quot;, y = &quot;Estimated Hamilton Depression Scale&quot;, linetype = &quot;Type of Depression&quot;) data_long %&gt;% dplyr::filter(!is.na(hamd)) %&gt;% ggplot(aes(x = week, y = hamd, group = endog, linetype = endog)) + geom_smooth(method = &quot;loess&quot;, color = &quot;black&quot;, alpha = .25) + theme_bw() + scale_linetype_manual(values = c(&quot;solid&quot;, &quot;longdash&quot;)) + theme(legend.position = c(1, 1), legend.justification = c(1.1, 1.1), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(2, &quot;cm&quot;)) + labs(x = &quot;Weeks Since Baseline&quot;, y = &quot;EsStimated Hamilton Depression Scale&quot;, linetype = &quot;Type of Depression&quot;) data_long %&gt;% dplyr::filter(!is.na(hamd)) %&gt;% ggplot(aes(x = week, y = hamd, group = endog, linetype = endog)) + geom_smooth(method = &quot;lm&quot;, color = &quot;black&quot;, alpha = .25) + theme_bw() + scale_linetype_manual(values = c(&quot;solid&quot;, &quot;longdash&quot;)) + theme(legend.position = c(1, 1), legend.justification = c(1.1, 1.1), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(1.5, &quot;cm&quot;)) + labs(x = &quot;Weeks Since Baseline&quot;, y = &quot;EStimated Hamilton Depression Scale&quot;, linetype = &quot;Type of Depression&quot;) 12.3 Patterns in the Outcome Over Time 12.3.1 Variance-Covariance 12.3.1.1 Full Matrix Variances are down the diagonal Increasing variance over time violates the ANOVA assumption of homogeity of variance data_wide %&gt;% dplyr::select(starts_with(&quot;hamd_&quot;)) %&gt;% # just the outcome(s) cov(use = &quot;complete.obs&quot;) %&gt;% # covariance matrix, LIST-wise deletion round(3) hamd_0 hamd_1 hamd_2 hamd_3 hamd_4 hamd_5 hamd_0 19.421 10.716 9.523 12.350 9.062 7.376 hamd_1 10.716 24.236 12.545 15.930 11.592 8.471 hamd_2 9.523 12.545 26.773 23.848 23.858 20.657 hamd_3 12.350 15.930 23.848 39.755 33.316 29.728 hamd_4 9.062 11.592 23.858 33.316 45.943 37.107 hamd_5 7.376 8.471 20.657 29.728 37.107 57.332 data_wide %&gt;% dplyr::select(starts_with(&quot;hamd_&quot;)) %&gt;% # just the outcome(s) cov(use = &quot;pairwise.complete.obs&quot;) %&gt;% # covariance matrix, PAIR-wise deletion round(3) hamd_0 hamd_1 hamd_2 hamd_3 hamd_4 hamd_5 hamd_0 20.551 10.115 10.139 10.086 7.191 6.278 hamd_1 10.115 22.071 12.277 12.550 10.264 7.720 hamd_2 10.139 12.277 30.091 25.126 24.626 18.384 hamd_3 10.086 12.550 25.126 41.153 37.339 23.992 hamd_4 7.191 10.264 24.626 37.339 48.594 30.513 hamd_5 6.278 7.720 18.384 23.992 30.513 52.120 12.3.1.2 Just Variances Notice the variance in scores increases over time, which is seen in the side-by-side boxplots. data_wide %&gt;% dplyr::select(starts_with(&quot;hamd_&quot;)) %&gt;% # just the outcome(s) cov(use = &quot;pairwise.complete.obs&quot;) %&gt;% # covariance matrix, PAIR-wise deletion diag() # extracts just the variances hamd_0 hamd_1 hamd_2 hamd_3 hamd_4 hamd_5 20.55082 22.07117 30.09135 41.15288 48.59447 52.12008 12.3.2 Correlation 12.3.2.1 Full Matrix Pairwise relationships are easier to eye-ball magnitude when presented as correlations, rather than covariances, due to the relative scale. data_wide %&gt;% dplyr::select(starts_with(&quot;hamd_&quot;)) %&gt;% # just the outcome(s) cor(use = &quot;complete.obs&quot;) %&gt;% # correlation matrix - LIST-wise deletion round(2) hamd_0 hamd_1 hamd_2 hamd_3 hamd_4 hamd_5 hamd_0 1.00 0.49 0.42 0.44 0.30 0.22 hamd_1 0.49 1.00 0.49 0.51 0.35 0.23 hamd_2 0.42 0.49 1.00 0.73 0.68 0.53 hamd_3 0.44 0.51 0.73 1.00 0.78 0.62 hamd_4 0.30 0.35 0.68 0.78 1.00 0.72 hamd_5 0.22 0.23 0.53 0.62 0.72 1.00 data_wide %&gt;% dplyr::select(starts_with(&quot;hamd_&quot;)) %&gt;% # just the outcome(s) cor(use = &quot;pairwise.complete.obs&quot;) %&gt;% # correlation matrix - PAIR-wise deletion round(2) hamd_0 hamd_1 hamd_2 hamd_3 hamd_4 hamd_5 hamd_0 1.00 0.49 0.41 0.33 0.23 0.18 hamd_1 0.49 1.00 0.49 0.41 0.31 0.22 hamd_2 0.41 0.49 1.00 0.74 0.67 0.46 hamd_3 0.33 0.41 0.74 1.00 0.82 0.57 hamd_4 0.23 0.31 0.67 0.82 1.00 0.65 hamd_5 0.18 0.22 0.46 0.57 0.65 1.00 12.3.2.2 Visualization Looking for patterns is always easier with a plot. All RM or mixed ANOVA assume sphericity or compound symmetry, meaning that all the correlations in the matrix would be the same. This is not the case for these data. Instead we see a classic pattern of corralary decay. Measures taken close in time, say 1 week apart, exhibit the highest degree of correlation. The farther apart in time that two measures are taken, the less correlated they are. Note that the adjacent measures become more highly correlated, too. This can be due to attrition; later time points having a smaller sample size. data_wide %&gt;% dplyr::select(starts_with(&quot;hamd_&quot;)) %&gt;% # just the outcome(s) cor(use = &quot;pairwise.complete.obs&quot;) %&gt;% # correlation matrix corrplot::corrplot.mixed(upper = &quot;ellipse&quot;) 12.3.3 For Each Group It can be a good ideal to investigate if the groups exhibit a similar pattern in correlation. Reactive Depression data_wide %&gt;% dplyr::filter(endog == &quot;Reactive&quot;) %&gt;% # filter observations for the REACTIVE group dplyr::select(starts_with(&quot;hamd_&quot;)) %&gt;% cor(use = &quot;pairwise.complete.obs&quot;) %&gt;% corrplot::corrplot.mixed(upper = &quot;ellipse&quot;) Endogenous Depression data_wide %&gt;% dplyr::filter(endog == &quot;Endogenous&quot;) %&gt;% # filter observations for the Endogenous group dplyr::select(starts_with(&quot;hamd_&quot;)) %&gt;% cor(use = &quot;pairwise.complete.obs&quot;) %&gt;% corrplot::corrplot.mixed(upper = &quot;ellipse&quot;) 12.4 MLM - Null or Emptly Models 12.4.1 Fit the model Random Intercepts, with Fixed Intercept and Time Slope (i.e. Trend)….@hedeker2006 section 4.3.5, starting on page 55 Since this situation deals with longitudinal data, it is more appropriate to start off including the time variable in the null model as a fixed effect only. fit_lmer_week_RI_reml &lt;- lmerTest::lmer(hamd ~ week + (1|id), data = data_long, REML = TRUE) 12.4.2 Table of Parameter Estimates texreg::screenreg(fit_lmer_week_RI_reml, single.row = TRUE, caption = &quot;MLM: Random Intercepts Null Model fit w/REML&quot;, caption.above = TRUE, custom.note = &quot;Reproduction of Hedeker&#39;s table 4.3 on page 55, except using REML here instead of ML&quot;) ======================================== Model 1 —————————————- (Intercept) 23.55 (0.64) week -2.38 (0.14) —————————————- AIC 2294.73 BIC 2310.43 Log Likelihood -1143.36 Num. obs. 375 Num. groups: id 66 Var: id (Intercept) 16.45 Var: Residual 19.10 ======================================== Reproduction of Hedeker’s table 4.3 on page 55, except using REML here instead of ML On average, patients start off with HDRS scores of 23.55 and then change by -2.38 points each week. This weekly improvement of about 2 points a week is statistically significant via the Wald test. 12.4.3 Estimated Marginal Means Plot Multilevel model on page 55 (Hedeker and Gibbons 2006) \\[ \\hat{y} = 23.552 + -2.376 week \\] The fastest way to plot a model is to use the sjPlot::plot_model() function. Note: you can’t use interactions::interact_plot() since there is only one predictor (i.e. independent variable) in this model. sjPlot::plot_model(fit_lmer_week_RI_reml, type = &quot;pred&quot;, terms = c(&quot;week&quot;)) 12.4.4 Estimated Marginal Means and Emperical Bayes Plots With a bit more code we can plot not only the marginal model (fixed effects only), but add the Best Linear Unbiased Predictions (BLUPs) or person-specific specific models (both fixed and random effects). data_long %&gt;% dplyr::mutate(pred_fixed = predict(fit_lmer_week_RI_reml, re.form = NA, # fixed effects only newdata = .)) %&gt;% dplyr::mutate(pred_wrand = predict(fit_lmer_week_RI_reml, # fixed and random effects both newdata = .)) %&gt;% ggplot(aes(x = week, y = hamd, group = id)) + geom_line(aes(y = pred_wrand, color = &quot;BLUP&quot;, size = &quot;BLUP&quot;, linetype = &quot;BLUP&quot;)) + geom_line(aes(y = pred_fixed, color = &quot;Marginal&quot;, size = &quot;Marginal&quot;, linetype = &quot;Marginal&quot;)) + theme_bw() + scale_color_manual(name = &quot;Type of Prediction&quot;, values = c(&quot;BLUP&quot; = &quot;gray50&quot;, &quot;Marginal&quot; = &quot;blue&quot;)) + scale_size_manual(name = &quot;Type of Prediction&quot;, values = c(&quot;BLUP&quot; = .5, &quot;Marginal&quot; = 1.25)) + scale_linetype_manual(name = &quot;Type of Prediction&quot;, values = c(&quot;BLUP&quot; = &quot;longdash&quot;, &quot;Marginal&quot; = &quot;solid&quot;)) + theme(legend.position = c(0, 0), legend.justification = c(-0.1, -0.1), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(1.5, &quot;cm&quot;)) + labs(x = &quot;Weeks Since Baseline&quot;, y = &quot;Estimated Hamilton Depression Score (HD)&quot;) Notice that in this model, all the BLUPs are parallel. That is because we are only letting the intercept vary from person-to-person while keeping the effect of time (slope) constant. Reproduce Table 4.4 on page 55 (Hedeker and Gibbons 2006) One way to judge a model is to compare the estimated means to the observed means to see how accuratedly they are represented by the model. This excellent fit of the estimated marginal means to the observed data supports the hypothesis that the change in depression across time is LINEAR. obs &lt;- data_long %&gt;% dplyr::group_by(week) %&gt;% dplyr::summarise(observed = mean(hamd, na.rm = TRUE)) effects::Effect(focal.predictors = &quot;week&quot;, mod = fit_lmer_week_RI_reml, xlevels = list(week = 0:5)) %&gt;% data.frame() %&gt;% dplyr::rename(estimated = fit) %&gt;% dplyr::left_join(obs, by = &quot;week&quot;) %&gt;% dplyr::select(week, observed, estimated) %&gt;% dplyr::mutate(diff = observed - estimated) %&gt;% pander::pander(caption = &quot;Observed and Estimated Means&quot;) Observed and Estimated Means week observed estimated diff 0 23 24 -0.11 1 22 21 0.67 2 18 19 -0.49 3 16 16 -0.01 4 14 14 -0.43 5 12 12 0.27 12.4.5 Intra-individual Correlation (ICC) performance::icc(fit_lmer_week_RI_reml) # Intraclass Correlation Coefficient Adjusted ICC: 0.463 Unadjusted ICC: 0.319 Interpretation Just less than a third (\\(\\rho_c = .32\\)) in baseline depression is explained by person-to-person differences. Thus, subjects display considerable heterogeneity in depression levels. This value of .46is an oversimplification of the correlation matrix above and may be thought of as the expected correlation between two randomly drawn weeks for any given person. performance::r2(fit_lmer_week_RI_reml) # R2 for Mixed Models Conditional R2: 0.629 Marginal R2: 0.310 Interpretation Linear growth accounts for 31% of the variance in Hamilton Depression Scores across the six weeks. Linear growth AND person-to-person differences account for a total 63% of this variance. This value of 46% is an oversimplification of the correlation matrix above and may be thought of as the expected correlation between two randomly drawn weeks for any given person. performance::r2(fit_lmer_week_RI_reml) # R2 for Mixed Models Conditional R2: 0.629 Marginal R2: 0.310 Note: The marginal \\(R^2\\) considers only the variance of the fixed effects, while the conditional \\(R^2\\) takes both the fixed and random effects into account. The random effect variances are actually the mean random effect variances, thus the \\(R^2\\) value is also appropriate for mixed models with random slopes or nested random effects (see Johnson 2014). Interpretation Linear growth accounts for 31% of the variance in Hamilton Depression Scores across the six weeks. Linear growth AND person-to-person differences account for a total 63% of this variance. Note: The marginal R^2 considers only the variance of the fixed effects, while the conditional R^2 takes both the fixed and random effects into account. The random effect variances are actually the mean random effect variances, thus the R^2 value is also appropriate for mixed models with random slopes or nested random effects (see Johnson 2014). 12.4.6 Compare to the Single-Level Null: No Random Effects Simple Linear Regression, (Hedeker and Gibbons 2006) To compare, fit the single level regression model fit_lm_week_ml &lt;- lm(hamd ~ week, data = data_long) 12.4.6.1 Table of Parameter Estimates texreg::screenreg(list(fit_lm_week_ml, fit_lmer_week_RI_reml), custom.model.names = c(&quot;Single-Level&quot;, &quot;Multilevel&quot;), single.row = TRUE, caption = &quot;MLM: Longitudinal Null Models&quot;, caption.above = TRUE, custom.note = &quot;The singel-level model treats are observations as being independent and unrelated to each other, even if they were made on the same person.&quot;) =========================================================== Single-Level Multilevel ———————————————————– (Intercept) 23.60 (0.55) *** 23.55 (0.64) week -2.41 (0.18) -2.38 (0.14) *** ———————————————————– R^2 0.32 Adj. R^2 0.32 Num. obs. 375 375 AIC 2294.73 BIC 2310.43 Log Likelihood -1143.36 Num. groups: id 66 Var: id (Intercept) 16.45 Var: Residual 19.10 =========================================================== The singel-level model treats are observations as being independent and unrelated to each other, even if they were made on the same person. For the multilevel model, the Wald tests indicated the fixed intercept is significant (no surprised that the depressions scores are not zero at baseline). More of note is the significance of the fixed effect of time. This signifies that depression scores are declining over time. On average, patients are improving (Hamilton Depression Scores get smaller) across time, by an average of 2.4’ish points a week. For the multilevel model, the Wald tests indicated the fixed intercept is significant (no surprised that the depressions scores are not zero at baseline). More of note is the significance of the fixed effect of time. This signifies that depression scores are declining over time. On average, patients are improving (Hamilton Depression Scores get smaller) across time, by an average of 2.4’ish points a week. 12.4.6.2 Residual Variance Note, the fixed estimates are very similar for the two models, but the standard errors are different. Additionally, whereas the single-level regression lumps all remaining variance together (\\(\\sigma^2\\)), the multilevel model seperates it into within-subjects (\\(\\sigma^2_{u0}\\) or \\(\\tau_{00}\\)) and between-subjects variance (\\(\\sigma^2_{e}\\) or \\(\\sigma^2\\)). sigma(fit_lm_week_ml)^2 [1] 35.3997 lme4::VarCorr(fit_lmer_week_RI_reml) %&gt;% # in longitudinal data, a group of observations = a participant or person print(comp = c(&quot;Variance&quot;, &quot;Std.Dev&quot;)) Groups Name Variance Std.Dev. id (Intercept) 16.446 4.0554 Residual 19.099 4.3703 “One statistician’s error term is another’s career!” (Hedeker and Gibbons 2006), page 56 12.5 MLM: Add Random Slope for Time (i.e. Trend) 12.5.1 Fit the Model fit_lmer_week_RIAS_reml &lt;- lmerTest::lmer(hamd ~ week + (week|id), # MLM-RIAS data = data_long, REML = TRUE) texreg::screenreg(list(fit_lmer_week_RI_reml, fit_lmer_week_RIAS_reml), custom.model.names = c(&quot;Random Intercepts&quot;, &quot; And Random Slopes&quot;), single.row = TRUE, caption = &quot;MLM: Null models fit w/REML&quot;, caption.above = TRUE, custom.note = &quot;Hedeker table 4.4 on page 55 and table 4.5 on page 58, except using REML here instead of ML&quot;) ================================================================== Random Intercepts And Random Slopes —————————————————————— (Intercept) 23.55 (0.64) *** 23.58 (0.55) week -2.38 (0.14) -2.38 (0.21) *** —————————————————————— AIC 2294.73 2231.92 BIC 2310.43 2255.48 Log Likelihood -1143.36 -1109.96 Num. obs. 375 375 Num. groups: id 66 66 Var: id (Intercept) 16.45 12.94 Var: Residual 19.10 12.21 Var: id week 2.13 Cov: id (Intercept) week -1.48 ================================================================== Hedeker table 4.4 on page 55 and table 4.5 on page 58, except using REML here instead of ML Visually, we can see that the unexplained or residual variance is less (12.21 vs 19.10) for the model that includes person-specific slopes (trajectories over time). Note: the negative covariance between random intercepts and random slopes (\\(\\sigma_{u01} = \\tau_{01} = -1.48\\)): “This suggests that patients who are initially more depressed (i.e. greater intercepts) improve at a greater rate (i.e. more pronounced negative slopes). An alternative explainatio, though,is that of a floor effect due to the HDRS rating scale. Simply put, patients with less depressed intitial scores have a more limited range of lower scores than those with higher initial scores.” (Hedeker and Gibbons 2006), page 58 12.5.2 Likelihood Ratio Test anova(fit_lmer_week_RI_reml, fit_lmer_week_RIAS_reml, model.names = c(&quot;RI&quot;, &quot;RIAS&quot;), refit = FALSE) %&gt;% pander::pander(caption = &quot;LRT: Assess Significance of Random Slopes&quot;) LRT: Assess Significance of Random Slopes   npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) RI 4 2295 2310 -1143 2287 NA NA NA RIAS 6 2232 2255 -1110 2220 67 2 0 Including the random slope for time significantly improved the model fit via the formal Likelihood Ratio Test. This rejects the assumption of compound symmetry. performance::compare_performance(fit_lmer_week_RI_reml, fit_lmer_week_RIAS_reml, rank = TRUE) # Comparison of Model Performance Indices Name | Model | R2 (cond.) | R2 (marg.) | ICC | RMSE | Sigma | AIC weights | AICc weights | BIC weights | Performance-Score ---------------------------------------------------------------------------------------------------------------------------------------------------------- fit_lmer_week_RIAS_reml | lmerModLmerTest | 0.769 | 0.302 | 0.669 | 2.993 | 3.495 | 1.000 | 1.000 | 1.000 | 87.50% fit_lmer_week_RI_reml | lmerModLmerTest | 0.629 | 0.310 | 0.463 | 4.031 | 4.370 | 3.20e-14 | 3.40e-14 | 1.63e-12 | 12.50% 12.5.3 Estimated Marginal Means Plot sjPlot::plot_model(fit_lmer_week_RIAS_reml, type = &quot;pred&quot;, terms = c(&quot;week&quot;)) Adding the random slopes didn’t change the estimates for the fixed effects much. 12.5.4 Estimated Marginal Means and Emperical Bayes Plots data_long %&gt;% dplyr::mutate(pred_fixed = predict(fit_lmer_week_RIAS_reml, re.form = NA, # fixed effects only newdata = .)) %&gt;% dplyr::mutate(pred_wrand = predict(fit_lmer_week_RIAS_reml, # fixed and random effects together newdata = .)) %&gt;% ggplot(aes(x = week, y = hamd, group = id)) + geom_line(aes(y = pred_wrand, color = &quot;BLUP&quot;, size = &quot;BLUP&quot;, linetype = &quot;BLUP&quot;)) + geom_line(aes(y = pred_fixed, color = &quot;Marginal&quot;, size = &quot;Marginal&quot;, linetype = &quot;Marginal&quot;)) + theme_bw() + scale_color_manual(name = &quot;Type of Prediction&quot;, values = c(&quot;BLUP&quot; = &quot;gray50&quot;, &quot;Marginal&quot; = &quot;blue&quot;)) + scale_size_manual(name = &quot;Type of Prediction&quot;, values = c(&quot;BLUP&quot; = .5, &quot;Marginal&quot; = 1.25)) + scale_linetype_manual(name = &quot;Type of Prediction&quot;, values = c(&quot;BLUP&quot; = &quot;longdash&quot;, &quot;Marginal&quot; = &quot;solid&quot;)) + theme(legend.position = c(0, 0), legend.justification = c(-0.1, -0.1), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(1.5, &quot;cm&quot;)) + labs(x = &quot;Weeks Since Baseline&quot;, y = &quot;Estimated Hamilton Depression Score&quot;) BLUPs are also refered to as Empirical Bayes Estimates and may be extracted from a model fit. In this cases there will be a specific intercept ((Intercept)) and time slope (week) for each individual or person (id). 12.5.4.1 Fixed Effects Marginal Model = within-subject effects fixef(fit_lmer_week_RIAS_reml) (Intercept) week 23.577044 -2.377047 12.5.4.2 Random Effects between-subjects effects ranef(fit_lmer_week_RIAS_reml)$id %&gt;% head() # only the first 6 participants (Intercept) week 101 1.0572022 -2.1151378 103 3.6707900 -0.4832479 104 2.6727551 -1.5008819 105 -3.0413391 0.2264496 106 0.3154240 1.0254750 107 -0.6148994 -0.4297385 12.5.4.3 BLUPs or Empirical Bayes Estimates fixed effects + random effects coef(fit_lmer_week_RIAS_reml)$id %&gt;% head() # only the first 6 participants (Intercept) week 101 24.63425 -4.492185 103 27.24783 -2.860295 104 26.24980 -3.877929 105 20.53571 -2.150598 106 23.89247 -1.351572 107 22.96214 -2.806786 We can create a scatterplot of these to see the correlation between them. coef(fit_lmer_week_RIAS_reml)$id %&gt;% ggplot(aes(x = week, y = `(Intercept)`)) + geom_point() + geom_hline(yintercept = fixef(fit_lmer_week_RIAS_reml)[&quot;(Intercept)&quot;], linetype = &quot;dashed&quot;) + geom_vline(xintercept = fixef(fit_lmer_week_RIAS_reml)[&quot;week&quot;], linetype = &quot;dashed&quot;) + geom_smooth(method = &quot;lm&quot;) + labs(title = &quot;Hedeker&#39;s Figure 4.4 on page 59&quot;, subtitle = &quot;Reisby data: Estimated random effects&quot;, x = &quot;Week Change in Depression&quot;, y = &quot;Baseline Depression Level&quot;) + theme_bw() 12.6 MLM: Coding of Time So far we have used the variable week to denote time as weeks since baseline = week \\(\\in 0, 1, 2, 3, 4, 5\\). But We could CENTER week at the middle of the study (week = 2.5). 12.6.1 Fit the Model fit_lmer_week_RIAS_reml_wc &lt;- lme4::lmer(hamd ~ I(week-2.5) + (I(week-2.5)|id), # MLM-RIAS data = data_long, REML = TRUE) 12.6.2 Table of Parameter Estimates texreg::screenreg(list(fit_lmer_week_RIAS_reml, fit_lmer_week_RIAS_reml_wc), custom.model.names = c(&quot;Random Intercepts&quot;, &quot; And Random Slopes&quot;), single.row = TRUE, caption = &quot;MLM: Null models fit w/REML&quot;, caption.above = TRUE, custom.note = &quot;Hedeker table table 4.5 on page 58 and table 4.6 on page 61, except using REML here instead of ML&quot;) =========================================================================== Random Intercepts And Random Slopes ————————————————————————— (Intercept) 23.58 (0.55) *** 17.63 (0.56) week -2.38 (0.21) week - 2.5 -2.38 (0.21) *** ————————————————————————— AIC 2231.92 2231.92 BIC 2255.48 2255.48 Log Likelihood -1109.96 -1109.96 Num. obs. 375 375 Num. groups: id 66 66 Var: id (Intercept) 12.94 18.85 Var: id week 2.13 Cov: id (Intercept) week -1.48 Var: Residual 12.21 12.21 Var: id I(week - 2.5) 2.13 Cov: id (Intercept) I(week - 2.5) 3.84 =========================================================================== Hedeker table table 4.5 on page 58 and table 4.6 on page 61, except using REML here instead of ML Unchanged model fit: AIC, BIC, -2LL, residual variance fixed effect of week variance for random intercepts Changed fixed intercept variance for random slopes covariance between random intercepts and random slopes model fit: AIC, BIC, -2LL, residual variance fixed effect of week variance for random intercepts Changed fixed intercept variance for random slopes covariance between random intercepts and random slopes 12.6.3 Estimated Marginal Means and Emperical Bayes Plots data_long %&gt;% dplyr::mutate(pred_fixed = predict(fit_lmer_week_RIAS_reml_wc, re.form = NA, newdata = .)) %&gt;% # fixed effects only dplyr::mutate(pred_wrand = predict(fit_lmer_week_RIAS_reml_wc, newdata = .)) %&gt;% # fixed and random effects together ggplot(aes(x = week, y = hamd, group = id)) + geom_line(aes(y = pred_wrand, color = &quot;BLUP&quot;, size = &quot;BLUP&quot;, linetype = &quot;BLUP&quot;)) + geom_line(aes(y = pred_fixed, color = &quot;Marginal&quot;, size = &quot;Marginal&quot;, linetype = &quot;Marginal&quot;)) + theme_bw() + scale_color_manual(name = &quot;Type of Prediction&quot;, values = c(&quot;BLUP&quot; = &quot;gray50&quot;, &quot;Marginal&quot; = &quot;blue&quot;)) + scale_size_manual(name = &quot;Type of Prediction&quot;, values = c(&quot;BLUP&quot; = .5, &quot;Marginal&quot; = 1.25)) + scale_linetype_manual(name = &quot;Type of Prediction&quot;, values = c(&quot;BLUP&quot; = &quot;longdash&quot;, &quot;Marginal&quot; = &quot;solid&quot;)) + theme(legend.position = c(0, 0), legend.justification = c(-0.1, -0.1), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(1.5, &quot;cm&quot;)) + labs(x = &quot;Weeks Since Baseline&quot;, y = &quot;Estimated Hamilton Depression Score (HD)&quot;) Again, centering time doesn’t change the interpretation at all, since there are no interactions. 12.7 MLM: Effect of DIagnosis on Time Trends (Fixed Interaction) The researcher specifically wants to know if the trajectory over time differs for the two types of depression. This translates into a fixed effects interaction between time and group. Start by comapring random intercepts only (RI) to a random intercetps and slopes (RIAS) model. 12.7.1 Fit the Models fit_lmer_week_RIAS_ml &lt;- lmerTest::lmer(hamd ~ week + (week|id), data = data_long, REML = FALSE) fit_lmer_wkdx_RIAS_ml &lt;- lmerTest::lmer(hamd ~ week*endog + (week|id), data = data_long, REML = FALSE) 12.7.2 Estimated Marginal Meanse Plot interactions::interact_plot(fit_lmer_wkdx_RIAS_ml, pred = week, modx = endog, legend.main = &quot;Type of Depression&quot;, interval = TRUE, main.title = &quot;Hedeker&#39;s Table 4.7 on page 64&quot;) + theme_bw() + labs(x = &quot;Weeks Since Baseline&quot;, y = &quot;Estimated Marginal Mean\\nHamilton Depression Scores (HD)&quot;) + theme(legend.background = element_rect(color = &quot;black&quot;), legend.position = c(0, 0), legend.justification = c(-0.1, -0.1), legend.key.width = unit(1.75, &quot;cm&quot;)) 12.7.3 Likelihood Ratio Test anova(fit_lmer_week_RIAS_ml, fit_lmer_wkdx_RIAS_ml, model.names = c(&quot;Just Time&quot;, &quot;Time X Dx&quot;)) %&gt;% pander::pander(caption = &quot;LRT: Assess Significance of Diagnosis Moderation of Trend&quot;) LRT: Assess Significance of Diagnosis Moderation of Trend   npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) Just Time 6 2231 2255 -1110 2219 NA NA NA Time X Dx 8 2231 2262 -1107 2215 4.1 2 0.13 performance::compare_performance(fit_lmer_week_RIAS_ml, fit_lmer_wkdx_RIAS_ml, rank = TRUE) # Comparison of Model Performance Indices Name | Model | R2 (cond.) | R2 (marg.) | ICC | RMSE | Sigma | AIC weights | AICc weights | BIC weights | Performance-Score -------------------------------------------------------------------------------------------------------------------------------------------------------- fit_lmer_week_RIAS_ml | lmerModLmerTest | 0.767 | 0.305 | 0.665 | 2.999 | 3.495 | 0.486 | 0.507 | 0.980 | 62.50% fit_lmer_wkdx_RIAS_ml | lmerModLmerTest | 0.768 | 0.324 | 0.656 | 3.005 | 3.495 | 0.514 | 0.493 | 0.020 | 37.50% The more complicated model (including the moderating effect of diagnosis) is NOT supported. The more complicated model (including the moderating effect of diagnosis) is NOT supported. 12.8 MLM: Quadratic Trend 12.8.1 Fit the Model fit_lmer_quad_RIAS_ml &lt;- lme4::lmer(hamd ~ week + I(week^2) + (week+ I(week^2)|id), data = data_long, REML = FALSE, control = lmerControl(optimizer = &quot;optimx&quot;, # get it to converge calc.derivs = FALSE, optCtrl = list(method = &quot;nlminb&quot;, starttests = FALSE, kkt = FALSE))) 12.8.2 Table of Parameter Estimates texreg::screenreg(list(fit_lmer_week_RIAS_ml, fit_lmer_quad_RIAS_ml), custom.model.names = c(&quot;Linear Trend&quot;, &quot;QUadratic Trend&quot;), single.row = TRUE, caption = &quot;MLM: RIAS models fit w/ML&quot;, caption.above = TRUE, custom.note = &quot;Hedeker table 4.5 on page 58 and table 5.1 on page 84&quot;) ======================================================================= Linear Trend QUadratic Trend ———————————————————————– (Intercept) 23.58 (0.55) *** 23.76 (0.55) week -2.38 (0.21) -2.63 (0.48) *** week^2 0.05 (0.09) ———————————————————————– AIC 2231.04 2227.65 BIC 2254.60 2266.92 Log Likelihood -1109.52 -1103.82 Num. obs. 375 375 Num. groups: id 66 66 Var: id (Intercept) 12.63 10.44 Var: id week 2.08 6.64 Cov: id (Intercept) week -1.42 -0.92 Var: Residual 12.22 10.52 Var: id I(week^2) 0.19 Cov: id (Intercept) I(week^2) -0.11 Cov: id week I(week^2) -0.94 ======================================================================= Hedeker table 4.5 on page 58 and table 5.1 on page 84 12.8.3 Likelihood Ratio Test anova(fit_lmer_week_RIAS_ml, fit_lmer_quad_RIAS_ml) Data: data_long Models: fit_lmer_week_RIAS_ml: hamd ~ week + (week | id) fit_lmer_quad_RIAS_ml: hamd ~ week + I(week^2) + (week + I(week^2) | id) npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) fit_lmer_week_RIAS_ml 6 2231.0 2254.6 -1109.5 2219.0 fit_lmer_quad_RIAS_ml 10 2227.7 2266.9 -1103.8 2207.7 11.39 4 0.02252 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 performance::compare_performance(fit_lmer_week_RIAS_ml, fit_lmer_quad_RIAS_ml, rank = TRUE) # Comparison of Model Performance Indices Name | Model | R2 (cond.) | R2 (marg.) | ICC | RMSE | Sigma | AIC weights | AICc weights | BIC weights | Performance-Score -------------------------------------------------------------------------------------------------------------------------------------------------------- fit_lmer_quad_RIAS_ml | lmerMod | 0.799 | 0.306 | 0.710 | 2.656 | 3.243 | 0.845 | 0.819 | 0.002 | 87.50% fit_lmer_week_RIAS_ml | lmerModLmerTest | 0.767 | 0.305 | 0.665 | 2.999 | 3.495 | 0.155 | 0.181 | 0.998 | 12.50% Even though the Wald test did not find the quadratic fixed time trend to be significant at the population level (marginal), the LRT and Bayes Factor both find that including the quadratic terms improves the model’s fit. 12.8.4 Estimated Marginal Means Plot fixef(fit_lmer_quad_RIAS_ml) (Intercept) week I(week^2) 23.7602494 -2.6325756 0.0514812 sjPlot::plot_model(fit_lmer_quad_RIAS_ml, type = &quot;pred&quot;, terms = &quot;week&quot;) At the population level, the curviture is very slight. 12.8.5 BLUPs or Emperical Bayes Estimates coef(fit_lmer_quad_RIAS_ml)$id (Intercept) week I(week^2) 101 25.16570 -5.28322240 0.150800674 103 27.50748 -3.43493956 0.119896419 104 25.99803 -3.09403830 -0.176872891 105 21.01142 -2.95259656 0.164659508 106 23.64345 -0.74433445 -0.141870771 107 22.70271 -1.98455785 -0.179103989 108 22.40954 -4.72427651 0.316018682 113 22.70933 -0.40479722 -0.023813901 114 21.71498 -4.51593943 0.305503156 115 21.54596 -2.55733919 0.139466228 117 20.68537 -3.87699771 0.196724980 118 25.37073 -2.57669407 -0.047611415 120 21.54364 -2.02854721 0.175309140 121 22.58017 -1.75375731 -0.038804946 123 19.25420 -2.87311398 0.017541231 302 21.58631 -3.89088841 0.294414722 303 22.46072 -3.40753058 0.044346959 304 24.67540 -0.37917299 -0.166164154 305 21.27400 -3.68029144 -0.010285277 308 23.09149 -2.47011411 0.005956521 309 22.90664 -1.55837647 -0.062499088 310 23.05814 -5.82990155 0.392499635 311 21.32623 -1.62724378 0.056406903 312 20.97723 -0.80434902 -0.285843319 313 21.61301 -4.45108620 0.355814498 315 25.23223 -4.58345009 0.080217902 316 27.75572 -1.45600475 0.069630733 318 20.56468 -0.29405182 -0.235309874 319 22.38191 -4.51711991 0.466348405 322 24.93395 1.26791161 -0.042516486 327 19.82905 -3.17595117 0.466782622 328 23.74555 1.29273058 -0.129018797 331 21.92609 -1.83148914 -0.074416221 333 23.11844 -0.64672472 -0.127987413 334 27.19417 -6.33068103 0.497614736 335 22.72435 -2.57422424 0.010287299 337 25.62004 -2.06606297 -0.118350517 338 22.89845 -0.54279462 -0.095537656 339 24.27807 -4.99290658 0.444972811 344 22.43030 -4.34890283 0.557804359 345 27.22532 -1.03851709 -0.044991723 346 24.66105 -2.09386084 0.138720903 347 20.11423 -3.85553234 0.239870351 348 23.42449 -4.05934676 0.152402219 349 20.49506 -3.30475825 0.269601249 350 23.29832 -3.86555727 0.351772013 351 27.85603 -2.54763708 -0.174396178 352 21.86125 -2.47215139 0.305548813 353 25.44900 -1.25790363 -0.261970988 354 26.94737 0.07803738 -0.289824519 355 24.47061 -2.72958140 -0.141017454 357 25.37351 -0.82942969 -0.114172086 360 24.04647 1.73527936 -0.131113157 361 25.48492 -7.37199852 0.953498020 501 27.83195 -1.46848824 -0.003539941 502 22.99362 -4.51402723 0.038610802 504 20.80201 -2.67922623 0.167674789 505 20.96546 -6.31343151 0.490343064 507 26.25982 0.08284389 -0.277546320 603 25.55712 -3.23921928 0.099756125 604 26.04326 -6.06913362 0.284971682 606 24.47426 -3.73162024 -0.178376508 607 31.08955 1.15553930 -1.091093331 608 23.46558 -4.87963779 0.180136913 609 25.91657 -2.23143989 -0.347099437 610 30.62475 -0.54536532 -0.593017239 For Illustration, two cases have been hand selected: id = 115 and 610. fun_115 &lt;- function(week){ coef(fit_lmer_quad_RIAS_ml)$id[&quot;115&quot;, &quot;(Intercept)&quot;] + coef(fit_lmer_quad_RIAS_ml)$id[&quot;115&quot;, &quot;week&quot;] * week + coef(fit_lmer_quad_RIAS_ml)$id[&quot;115&quot;, &quot;I(week^2)&quot;] * week^2 } fun_610 &lt;- function(week){ coef(fit_lmer_quad_RIAS_ml)$id[&quot;610&quot;, &quot;(Intercept)&quot;] + coef(fit_lmer_quad_RIAS_ml)$id[&quot;610&quot;, &quot;week&quot;] * week + coef(fit_lmer_quad_RIAS_ml)$id[&quot;610&quot;, &quot;I(week^2)&quot;] * week^2 } data_long %&gt;% dplyr::mutate(pred_fixed = predict(fit_lmer_quad_RIAS_ml, re.form = NA, newdata = .)) %&gt;% # fixed effects only dplyr::mutate(pred_wrand = predict(fit_lmer_quad_RIAS_ml, newdata = .)) %&gt;% # fixed and random effects together ggplot(aes(x = week, y = hamd, group = id)) + stat_function(fun = fun_115) + # add cure for ID = 115 stat_function(fun = fun_610) + # add cure for ID = 610 geom_line(aes(y = pred_fixed), color = &quot;blue&quot;, size = 1.25) + theme_bw() + theme(legend.position = c(0, 0), legend.justification = c(-0.1, -0.1), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(1.5, &quot;cm&quot;)) + labs(x = &quot;Weeks Since Baseline&quot;, y = &quot;Estimated Hamilton Depression Scores (HD)&quot;, title = &quot;Similar to Hedeker&#39;s Figure 5.3 on page 84&quot;, subtitle = &quot;Marginal Mean show in thicker blue\\nBLUPs for two of the participant in thinner black&quot;) Figure 12.1: Two Example BLUPS for two different participants These two individuals have quite different curvatures and illustrated how this type of curvatures in person-specific trajectories may end up cancelling each other out to arrive at a fairly linear marginal model. 12.8.6 Estimated Marginal Means and Emperical Bayes Plots Note: although the BLUPs are shown for all participants, the predictions are just connects and are therefore slightly jagged and now smoother like the lines on the plot above. data_long %&gt;% dplyr::mutate(pred_fixed = predict(fit_lmer_quad_RIAS_ml, re.form = NA, newdata = .)) %&gt;% # fixed effects only dplyr::mutate(pred_wrand = predict(fit_lmer_quad_RIAS_ml, newdata = .)) %&gt;% # fixed and random effects together ggplot(aes(x = week, y = hamd, group = id)) + geom_line(aes(y = pred_wrand, color = &quot;BLUP&quot;, size = &quot;BLUP&quot;, linetype = &quot;BLUP&quot;)) + geom_line(aes(y = pred_fixed, color = &quot;Marginal&quot;, size = &quot;Marginal&quot;, linetype = &quot;Marginal&quot;)) + theme_bw() + scale_color_manual(name = &quot;Type of Prediction&quot;, values = c(&quot;BLUP&quot; = &quot;gray50&quot;, &quot;Marginal&quot; = &quot;blue&quot;)) + scale_size_manual(name = &quot;Type of Prediction&quot;, values = c(&quot;BLUP&quot; = .5, &quot;Marginal&quot; = 1.25)) + scale_linetype_manual(name = &quot;Type of Prediction&quot;, values = c(&quot;BLUP&quot; = &quot;longdash&quot;, &quot;Marginal&quot; = &quot;solid&quot;)) + theme(legend.position = c(0, 0), legend.justification = c(-0.1, -0.1), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(1.5, &quot;cm&quot;)) + labs(x = &quot;Weeks Since Baseline&quot;, y = &quot;Estimated Hamilton Depression Scores&quot;, title = &quot;Hedeker&#39;s Figure 5.4 on page 85&quot;) Figure 12.2: EStimated curvilinear trends At the person-level, the curvature is very diverse (heterogeneous). Some individuals have accelerating downward tend while other have accelerating upward trends. The improvement that the curvi-linear model provides in describing change across time is perhaps modest. "],["mlm-longitudinal-hox-ch-5---student-gpa.html", "13 MLM, Longitudinal: Hox ch 5 - student GPA 13.1 Background 13.2 MLM", " 13 MLM, Longitudinal: Hox ch 5 - student GPA library(tidyverse) # all things tidy library(haven) library(pander) # nice looking genderal tabulations library(furniture) # nice Table1() descriptives library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(psych) # contains some useful functions, like headTail library(sjstats) # ICC calculations library(sjPlot) # Visualization for Models library(lme4) # non-linear mixed-effects models library(lmerTest) library(effects) # Effect displays for Models library(effectsize) library(interactions) library(performance) 13.1 Background The text “Multilevel Analysis: Techniques and Applications, Third Edition” (Hox, Moerbeek, and Van de Schoot 2017) has a companion website which includes links to all the data files used throughout the book (housed on the book’s GitHub repository). The following example is used through out Hox, Moerbeek, and Van de Schoot (2017)’s chapater 5. The GPA for 200 college students were followed for 6 consecutive semesters (simulated). Job status was also measured as number of hours worked for the same size occations. Time-invariant covariates are the student’s gender and high school GPA. The variable admitted will not be used. data_raw &lt;- haven::read_sav(&quot;https://github.com/MultiLevelAnalysis/Datasets-third-edition-Multilevel-book/raw/master/chapter%205/GPA2/gpa2long.sav&quot;) %&gt;% haven::as_factor() # retain the labels from SPSS --&gt; factor tibble::glimpse(data_raw) Rows: 1,200 Columns: 7 $ student &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4… $ occas &lt;fct&gt; year 1 semester 1, year 1 semester 2, year 2 semester 1, year… $ gpa &lt;dbl&gt; 2.3, 2.1, 3.0, 3.0, 3.0, 3.3, 2.2, 2.5, 2.6, 2.6, 3.0, 2.8, 2… $ job &lt;fct&gt; 2 hours, 2 hours, 2 hours, 2 hours, 2 hours, 2 hours, 2 hours… $ sex &lt;fct&gt; female, female, female, female, female, female, male, male, m… $ highgpa &lt;dbl&gt; 2.8, 2.8, 2.8, 2.8, 2.8, 2.8, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2… $ admitted &lt;fct&gt; yes, yes, yes, yes, yes, yes, no, no, no, no, no, no, yes, ye… data_raw %&gt;% dplyr::select(occas, job) %&gt;% table(useNA = &quot;always&quot;) job occas no job 1 hour 2 hours 3 hours 4 or more hours &lt;NA&gt; year 1 semester 1 0 0 172 28 0 0 year 1 semester 2 0 0 169 31 0 0 year 2 semester 1 0 7 159 34 0 0 year 2 semester 2 0 5 169 26 0 0 year 3 semester 1 0 18 150 32 0 0 year 3 semester 2 0 22 148 30 0 0 &lt;NA&gt; 0 0 0 0 0 0 data_long &lt;- data_raw %&gt;% dplyr::mutate(student = factor(student)) %&gt;% dplyr::mutate(sem = case_when(occas == &quot;year 1 semester 1&quot; ~ 1, occas == &quot;year 1 semester 2&quot; ~ 2, occas == &quot;year 2 semester 1&quot; ~ 3, occas == &quot;year 2 semester 2&quot; ~ 4, occas == &quot;year 3 semester 1&quot; ~ 5, occas == &quot;year 3 semester 2&quot; ~ 6)) %&gt;% dplyr::mutate(job = fct_drop(job)) %&gt;% dplyr::mutate(hrs = case_when(job == &quot;no job&quot; ~ 0, job == &quot;1 hour&quot; ~ 1, job == &quot;2 hours&quot; ~ 2, job == &quot;3 hours&quot; ~ 3, job == &quot;4 or more hours&quot; ~ 4)) %&gt;% dplyr::select(student, sex, highgpa, sem, job, hrs, gpa) %&gt;% dplyr::arrange(student, sem) psych::headTail(data_long, top = 10) student sex highgpa sem job hrs gpa 1 1 female 2.8 1 2 hours 2 2.3 2 1 female 2.8 2 2 hours 2 2.1 3 1 female 2.8 3 2 hours 2 3 4 1 female 2.8 4 2 hours 2 3 5 1 female 2.8 5 2 hours 2 3 6 1 female 2.8 6 2 hours 2 3.3 7 2 male 2.5 1 2 hours 2 2.2 8 2 male 2.5 2 3 hours 3 2.5 9 2 male 2.5 3 2 hours 2 2.6 10 2 male 2.5 4 2 hours 2 2.6 11 &lt;NA&gt; &lt;NA&gt; ... ... &lt;NA&gt; ... ... 12 200 male 3.4 3 2 hours 2 3.4 13 200 male 3.4 4 2 hours 2 3.5 14 200 male 3.4 5 1 hour 1 3.3 15 200 male 3.4 6 1 hour 1 3.4 data_wide &lt;- data_long %&gt;% tidyr::pivot_wider(names_from = sem, values_from = c(job, hrs, gpa), names_sep = &quot;_&quot;) psych::headTail(data_wide) student sex highgpa job_1 job_2 job_3 job_4 job_5 job_6 hrs_1 1 1 female 2.8 2 hours 2 hours 2 hours 2 hours 2 hours 2 hours 2 2 2 male 2.5 2 hours 3 hours 2 hours 2 hours 2 hours 2 hours 2 3 3 female 2.5 2 hours 2 hours 2 hours 3 hours 2 hours 2 hours 2 4 4 male 3.8 3 hours 2 hours 2 hours 2 hours 2 hours 2 hours 3 5 &lt;NA&gt; &lt;NA&gt; ... &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ... 6 197 female 2.1 2 hours 2 hours 3 hours 2 hours 2 hours 2 hours 2 7 198 male 4 2 hours 2 hours 2 hours 2 hours 1 hour 2 hours 2 8 199 female 2.3 2 hours 2 hours 3 hours 2 hours 2 hours 3 hours 2 9 200 male 3.4 2 hours 2 hours 2 hours 2 hours 1 hour 1 hour 2 hrs_2 hrs_3 hrs_4 hrs_5 hrs_6 gpa_1 gpa_2 gpa_3 gpa_4 gpa_5 gpa_6 1 2 2 2 2 2 2.3 2.1 3 3 3 3.3 2 3 2 2 2 2 2.2 2.5 2.6 2.6 3 2.8 3 2 2 3 2 2 2.4 2.9 3 2.8 3.3 3.4 4 2 2 2 2 2 2.5 2.7 2.4 2.7 2.9 2.7 5 ... ... ... ... ... ... ... ... ... ... ... 6 2 3 2 2 2 2.9 2.5 2.8 3.2 3.3 3.8 7 2 2 2 1 2 2.5 2.9 3 3.2 3.3 3.4 8 2 3 2 2 3 2.6 2.3 2.7 2.7 2.8 2.8 9 2 2 2 1 1 2.8 3.2 3.4 3.5 3.3 3.4 data_wide %&gt;% dplyr::group_by(sex) %&gt;% furniture::table1(&quot;High School GPA&quot; = highgpa, &quot;Initial College GPA&quot; = gpa_1, &quot;Initial Job&quot; = job_1, &quot;Initial Hrs&quot; = hrs_1, output = &quot;markdown&quot;, digits = 3, total = TRUE, test = TRUE) Total male female P-Value n = 200 n = 95 n = 105 High School GPA 0.31 2.987 (0.596) 3.033 (0.592) 2.947 (0.600) Initial College GPA 0.094 2.594 (0.312) 2.555 (0.314) 2.629 (0.307) Initial Job 0.192 1 hour 0 (0%) 0 (0%) 0 (0%) 2 hours 172 (86%) 78 (82.1%) 94 (89.5%) 3 hours 28 (14%) 17 (17.9%) 11 (10.5%) Initial Hrs 0.132 2.140 (0.348) 2.179 (0.385) 2.105 (0.308) data_wide %&gt;% dplyr::group_by(sex) %&gt;% furniture::table1(gpa_1, gpa_2, gpa_3, gpa_4, gpa_5, gpa_6, output = &quot;markdown&quot;, digits = 3, total = TRUE, test = TRUE, caption = &quot;Hox Table 5.2 (page 77) GPA means at six occations, for male and female students&quot;, caption.above = TRUE) Table 13.1: Hox Table 5.2 (page 77) GPA means at six occations, for male and female students Total male female n = 200 n = 95 n = 105 gpa_1 2.594 (0.312) 2.555 (0.314) 2.629 (0.307) gpa_2 2.716 (0.336) 2.666 (0.318) 2.760 (0.348) gpa_3 2.810 (0.354) 2.742 (0.363) 2.871 (0.335) gpa_4 2.918 (0.355) 2.818 (0.351) 3.009 (0.335) gpa_5 3.019 (0.358) 2.915 (0.359) 3.113 (0.332) gpa_6 3.134 (0.377) 3.028 (0.375) 3.230 (0.354) 13.2 MLM 13.2.1 Null Models and ICC fit_lmer_0_re &lt;- lmerTest::lmer(gpa ~ 1 + (1|student), data = data_long, REML = TRUE) performance::icc(fit_lmer_0_re) # Intraclass Correlation Coefficient Adjusted ICC: 0.369 Unadjusted ICC: 0.369 Over a third of the variance in the 6 GPA measures is variance between individuals, and about two-thirds is variance within individuals across time, \\(\\rho = .369\\). fit_lmer_1_re &lt;- lmerTest::lmer(gpa ~ I(sem - 1) + (1|student), data = data_long, REML = TRUE) performance::icc(fit_lmer_1_re) # Intraclass Correlation Coefficient Adjusted ICC: 0.523 Unadjusted ICC: 0.412 After accounting for the linear change in GPA over semesters, about half of the remaining variance in GPA scores is attribuTable person-to-person difference. Another way to say it, is about half of the variance in initial GPAs is due to student-to-student differences. 13.2.2 Fixed Effects fit_lmer_0_ml &lt;- lmerTest::lmer(gpa ~ 1 + (1|student), data = data_long, REML = FALSE) fit_lmer_1_ml &lt;- lmerTest::lmer(gpa ~ I(sem - 1) + (1|student), data = data_long, REML = FALSE) fit_lmer_2_ml &lt;- lmerTest::lmer(gpa ~ I(sem - 1) + hrs + (1|student), data = data_long, REML = FALSE) fit_lmer_3_ml &lt;- lmerTest::lmer(gpa ~ I(sem - 1) + hrs + highgpa + sex + (1|student), data = data_long, REML = FALSE) texreg::knitreg(list(fit_lmer_0_ml, fit_lmer_1_ml, fit_lmer_2_ml, fit_lmer_3_ml)) Statistical models   Model 1 Model 2 Model 3 Model 4 (Intercept) 2.87*** 2.60*** 2.97*** 2.64***   (0.02) (0.02) (0.04) (0.10) sem - 1   0.11*** 0.10*** 0.10***     (0.00) (0.00) (0.00) hrs     -0.17*** -0.17***       (0.02) (0.02) highgpa       0.08**         (0.03) sexfemale       0.15***         (0.03) AIC 919.46 401.65 318.40 296.76 BIC 934.73 422.01 343.85 332.39 Log Likelihood -456.73 -196.82 -154.20 -141.38 Num. obs. 1200 1200 1200 1200 Num. groups: student 200 200 200 200 Var: student (Intercept) 0.06 0.06 0.05 0.04 Var: Residual 0.10 0.06 0.06 0.06 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 texreg::knitreg(list(fit_lmer_0_ml, fit_lmer_1_ml, fit_lmer_2_ml, fit_lmer_3_ml), custom.model.names = c(&quot;M1: Null&quot;, &quot;M2: Occ&quot;, &quot;M3: Job&quot;, &quot;M4: GPA, Sex&quot;), custom.coef.map = list(&quot;(Intercept)&quot; = &quot;(Intercept)&quot;, &quot;I(sem - 1)&quot; = &quot;Semester&quot;, &quot;hrs&quot; = &quot;Hours Working&quot;, &quot;highgpa&quot; = &quot;High School GPA&quot;, &quot;sexfemale&quot; = &quot;Female vs. Male&quot;), groups = list(&quot;Level 1 Main Effects, Occasion-Specific&quot; = 2:3, &quot;Level 2 Main Effects, Person-Specific&quot; = 4:5), custom.note = &quot;%stars. \\nNote: Intercept refers to population mean for a Male who is not working during their first semester&quot;, caption = &quot;Hox Table 5.3 (page 78) Results of Multilevel Anlaysis of GPA, Fixed Effects&quot;, caption.above = TRUE, digits = 3) Hox Table 5.3 (page 78) Results of Multilevel Anlaysis of GPA, Fixed Effects   M1: Null M2: Occ M3: Job M4: GPA, Sex (Intercept) 2.865*** 2.599*** 2.970*** 2.641***   (0.019) (0.022) (0.044) (0.098) Level 1 Main Effects, Occasion-Specific                        Semester   0.106*** 0.102*** 0.102***     (0.004) (0.004) (0.004)      Hours Working     -0.171*** -0.172***       (0.018) (0.018) Level 2 Main Effects, Person-Specific                        High School GPA       0.085**         (0.028)      Female vs. Male       0.147***         (0.033) AIC 919.456 401.649 318.399 296.760 BIC 934.726 422.009 343.849 332.390 Log Likelihood -456.728 -196.825 -154.200 -141.380 Num. obs. 1200 1200 1200 1200 Num. groups: student 200 200 200 200 Var: student (Intercept) 0.057 0.063 0.052 0.045 Var: Residual 0.098 0.058 0.055 0.055 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05. Note: Intercept refers to population mean for a Male who is not working during their first semester anova(fit_lmer_0_ml, fit_lmer_1_ml, fit_lmer_2_ml, fit_lmer_3_ml) Data: data_long Models: fit_lmer_0_ml: gpa ~ 1 + (1 | student) fit_lmer_1_ml: gpa ~ I(sem - 1) + (1 | student) fit_lmer_2_ml: gpa ~ I(sem - 1) + hrs + (1 | student) fit_lmer_3_ml: gpa ~ I(sem - 1) + hrs + highgpa + sex + (1 | student) npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) fit_lmer_0_ml 3 919.46 934.73 -456.73 913.46 fit_lmer_1_ml 4 401.65 422.01 -196.82 393.65 519.807 1 &lt; 2.2e-16 *** fit_lmer_2_ml 5 318.40 343.85 -154.20 308.40 85.250 1 &lt; 2.2e-16 *** fit_lmer_3_ml 7 296.76 332.39 -141.38 282.76 25.639 2 2.707e-06 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 13.2.3 Variance Explained by linear TIME at Level ONE lme4::VarCorr(fit_lmer_0_ml) # baseline Groups Name Std.Dev. student (Intercept) 0.23826 Residual 0.31239 lme4::VarCorr(fit_lmer_1_ml) # model to compare Groups Name Std.Dev. student (Intercept) 0.25172 Residual 0.24090 lme4::VarCorr(fit_lmer_0_ml) %&gt;% # baseline print(comp = c(&quot;Variance&quot;, &quot;Std.Dev&quot;), digits = 3) Groups Name Variance Std.Dev. student (Intercept) 0.0568 0.238 Residual 0.0976 0.312 lme4::VarCorr(fit_lmer_1_ml) %&gt;% # model to compare print(comp = c(&quot;Variance&quot;, &quot;Std.Dev&quot;), digits = 3) Groups Name Variance Std.Dev. student (Intercept) 0.0634 0.252 Residual 0.0580 0.241 13.2.3.1 Raudenbush and Bryk Explained variance is a proportion of first-level variance only A good option when the multilevel sampling process is is close to two-stage simple random sampling Raudenbush and Bryk Approximate Formula - Level 1 approximate \\[ approx \\;R^2_1 = \\frac{\\sigma^2_{e-BL} - \\sigma^2_{e-MC}} {\\sigma^2_{e-BL} } \\tag{Hox 4.8} \\] (0.098 - 0.058) / 0.098 [1] 0.4081633 13.2.3.2 Snijders and Bosker Snijders and Bosker Formula - Level 1 Random Intercepts Models Only, address potential negative \\(R^2\\) issue \\[ R^2_1 = 1 - \\frac{\\sigma^2_{e-MC} + \\sigma^2_{u0-MC}} {\\sigma^2_{e-BL} + \\sigma^2_{u0-BL}} \\] 1 - (0.058 + 0.063)/(0.098 + 0.057) [1] 0.2193548 13.2.4 Variance Explained by linear TIME at Level TWO 13.2.4.1 Raudenbush and Bryk Raudenbush and Bryk Approximate Formula - Level 2 \\[ approx \\; R^2_s = \\frac{\\sigma^2_{u0-BL} - \\sigma^2_{u0-MC}} {\\sigma^2_{u0-BL} } \\tag{Hox 4.9} \\] (0.057 - 0.063)/ 0.057 [1] -0.1052632 YIKES! Negative Variance explained! 13.2.4.2 Snijders and Bosker Snijders and Bosker Formula Extended - Level 2 \\[ R^2_2 = 1 - \\frac{\\frac{\\sigma^2_{e-MC}}{B} + \\sigma^2_{u0-MC}} {\\frac{\\sigma^2_{e-BL}}{B} + \\sigma^2_{u0-BL}} \\] \\(B\\) is the average size of the Level 2 units. Technically, you should use the harmonic mean, but unless the clusters differ greatly in size, it doesn’t make a huge difference. 1 - (0.058/6 + 0.063) / (0.098/6 + 0.057) [1] 0.009090909 Reason: The intercept only model overestimates the variance at the occasion level and underestimates the variance at the subject level (see chapter 4 of Hox, Moerbeek, and Van de Schoot (2017)) 13.2.5 Random Effects fit_lmer_3_re &lt;- lmerTest::lmer(gpa ~ I(sem-1) + hrs + highgpa + sex + (1|student), data = data_long, REML = TRUE) fit_lmer_4_re &lt;- lmerTest::lmer(gpa ~ I(sem-1) + hrs + highgpa + sex + (I(sem-1)|student), data = data_long, REML = TRUE, control = lmerControl(optimizer =&quot;Nelder_Mead&quot;)) anova(fit_lmer_3_re, fit_lmer_4_re, refit = FALSE) Data: data_long Models: fit_lmer_3_re: gpa ~ I(sem - 1) + hrs + highgpa + sex + (1 | student) fit_lmer_4_re: gpa ~ I(sem - 1) + hrs + highgpa + sex + (I(sem - 1) | student) npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) fit_lmer_3_re 7 328.84 364.47 -157.42 314.84 fit_lmer_4_re 9 219.93 265.75 -100.97 201.94 112.9 2 &lt; 2.2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 13.2.6 Cross-Level Interaction fit_lmer_4_ml &lt;- lmerTest::lmer(gpa ~ I(sem-1) + hrs + highgpa + sex + (I(sem-1)|student), data = data_long, REML = FALSE) fit_lmer_4_re &lt;- lmerTest::lmer(gpa ~ I(sem-1) + hrs + highgpa + sex + (I(sem-1)|student), data = data_long, REML = TRUE, control = lmerControl(optimizer =&quot;Nelder_Mead&quot;)) fit_lmer_5_ml &lt;- lmerTest::lmer(gpa ~ I(sem-1) + hrs + highgpa + sex + I(sem-1):sex + (I(sem-1)|student), data = data_long, REML = FALSE) fit_lmer_5_re &lt;- lmerTest::lmer(gpa ~ I(sem-1) + hrs + highgpa + sex + I(sem-1):sex + (I(sem-1)|student), data = data_long, REML = FALSE) texreg::knitreg(list(fit_lmer_4_re, fit_lmer_5_re), single.row = TRUE) Statistical models   Model 1 Model 2 (Intercept) 2.56 (0.09)*** 2.58 (0.09)*** sem - 1 0.10 (0.01)*** 0.09 (0.01)*** hrs -0.13 (0.02)*** -0.13 (0.02)*** highgpa 0.09 (0.03)*** 0.09 (0.03)*** sexfemale 0.12 (0.03)*** 0.08 (0.03)* sem - 1:sexfemale   0.03 (0.01)** AIC 219.93 182.97 BIC 265.75 233.87 Log Likelihood -100.97 -81.49 Num. obs. 1200 1200 Num. groups: student 200 200 Var: student (Intercept) 0.04 0.04 Var: student I(sem - 1) 0.00 0.00 Cov: student (Intercept) I(sem - 1) -0.00 -0.00 Var: Residual 0.04 0.04 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 texreg::knitreg(list(fit_lmer_4_re, fit_lmer_5_re), single.row = TRUE, custom.model.names = c(&quot;M5: Occ Rand&quot;, &quot;M6: Xlevel Int&quot;), custom.coef.map = list(&quot;(Intercept)&quot; = &quot;(Intercept)&quot;, &quot;I(sem - 1)&quot; = &quot;Semester&quot;, &quot;hrs&quot; = &quot;Hours Working&quot;, &quot;sexfemale&quot; = &quot;Sex: Female vs. Male&quot;, &quot;highgpa&quot; = &quot;High School GPA&quot;, &quot;I(sem - 1):sexfemale&quot; = &quot;Semester X Sex&quot;), groups = list(&quot;Level 1 Main Effects, Occasion-Specific&quot; = 2:3, &quot;Level 2 Main Effects, Person-Specific&quot; = 4:5, &quot;Cross Level Interaction&quot; = 6), custom.note = &quot;%stars. \\nNote: Intercept refers to population mean for a Male who is not working during their first semester&quot;, caption = &quot;Hox Table 5.4 (page 80) Results of Multilevel Anlaysis of GPA, Varying Effects for Occation&quot;, caption.above = TRUE, digits = 3) Hox Table 5.4 (page 80) Results of Multilevel Anlaysis of GPA, Varying Effects for Occation   M5: Occ Rand M6: Xlevel Int (Intercept) 2.557 (0.093)*** 2.581 (0.092)*** Level 1 Main Effects, Occasion-Specific          Semester 0.103 (0.006)*** 0.088 (0.008)***      Hours Working -0.131 (0.017)*** -0.132 (0.017)*** Level 2 Main Effects, Person-Specific          Sex: Female vs. Male 0.116 (0.032)*** 0.076 (0.035)*      High School GPA 0.089 (0.026)*** 0.089 (0.026)*** Cross Level Interaction          Semester X Sex   0.030 (0.011)** AIC 219.935 182.971 BIC 265.746 233.872 Log Likelihood -100.967 -81.486 Num. obs. 1200 1200 Num. groups: student 200 200 Var: student (Intercept) 0.039 0.038 Var: student I(sem - 1) 0.004 0.004 Cov: student (Intercept) I(sem - 1) -0.003 -0.002 Var: Residual 0.042 0.042 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05. Note: Intercept refers to population mean for a Male who is not working during their first semester anova(fit_lmer_4_ml, fit_lmer_5_ml) Data: data_long Models: fit_lmer_4_ml: gpa ~ I(sem - 1) + hrs + highgpa + sex + (I(sem - 1) | student) fit_lmer_5_ml: gpa ~ I(sem - 1) + hrs + highgpa + sex + I(sem - 1):sex + (I(sem - 1) | student) npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) fit_lmer_4_ml 9 188.12 233.93 -85.059 170.12 fit_lmer_5_ml 10 182.97 233.87 -81.486 162.97 7.1464 1 0.007511 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 13.2.7 Visualize the Model interactions::interact_plot(model = fit_lmer_5_re, pred = sem, modx = sex, interval = TRUE) Figure 13.1: Hox Figure 5.4 (page 82) Multilevel model (M6) comapring linear increase in GPA over semester, but student’s sex. interactions::interact_plot(model = fit_lmer_5_re, pred = sem, modx = sex, mod2 = hrs, mod2.values = c(1, 2, 3), interval = TRUE) interactions::interact_plot(model = fit_lmer_5_re, pred = sem, modx = sex, mod2 = highgpa, mod2.values = c(2, 3, 4), interval = TRUE) 13.2.8 Effect Sizes 13.2.8.1 Standardized Parameters effectsize::standardize_parameters(fit_lmer_5_re) # Standardization method: refit Parameter | Std. Coef. | 95% CI -------------------------------------------------- (Intercept) | 0.18 | [ 0.02, 0.34] I(sem - 1) | 0.38 | [ 0.31, 0.45] hrs | -0.14 | [-0.18, -0.11] highgpa | 0.13 | [ 0.06, 0.21] sexfemale | 0.51 | [ 0.29, 0.73] I(sem - 1):sexfemale | 0.13 | [ 0.04, 0.22] 13.2.8.2 R-squared type measures performance::r2(fit_lmer_5_re, by_group = TRUE) # Explained Variance by Level Level | R2 --------------- Level 1 | 0.020 student | 0.348 13.2.9 Significance 13.2.9.1 Fixed Effects The Likelyhood Ratio Test (Deviance Difference Test) is best for establishing significance of fixed effects. Wald-tests summary(fit_lmer_5_re) Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s method [lmerModLmerTest] Formula: gpa ~ I(sem - 1) + hrs + highgpa + sex + I(sem - 1):sex + (I(sem - 1) | student) Data: data_long AIC BIC logLik deviance df.resid 183.0 233.9 -81.5 163.0 1190 Scaled residuals: Min 1Q Median 3Q Max -3.0044 -0.5268 -0.0138 0.5290 3.3513 Random effects: Groups Name Variance Std.Dev. Corr student (Intercept) 0.037811 0.19445 I(sem - 1) 0.003614 0.06012 -0.19 Residual 0.041555 0.20385 Number of obs: 1200, groups: student, 200 Fixed effects: Estimate Std. Error df t value Pr(&gt;|t|) (Intercept) 2.581e+00 9.239e-02 2.965e+02 27.938 &lt; 2e-16 *** I(sem - 1) 8.783e-02 7.951e-03 1.987e+02 11.046 &lt; 2e-16 *** hrs -1.321e-01 1.723e-02 1.042e+03 -7.670 3.93e-14 *** highgpa 8.850e-02 2.627e-02 1.976e+02 3.369 0.000907 *** sexfemale 7.551e-02 3.465e-02 2.003e+02 2.179 0.030499 * I(sem - 1):sexfemale 2.956e-02 1.096e-02 1.977e+02 2.698 0.007581 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Correlation of Fixed Effects: (Intr) I(s-1) hrs highgp sexfml I(sem - 1) -0.140 hrs -0.428 0.054 highgpa -0.871 0.001 0.022 sexfemale -0.265 0.312 0.030 0.066 I(sm-1):sxf 0.088 -0.724 -0.008 0.000 -0.430 F-test with Satterthwaite adjusted degrees of freedom anova(fit_lmer_5_re) Type III Analysis of Variance Table with Satterthwaite&#39;s method Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) I(sem - 1) 14.5023 14.5023 1 199.46 348.9910 &lt; 2.2e-16 *** hrs 2.4447 2.4447 1 1041.66 58.8305 3.933e-14 *** highgpa 0.4716 0.4716 1 197.63 11.3493 0.0009074 *** sex 0.1973 0.1973 1 200.28 4.7481 0.0304985 * I(sem - 1):sex 0.3025 0.3025 1 197.72 7.2787 0.0075806 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 13.2.9.2 Random Effects Likelyhood Ratio Tests (Deviance Difference Test), by single term deletion lmerTest::ranova(fit_lmer_5_re) ANOVA-like table for random-effects: Single term deletions Model: gpa ~ I(sem - 1) + hrs + highgpa + sex + (I(sem - 1) | student) + I(sem - 1):sex npar logLik AIC LRT Df Pr(&gt;Chisq) &lt;none&gt; 10 -81.486 182.97 I(sem - 1) in (I(sem - 1) | student) 8 -134.321 284.64 105.67 2 &lt; 2.2e-16 &lt;none&gt; I(sem - 1) in (I(sem - 1) | student) *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 "],["mlm-longitudinal-rct---exercise-and-diet.html", "14 MLM, Longitudinal: RCT - Exercise and Diet 14.1 The dataset 14.2 Exploratory Data Analysis 14.3 Multilevel Modeling 14.4 Autoregression 1", " 14 MLM, Longitudinal: RCT - Exercise and Diet library(tidyverse) # all things tidy library(pander) # nice looking genderal tabulations library(furniture) # nice table1() descriptives library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(psych) # contains some useful functions, like headTail library(performance) # ICC calculations library(interactions) library(sjPlot) # Visualization for Models library(effects) # Effec displays for Models library(lme4) # non-linear mixed-effects models 14.1 The dataset This comes from a Randomized Controled Trial. data_raw &lt;- read.table(&quot;https://raw.githubusercontent.com/CEHS-research/data/master/MLM/exercise_diet.txt&quot;, header = TRUE, sep = &quot;,&quot;) tibble::glimpse(data_raw) Rows: 120 Columns: 5 $ id &lt;int&gt; 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 6… $ exertype &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… $ diet &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2… $ pulse &lt;int&gt; 90, 92, 93, 93, 90, 92, 93, 93, 97, 97, 94, 94, 80, 82, 83, 8… $ time &lt;int&gt; 0, 228, 296, 639, 0, 56, 434, 538, 0, 150, 295, 541, 0, 121, … data_long &lt;- data_raw %&gt;% dplyr::mutate(id = id %&gt;% factor) %&gt;% dplyr::mutate(exertype = exertype %&gt;% factor(levels = 1:3, labels = c(&quot;At Rest&quot;, &quot;Leisurely Walking&quot;, &quot;Moderate Running&quot;))) %&gt;% dplyr::mutate(diet = diet %&gt;% factor(levels = 1:2, labels = c(&quot;low-fat&quot;, &quot;non-fat&quot;))) %&gt;% dplyr::mutate(time_min = time / 60) data_long %&gt;% psych::headTail(top = 10, bottom = 10) %&gt;% pander::pander(caption = &quot;Raw Data&quot;) Raw Data   id exertype diet pulse time time_min 1 1 At Rest low-fat 90 0 0 2 1 At Rest low-fat 92 228 3.8 3 1 At Rest low-fat 93 296 4.93 4 1 At Rest low-fat 93 639 10.65 5 2 At Rest low-fat 90 0 0 6 2 At Rest low-fat 92 56 0.93 7 2 At Rest low-fat 93 434 7.23 8 2 At Rest low-fat 93 538 8.97 9 3 At Rest low-fat 97 0 0 10 3 At Rest low-fat 97 150 2.5 … NA NA NA … … … 111 28 Moderate Running non-fat 140 263 4.38 112 28 Moderate Running non-fat 143 588 9.8 113 29 Moderate Running non-fat 94 0 0 114 29 Moderate Running non-fat 135 164 2.73 115 29 Moderate Running non-fat 130 353 5.88 116 29 Moderate Running non-fat 137 560 9.33 117 30 Moderate Running non-fat 99 0 0 118 30 Moderate Running non-fat 111 114 1.9 119 30 Moderate Running non-fat 140 362 6.03 120 30 Moderate Running non-fat 148 501 8.35 14.2 Exploratory Data Analysis 14.2.1 Participant Summary In this experiment, both exercise (exertype) and diet (diet) were randomized at the subject level to create a 2x3 = 6 combinations each with exactly 5 participants. data_long %&gt;% dplyr::filter(time == 0) %&gt;% dplyr::group_by(exertype) %&gt;% furniture::table1(&quot;Diet, randomized&quot; = diet, caption = &quot;Participants&quot;, output = &quot;markdown&quot;) Table 14.1: Participants At Rest Leisurely Walking Moderate Running n = 10 n = 10 n = 10 Diet, randomized low-fat 5 (50%) 5 (50%) 5 (50%) non-fat 5 (50%) 5 (50%) 5 (50%) 14.2.2 Baseline Summary data_long %&gt;% dplyr::filter(time == 0) %&gt;% dplyr::group_by(exertype, diet) %&gt;% dplyr::summarise(mean = mean(pulse)) %&gt;% dplyr::ungroup() %&gt;% tidyr::pivot_wider(names_from = diet, values_from = mean) %&gt;% pander::pander(caption = &quot;Baseline Pulse, Means&quot;) summarise() has grouped output by ‘exertype’. You can override using the .groups argument. Baseline Pulse, Means exertype low-fat non-fat At Rest 90 92 Leisurely Walking 91 96 Moderate Running 94 98 14.2.3 Raw Trajectories - Person Profile Plot 14.2.3.1 Connect the dots data_long %&gt;% ggplot(aes(x = time_min, y = pulse)) + geom_point() + geom_line(aes(group = id)) + facet_grid(diet ~ exertype) + theme_bw() 14.2.3.2 Loess - Moving Average Smoothers data_long %&gt;% ggplot(aes(x = time_min, y = pulse, color = diet)) + geom_line(aes(group = id)) + facet_grid(~ exertype) + theme_bw() + geom_smooth(method = &quot;loess&quot;, se = FALSE, size = 2, span = 5) + theme(legend.position = c(0.08, 0.85), legend.background = element_rect(color = &quot;black&quot;)) + labs(title = &quot;Raw Pulse Trajectories&quot;, subtitle = &quot;By Exercise and Diet Groupings&quot;, x = &quot;Time (Minutes Post-Baseline)&quot;, y = &quot;Pulse (Beats per Minute)&quot;, color = &quot;Diet Plan&quot;) 14.3 Multilevel Modeling 14.3.1 Null Model fit_lmer_0re &lt;- lmerTest::lmer(pulse ~ 1 + (1 | id), data = data_long) texreg::knitreg(fit_lmer_0re, single.row = TRUE) Statistical models   Model 1 (Intercept) 102.13 (2.54)*** AIC 963.89 BIC 972.25 Log Likelihood -478.95 Num. obs. 120 Num. groups: id 30 Var: id (Intercept) 165.84 Var: Residual 109.39 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 14.3.2 ICC &amp; R-squared performance::icc(fit_lmer_0re) # Intraclass Correlation Coefficient Adjusted ICC: 0.603 Unadjusted ICC: 0.603 performance::r2(fit_lmer_0re) # R2 for Mixed Models Conditional R2: 0.603 Marginal R2: 0.000 14.3.3 Add fixed effects: level specific 14.3.3.1 Fit nested models # Null Model (random intercept only) fit_lmer_0ml &lt;- lmerTest::lmer(pulse ~ 1 + (1 | id), data = data_long, REML = FALSE) # Add quadratic time fit_lmer_1ml &lt;- lmerTest::lmer(pulse ~ time_min + I(time_min^2) + (1 | id), data = data_long, REML = FALSE) # Add main effects for 2 interventions (person-specific, i.e. level-2 factors) fit_lmer_2ml &lt;- lmerTest::lmer(pulse ~ diet + exertype + time_min + I(time_min^2) + (1 | id), data = data_long, REML = FALSE) # Add interaction between level-2 factors fit_lmer_3ml &lt;- lmerTest::lmer(pulse ~ diet*exertype + time_min + I(time_min^2) + (1 | id), data = data_long, REML = FALSE) # Add exercise interacting with [time &amp; time-squared] fit_lmer_4ml &lt;- lmerTest::lmer(pulse ~ diet*exertype + exertype*time_min + exertype*I(time_min^2) + (1 | id), data = data_long, REML = FALSE) # Add diet interacting with [time &amp; time-squared] fit_lmer_5ml &lt;- lmerTest::lmer(pulse ~ diet*exertype*time_min + diet*exertype*I(time_min^2) + (1 | id), data = data_long, REML = FALSE) texreg::knitreg(list(fit_lmer_1ml, fit_lmer_2ml, fit_lmer_3ml, fit_lmer_4ml, fit_lmer_5ml)) Statistical models   Model 1 Model 2 Model 3 Model 4 Model 5 (Intercept) 94.05*** 79.30*** 82.15*** 89.89*** 89.81***   (2.71) (2.46) (2.64) (2.69) (2.78) time_min 3.57*** 3.58*** 3.44*** 0.24 0.37   (0.65) (0.64) (0.64) (0.62) (0.87) time_min^2 -0.21*** -0.21*** -0.20*** -0.01 -0.03   (0.06) (0.06) (0.06) (0.05) (0.09) dietnon-fat   8.36*** 2.89 1.99 2.11     (2.21) (3.36) (3.45) (3.89) exertypeLeisurely Walking   5.20 3.81 0.84 1.40     (2.70) (3.34) (3.78) (3.92) exertypeModerate Running   26.43*** 19.71*** 0.53 5.71     (2.70) (3.34) (3.77) (3.92) dietnon-fat:exertypeLeisurely Walking     2.83 3.70 2.53       (4.73) (4.86) (5.50) dietnon-fat:exertypeModerate Running     13.47** 14.02** 3.99       (4.74) (4.86) (5.50) exertypeLeisurely Walking:time_min       1.17 1.09         (0.87) (1.17) exertypeModerate Running:time_min       8.19*** 5.77***         (0.90) (1.20) exertypeLeisurely Walking:time_min^2       -0.07 -0.08         (0.08) (0.11) exertypeModerate Running:time_min^2       -0.48*** -0.33**         (0.08) (0.11) dietnon-fat:time_min         -0.17           (1.14) dietnon-fat:time_min^2         0.02           (0.10) dietnon-fat:exertypeLeisurely Walking:time_min         0.21           (1.56) dietnon-fat:exertypeModerate Running:time_min         4.42**           (1.61) dietnon-fat:exertypeLeisurely Walking:time_min^2         0.01           (0.14) dietnon-fat:exertypeModerate Running:time_min^2         -0.27           (0.15) AIC 927.70 884.96 881.11 785.34 769.23 BIC 941.64 907.26 908.99 824.36 824.98 Log Likelihood -458.85 -434.48 -430.56 -378.67 -364.62 Num. obs. 120 120 120 120 120 Num. groups: id 30 30 30 30 30 Var: id (Intercept) 167.58 19.46 11.03 24.13 25.64 Var: Residual 67.47 67.47 67.52 20.95 15.32 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 14.3.3.2 Evaluate Model Fit, i.e. variable significance anova(fit_lmer_1ml, fit_lmer_2ml, fit_lmer_3ml, fit_lmer_4ml, fit_lmer_5ml) Data: data_long Models: fit_lmer_1ml: pulse ~ time_min + I(time_min^2) + (1 | id) fit_lmer_2ml: pulse ~ diet + exertype + time_min + I(time_min^2) + (1 | id) fit_lmer_3ml: pulse ~ diet * exertype + time_min + I(time_min^2) + (1 | id) fit_lmer_4ml: pulse ~ diet * exertype + exertype * time_min + exertype * I(time_min^2) + (1 | id) fit_lmer_5ml: pulse ~ diet * exertype * time_min + diet * exertype * I(time_min^2) + (1 | id) npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) fit_lmer_1ml 5 927.70 941.64 -458.85 917.70 fit_lmer_2ml 8 884.96 907.26 -434.48 868.96 48.742 3 1.480e-10 *** fit_lmer_3ml 10 881.11 908.99 -430.56 861.11 7.847 2 0.01977 * fit_lmer_4ml 14 785.34 824.36 -378.67 757.34 103.776 4 &lt; 2.2e-16 *** fit_lmer_5ml 20 769.23 824.98 -364.62 729.23 28.108 6 8.968e-05 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 performance::compare_performance(fit_lmer_1ml, fit_lmer_2ml, fit_lmer_3ml, fit_lmer_4ml, fit_lmer_5ml, rank = TRUE) # Comparison of Model Performance Indices Name | Model | R2 (cond.) | R2 (marg.) | ICC | RMSE | Sigma | AIC weights | AICc weights | BIC weights | Performance-Score ----------------------------------------------------------------------------------------------------------------------------------------------- fit_lmer_5ml | lmerModLmerTest | 0.944 | 0.850 | 0.626 | 3.462 | 3.914 | 1.000 | 0.997 | 0.423 | 94.78% fit_lmer_4ml | lmerModLmerTest | 0.923 | 0.835 | 0.535 | 4.080 | 4.577 | 3.18e-04 | 0.003 | 0.577 | 65.87% fit_lmer_1ml | lmerModLmerTest | 0.748 | 0.121 | 0.713 | 7.221 | 8.214 | 3.87e-35 | 2.07e-33 | 1.97e-26 | 14.18% fit_lmer_2ml | lmerModLmerTest | 0.750 | 0.678 | 0.224 | 7.644 | 8.214 | 7.40e-26 | 2.68e-24 | 5.75e-19 | 11.97% fit_lmer_3ml | lmerModLmerTest | 0.750 | 0.710 | 0.140 | 7.800 | 8.217 | 5.07e-25 | 1.28e-23 | 2.42e-19 | 10.27% 14.3.4 Final Model Refit via REML fit_lmer_5re &lt;- lmerTest::lmer(pulse ~ diet*exertype*time_min + diet*exertype*I(time_min^2) + (1 | id), data = data_long, REML = TRUE) 14.3.4.1 Visualize sjPlot::plot_model(fit_lmer_5re, type = &quot;pred&quot;, terms = c(&quot;time_min&quot;, &quot;diet&quot;, &quot;exertype&quot;)) interactions::interact_plot(fit_lmer_5re, pred = time_min, modx = diet, mod2 = exertype, interval = TRUE) effects::Effect(focal.predictors = c(&quot;diet&quot;, &quot;exertype&quot;, &quot;time_min&quot;), mod = fit_lmer_5re) %&gt;% data.frame %&gt;% ggplot(aes(x = time_min, y = fit, fill = diet, color = diet)) + geom_line(size = 1.5) + theme_bw() + facet_grid(~ exertype) + theme(legend.position = c(0.08, 0.85), legend.background = element_rect(color = &quot;black&quot;)) + labs(title = &quot;Raw Pulse Trajectories&quot;, subtitle = &quot;By Exercise and Diet Groupings&quot;, x = &quot;Time (Minutes Post-Baseline)&quot;, y = &quot;Estimated Marginal Mean\\nPulse (Beats per Minute)&quot;, fill = &quot;Diet Plan&quot;, color = &quot;Diet Plan&quot;) effects::Effect(focal.predictors = c(&quot;diet&quot;, &quot;exertype&quot;, &quot;time_min&quot;), mod = fit_lmer_5re, xlevels = list(&quot;time_min&quot; = seq(from = 0, to = 12, by = 0.5))) %&gt;% data.frame %&gt;% dplyr::mutate(diet = fct_rev(diet)) %&gt;% # reverse the order of the levels ggplot(aes(x = time_min, y = fit)) + geom_ribbon(aes(ymin = fit - se, ymax = fit + se, fill = diet), alpha = 0.3) + geom_line(aes(linetype = diet), size = 1) + theme_bw() + facet_grid(~ exertype) + theme(legend.position = c(0.12, 0.85), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(2, &quot;cm&quot;)) + labs(title = &quot;Raw Pulse Trajectories&quot;, subtitle = &quot;By Randomized Exercise and Diet Intervention&quot;, x = &quot;Time (Minutes Post-Baseline)&quot;, y = &quot;Estimated Marginal Mean\\nPulse (Beats per Minute)&quot;, fill = &quot;Diet Plan&quot;, color = &quot;Diet Plan&quot;, linetype = &quot;Diet Plan&quot;) + scale_fill_manual(values = c(&quot;black&quot;, &quot;gray50&quot;)) + scale_linetype_manual(values = c(&quot;solid&quot;, &quot;longdash&quot;)) + scale_x_continuous(breaks = seq(from = 0, to = 14, by = 5)) library(tidyverse) library(nlme) df_long &lt;- read.table(&quot;data/bock1983_wpss_tca.txt&quot;, col.names = c(&quot;id&quot;, &quot;wpss&quot;, &quot;int&quot;, &quot;linear_time&quot;, &quot;change_time&quot;, &quot;group&quot;, &quot;x2&quot;, &quot;x3&quot;)) %&gt;% dplyr::mutate(week = 3.5 + linear_time) %&gt;% dplyr::mutate(group = factor(group, levels = c(0, 1), labels = c(&quot;TCA-None&quot;, &quot;None-TCA&quot;))) %&gt;% dplyr::select(id, group, week, wpss) df_long id group week wpss 1 1 TCA-None 1 1 2 1 TCA-None 2 1 3 1 TCA-None 3 1 4 1 TCA-None 4 1 5 1 TCA-None 5 1 6 1 TCA-None 6 1 7 2 TCA-None 1 4 8 2 TCA-None 2 3 9 2 TCA-None 3 3 10 2 TCA-None 4 3 11 2 TCA-None 5 4 12 2 TCA-None 6 4 13 3 TCA-None 1 4 14 3 TCA-None 2 3 15 3 TCA-None 3 3 16 3 TCA-None 4 4 17 3 TCA-None 5 4 18 3 TCA-None 6 4 19 4 TCA-None 1 5 20 4 TCA-None 2 3 21 4 TCA-None 3 2 22 4 TCA-None 4 2 23 4 TCA-None 5 5 24 4 TCA-None 6 5 25 5 TCA-None 1 5 26 5 TCA-None 2 5 27 5 TCA-None 3 1 28 5 TCA-None 4 2 29 5 TCA-None 5 2 30 5 TCA-None 6 2 31 6 TCA-None 1 3 32 6 TCA-None 2 2 33 6 TCA-None 3 2 34 6 TCA-None 4 2 35 6 TCA-None 5 1 36 6 TCA-None 6 1 37 7 TCA-None 1 5 38 7 TCA-None 2 5 39 7 TCA-None 3 5 40 7 TCA-None 4 5 41 7 TCA-None 5 1 42 7 TCA-None 6 1 43 8 TCA-None 1 3 44 8 TCA-None 2 3 45 8 TCA-None 3 3 46 8 TCA-None 4 4 47 8 TCA-None 5 4 48 8 TCA-None 6 4 49 9 TCA-None 1 2 50 9 TCA-None 2 1 51 9 TCA-None 3 1 52 9 TCA-None 4 1 53 9 TCA-None 5 1 54 9 TCA-None 6 1 55 10 TCA-None 1 5 56 10 TCA-None 2 5 57 10 TCA-None 3 5 58 10 TCA-None 4 5 59 10 TCA-None 5 5 60 10 TCA-None 6 5 61 11 TCA-None 1 3 62 11 TCA-None 2 3 63 11 TCA-None 3 3 64 11 TCA-None 4 3 65 11 TCA-None 5 1 66 11 TCA-None 6 1 67 12 TCA-None 1 3 68 12 TCA-None 2 3 69 12 TCA-None 3 2 70 12 TCA-None 4 2 71 12 TCA-None 5 3 72 12 TCA-None 6 3 73 13 TCA-None 1 5 74 13 TCA-None 2 4 75 13 TCA-None 3 3 76 13 TCA-None 4 1 77 13 TCA-None 5 1 78 13 TCA-None 6 1 79 14 TCA-None 1 6 80 14 TCA-None 2 5 81 14 TCA-None 3 5 82 14 TCA-None 4 5 83 14 TCA-None 5 5 84 14 TCA-None 6 6 85 15 TCA-None 1 1 86 15 TCA-None 2 1 87 15 TCA-None 3 1 88 15 TCA-None 4 1 89 15 TCA-None 5 1 90 15 TCA-None 6 1 91 16 TCA-None 1 2 92 16 TCA-None 2 2 93 16 TCA-None 3 3 94 16 TCA-None 4 3 95 16 TCA-None 5 3 96 16 TCA-None 6 3 97 17 TCA-None 1 1 98 17 TCA-None 2 1 99 17 TCA-None 3 1 100 17 TCA-None 4 1 101 17 TCA-None 5 1 102 17 TCA-None 6 1 103 18 TCA-None 1 5 104 18 TCA-None 2 4 105 18 TCA-None 3 4 106 18 TCA-None 4 1 107 18 TCA-None 5 1 108 18 TCA-None 6 1 109 19 TCA-None 1 5 110 19 TCA-None 2 5 111 19 TCA-None 3 5 112 19 TCA-None 4 4 113 19 TCA-None 5 4 114 19 TCA-None 6 3 115 20 TCA-None 1 4 116 20 TCA-None 2 4 117 20 TCA-None 3 3 118 20 TCA-None 4 3 119 20 TCA-None 5 3 120 20 TCA-None 6 3 121 21 TCA-None 1 2 122 21 TCA-None 2 1 123 21 TCA-None 3 1 124 21 TCA-None 4 1 125 21 TCA-None 5 1 126 21 TCA-None 6 1 127 22 TCA-None 1 2 128 22 TCA-None 2 3 129 22 TCA-None 3 4 130 22 TCA-None 4 3 131 22 TCA-None 5 4 132 22 TCA-None 6 2 133 23 TCA-None 1 3 134 23 TCA-None 2 3 135 23 TCA-None 3 3 136 23 TCA-None 4 3 137 23 TCA-None 5 3 138 23 TCA-None 6 3 139 24 TCA-None 1 5 140 24 TCA-None 2 6 141 24 TCA-None 3 5 142 24 TCA-None 4 6 143 24 TCA-None 5 5 144 24 TCA-None 6 6 145 25 TCA-None 1 5 146 25 TCA-None 2 5 147 25 TCA-None 3 5 148 25 TCA-None 4 5 149 25 TCA-None 5 5 150 25 TCA-None 6 5 151 26 TCA-None 1 3 152 26 TCA-None 2 3 153 26 TCA-None 3 3 154 26 TCA-None 4 1 155 26 TCA-None 5 1 156 26 TCA-None 6 1 157 27 TCA-None 1 6 158 27 TCA-None 2 6 159 27 TCA-None 3 6 160 27 TCA-None 4 6 161 27 TCA-None 5 6 162 27 TCA-None 6 6 163 28 TCA-None 1 3 164 28 TCA-None 2 3 165 28 TCA-None 3 3 166 28 TCA-None 4 3 167 28 TCA-None 5 3 168 28 TCA-None 6 3 169 29 TCA-None 1 4 170 29 TCA-None 2 4 171 29 TCA-None 3 4 172 29 TCA-None 4 4 173 29 TCA-None 5 4 174 29 TCA-None 6 4 175 30 TCA-None 1 6 176 30 TCA-None 2 6 177 30 TCA-None 3 6 178 30 TCA-None 4 6 179 30 TCA-None 5 6 180 30 TCA-None 6 6 181 31 TCA-None 1 3 182 31 TCA-None 2 3 183 31 TCA-None 3 3 184 31 TCA-None 4 3 185 31 TCA-None 5 3 186 31 TCA-None 6 3 187 32 TCA-None 1 5 188 32 TCA-None 2 5 189 32 TCA-None 3 5 190 32 TCA-None 4 5 191 32 TCA-None 5 5 192 32 TCA-None 6 5 193 33 TCA-None 1 4 194 33 TCA-None 2 4 195 33 TCA-None 3 4 196 33 TCA-None 4 4 197 33 TCA-None 5 4 198 33 TCA-None 6 4 199 34 TCA-None 1 3 200 34 TCA-None 2 3 201 34 TCA-None 3 3 202 34 TCA-None 4 3 203 34 TCA-None 5 3 204 34 TCA-None 6 2 205 35 TCA-None 1 5 206 35 TCA-None 2 4 207 35 TCA-None 3 4 208 35 TCA-None 4 3 209 35 TCA-None 5 1 210 35 TCA-None 6 1 211 36 TCA-None 1 4 212 36 TCA-None 2 3 213 36 TCA-None 3 1 214 36 TCA-None 4 1 215 36 TCA-None 5 1 216 36 TCA-None 6 1 217 37 TCA-None 1 2 218 37 TCA-None 2 2 219 37 TCA-None 3 2 220 37 TCA-None 4 2 221 37 TCA-None 5 2 222 37 TCA-None 6 2 223 38 TCA-None 1 4 224 38 TCA-None 2 4 225 38 TCA-None 3 3 226 38 TCA-None 4 3 227 38 TCA-None 5 3 228 38 TCA-None 6 3 229 39 TCA-None 1 3 230 39 TCA-None 2 3 231 39 TCA-None 3 3 232 39 TCA-None 4 2 233 39 TCA-None 5 2 234 39 TCA-None 6 2 235 40 TCA-None 1 5 236 40 TCA-None 2 4 237 40 TCA-None 3 1 238 40 TCA-None 4 1 239 40 TCA-None 5 1 240 40 TCA-None 6 1 241 41 TCA-None 1 5 242 41 TCA-None 2 5 243 41 TCA-None 3 5 244 41 TCA-None 4 4 245 41 TCA-None 5 4 246 41 TCA-None 6 4 247 42 TCA-None 1 4 248 42 TCA-None 2 3 249 42 TCA-None 3 3 250 42 TCA-None 4 3 251 42 TCA-None 5 3 252 42 TCA-None 6 3 253 43 TCA-None 1 4 254 43 TCA-None 2 3 255 43 TCA-None 3 3 256 43 TCA-None 4 2 257 43 TCA-None 5 2 258 43 TCA-None 6 2 259 44 TCA-None 1 5 260 44 TCA-None 2 5 261 44 TCA-None 3 4 262 44 TCA-None 4 4 263 44 TCA-None 5 4 264 44 TCA-None 6 3 265 45 TCA-None 1 3 266 45 TCA-None 2 2 267 45 TCA-None 3 2 268 45 TCA-None 4 1 269 45 TCA-None 5 1 270 45 TCA-None 6 1 271 46 TCA-None 1 3 272 46 TCA-None 2 3 273 46 TCA-None 3 1 274 46 TCA-None 4 1 275 46 TCA-None 5 1 276 46 TCA-None 6 1 277 47 None-TCA 1 6 278 47 None-TCA 2 6 279 47 None-TCA 3 6 280 47 None-TCA 4 6 281 47 None-TCA 5 6 282 47 None-TCA 6 6 283 48 None-TCA 1 6 284 48 None-TCA 2 6 285 48 None-TCA 3 6 286 48 None-TCA 4 6 287 48 None-TCA 5 6 288 48 None-TCA 6 6 289 49 None-TCA 1 5 290 49 None-TCA 2 5 291 49 None-TCA 3 5 292 49 None-TCA 4 5 293 49 None-TCA 5 4 294 49 None-TCA 6 3 295 50 None-TCA 1 6 296 50 None-TCA 2 6 297 50 None-TCA 3 6 298 50 None-TCA 4 6 299 50 None-TCA 5 6 300 50 None-TCA 6 6 301 51 None-TCA 1 5 302 51 None-TCA 2 5 303 51 None-TCA 3 5 304 51 None-TCA 4 5 305 51 None-TCA 5 5 306 51 None-TCA 6 5 307 52 None-TCA 1 3 308 52 None-TCA 2 2 309 52 None-TCA 3 2 310 52 None-TCA 4 2 311 52 None-TCA 5 1 312 52 None-TCA 6 1 313 53 None-TCA 1 5 314 53 None-TCA 2 4 315 53 None-TCA 3 4 316 53 None-TCA 4 4 317 53 None-TCA 5 4 318 53 None-TCA 6 4 319 54 None-TCA 1 5 320 54 None-TCA 2 5 321 54 None-TCA 3 5 322 54 None-TCA 4 5 323 54 None-TCA 5 5 324 54 None-TCA 6 5 325 55 None-TCA 1 5 326 55 None-TCA 2 3 327 55 None-TCA 3 3 328 55 None-TCA 4 3 329 55 None-TCA 5 3 330 55 None-TCA 6 2 331 56 None-TCA 1 3 332 56 None-TCA 2 3 333 56 None-TCA 3 2 334 56 None-TCA 4 2 335 56 None-TCA 5 3 336 56 None-TCA 6 3 337 57 None-TCA 1 3 338 57 None-TCA 2 3 339 57 None-TCA 3 3 340 57 None-TCA 4 3 341 57 None-TCA 5 4 342 57 None-TCA 6 4 343 58 None-TCA 1 5 344 58 None-TCA 2 5 345 58 None-TCA 3 5 346 58 None-TCA 4 5 347 58 None-TCA 5 5 348 58 None-TCA 6 5 349 59 None-TCA 1 5 350 59 None-TCA 2 5 351 59 None-TCA 3 4 352 59 None-TCA 4 4 353 59 None-TCA 5 4 354 59 None-TCA 6 4 355 60 None-TCA 1 5 356 60 None-TCA 2 5 357 60 None-TCA 3 5 358 60 None-TCA 4 5 359 60 None-TCA 5 5 360 60 None-TCA 6 4 361 61 None-TCA 1 5 362 61 None-TCA 2 5 363 61 None-TCA 3 5 364 61 None-TCA 4 5 365 61 None-TCA 5 4 366 61 None-TCA 6 3 367 62 None-TCA 1 3 368 62 None-TCA 2 3 369 62 None-TCA 3 4 370 62 None-TCA 4 4 371 62 None-TCA 5 3 372 62 None-TCA 6 3 373 63 None-TCA 1 4 374 63 None-TCA 2 5 375 63 None-TCA 3 5 376 63 None-TCA 4 5 377 63 None-TCA 5 5 378 63 None-TCA 6 4 379 64 None-TCA 1 3 380 64 None-TCA 2 3 381 64 None-TCA 3 3 382 64 None-TCA 4 3 383 64 None-TCA 5 3 384 64 None-TCA 6 3 385 65 None-TCA 1 4 386 65 None-TCA 2 4 387 65 None-TCA 3 4 388 65 None-TCA 4 5 389 65 None-TCA 5 5 390 65 None-TCA 6 5 391 66 None-TCA 1 5 392 66 None-TCA 2 5 393 66 None-TCA 3 5 394 66 None-TCA 4 5 395 66 None-TCA 5 5 396 66 None-TCA 6 5 397 67 None-TCA 1 6 398 67 None-TCA 2 6 399 67 None-TCA 3 6 400 67 None-TCA 4 6 401 67 None-TCA 5 4 402 67 None-TCA 6 4 403 68 None-TCA 1 6 404 68 None-TCA 2 6 405 68 None-TCA 3 6 406 68 None-TCA 4 6 407 68 None-TCA 5 6 408 68 None-TCA 6 6 409 69 None-TCA 1 5 410 69 None-TCA 2 5 411 69 None-TCA 3 5 412 69 None-TCA 4 5 413 69 None-TCA 5 5 414 69 None-TCA 6 5 415 70 None-TCA 1 5 416 70 None-TCA 2 5 417 70 None-TCA 3 4 418 70 None-TCA 4 3 419 70 None-TCA 5 2 420 70 None-TCA 6 2 421 71 None-TCA 1 4 422 71 None-TCA 2 4 423 71 None-TCA 3 5 424 71 None-TCA 4 5 425 71 None-TCA 5 5 426 71 None-TCA 6 4 427 72 None-TCA 1 5 428 72 None-TCA 2 5 429 72 None-TCA 3 5 430 72 None-TCA 4 3 431 72 None-TCA 5 2 432 72 None-TCA 6 2 433 73 None-TCA 1 5 434 73 None-TCA 2 4 435 73 None-TCA 3 3 436 73 None-TCA 4 3 437 73 None-TCA 5 2 438 73 None-TCA 6 2 439 74 None-TCA 1 5 440 74 None-TCA 2 6 441 74 None-TCA 3 6 442 74 None-TCA 4 5 443 74 None-TCA 5 5 444 74 None-TCA 6 3 445 75 None-TCA 1 5 446 75 None-TCA 2 5 447 75 None-TCA 3 5 448 75 None-TCA 4 5 449 75 None-TCA 5 5 450 75 None-TCA 6 4 df_wide &lt;- df_long %&gt;% tidyr::pivot_wider(names_from = week, names_prefix = &quot;wpss_&quot;, values_from = wpss) df_wide %&gt;% dplyr::select(starts_with(&quot;wpss&quot;)) %&gt;% cor() %&gt;% corrplot::corrplot.mixed() df_long %&gt;% ggplot(aes(x = week, y = wpss)) + geom_line(aes(group = id), alpha = .2) + theme_bw() + facet_grid(. ~ group) + stat_summary() + stat_summary(fun = &quot;mean&quot;, geom = &quot;line&quot;) + geom_vline(xintercept = 3.5, linetype = &quot;dashed&quot;) 14.4 Autoregression 1 fit_cpm_ar1 &lt;- nlme::gls(wpss ~ group*I(week-3.5) + group*I((week - 3.5)^2), data = df_long, correlation = corAR1(form = ~ 1 | id)) summary(fit_cpm_ar1) Generalized least squares fit by REML Model: wpss ~ group * I(week - 3.5) + group * I((week - 3.5)^2) Data: df_long AIC BIC logLik 1037.232 1069.999 -510.616 Correlation Structure: AR(1) Formula: ~1 | id Parameter estimate(s): Phi 0.8859207 Coefficients: Value Std.Error t-value p-value (Intercept) 3.0188076 0.1994448 15.136058 0.0000 groupNone-TCA 1.4748315 0.3207409 4.598203 0.0000 I(week - 3.5) -0.2044912 0.0393299 -5.199385 0.0000 I((week - 3.5)^2) 0.0369968 0.0158297 2.337173 0.0199 groupNone-TCA:I(week - 3.5) 0.0393150 0.0632491 0.621590 0.5345 groupNone-TCA:I((week - 3.5)^2) -0.0663268 0.0254568 -2.605459 0.0095 Correlation: (Intr) gN-TCA I(-3.5 I((-3. gN-TCA:I(-3. groupNone-TCA -0.622 I(week - 3.5) 0.000 0.000 I((week - 3.5)^2) -0.422 0.263 0.000 groupNone-TCA:I(week - 3.5) 0.000 0.000 -0.622 0.000 groupNone-TCA:I((week - 3.5)^2) 0.263 -0.422 0.000 -0.622 0.000 Standardized residuals: Min Q1 Med Q3 Max -2.2724244 -0.7712090 0.1462741 0.7879557 2.3305293 Residual standard error: 1.399335 Degrees of freedom: 450 total; 444 residual texreg::knitreg(fit_cpm_ar1, single.row = TRUE) &lt;table class=&quot;texreg&quot; style=&quot;margin: 10px auto;border-collapse: collapse;border-spacing: 0px;caption-side: bottom;color: #000000;border-top: 2px solid #000000;&quot;&gt; &lt;caption&gt;Statistical models&lt;/caption&gt; &lt;thead&gt; &lt;tr&gt; &lt;th style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;&amp;nbsp;&lt;/th&gt; &lt;th style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;Model 1&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr style=&quot;border-top: 1px solid #000000;&quot;&gt; &lt;td style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;(Intercept)&lt;/td&gt; &lt;td style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;3.02 (0.20)&lt;sup&gt;&amp;#42;&amp;#42;&amp;#42;&lt;/sup&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;groupNone-TCA&lt;/td&gt; &lt;td style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;1.47 (0.32)&lt;sup&gt;&amp;#42;&amp;#42;&amp;#42;&lt;/sup&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;week - 3.5&lt;/td&gt; &lt;td style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;-0.20 (0.04)&lt;sup&gt;&amp;#42;&amp;#42;&amp;#42;&lt;/sup&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;(week - 3.5)^2&lt;/td&gt; &lt;td style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;0.04 (0.02)&lt;sup&gt;&amp;#42;&lt;/sup&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;groupNone-TCA:week - 3.5&lt;/td&gt; &lt;td style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;0.04 (0.06)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;groupNone-TCA:(week - 3.5)^2&lt;/td&gt; &lt;td style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;-0.07 (0.03)&lt;sup&gt;&amp;#42;&amp;#42;&lt;/sup&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr style=&quot;border-top: 1px solid #000000;&quot;&gt; &lt;td style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;AIC&lt;/td&gt; &lt;td style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;1037.23&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;BIC&lt;/td&gt; &lt;td style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;1070.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;Log Likelihood&lt;/td&gt; &lt;td style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;-510.62&lt;/td&gt; &lt;/tr&gt; &lt;tr style=&quot;border-bottom: 2px solid #000000;&quot;&gt; &lt;td style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;Num. obs.&lt;/td&gt; &lt;td style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;450&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;tfoot&gt; &lt;tr&gt; &lt;td style=&quot;font-size: 0.8em;&quot; colspan=&quot;2&quot;&gt;&lt;sup&gt;&amp;#42;&amp;#42;&amp;#42;&lt;/sup&gt;p &amp;lt; 0.001; &lt;sup&gt;&amp;#42;&amp;#42;&lt;/sup&gt;p &amp;lt; 0.01; &lt;sup&gt;&amp;#42;&lt;/sup&gt;p &amp;lt; 0.05&lt;/td&gt; &lt;/tr&gt; &lt;/tfoot&gt; &lt;/table&gt; fit_cpm_ar1 %&gt;% emmeans::emmeans(~ week*group, at = list(week = seq(from = 1, to = 6, by = .01))) %&gt;% data.frame() %&gt;% ggplot(aes(x = week, y = emmean, fill = group)) + geom_ribbon(aes(ymin = emmean - SE, ymax = emmean + SE), alpha = .5) + geom_line(aes(linetype = group)) + theme_bw() + geom_vline(xintercept = 3.5, linetype = &quot;dashed&quot;) 14.4.1 Unstructured fit_cpm_uns &lt;- nlme::gls(wpss ~ group*I(week-3.5) + group*I((week - 3.5)^2), data = df_long, correlation = corSymm(form = ~ 1 | id)) summary(fit_cpm_uns) Generalized least squares fit by REML Model: wpss ~ group * I(week - 3.5) + group * I((week - 3.5)^2) Data: df_long AIC BIC logLik 1022.343 1112.451 -489.1713 Correlation Structure: General Formula: ~1 | id Parameter estimate(s): Correlation: 1 2 3 4 5 2 0.912 3 0.723 0.846 4 0.633 0.775 0.889 5 0.505 0.621 0.713 0.830 6 0.525 0.603 0.658 0.799 0.941 Coefficients: Value Std.Error t-value p-value (Intercept) 3.0219122 0.19193537 15.744426 0.0000 groupNone-TCA 1.4867418 0.30866447 4.816693 0.0000 I(week - 3.5) -0.1987214 0.03646787 -5.449219 0.0000 I((week - 3.5)^2) 0.0336681 0.01503318 2.239588 0.0256 groupNone-TCA:I(week - 3.5) 0.0179178 0.05864649 0.305522 0.7601 groupNone-TCA:I((week - 3.5)^2) -0.0633258 0.02417589 -2.619379 0.0091 Correlation: (Intr) gN-TCA I(-3.5 I((-3. gN-TCA:I(-3. groupNone-TCA -0.622 I(week - 3.5) 0.149 -0.093 I((week - 3.5)^2) -0.396 0.246 -0.251 groupNone-TCA:I(week - 3.5) -0.093 0.149 -0.622 0.156 groupNone-TCA:I((week - 3.5)^2) 0.246 -0.396 0.156 -0.622 -0.251 Standardized residuals: Min Q1 Med Q3 Max -2.2819415 -0.7772744 0.1442383 0.8123278 2.3494103 Residual standard error: 1.389483 Degrees of freedom: 450 total; 444 residual fit_cpm_uns %&gt;% emmeans::emmeans(~ week*group, at = list(week = seq(from = 1, to = 6, by = .01))) %&gt;% data.frame() %&gt;% ggplot(aes(x = week, y = emmean, fill = group)) + geom_ribbon(aes(ymin = emmean - SE, ymax = emmean + SE), alpha = .5) + geom_line(aes(linetype = group)) + theme_bw() + geom_vline(xintercept = 3.5, linetype = &quot;dashed&quot;) 14.4.2 Compound Symmetry (with constant correlation) fit_cpm_cps &lt;- nlme::gls(wpss ~ group*I(week-3.5) + group*I((week - 3.5)^2), data = df_long, correlation = corCompSymm(form = ~ 1 | id)) fit_cpm_cps %&gt;% emmeans::emmeans(~ week*group, at = list(week = seq(from = 1, to = 6, by = .01))) %&gt;% data.frame() %&gt;% ggplot(aes(x = week, y = emmean, fill = group)) + geom_ribbon(aes(ymin = emmean - SE, ymax = emmean + SE), alpha = .5) + geom_line(aes(linetype = group)) + theme_bw() + geom_vline(xintercept = 3.5, linetype = &quot;dashed&quot;) 14.4.3 Compare performance performance::compare_performance(fit_cpm_ar1, fit_cpm_cps, fit_cpm_uns) # Comparison of Model Performance Indices Name | Model | AIC (weights) | BIC (weights) | R2 | RMSE | Sigma ----------------------------------------------------------------------------- fit_cpm_ar1 | gls | 1013.2 (&lt;.001) | 1046.1 (&gt;.999) | 0.208 | 1.382 | 1.399 fit_cpm_cps | gls | 1202.1 (&lt;.001) | 1235.0 (&lt;.001) | 0.208 | 1.382 | 1.398 fit_cpm_uns | gls | 997.6 (&gt;.999) | 1088.0 (&lt;.001) | 0.208 | 1.382 | 1.389 "],["gee-continuous-outcome-beat-the-blues.html", "15 GEE, Continuous Outcome: Beat the Blues 15.1 Packages 15.2 Background 15.3 Exploratory Data Analysis 15.4 Multiple Regression (OLS) 15.5 Multilevel Models (MLM) 15.6 General Estimating Equations, GEE 15.7 Conclusion", " 15 GEE, Continuous Outcome: Beat the Blues 15.1 Packages 15.1.1 CRAN library(tidyverse) # all things tidy library(pander) # nice looking genderal tabulations library(furniture) # nice table1() descriptives library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(psych) # contains some useful functions, like headTail library(interactions) library(performance) library(lme4) # Linear, generalized linear, &amp; nonlinear mixed models library(corrplot) # Vizualize correlation matrix library(gee) # Genderalized Estimation Equation Solver library(geepack) # Genderalized Estimation Equation Package library(MuMIn) # Multi-Model Inference (caluclate QIC) library(HSAUR) # package with the dataset 15.1.2 GitHub Helper extract functions for exponentiation of parameters form generalized regression models within a texreg table of model parameters. # remotes::install_github(&quot;sarbearschwartz/texreghelpr&quot;) # first time library(texreghelpr) 15.2 Background This dataset was used as an example in Chapter 11 of “A Handbook of Statistical Analysis using R” by Brian S. Everitt and Torsten Hothorn. The authors include this data set in their HSAUR package on CRAN. The Background “These data resulted from a clinical trial of an interactive multimedia program called ‘Beat the Blues’ designed to deliver cognitive behavioral therapy (CBT) to depressed patients via a computer terminal. Full details are given here: Proudfoot et. al (2003), but in essence Beat the Blues is an interactive program using multimedia techniques, in particular video vignettes.” “In a randomized controlled trial of the program, patients with depression recruited in primary care were randomized to either the Beating the Blues program or to”Treatment as Usual” (TAU). Patients randomized to the BEat the Blues also received pharmacology and/or general practice (GP) support and practical/social help,offered as part of treatment as usual, with the exception of any face-to-face counseling or psychological intervention. Patients allocated to TAU received whatever treatment their GP prescribed. The latter included, besides any medication, discussion of problems with GP, provisions of practical/social help, referral to a counselor, referral to a practice nurse, referral to mental health professionals, or further physical examination. The Research Question Net of other factors (use of antidepressants and length of the current episode), does the Beat-the-Blues program results in better depression trajectories over treatment as usual? The Data The variables are as follows: drug did the patient take anti-depressant drugs (No or Yes) length the length of the current episode of depression, a factor with levels: “&lt;6m” less than six months “&gt;6m” more than six months treatment treatment group, a factor with levels: “TAU” treatment as usual “BtheB” Beat the Blues bdi.pre Beck Depression Inventory II, before treatment bdi.2m Beck Depression Inventory II, after 2 months bdi.4m Beck Depression Inventory II, after 4 months bdi.6m Beck Depression Inventory II, after 6 months bdi.8m Beck Depression Inventory II, after 8 months 15.2.1 Read in the data data(BtheB, package = &quot;HSAUR&quot;) BtheB %&gt;% psych::headTail() drug length treatment bdi.pre bdi.2m bdi.4m bdi.6m bdi.8m 1 No &gt;6m TAU 29 2 2 &lt;NA&gt; &lt;NA&gt; 2 Yes &gt;6m BtheB 32 16 24 17 20 3 Yes &lt;6m TAU 25 20 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 4 No &gt;6m BtheB 21 17 16 10 9 ... &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ... ... ... ... ... 97 Yes &lt;6m TAU 28 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 98 No &gt;6m BtheB 11 22 9 11 11 99 No &lt;6m TAU 13 5 5 0 6 100 Yes &lt;6m TAU 43 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 15.2.2 Wide Format Tidy up the dataset WIDE format * One line per person * Good for descriptive tables &amp; correlation matrices btb_wide &lt;- BtheB %&gt;% dplyr::mutate(id = row_number()) %&gt;% # create a new variable to ID participants dplyr::select(id, treatment, # specify that ID variable is first drug, length, bdi.pre, bdi.2m, bdi.4m, bdi.6m, bdi.8m) btb_wide %&gt;% psych::headTail(top = 10, bottom = 10) id treatment drug length bdi.pre bdi.2m bdi.4m bdi.6m bdi.8m 1 1 TAU No &gt;6m 29 2 2 &lt;NA&gt; &lt;NA&gt; 2 2 BtheB Yes &gt;6m 32 16 24 17 20 3 3 TAU Yes &lt;6m 25 20 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 4 4 BtheB No &gt;6m 21 17 16 10 9 5 5 BtheB Yes &gt;6m 26 23 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 6 6 BtheB Yes &lt;6m 7 0 0 0 0 7 7 TAU Yes &lt;6m 17 7 7 3 7 8 8 TAU No &gt;6m 20 20 21 19 13 9 9 BtheB Yes &lt;6m 18 13 14 20 11 10 10 BtheB Yes &gt;6m 20 5 5 8 12 ... ... &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ... ... ... ... ... 91 91 TAU No &lt;6m 16 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 92 92 BtheB Yes &lt;6m 30 26 28 &lt;NA&gt; &lt;NA&gt; 93 93 BtheB Yes &lt;6m 17 8 7 12 &lt;NA&gt; 94 94 BtheB No &gt;6m 19 4 3 3 3 95 95 BtheB No &gt;6m 16 11 4 2 3 96 96 BtheB Yes &gt;6m 16 16 10 10 8 97 97 TAU Yes &lt;6m 28 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 98 98 BtheB No &gt;6m 11 22 9 11 11 99 99 TAU No &lt;6m 13 5 5 0 6 100 100 TAU Yes &lt;6m 43 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 15.2.3 Long Format Restructure to long format LONG FORMAT * One line per observation * Good for plots and modeling btb_long &lt;- btb_wide %&gt;% tidyr::pivot_longer(cols = c(bdi.2m, bdi.4m, bdi.6m, bdi.8m), # all existing variables (not quoted) names_to = &quot;month&quot;, names_pattern = &quot;bdi.(.)m&quot;, values_to = &quot;bdi&quot;) %&gt;% dplyr::mutate(month = as.numeric(month)) %&gt;% dplyr::filter(complete.cases(id, bdi, treatment, month)) %&gt;% dplyr::arrange(id, month) %&gt;% dplyr::select(id, treatment, drug, length, bdi.pre, month, bdi) btb_long %&gt;% psych::headTail(top = 10, bottom = 10) id treatment drug length bdi.pre month bdi 1 1 TAU No &gt;6m 29 2 2 2 1 TAU No &gt;6m 29 4 2 3 2 BtheB Yes &gt;6m 32 2 16 4 2 BtheB Yes &gt;6m 32 4 24 5 2 BtheB Yes &gt;6m 32 6 17 6 2 BtheB Yes &gt;6m 32 8 20 7 3 TAU Yes &lt;6m 25 2 20 8 4 BtheB No &gt;6m 21 2 17 9 4 BtheB No &gt;6m 21 4 16 10 4 BtheB No &gt;6m 21 6 10 11 ... &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ... ... ... 12 96 BtheB Yes &gt;6m 16 6 10 13 96 BtheB Yes &gt;6m 16 8 8 14 98 BtheB No &gt;6m 11 2 22 15 98 BtheB No &gt;6m 11 4 9 16 98 BtheB No &gt;6m 11 6 11 17 98 BtheB No &gt;6m 11 8 11 18 99 TAU No &lt;6m 13 2 5 19 99 TAU No &lt;6m 13 4 5 20 99 TAU No &lt;6m 13 6 0 21 99 TAU No &lt;6m 13 8 6 15.3 Exploratory Data Analysis 15.3.1 Visualize: Person-profile Plots Create spaghetti plots of the raw, observed data btb_long %&gt;% ggplot(aes(x = month, y = bdi)) + geom_point() + geom_line(aes(group = id), size = 1, alpha = 0.3) + geom_smooth(method = &quot;lm&quot;) + theme_bw() + facet_grid(.~ treatment) + labs(title = &quot;BtheB - Observed Data Across Time with LM Smoother&quot;, subtitle = &quot;Seperate by Treatment&quot;) btb_long %&gt;% ggplot(aes(x = month, y = bdi)) + geom_point() + geom_line(aes(group = id), size = 1, alpha = 0.3) + geom_smooth(method = &quot;lm&quot;) + facet_grid(drug~ treatment, labeller = label_both) + theme_bw() + labs(title = &quot;BtheB - Observed Data Across Time with LM Smoother&quot;, subtitle = &quot;Seperate by Treatment &amp; Antidepressant Use&quot;) btb_long %&gt;% ggplot(aes(x = month, y = bdi)) + geom_point() + geom_line(aes(group = id, color = length), size = 1, alpha = 0.3) + geom_smooth(aes(color = length), method = &quot;lm&quot;, size = 1.25, se = FALSE) + facet_grid(drug~ treatment, labeller = label_both) + theme_bw() + labs(title = &quot;BtheB - Observed Data Across Time with LM Smoother&quot;, subtitle = &quot;Seperate by Treatment, Antidepressant Use, &amp; Length of Current Episode&quot;) btb_long %&gt;% ggplot(aes(x = month, y = bdi, color = treatment, fill = treatment)) + geom_smooth(method = &quot;lm&quot;) + theme_bw() + labs(title = &quot;BtheB - Predictions from TWO Seperate Single Simple Linear Models (lm)&quot;, subtitle = &quot;Assumes Independence of Repeated Measures&quot;) 15.3.2 Calculate the Observed Correlation Structure bdi_corr &lt;- btb_wide %&gt;% dplyr::select(starts_with(&quot;bdi&quot;)) %&gt;% stats::cor(use=&quot;pairwise.complete.obs&quot;) bdi_corr bdi.pre bdi.2m bdi.4m bdi.6m bdi.8m bdi.pre 1.0000000 0.6142207 0.5691248 0.5077286 0.3835090 bdi.2m 0.6142207 1.0000000 0.7903346 0.7849188 0.7038158 bdi.4m 0.5691248 0.7903346 1.0000000 0.8166591 0.7220149 bdi.6m 0.5077286 0.7849188 0.8166591 1.0000000 0.8107773 bdi.8m 0.3835090 0.7038158 0.7220149 0.8107773 1.0000000 15.3.3 Plot the correlation matrix to get a better feel for the pattern bdi_corr %&gt;% corrplot::corrplot.mixed(upper = &quot;ellipse&quot;) 15.4 Multiple Regression (OLS) This ignores any correlation between repeated measures on the same individual and treats all observations as independent. 15.4.1 Fit the models btb_lm_1 &lt;- stats::lm(bdi ~ bdi.pre + length + drug + treatment + month, data = btb_long) btb_lm_2 &lt;- stats::lm(bdi ~ bdi.pre + length + drug + treatment*month, data = btb_long) btb_lm_3 &lt;- stats::lm(bdi ~ bdi.pre + length + drug + treatment + drug*month, data = btb_long) btb_lm_4 &lt;- stats::lm(bdi ~ bdi.pre + length + drug*treatment*month, data = btb_long) 15.4.2 Parameter Estimates Table texreg::knitreg(list(btb_lm_1, btb_lm_2, btb_lm_3, btb_lm_4), label = &quot;lm&quot;, caption = &quot;OLS&quot;) OLS   Model 1 Model 2 Model 3 Model 4 (Intercept) 7.88*** 7.77*** 7.21*** 7.33**   (1.78) (2.08) (2.03) (2.30) bdi.pre 0.57*** 0.57*** 0.57*** 0.56***   (0.05) (0.05) (0.05) (0.05) length&gt;6m 1.75 1.75 1.78 1.86   (1.11) (1.11) (1.11) (1.10) drugYes -3.55** -3.55** -2.10 -2.00   (1.14) (1.15) (2.39) (3.75) treatmentBtheB -3.35** -3.13 -3.36** -3.31   (1.10) (2.36) (1.10) (3.13) month -0.96*** -0.93** -0.82** -0.60   (0.23) (0.34) (0.31) (0.40) treatmentBtheB:month   -0.05   -0.56     (0.47)   (0.63) drugYes:month     -0.32 -1.02       (0.47) (0.73) drugYes:treatmentBtheB       -0.23         (4.92) drugYes:treatmentBtheB:month       1.31         (0.98) R2 0.40 0.40 0.40 0.42 Adj. R2 0.39 0.38 0.39 0.40 Num. obs. 280 280 280 280 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 15.4.3 Plot the model predictions interactions::interact_plot(model = btb_lm_1, pred = month, modx = treatment, main.title = &quot;LM with 95% CI&quot;, interval = TRUE) # default = 95% confidence interval interactions::interact_plot(model = btb_lm_1, pred = month, modx = treatment, main.title = &quot;LM with 68% CI (M +/- SEM)&quot;, interval = TRUE, int.width = 0.68) # change to 68% CI, which is +/- 1 SEM interactions::interact_plot(model = btb_lm_1, pred = month, modx = treatment, interval = TRUE, colors = rep(&quot;black&quot;, times = 3), x.label = &quot;Month&quot;, y.label = &quot;Predicted BDI&quot;, legend.main = &quot;Baseline BDI:&quot;) + theme_bw() + theme(legend.position = &quot;bottom&quot;, legend.key.width = unit(2, &quot;cm&quot;)) + labs(title = &quot;BtheB - Predictions from the LM model (95% CI)&quot;, subtitle = &quot;Trajectory for a person with BL depression &lt; 6 months and randomized to TAU&quot;) effects::Effect(focal.predictors = c(&quot;treatment&quot;, &quot;month&quot;), mod = btb_lm_1) %&gt;% data.frame %&gt;% dplyr::mutate(treatment = fct_reorder2(treatment, month, fit)) %&gt;% ggplot(aes(x = month, y = fit)) + geom_line(aes(color = treatment)) + geom_ribbon(aes(ymin = lower, # 95% confidence intervals ymax = upper, fill = treatment), alpha = 0.3) + geom_ribbon(aes(ymin = fit - se, # Mean +/- 1 standard error for the mean (SEM) ymax = fit + se, fill = treatment), alpha = 0.3) + theme_bw() + labs(title = &quot;BtheB - Predictions from a Single Linear Model (lm)&quot;, subtitle = &quot;Assumes Independence of Repeated Measures&quot;) + theme(legend.position = c(1, 1), legend.justification = c(1.1, 1.1), legend.background = element_rect(color = &quot;black&quot;)) 15.5 Multilevel Models (MLM) 15.5.1 Fit the models btb_lmer_RI &lt;- lmerTest::lmer(bdi ~ bdi.pre + length + drug + treatment + month + (1 | id), data = btb_long, REML = TRUE) btb_lmer_RIAS &lt;- lmerTest::lmer(bdi ~ bdi.pre + length + drug + treatment + month + (month | id), data = btb_long, REML = TRUE, control = lmerControl(optimizer = &quot;Nelder_Mead&quot;)) 15.5.2 Parameter Estimates Table texreg::knitreg(list(btb_lm_1, btb_lmer_RI, btb_lmer_RIAS), custom.model.names = c(&quot;OLS&quot;, &quot;MLM-RI&quot;, &quot;MLM-RIAS&quot;), label = &quot;mlm&quot;, caption = &quot;LM vs. MLM&quot;) LM vs. MLM   OLS MLM-RI MLM-RIAS (Intercept) 7.88*** 5.92* 5.94**   (1.78) (2.31) (2.30) bdi.pre 0.57*** 0.64*** 0.64***   (0.05) (0.08) (0.08) length&gt;6m 1.75 0.24 0.10   (1.11) (1.68) (1.67) drugYes -3.55** -2.79 -2.89   (1.14) (1.77) (1.76) treatmentBtheB -3.35** -2.36 -2.49   (1.10) (1.71) (1.71) month -0.96*** -0.71*** -0.70***   (0.23) (0.15) (0.16) R2 0.40     Adj. R2 0.39     Num. obs. 280 280 280 AIC   1882.08 1885.16 BIC   1911.16 1921.50 Log Likelihood   -933.04 -932.58 Num. groups: id   97 97 Var: id (Intercept)   51.44 50.56 Var: Residual   25.27 23.87 Var: id month     0.23 Cov: id (Intercept) month     -0.31 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 15.5.3 Likelihood Ratio Test anova(btb_lmer_RI, btb_lmer_RIAS, refit = FALSE) Data: btb_long Models: btb_lmer_RI: bdi ~ bdi.pre + length + drug + treatment + month + (1 | id) btb_lmer_RIAS: bdi ~ bdi.pre + length + drug + treatment + month + (month | id) npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) btb_lmer_RI 8 1882.1 1911.2 -933.04 1866.1 btb_lmer_RIAS 10 1885.2 1921.5 -932.58 1865.2 0.9236 2 0.6301 performance::compare_performance(btb_lm_1, btb_lmer_RI, btb_lmer_RIAS, rank = TRUE) # Comparison of Model Performance Indices Name | Model | RMSE | Sigma | AIC weights | BIC weights | Performance-Score ----------------------------------------------------------------------------------------------- btb_lmer_RI | lmerModLmerTest | 4.234 | 5.027 | 0.831 | 0.995 | 97.86% btb_lmer_RIAS | lmerModLmerTest | 4.016 | 4.885 | 0.169 | 0.005 | 55.20% btb_lm_1 | lm | 8.560 | 8.654 | 8.42e-28 | 6.20e-27 | 0.00% 15.5.4 Plot the model predictions interactions::interact_plot(model = btb_lmer_RI, pred = month, modx = treatment, mod2 = drug, interval = TRUE) + theme_bw() + labs(title = &quot;BtheB - Predictions from a Multilevel Model (lmer)&quot;) + theme(legend.position = c(0, 0), legend.justification = c(-0.1, -0.1), legend.background = element_rect(color = &quot;black&quot;)) effects::Effect(c(&quot;treatment&quot;, &quot;month&quot;, &quot;drug&quot;), mod = btb_lmer_RI) %&gt;% data.frame %&gt;% dplyr::mutate(treatment = fct_reorder2(treatment, month, fit)) %&gt;% ggplot(aes(x = month, y = fit)) + geom_line(aes(color = treatment)) + geom_ribbon(aes(ymin = lower, ymax = upper, fill = treatment), alpha = 0.3) + theme_bw() + facet_grid(.~ drug, labeller = label_both) + labs(title = &quot;BtheB - Predictions from a Multilevel Model (lmer)&quot;) + theme(legend.position = c(0, 0), legend.justification = c(-0.1, -0.1), legend.background = element_rect(color = &quot;black&quot;)) 15.6 General Estimating Equations, GEE 15.6.1 Explore Various Correlation Structures 15.6.1.1 Fit the models - Main effects to determine correlation structure Use the gee() function from the gee package for the results to be used in a texreg::knitreg() tables. The output below each model is the ‘starting’ model assuming independence, so they will all be the same here. btb_gee_in &lt;- gee::gee(bdi ~ bdi.pre + length + drug + treatment + month, data = btb_long, id = id, family = gaussian, corstr = &#39;independence&#39;) (Intercept) bdi.pre length&gt;6m drugYes treatmentBtheB 7.8830747 0.5723729 1.7530800 -3.5460058 -3.3539662 month -0.9608077 btb_gee_ex &lt;- gee::gee(bdi ~ bdi.pre + length + drug + treatment + month, data = btb_long, id = id, family = gaussian, corstr = &#39;exchangeable&#39;) (Intercept) bdi.pre length&gt;6m drugYes treatmentBtheB 7.8830747 0.5723729 1.7530800 -3.5460058 -3.3539662 month -0.9608077 # The AR-1 fails if any subjects have only 1 observation # to use this one, we would need to remove participants with only 1 BDI # btb_gee_ar &lt;- gee(bdi ~ bdi.pre + length + drug + treatment + month, # data = btb_long, # id = id, # family = gaussian, # corstr = &#39;AR-M&#39;, # Mv = 1) btb_gee_un &lt;- gee::gee(bdi ~ bdi.pre + length + drug + treatment + month, data = btb_long, id = id, family = gaussian, corstr = &#39;unstructured&#39;) (Intercept) bdi.pre length&gt;6m drugYes treatmentBtheB 7.8830747 0.5723729 1.7530800 -3.5460058 -3.3539662 month -0.9608077 summary(btb_gee_in) GEE: GENERALIZED LINEAR MODELS FOR DEPENDENT DATA gee S-function, version 4.13 modified 98/01/27 (1998) Model: Link: Identity Variance to Mean Relation: Gaussian Correlation Structure: Independent Call: gee::gee(formula = bdi ~ bdi.pre + length + drug + treatment + month, id = id, data = btb_long, family = gaussian, corstr = &quot;independence&quot;) Summary of Residuals: Min 1Q Median 3Q Max -24.20158432 -5.31202378 0.01101526 5.29503741 27.77789553 Coefficients: Estimate Naive S.E. Naive z Robust S.E. Robust z (Intercept) 7.8830747 1.78048908 4.427477 2.19973944 3.583640 bdi.pre 0.5723729 0.05486079 10.433188 0.08853253 6.465114 length&gt;6m 1.7530800 1.10849861 1.581490 1.41954159 1.234962 drugYes -3.5460058 1.14469086 -3.097785 1.73069664 -2.048889 treatmentBtheB -3.3539662 1.09831939 -3.053726 1.71390982 -1.956909 month -0.9608077 0.23263437 -4.130119 0.17688635 -5.431780 Estimated Scale Parameter: 74.8854 Number of Iterations: 1 Working Correlation [,1] [,2] [,3] [,4] [1,] 1 0 0 0 [2,] 0 1 0 0 [3,] 0 0 1 0 [4,] 0 0 0 1 summary(btb_gee_ex) GEE: GENERALIZED LINEAR MODELS FOR DEPENDENT DATA gee S-function, version 4.13 modified 98/01/27 (1998) Model: Link: Identity Variance to Mean Relation: Gaussian Correlation Structure: Exchangeable Call: gee::gee(formula = bdi ~ bdi.pre + length + drug + treatment + month, id = id, data = btb_long, family = gaussian, corstr = &quot;exchangeable&quot;) Summary of Residuals: Min 1Q Median 3Q Max -25.4478843 -6.3276726 -0.8152833 4.3622258 25.4078115 Coefficients: Estimate Naive S.E. Naive z Robust S.E. Robust z (Intercept) 5.8855129 2.32380381 2.5327065 2.10712166 2.7931529 bdi.pre 0.6399964 0.08033495 7.9665999 0.07931263 8.0692874 length&gt;6m 0.2084783 1.69179766 0.1232288 1.48052530 0.1408137 drugYes -2.7742506 1.78397557 -1.5550945 1.64824318 -1.6831561 treatmentBtheB -2.3360241 1.72621751 -1.3532617 1.66217026 -1.4054060 month -0.7078407 0.14254124 -4.9658660 0.15394156 -4.5981134 Estimated Scale Parameter: 77.14393 Number of Iterations: 5 Working Correlation [,1] [,2] [,3] [,4] [1,] 1.0000000 0.6915241 0.6915241 0.6915241 [2,] 0.6915241 1.0000000 0.6915241 0.6915241 [3,] 0.6915241 0.6915241 1.0000000 0.6915241 [4,] 0.6915241 0.6915241 0.6915241 1.0000000 summary(btb_gee_un) GEE: GENERALIZED LINEAR MODELS FOR DEPENDENT DATA gee S-function, version 4.13 modified 98/01/27 (1998) Model: Link: Identity Variance to Mean Relation: Gaussian Correlation Structure: Unstructured Call: gee::gee(formula = bdi ~ bdi.pre + length + drug + treatment + month, id = id, data = btb_long, family = gaussian, corstr = &quot;unstructured&quot;) Summary of Residuals: Min 1Q Median 3Q Max -25.1527937 -6.1091139 -0.5896205 4.7316139 25.9041542 Coefficients: Estimate Naive S.E. Naive z Robust S.E. Robust z (Intercept) 6.3905215 2.28769760 2.793429 2.15668950 2.9631162 bdi.pre 0.6171798 0.07744569 7.969195 0.08081777 7.6366846 length&gt;6m 0.5834398 1.61626647 0.360980 1.46837275 0.3973377 drugYes -2.7908835 1.69816226 -1.643473 1.63741987 -1.7044398 treatmentBtheB -2.4261698 1.64272613 -1.476917 1.65519523 -1.4657907 month -0.7628336 0.18121518 -4.209546 0.15643591 -4.8763329 Estimated Scale Parameter: 76.40371 Number of Iterations: 5 Working Correlation [,1] [,2] [,3] [,4] [1,] 1.0000000 0.7069560 0.5704892 0.4714744 [2,] 0.7069560 1.0000000 0.6086188 0.4637445 [3,] 0.5704892 0.6086188 1.0000000 0.5454963 [4,] 0.4714744 0.4637445 0.5454963 1.0000000 15.6.1.2 Parameter Estimates Table texreg::knitreg(list(btb_lm_1, btb_lmer_RI, btb_gee_in, btb_gee_ex, btb_gee_un), custom.model.names = c(&quot;OLS&quot;, &quot;MLM-RI&quot;, &quot;GEE-in&quot;, &quot;GEE-ex&quot;, &quot;GEE-un&quot;), label = &quot;GEEs&quot;, caption = &quot;LM, MLM, and GEE&quot;) LM, MLM, and GEE   OLS MLM-RI GEE-in GEE-ex GEE-un (Intercept) 7.88*** 5.92* 7.88*** 5.89** 6.39**   (1.78) (2.31) (2.20) (2.11) (2.16) bdi.pre 0.57*** 0.64*** 0.57*** 0.64*** 0.62***   (0.05) (0.08) (0.09) (0.08) (0.08) length&gt;6m 1.75 0.24 1.75 0.21 0.58   (1.11) (1.68) (1.42) (1.48) (1.47) drugYes -3.55** -2.79 -3.55* -2.77 -2.79   (1.14) (1.77) (1.73) (1.65) (1.64) treatmentBtheB -3.35** -2.36 -3.35 -2.34 -2.43   (1.10) (1.71) (1.71) (1.66) (1.66) month -0.96*** -0.71*** -0.96*** -0.71*** -0.76***   (0.23) (0.15) (0.18) (0.15) (0.16) R2 0.40         Adj. R2 0.39         Num. obs. 280 280 280 280 280 AIC   1882.08       BIC   1911.16       Log Likelihood   -933.04       Num. groups: id   97       Var: id (Intercept)   51.44       Var: Residual   25.27       Scale     74.89 77.14 76.40 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 15.6.1.3 Re-Fit Models Use the geeglm() function from the geepack package for the results to be used in a anova() table and interaction plots. This function does NOT produce the same starting model output as gee::gee(). btb_geeglm_in &lt;- geepack::geeglm(bdi ~ bdi.pre + length + drug + treatment + month, data = btb_long, id = id, wave = month, family = gaussian, corstr = &#39;independence&#39;) btb_geeglm_ex &lt;- geepack::geeglm(bdi ~ bdi.pre + length + drug + treatment + month, data = btb_long, id = id, wave = month, family = gaussian, corstr = &#39;exchangeable&#39;) btb_geeglm_ar &lt;- geepack::geeglm(bdi ~ bdi.pre + length + drug + treatment + month, data = btb_long, id = id, wave = month, family = gaussian, corstr = &#39;ar1&#39;) btb_geeglm_un &lt;- geepack::geeglm(bdi ~ bdi.pre + length + drug + treatment + month, data = btb_long, id = id, wave = month, family = gaussian, corstr = &#39;unstructured&#39;) Can’t Use the Likelihood Ratio Test The anova() function is used to compare nested models for parameters (fixed effects), not correlation structures. anova(btb_geeglm_in, btb_geeglm_ex) Models are identical NULL anova(btb_geeglm_in, btb_geeglm_ar) Models are identical NULL anova(btb_geeglm_in, btb_geeglm_un) Models are identical NULL 15.6.1.4 Variaous QIC Measures of Fit References: Pan, W. 2001. Akaike’s information criterion in generalized estimating equations. Biometrics 57:120-125. https://onlinelibrary.wiley.com/doi/abs/10.1111/j.0006-341X.2001.00120.x Burnham, K. P. and D. R. Anderson. 2002. Model selection and multimodel inference: a practical information-theoretic approach. Second edition. Springer Science and Business Media, Inc., New York. https://cds.cern.ch/record/1608735/files/9780387953649_TOC.pdf The QIC() is one way to try to measure model fit. You can enter more than one model into a single function call. QIC(I) based on independence model &lt;– suggested by Pan (Biometric, March 2001), asymptotically unbiased estimator (choose the correlation stucture that produces the smallest QIC(I), p122) MuMIn::QIC(btb_geeglm_in, btb_geeglm_ex, btb_geeglm_ar, btb_geeglm_un, typeR = FALSE) %&gt;% # default pander::pander(caption = &quot;QIC&quot;) QIC   QIC btb_geeglm_in 307 btb_geeglm_ex 296 btb_geeglm_ar 298 btb_geeglm_un 297 QIC(R) is based on quasi-likelihood of a working correlation R model, can NOT be used to select the working correlation matrix. MuMIn::QIC(btb_geeglm_in, btb_geeglm_ex, btb_geeglm_ar, btb_geeglm_un, typeR = TRUE) # NOT the default QIC btb_geeglm_in 306.5589 btb_geeglm_ex 304.5003 btb_geeglm_ar 304.6425 btb_geeglm_un 304.4087 QIC_U(R) approximates QIC(R), and while both are useful for variable selection, they can NOT be applied to select the working correlation matrix. MuMIn::QICu(btb_geeglm_in, btb_geeglm_ex, btb_geeglm_ar, btb_geeglm_un) QICu btb_geeglm_in 292.0000 btb_geeglm_ex 283.7551 btb_geeglm_ar 285.6132 btb_geeglm_un 284.1707 MuMIn::model.sel(btb_geeglm_in, btb_geeglm_ex, btb_geeglm_ar, btb_geeglm_un, rank = &quot;QIC&quot;) #sorts the best to the TOP, uses QIC(I) Model selection table (Int) bdi.pre drg lng mnt trt corstr qLik QIC delta weight btb_geeglm_ex 5.880 0.6402 + + -0.7070 + exchng -140 296.3 0.00 0.450 btb_geeglm_un 6.068 0.6307 + + -0.7061 + unstrc -140 296.6 0.32 0.382 btb_geeglm_ar 6.620 0.5956 + + -0.7357 + ar1 -140 298.3 2.00 0.165 btb_geeglm_in 7.883 0.5724 + + -0.9608 + indpnd -140 306.6 10.30 0.003 Abbreviations: corstr: exchng = &#39;exchangeable&#39;, indpnd = &#39;independence&#39;, unstrc = &#39;unstructured&#39; Models ranked by QIC(x) 15.6.2 Final “Best” Model 15.6.2.1 Tabulate Model Parameters texreg::knitreg(list(btb_gee_ex), custom.model.names = c(&quot;GEE-ex&quot;), single.row = TRUE, caption = &quot;Final GEE, exchangeable correlation&quot;) Final GEE, exchangeable correlation   GEE-ex (Intercept) 5.89 (2.11)** bdi.pre 0.64 (0.08)*** length&gt;6m 0.21 (1.48) drugYes -2.77 (1.65) treatmentBtheB -2.34 (1.66) month -0.71 (0.15)*** Scale 77.14 Num. obs. 280 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 15.6.2.2 Plot the Marginal Effects interactions::interact_plot(model = btb_geeglm_ex, pred = &quot;month&quot;, modx = &quot;treatment&quot;) Do not worry about confidence intervals. expand.grid(bdi.pre = 23, length = &quot;&lt;6m&quot;, drug = &quot;No&quot;, treatment = levels(btb_long$treatment), month = seq(from = 2, to = 8, by = 2)) %&gt;% dplyr::mutate(fit_in = predict(btb_geeglm_in, newdata = ., type = &quot;response&quot;)) %&gt;% dplyr::mutate(fit_ex = predict(btb_geeglm_ex, newdata = ., type = &quot;response&quot;)) %&gt;% dplyr::mutate(fit_ar = predict(btb_geeglm_ar, newdata = ., type = &quot;response&quot;)) %&gt;% dplyr::mutate(fit_un = predict(btb_geeglm_un, newdata = ., type = &quot;response&quot;)) %&gt;% tidyr::pivot_longer(cols = starts_with(&quot;fit_&quot;), names_to = &quot;covR&quot;, names_pattern = &quot;fit_(.*)&quot;, names_ptype = list(covR = &quot;factor()&quot;), values_to = &quot;fit&quot;) %&gt;% dplyr::mutate(covR = factor(covR, levels = c(&quot;un&quot;, &quot;ar&quot;, &quot;ex&quot;, &quot;in&quot;), labels = c(&quot;Unstructured&quot;, &quot;Auto-Regressive&quot;, &quot;Compound Symetry&quot;, &quot;Independence&quot;))) %&gt;% ggplot(aes(x = month, y = fit, linetype = treatment)) + geom_line(alpha = 0.6) + theme_bw() + labs(title = &quot;BtheB - Predictions from four GEE models (geeglm)&quot;, x = &quot;Month&quot;, y = &quot;Predicted BDI&quot;, linetype = &quot;Treatment:&quot;) + scale_linetype_manual(values = c(&quot;solid&quot;, &quot;longdash&quot;)) + theme(legend.key.width = unit(1, &quot;cm&quot;)) + facet_wrap(~ covR) expand.grid(bdi.pre = 23, length = &quot;&lt;6m&quot;, drug = &quot;No&quot;, treatment = levels(btb_long$treatment), month = seq(from = 2, to = 8, by = 2)) %&gt;% dplyr::mutate(fit_in = predict(btb_geeglm_in, newdata = ., type = &quot;response&quot;)) %&gt;% dplyr::mutate(fit_ex = predict(btb_geeglm_ex, newdata = ., type = &quot;response&quot;)) %&gt;% dplyr::mutate(fit_ar = predict(btb_geeglm_ar, newdata = ., type = &quot;response&quot;)) %&gt;% dplyr::mutate(fit_un = predict(btb_geeglm_un, newdata = ., type = &quot;response&quot;)) %&gt;% tidyr::pivot_longer(cols = starts_with(&quot;fit_&quot;), names_to = &quot;covR&quot;, names_pattern = &quot;fit_(.*)&quot;, names_ptype = list(covR = &quot;factor()&quot;), values_to = &quot;fit&quot;) %&gt;% dplyr::mutate(covR = factor(covR, levels = c(&quot;un&quot;, &quot;ar&quot;, &quot;ex&quot;, &quot;in&quot;), labels = c(&quot;Unstructured&quot;, &quot;Auto-Regressive&quot;, &quot;Compound Symetry&quot;, &quot;Independence&quot;))) %&gt;% ggplot(aes(x = month, y = fit, color = covR, linetype = treatment)) + geom_line(alpha = 0.6) + theme_bw() + labs(title = &quot;BtheB - Predictions from four GEE models (geeglm)&quot;, x = &quot;Month&quot;, y = &quot;Predicted BDI&quot;, color = &quot;Covariance Pattern:&quot;, linetype = &quot;Treatment:&quot;) + scale_linetype_manual(values = c(&quot;solid&quot;, &quot;longdash&quot;)) + scale_size_manual(values = c(2, 1, 1, 1)) + scale_color_manual(values = c(&quot;red&quot;, &quot;dodgerblue&quot;, &quot;blue&quot;, &quot;darkgreen&quot;)) + theme(legend.key.width = unit(1, &quot;cm&quot;)) 15.6.3 Investigate interactions NOT with time (month) btb_geeglm_ex_1 &lt;- geepack::geeglm(bdi ~ bdi.pre*length + drug + treatment + month, data = btb_long, id = id, wave = month, family = gaussian, corstr = &#39;exchangeable&#39;) btb_geeglm_ex_2 &lt;- geepack::geeglm(bdi ~ bdi.pre*drug + length + treatment + month, data = btb_long, id = id, wave = month, family = gaussian, corstr = &#39;exchangeable&#39;) btb_geeglm_ex_3 &lt;- geepack::geeglm(bdi ~ bdi.pre*treatment + length + drug + month, data = btb_long, id = id, wave = month, family = gaussian, corstr = &#39;exchangeable&#39;) texreg::knitreg(list(btb_geeglm_ex, btb_geeglm_ex_1, btb_geeglm_ex_2, btb_geeglm_ex_3), custom.model.names = c(&quot;None&quot;, &quot;Length&quot;, &quot;Antidpressant&quot;, &quot;Treatment&quot;), label = &quot;GEE_inter1&quot;, caption = &quot;GEE (exchangable): Interactions, not with Time&quot;) GEE (exchangable): Interactions, not with Time   None Length Antidpressant Treatment (Intercept) 5.88** 3.24 3.37 3.62   (2.11) (1.99) (2.68) (2.78) bdi.pre 0.64*** 0.76*** 0.75*** 0.74***   (0.08) (0.08) (0.12) (0.12) length&gt;6m 0.20 5.66 0.47 0.22   (1.48) (3.27) (1.46) (1.48) drugYes -2.77 -2.25 1.47 -2.76   (1.65) (1.62) (3.20) (1.64) treatmentBtheB -2.33 -2.42 -2.19 1.24   (1.66) (1.63) (1.67) (3.38) month -0.71*** -0.70*** -0.71*** -0.71***   (0.15) (0.15) (0.15) (0.15) bdi.pre:length&gt;6m   -0.23         (0.16)     bdi.pre:drugYes     -0.19         (0.17)   bdi.pre:treatmentBtheB       -0.15         (0.17) Scale parameter: gamma 75.50 74.56 73.92 74.15 Scale parameter: SE 10.68 10.71 10.15 10.72 Correlation parameter: alpha 0.69 0.69 0.68 0.68 Correlation parameter: SE 0.11 0.12 0.10 0.11 Num. obs. 280 280 280 280 Num. clust. 97 97 97 97 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 15.6.4 Investigate interactions with time (month) btb_geeglm_ex_11 &lt;- geepack::geeglm(bdi ~ bdi.pre + length + drug + treatment*month, data = btb_long, id = id, wave = month, family = gaussian, corstr = &#39;exchangeable&#39;) btb_geeglm_ex_12 &lt;- geepack::geeglm(bdi ~ bdi.pre + length + treatment + drug*month, data = btb_long, id = id, wave = month, family = gaussian, corstr = &#39;exchangeable&#39;) btb_geeglm_ex_13 &lt;- geepack::geeglm(bdi ~ bdi.pre + drug + treatment + length*month, data = btb_long, id = id, wave = month, family = gaussian, corstr = &#39;exchangeable&#39;) btb_geeglm_ex_14 &lt;- geepack::geeglm(bdi ~ length + drug + treatment + bdi.pre*month, data = btb_long, id = id, wave = month, family = gaussian, corstr = &#39;exchangeable&#39;) texreg::knitreg(list(btb_geeglm_ex, btb_geeglm_ex_11, btb_geeglm_ex_12, btb_geeglm_ex_13, btb_geeglm_ex_14), custom.model.names = c(&quot;None&quot;, &quot;Treatment&quot;, &quot;Antidepressant&quot;, &quot;Length&quot;, &quot;BL BDI&quot;), label=&quot;GEEinter2&quot;, caption = &quot;GEE (exchangable): Interactions with Time&quot;) GEE (exchangable): Interactions with Time   None Treatment Antidepressant Length BL BDI (Intercept) 5.88** 6.83** 6.63** 6.28** 5.09*   (2.11) (2.22) (2.21) (2.15) (2.34) bdi.pre 0.64*** 0.64*** 0.64*** 0.64*** 0.67***   (0.08) (0.08) (0.08) (0.08) (0.09) length&gt;6m 0.20 0.25 0.16 -0.44 0.27   (1.48) (1.49) (1.49) (1.83) (1.48) drugYes -2.77 -2.74 -4.31* -2.79 -2.72   (1.65) (1.66) (2.01) (1.64) (1.66) treatmentBtheB -2.33 -4.23* -2.32 -2.31 -2.36   (1.66) (2.00) (1.67) (1.66) (1.66) month -0.71*** -0.95*** -0.88*** -0.80*** -0.50   (0.15) (0.26) (0.21) (0.16) (0.35) treatmentBtheB:month   0.47           (0.30)       drugYes:month     0.38           (0.30)     length&gt;6m:month       0.16           (0.29)   bdi.pre:month         -0.01           (0.01) Scale parameter: gamma 75.50 76.03 76.13 75.21 75.17 Scale parameter: SE 10.68 10.83 10.81 10.63 10.79 Correlation parameter: alpha 0.69 0.70 0.70 0.69 0.69 Correlation parameter: SE 0.11 0.11 0.11 0.11 0.11 Num. obs. 280 280 280 280 280 Num. clust. 97 97 97 97 97 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 Now only plot the significant variables for the ‘best’ model. interactions::interact_plot(model = btb_geeglm_ex, pred = &quot;month&quot;, modx = &quot;bdi.pre&quot;) interactions::interact_plot(model = btb_geeglm_ex, pred = &quot;month&quot;, modx = &quot;bdi.pre&quot;, modx.values = c(10, 20, 30), at = list(length = &quot;&lt;6m&quot;, drug = &quot;No&quot;, treatment = &quot;TAU&quot;), colors = rep(&quot;black&quot;, times = 3), x.label = &quot;Month&quot;, y.label = &quot;Predicted BDI&quot;, legend.main = &quot;Baseline BDI:&quot;) + theme_bw() + theme(legend.position = &quot;bottom&quot;, legend.key.width = unit(2, &quot;cm&quot;)) + labs(title = &quot;BtheB - Predictions from the GEE model (exchangable)&quot;, subtitle = &quot;Trajectory for a person with BL depression &lt; 6 months and randomized to TAU&quot;) 15.7 Conclusion The Research Question Does the Beat-the-Blues program Net of other factors (use of antidepressants and length of the current episode), does the Beat-the-Blues program results in better depression trajectories over treatment as usual? The Conclusion There is no evidence that depression trajectories differ between participants randomized to the Beat the Blues program or the Treatment as Usual condition, after accounting for covariates and the correlation between repeated measurements. "],["gee-binary-outcome-respiratory-illness.html", "16 GEE, Binary Outcome: Respiratory Illness 16.1 Packages 16.2 Background 16.3 Exploratory Data Analysis 16.4 Logisitc Regression (GLM) 16.5 Generalized Estimating Equations (GEE) 16.6 Conclusion", " 16 GEE, Binary Outcome: Respiratory Illness 16.1 Packages 16.1.1 CRAN library(tidyverse) # all things tidy library(pander) # nice looking genderal tabulations library(furniture) # nice table1() descriptives library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(psych) # contains some useful functions, like headTail library(interactions) library(performance) library(lme4) # Linear, generalized linear, &amp; nonlinear mixed models library(corrplot) # Vizualize correlation matrix library(gee) # Genderalized Estimation Equation Solver library(geepack) # Genderalized Estimation Equation Package library(MuMIn) # Multi-Model Inference (caluclate QIC) library(HSAUR) # package with the dataset 16.1.2 GitHub Helper extract functions for exponentiation of parameters form generalized regression models within a texreg table of model parameters. # remotes::install_github(&quot;sarbearschwartz/texreghelpr&quot;) # first time library(texreghelpr) 16.2 Background This dataset was used as an example in Chapter 11 of “A Handbook of Statistical Analysis using R” by Brian S. Everitt and Torsten Hothorn. The authors include this data set in their HSAUR package on CRAN. The Background In each of two centers, eligible patients were randomly assigned to active treatment or placebo. During the treatment, the respiratory status (categorized poor or good) was determined at each of four, monthly visits. The trial recruited 111 participants (54 in the active group, 57 in the placebo group) and there were no missing data for either the responses or the covariates. The Research Question The question of interest is to assess whether the treatment is effective and to estimate its effect. Note: We are NOT interested in change over time, but rather mean differences in the treatment group compared to the placebo group, net of any potential confounding due to age, sex, and site. The Data Note that the data (555 observations on the following 7 variables) are in long form, i.e, repeated measurements are stored as additional rows in the data frame. Indicators subject the patient ID, a factor with levels 1 to 111 centre the study center, a factor with levels 1 and 2 month the month, each patient was examined at months 0, 1, 2, 3 and 4 Outcome or dependent variable status the respiratory status (response variable), a factor with levels poor and good Main predictor or independent variable of interest treatment the treatment arm, a factor with levels placebo and treatment Time-invariant Covariates to control for sex a factor with levels female and male age the age of the patient 16.2.1 Read in the data data(&quot;respiratory&quot;, package = &quot;HSAUR&quot;) str(respiratory) &#39;data.frame&#39;: 555 obs. of 7 variables: $ centre : Factor w/ 2 levels &quot;1&quot;,&quot;2&quot;: 1 1 1 1 1 1 1 1 1 1 ... $ treatment: Factor w/ 2 levels &quot;placebo&quot;,&quot;treatment&quot;: 1 1 1 1 1 1 1 1 1 1 ... $ sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 1 1 1 1 1 1 1 1 1 1 ... $ age : num 46 46 46 46 46 28 28 28 28 28 ... $ status : Factor w/ 2 levels &quot;poor&quot;,&quot;good&quot;: 1 1 1 1 1 1 1 1 1 1 ... $ month : Ord.factor w/ 5 levels &quot;0&quot;&lt;&quot;1&quot;&lt;&quot;2&quot;&lt;&quot;3&quot;&lt;..: 1 2 3 4 5 1 2 3 4 5 ... $ subject : Factor w/ 111 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 1 1 1 2 2 2 2 2 ... psych::headTail(respiratory) centre treatment sex age status month subject 1 1 placebo female 46 poor 0 1 112 1 placebo female 46 poor 1 1 223 1 placebo female 46 poor 2 1 334 1 placebo female 46 poor 3 1 ... &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ... &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 222 2 treatment female 31 good 1 111 333 2 treatment female 31 good 2 111 444 2 treatment female 31 good 3 111 555 2 treatment female 31 good 4 111 16.2.2 Wide Format Wide format has one line per participant. data_wide &lt;- respiratory %&gt;% tidyr::spread(key = month, value = status, sep = &quot;_&quot;) %&gt;% dplyr::rename(&quot;BL_status&quot; = &quot;month_0&quot;) %&gt;% dplyr::arrange(subject) %&gt;% dplyr::select(subject, centre, sex, age, treatment, BL_status, starts_with(&quot;month&quot;)) tibble::glimpse(data_wide) Rows: 111 Columns: 10 $ subject &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1… $ centre &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … $ sex &lt;fct&gt; female, female, female, female, male, female, female, female… $ age &lt;dbl&gt; 46, 28, 23, 44, 13, 34, 43, 28, 31, 37, 30, 14, 23, 30, 20, … $ treatment &lt;fct&gt; placebo, placebo, treatment, placebo, placebo, treatment, pl… $ BL_status &lt;fct&gt; poor, poor, good, good, good, poor, poor, poor, good, good, … $ month_1 &lt;fct&gt; poor, poor, good, good, good, poor, good, poor, good, poor, … $ month_2 &lt;fct&gt; poor, poor, good, good, good, poor, poor, poor, good, good, … $ month_3 &lt;fct&gt; poor, poor, good, good, good, poor, good, poor, good, good, … $ month_4 &lt;fct&gt; poor, poor, good, poor, good, poor, good, poor, good, poor, … psych::headTail(data_wide) subject centre sex age treatment BL_status month_1 month_2 month_3 1 1 1 female 46 placebo poor poor poor poor 2 2 1 female 28 placebo poor poor poor poor 3 3 1 female 23 treatment good good good good 4 4 1 female 44 placebo good good good good ... &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ... &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 108 108 2 male 39 treatment poor good good good 109 109 2 female 68 treatment poor good good good 110 110 2 male 63 treatment good good good good 111 111 2 female 31 treatment good good good good month_4 1 poor 2 poor 3 good 4 poor ... &lt;NA&gt; 108 good 109 good 110 good 111 good 16.2.3 Long Format Long format has one line per observation. data_long &lt;- data_wide%&gt;% tidyr::gather(key = month, value = status, starts_with(&quot;month&quot;)) %&gt;% dplyr::mutate(month = str_sub(month, start = -1) %&gt;% as.numeric) %&gt;% dplyr::mutate(status = case_when(status == &quot;poor&quot; ~ 0, status == &quot;good&quot; ~ 1)) %&gt;% dplyr::arrange(subject, month) %&gt;% dplyr::select(subject, centre, sex, age, treatment, BL_status, month, status) tibble::glimpse(data_long) Rows: 444 Columns: 8 $ subject &lt;fct&gt; 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, … $ centre &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … $ sex &lt;fct&gt; female, female, female, female, female, female, female, fema… $ age &lt;dbl&gt; 46, 46, 46, 46, 28, 28, 28, 28, 23, 23, 23, 23, 44, 44, 44, … $ treatment &lt;fct&gt; placebo, placebo, placebo, placebo, placebo, placebo, placeb… $ BL_status &lt;fct&gt; poor, poor, poor, poor, poor, poor, poor, poor, good, good, … $ month &lt;dbl&gt; 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, … $ status &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, … psych::headTail(data_long) subject centre sex age treatment BL_status month status 1 1 1 female 46 placebo poor 1 0 2 1 1 female 46 placebo poor 2 0 3 1 1 female 46 placebo poor 3 0 4 1 1 female 46 placebo poor 4 0 ... &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ... &lt;NA&gt; &lt;NA&gt; ... ... 441 111 2 female 31 treatment good 1 1 442 111 2 female 31 treatment good 2 1 443 111 2 female 31 treatment good 3 1 444 111 2 female 31 treatment good 4 1 16.3 Exploratory Data Analysis 16.3.1 Summary Statistics 16.3.1.1 Demographics and Baseline Measure Notice that numerical summaries are computed for all variables, even the categorical variables (i.e. factors). The have an * after the variable name to remind you that the mean, sd, and se are of limited use. Notice: the mean age is 33 data_wide %&gt;% psych::describe(skew = FALSE) vars n mean sd min max range se subject* 1 111 56.00 32.19 1 111 110 3.06 centre* 2 111 1.50 0.50 1 2 1 0.05 sex* 3 111 1.21 0.41 1 2 1 0.04 age 4 111 33.28 13.65 11 68 57 1.30 treatment* 5 111 1.49 0.50 1 2 1 0.05 BL_status* 6 111 1.45 0.50 1 2 1 0.05 month_1* 7 111 1.59 0.49 1 2 1 0.05 month_2* 8 111 1.54 0.50 1 2 1 0.05 month_3* 9 111 1.59 0.49 1 2 1 0.05 month_4* 10 111 1.53 0.50 1 2 1 0.05 data_wide %&gt;% dplyr::group_by(treatment) %&gt;% furniture::table1(&quot;Center&quot; = centre, &quot;Sex&quot; = sex, &quot;Age&quot; = age, &quot;Baseline Status&quot; = BL_status, caption = &quot;Participant Demographics&quot;, output = &quot;markdown&quot;, na.rm = FALSE, total = TRUE, test = TRUE) Table 16.1: Participant Demographics Total placebo treatment P-Value n = 111 n = 57 n = 54 Center 1 1 56 (50.5%) 29 (50.9%) 27 (50%) 2 55 (49.5%) 28 (49.1%) 27 (50%) NA 0 (0%) 0 (0%) 0 (0%) Sex 0.028 female 88 (79.3%) 40 (70.2%) 48 (88.9%) male 23 (20.7%) 17 (29.8%) 6 (11.1%) NA 0 (0%) 0 (0%) 0 (0%) Age 0.771 33.3 (13.7) 33.6 (13.4) 32.9 (14.0) Baseline Status 1 poor 61 (55%) 31 (54.4%) 30 (55.6%) good 50 (45%) 26 (45.6%) 24 (44.4%) NA 0 (0%) 0 (0%) 0 (0%) 16.3.1.2 Status Over Time data_wide %&gt;% dplyr::group_by(treatment) %&gt;% furniture::table1(&quot;Month One&quot; = month_1, &quot;Month Two&quot; = month_2, &quot;Month Three&quot; = month_3, &quot;Month Four&quot; = month_4, caption = &quot;Respiratory Status Over Time&quot;, output = &quot;markdown&quot;, na.rm = FALSE, total = TRUE, test = TRUE) Table 16.2: Respiratory Status Over Time Total placebo treatment P-Value n = 111 n = 57 n = 54 Month One 0.06 poor 46 (41.4%) 29 (50.9%) 17 (31.5%) good 65 (58.6%) 28 (49.1%) 37 (68.5%) NA 0 (0%) 0 (0%) 0 (0%) Month Two 0.002 poor 51 (45.9%) 35 (61.4%) 16 (29.6%) good 60 (54.1%) 22 (38.6%) 38 (70.4%) NA 0 (0%) 0 (0%) 0 (0%) Month Three 0.008 poor 46 (41.4%) 31 (54.4%) 15 (27.8%) good 65 (58.6%) 26 (45.6%) 39 (72.2%) NA 0 (0%) 0 (0%) 0 (0%) Month Four 0.068 poor 52 (46.8%) 32 (56.1%) 20 (37%) good 59 (53.2%) 25 (43.9%) 34 (63%) NA 0 (0%) 0 (0%) 0 (0%) Correlation between repeated observations: data_wide %&gt;% dplyr::select(starts_with(&quot;month&quot;)) %&gt;% dplyr::mutate_all(function(x) x == &quot;good&quot;) %&gt;% cor() %&gt;% corrplot::corrplot.mixed() data_month_trt_prop &lt;- data_long %&gt;% dplyr::group_by(treatment, month) %&gt;% dplyr::summarise(n = n(), prop_good = mean(status), prop_sd = sd(status), prop_se = prop_sd/sqrt(n)) data_month_trt_prop # A tibble: 8 × 6 # Groups: treatment [2] treatment month n prop_good prop_sd prop_se &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 placebo 1 57 0.491 0.504 0.0668 2 placebo 2 57 0.386 0.491 0.0651 3 placebo 3 57 0.456 0.503 0.0666 4 placebo 4 57 0.439 0.501 0.0663 5 treatment 1 54 0.685 0.469 0.0638 6 treatment 2 54 0.704 0.461 0.0627 7 treatment 3 54 0.722 0.452 0.0615 8 treatment 4 54 0.630 0.487 0.0663 16.3.2 Visualization 16.3.2.1 Age data_wide %&gt;% ggplot(aes(age)) + geom_histogram(binwidth = 5, color = &quot;black&quot;, alpha = .3) + geom_vline(xintercept = c(20, 35, 45), color = &quot;red&quot;) + theme_bw() + facet_grid(treatment ~ .) 16.3.2.2 Status by Age data_wide %&gt;% dplyr::mutate(n_good = furniture::rowsums(month_1 == &quot;good&quot;, month_2 == &quot;good&quot;, month_3 == &quot;good&quot;, month_4 == &quot;good&quot;)) %&gt;% ggplot(aes(x = age, y = n_good)) + geom_count() + geom_smooth() + theme_bw() + labs(x = &quot;Age in Years&quot;, y = &quot;Number of Visits out of Four, with &#39;Good&#39; Respiration&quot;) 16.3.2.3 Status Over Time It apears that status is fairly constant over time. data_month_trt_prop %&gt;% ggplot(aes(x = month, y = prop_good, group = treatment, color = treatment)) + geom_errorbar(aes(ymin = prop_good - prop_se, ymax = prop_good + prop_se), width = .25) + geom_point() + geom_line() + theme_bw() + labs(x = &quot;Time, months&quot;, y = &quot;Proportion of Participants\\nwith &#39;Good&#39; Respiratory Status&quot;) + theme(legend.position = &quot;bottom&quot;, legend.key.width = unit(1.5, &quot;cm&quot;)) It is NOT the purpose of this analysis to investigate change over time! Since status is largely stable over time, no linear (or even quadratic) effect of the month variable will be modeled. Instead, the four observations on each subject are treated as correlated (at least with non-independent correlation structure in GEE), but no time trend will be included. 16.4 Logisitc Regression (GLM) Note: THIS IS NEVER APPROPRIATE TO CONDUCT A GLM ON REPEATED MEASURES. THIS IS DONE FOR ILLUSTRATION PURPOSES ONLY! Since participants in the middle ages seem to do worse than either extreme, the potential quadratic effect of age will be included. Age is also being grand-mean centered to make the intercept more meaningful. 16.4.1 Fit the Model resp_glm &lt;- glm(status ~ centre + treatment + sex + BL_status + I(age-33) + I((age-33)^2), data = data_long, family = binomial(link = &quot;logit&quot;)) summary(resp_glm) Call: glm(formula = status ~ centre + treatment + sex + BL_status + I(age - 33) + I((age - 33)^2), family = binomial(link = &quot;logit&quot;), data = data_long) Deviance Residuals: Min 1Q Median 3Q Max -2.5965 -0.9178 0.3985 0.8388 2.0988 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -1.9685725 0.2733549 -7.202 5.95e-13 *** centre2 0.5347938 0.2464412 2.170 0.030002 * treatmenttreatment 1.3561814 0.2447533 5.541 3.01e-08 *** sexmale 0.4263433 0.3175081 1.343 0.179343 BL_statusgood 1.9193401 0.2500033 7.677 1.63e-14 *** I(age - 33) -0.0368535 0.0106382 -3.464 0.000532 *** I((age - 33)^2) 0.0025169 0.0006352 3.963 7.41e-05 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 608.93 on 443 degrees of freedom Residual deviance: 465.25 on 437 degrees of freedom AIC: 479.25 Number of Fisher Scoring iterations: 4 16.4.2 Tabulate the Model Parameters texreg::knitreg(list(resp_glm, texreghelpr::extract_glm_exp(resp_glm, include.any = FALSE)), custom.model.names = c(&quot;b (SE)&quot;, &quot;OR [95 CI]&quot;), caption = &quot;Logistic Generalized Linear Regression (GLM)&quot;, single.row = TRUE, digits = 3, ci.test = 1) Logistic Generalized Linear Regression (GLM)   b (SE) OR [95 CI] (Intercept) -1.969 (0.273)*** 0.140 [0.080; 0.234]* centre2 0.535 (0.246)* 1.707 [1.054; 2.775]* treatmenttreatment 1.356 (0.245)*** 3.881 [2.422; 6.334]* sexmale 0.426 (0.318) 1.532 [0.826; 2.875] BL_statusgood 1.919 (0.250)*** 6.816 [4.221; 11.271]* age - 33 -0.037 (0.011)*** 0.964 [0.943; 0.984]* (age - 33)^2 0.003 (0.001)*** 1.003 [1.001; 1.004]* AIC 479.253   BIC 507.924   Log Likelihood -232.626   Deviance 465.253   Num. obs. 444   ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 (or Null hypothesis value outside the confidence interval). 16.5 Generalized Estimating Equations (GEE) Again, since participants age is likely to be a risk at either end of the spectrum, the potential quadratic effect of age will be modeled. Age is also being grand-mean centered to make the intercept more meaningful. 16.5.1 Indepdendence Using an \"independence\" correlation structure is equivalent to using a GLM analysis (logistic regression in this case) and is never appropriate for repeated measures data. It is only being done here for comparison purposes. resp_gee_in &lt;- gee::gee(status ~ centre + treatment + sex + BL_status + I(age-33) + I((age-33)^2), data = data_long, family = binomial(link = &quot;logit&quot;), id = subject, corstr = &quot;independence&quot;, scale.fix = TRUE, scale.value = 1) (Intercept) centre2 treatmenttreatment sexmale -1.968572485 0.534793799 1.356181372 0.426343291 BL_statusgood I(age - 33) I((age - 33)^2) 1.919340141 -0.036853528 0.002516859 summary(resp_gee_in) GEE: GENERALIZED LINEAR MODELS FOR DEPENDENT DATA gee S-function, version 4.13 modified 98/01/27 (1998) Model: Link: Logit Variance to Mean Relation: Binomial Correlation Structure: Independent Call: gee::gee(formula = status ~ centre + treatment + sex + BL_status + I(age - 33) + I((age - 33)^2), id = subject, data = data_long, family = binomial(link = &quot;logit&quot;), corstr = &quot;independence&quot;, scale.fix = TRUE, scale.value = 1) Summary of Residuals: Min 1Q Median 3Q Max -0.96563962 -0.34372730 0.07631922 0.29658264 0.88947816 Coefficients: Estimate Naive S.E. Naive z Robust S.E. Robust z (Intercept) -1.968572493 0.2733635751 -7.201298 0.4457014141 -4.4167966 centre2 0.534793799 0.2464443046 2.170039 0.3795759846 1.4089242 treatmenttreatment 1.356181377 0.2447584751 5.540896 0.3777998909 3.5896818 sexmale 0.426343293 0.3175134753 1.342757 0.4832336627 0.8822715 BL_statusgood 1.919340146 0.2500092510 7.677077 0.3772812271 5.0872930 I(age - 33) -0.036853528 0.0106384086 -3.464196 0.0150120266 -2.4549336 I((age - 33)^2) 0.002516859 0.0006351834 3.962414 0.0007592432 3.3149582 Estimated Scale Parameter: 1 Number of Iterations: 1 Working Correlation [,1] [,2] [,3] [,4] [1,] 1 0 0 0 [2,] 0 1 0 0 [3,] 0 0 1 0 [4,] 0 0 0 1 The results for GEE fit with the independence correlation structure produces results that are nearly identical to the GLM model. The robust (sandwich) standard errors are however much larger than the naive standard errors. 16.5.2 Exchangeable resp_gee_ex &lt;- gee::gee(status ~ centre + treatment + sex + BL_status + I(age-33) + I((age-33)^2), data = data_long, family = binomial(link = &quot;logit&quot;), id = subject, corstr = &quot;exchangeable&quot;, scale.fix = TRUE, scale.value = 1) (Intercept) centre2 treatmenttreatment sexmale -1.968572485 0.534793799 1.356181372 0.426343291 BL_statusgood I(age - 33) I((age - 33)^2) 1.919340141 -0.036853528 0.002516859 summary(resp_gee_ex) GEE: GENERALIZED LINEAR MODELS FOR DEPENDENT DATA gee S-function, version 4.13 modified 98/01/27 (1998) Model: Link: Logit Variance to Mean Relation: Binomial Correlation Structure: Exchangeable Call: gee::gee(formula = status ~ centre + treatment + sex + BL_status + I(age - 33) + I((age - 33)^2), id = subject, data = data_long, family = binomial(link = &quot;logit&quot;), corstr = &quot;exchangeable&quot;, scale.fix = TRUE, scale.value = 1) Summary of Residuals: Min 1Q Median 3Q Max -0.96563962 -0.34372730 0.07631922 0.29658264 0.88947816 Coefficients: Estimate Naive S.E. Naive z Robust S.E. Robust z (Intercept) -1.968572493 0.379830796 -5.1827617 0.4457014141 -4.4167966 centre2 0.534793799 0.342427246 1.5617735 0.3795759846 1.4089242 treatmenttreatment 1.356181377 0.340084835 3.9877737 0.3777998909 3.5896818 sexmale 0.426343293 0.441175807 0.9663796 0.4832336627 0.8822715 BL_statusgood 1.919340146 0.347380636 5.5251789 0.3772812271 5.0872930 I(age - 33) -0.036853528 0.014781762 -2.4931757 0.0150120266 -2.4549336 I((age - 33)^2) 0.002516859 0.000882569 2.8517424 0.0007592432 3.3149582 Estimated Scale Parameter: 1 Number of Iterations: 1 Working Correlation [,1] [,2] [,3] [,4] [1,] 1.00000 0.31021 0.31021 0.31021 [2,] 0.31021 1.00000 0.31021 0.31021 [3,] 0.31021 0.31021 1.00000 0.31021 [4,] 0.31021 0.31021 0.31021 1.00000 Notice that the naive standard errors are more similar to the robust (sandwich) standard errors, supporting that this is a better fitting model 16.5.3 Paramgeter Estimates Table The GEE models display the robust (sandwich) standard errors. 16.5.3.1 Raw Estimates (Logit Scale) texreg::knitreg(list(resp_glm, resp_gee_in, resp_gee_ex), custom.model.names = c(&quot;GLM&quot;, &quot;GEE-INDEP&quot;, &quot;GEE-EXCH&quot;), caption = &quot;Estimates on Logit Scale&quot;, single.row = TRUE, digits = 4) Estimates on Logit Scale   GLM GEE-INDEP GEE-EXCH (Intercept) -1.9686 (0.2734)*** -1.9686 (0.4457)*** -1.9686 (0.4457)*** centre2 0.5348 (0.2464)* 0.5348 (0.3796) 0.5348 (0.3796) treatmenttreatment 1.3562 (0.2448)*** 1.3562 (0.3778)*** 1.3562 (0.3778)*** sexmale 0.4263 (0.3175) 0.4263 (0.4832) 0.4263 (0.4832) BL_statusgood 1.9193 (0.2500)*** 1.9193 (0.3773)*** 1.9193 (0.3773)*** age - 33 -0.0369 (0.0106)*** -0.0369 (0.0150)* -0.0369 (0.0150)* (age - 33)^2 0.0025 (0.0006)*** 0.0025 (0.0008)*** 0.0025 (0.0008)*** AIC 479.2530     BIC 507.9238     Log Likelihood -232.6265     Deviance 465.2530     Num. obs. 444 444 444 Scale   1.0000 1.0000 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 Comparing the two GEE models: parameters are identical and so are the robust (sandwich) standard errors. 16.5.3.2 Exponentiate the Estimates (odds ratio scale) texreg::knitreg(list(extract_glm_exp(resp_glm), extract_gee_exp(resp_gee_in), extract_gee_exp(resp_gee_ex)), custom.model.names = c(&quot;GLM&quot;, &quot;GEE-INDEP&quot;, &quot;GEE-EXCH&quot;), caption = &quot;Estimates on Odds-Ratio Scale&quot;, single.row = TRUE, ci.test = 1, digits = 3) Estimates on Odds-Ratio Scale   GLM GEE-INDEP GEE-EXCH (Intercept) 0.140 [0.080; 0.234]* 0.140 [0.058; 0.335]* 0.140 [0.058; 0.335]* centre2 1.707 [1.054; 2.775]* 1.707 [0.811; 3.592] 1.707 [0.811; 3.592] treatmenttreatment 3.881 [2.422; 6.334]* 3.881 [1.851; 8.139]* 3.881 [1.851; 8.139]* sexmale 1.532 [0.826; 2.875] 1.532 [0.594; 3.949] 1.532 [0.594; 3.949] BL_statusgood 6.816 [4.221; 11.271]* 6.816 [3.254; 14.279]* 6.816 [3.254; 14.279]* age - 33 0.964 [0.943; 0.984]* 0.964 [0.936; 0.993]* 0.964 [0.936; 0.993]* (age - 33)^2 1.003 [1.001; 1.004]* 1.003 [1.001; 1.004]* 1.003 [1.001; 1.004]* Dispersion   1.000 1.000 * Null hypothesis value outside the confidence interval. 16.5.3.3 Manual Extraction resp_gee_ex %&gt;% coef() %&gt;% exp() (Intercept) centre2 treatmenttreatment sexmale 0.1396561 1.7070962 3.8813436 1.5316465 BL_statusgood I(age - 33) I((age - 33)^2) 6.8164591 0.9638173 1.0025200 trt_EST &lt;- summary(resp_gee_ex)$coeff[&quot;treatmenttreatment&quot;, &quot;Estimate&quot;] trt_EST [1] 1.356181 exp(trt_EST) [1] 3.881344 Trt_SE &lt;- summary(resp_gee_ex)$coeff[&quot;treatmenttreatment&quot;, &quot;Robust S.E.&quot;] Trt_SE [1] 0.3777999 trt_95ci &lt;- trt_EST + c(-1, +1)*1.96*Trt_SE trt_95ci [1] 0.6156936 2.0966692 exp(trt_95ci) [1] 1.850940 8.139015 16.5.4 Final Model 16.5.4.1 Estimates on both the logit and odds-ratio scales texreg::knitreg(list(resp_gee_ex, texreghelpr::extract_gee_exp(resp_gee_ex, include.any = FALSE)), custom.model.names = c(&quot;b (SE)&quot;, &quot;OR [95 CI]&quot;), custom.coef.map = list(&quot;(Intercept)&quot; = &quot;(Intercept)&quot;, centre2 = &quot;Center 2 vs. 1&quot;, sexmale = &quot;Male vs. Female&quot;, BL_statusgood = &quot;BL Good vs. Poor&quot;, &quot;I(age - 33)&quot; = &quot;Age (Yrs post 33)&quot;, &quot;I((age - 33)^2)&quot; = &quot;Age-Squared&quot;, treatmenttreatment = &quot;Treatment&quot;), caption = &quot;GEE: Final Model (exchangable)&quot;, ci.test = 1, single.row = TRUE, digits = 3) GEE: Final Model (exchangable)   b (SE) OR [95 CI] (Intercept) -1.969 (0.446)*** 0.140 [0.058; 0.335]* Center 2 vs. 1 0.535 (0.380) 1.707 [0.811; 3.592] Male vs. Female 0.426 (0.483) 1.532 [0.594; 3.949] BL Good vs. Poor 1.919 (0.377)*** 6.816 [3.254; 14.279]* Age (Yrs post 33) -0.037 (0.015)* 0.964 [0.936; 0.993]* Age-Squared 0.003 (0.001)*** 1.003 [1.001; 1.004]* Treatment 1.356 (0.378)*** 3.881 [1.851; 8.139]* Scale 1.000   Num. obs. 444   Dispersion   1.000 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 (or Null hypothesis value outside the confidence interval). 16.5.4.2 Interpretation centre: Controlling for baseline status, sex, age, and treatment, participants at the two centers did not significantly differ in respiratory status during the intervention sex: Controlling for baseline status, center, age, and treatment, a participant’s respiratory status did not differ between the two sexes. BL_status: Controlling for sex, center, age, and treatment, those with good baseline status had nearly 7 times higher odds of having a good respiratory status, compared to participants that starts out poor. age: Controlling for baseline status, sex, center, and treatment, the role of age was non-linear, such that the odds of a good respiratory status was lowest for patients age 40 and better for those that were either younger or older. Most importantly: treatment: Controlling for baseline status, sex, age, and center, those on the treatment had 3.88 time higher odds of having a good respiratory status, compared to similar participants that were randomized to the placebo. 16.5.5 Predicted Probabilities 16.5.5.1 Refit with the geepack package This package lets you plot, but not put the results in a table. resp_geeglm_ex &lt;- geepack::geeglm(status ~ centre + treatment + sex + BL_status + I(age-33) + I((age-33)^2), data = data_long, family = binomial(link = &quot;logit&quot;), id = subject, waves = month, corstr = &quot;exchangeable&quot;) 16.5.5.2 Make predictions What is the change a 40 year old man in poor condition at center 1 change of being rated as being in “Good” respiratory condition? resp_geeglm_ex %&gt;% emmeans::emmeans(pairwise ~ treatment, at = list(centre = &quot;1&quot;, sex = &quot;male&quot;, age = 40, BL_status = &quot;poor&quot;), type = &quot;response&quot;) $emmeans treatment prob SE df asymp.LCL asymp.UCL placebo 0.158 0.0548 Inf 0.0768 0.296 treatment 0.421 0.1164 Inf 0.2215 0.649 Covariance estimate used: vbeta Confidence level used: 0.95 Intervals are back-transformed from the logit scale $contrasts contrast odds.ratio SE df null z.ratio p.value placebo / treatment 0.258 0.0973 Inf 1 -3.590 0.0003 Tests are performed on the log odds ratio scale A 40 year old man in poor condition at center 1 has a 15.8% change of being rated as being in “Good” respiratory condition if he was randomized to placebo. A 40 year old man in poor condition at center 1 has a 42.1% change of being rated as being in “Good” respiratory condition if he was randomized to placebo. The odds ratio for treatment is: (.421/(1 -.421) )/( .158/(1 - .158)) [1] 3.874882 16.5.5.3 Quick 16.5.6 Marginal Plot to Visualize the Model 16.5.6.1 Quickest Version The interactions::interact_plot() function can only investigate 3 variables at once: pred the x-axis (must be continuous) modx different lines (may be categorical or continuous) mod2 side-by-side panels (may be categorical or continuous) All other variables must be held constant. interactions::interact_plot(model = resp_geeglm_ex, # model name pred = age, # x-axis modx = treatment, # lines mod2 = sex, # panels data = data_long, at = list(centre = &quot;1&quot;, BL_status = &quot;good&quot;), # hold constant type = &quot;mean_subject&quot;) + theme_bw() 16.5.6.2 Full Version Makes a dataset with all combinations resp_geeglm_ex %&gt;% emmeans::emmeans(~ centre + treatment + sex + age + BL_status, at = list(age = c(20, 35, 50)), type = &quot;response&quot;) %&gt;% data.frame() centre treatment sex age BL_status prob SE df asymp.LCL 1 1 placebo female 20 poor 0.2565215 0.06548740 Inf 0.14967845 2 2 placebo female 20 poor 0.3706723 0.12112814 Inf 0.17551296 3 1 treatment female 20 poor 0.5724989 0.07094442 Inf 0.43141390 4 2 treatment female 20 poor 0.6956880 0.09012479 Inf 0.49811941 5 1 placebo male 20 poor 0.3457476 0.10327449 Inf 0.17761271 6 2 placebo male 20 poor 0.4742752 0.11897826 Inf 0.26149005 7 1 treatment male 20 poor 0.6722540 0.13353316 Inf 0.38474268 8 2 treatment male 20 poor 0.7778518 0.09957372 Inf 0.53090485 9 1 placebo female 35 poor 0.1158621 0.04715858 Inf 0.05047400 10 2 placebo female 35 poor 0.1828109 0.09160156 Inf 0.06302045 11 1 treatment female 35 poor 0.3371478 0.06614826 Inf 0.22163454 12 2 treatment female 35 poor 0.4647493 0.11044788 Inf 0.26669467 13 1 placebo male 35 poor 0.1671630 0.05669685 Inf 0.08286363 14 2 placebo male 35 poor 0.2551987 0.08445543 Inf 0.12543357 15 1 treatment male 35 poor 0.4379004 0.11914059 Inf 0.23176597 16 2 treatment male 35 poor 0.5707977 0.11262714 Inf 0.35070552 17 1 placebo female 50 poor 0.1338069 0.05877875 Inf 0.05408046 18 2 placebo female 50 poor 0.2086774 0.10132831 Inf 0.07340015 19 1 treatment female 50 poor 0.3748352 0.08438493 Inf 0.22840791 20 2 treatment female 50 poor 0.5058160 0.11024112 Inf 0.30129761 21 1 placebo male 50 poor 0.1913338 0.06744244 Inf 0.09148083 22 2 placebo male 50 poor 0.2877016 0.08618754 Inf 0.15047521 23 1 treatment male 50 poor 0.4787165 0.12607341 Inf 0.25438187 24 2 treatment male 50 poor 0.6105461 0.10299001 Inf 0.40147684 25 1 placebo female 20 good 0.7016595 0.07358528 Inf 0.54146521 26 2 placebo female 20 good 0.8005933 0.06259978 Inf 0.65055094 27 1 treatment female 20 good 0.9012681 0.04083868 Inf 0.78782754 28 2 treatment female 20 good 0.9396977 0.02445873 Inf 0.86991509 29 1 placebo male 20 good 0.7827145 0.10216134 Inf 0.52603135 30 2 placebo male 20 good 0.8601276 0.06117585 Inf 0.69417730 31 1 treatment male 20 good 0.9332512 0.04985998 Inf 0.74440141 32 2 treatment male 20 good 0.9597874 0.02684720 Inf 0.85926193 33 1 placebo female 35 good 0.4718119 0.09433741 Inf 0.29842235 34 2 placebo female 35 good 0.6039430 0.10310876 Inf 0.39581143 35 1 treatment female 35 good 0.7761395 0.06499212 Inf 0.62484433 36 2 treatment female 35 good 0.8554625 0.04422313 Inf 0.74594602 37 1 placebo male 35 good 0.5777323 0.12049412 Inf 0.34195766 38 2 placebo male 35 good 0.7002031 0.08237976 Inf 0.51976564 39 1 treatment male 35 good 0.8415295 0.08778590 Inf 0.59374248 40 2 treatment male 35 good 0.9006481 0.04809252 Inf 0.75970156 41 1 placebo female 50 good 0.5129046 0.11084368 Inf 0.30619949 42 2 placebo female 50 good 0.6425442 0.10133622 Inf 0.43086572 43 1 treatment female 50 good 0.8034205 0.06889169 Inf 0.63480235 44 2 treatment female 50 good 0.8746381 0.04007282 Inf 0.77316863 45 1 placebo male 50 good 0.6172692 0.12419998 Inf 0.36530393 46 2 placebo male 50 good 0.7335613 0.07358316 Inf 0.56828932 47 1 treatment male 50 good 0.8622559 0.08077815 Inf 0.62272870 48 2 treatment male 50 good 0.9144286 0.04091477 Inf 0.79316718 asymp.UCL 1 0.4034454 2 0.6197250 3 0.7027009 4 0.8404014 5 0.5639080 6 0.6968321 7 0.8705984 8 0.9154912 9 0.2441759 10 0.4266250 11 0.4760446 12 0.6745823 13 0.3083856 14 0.4501170 15 0.6679641 16 0.7660522 17 0.2944776 18 0.4674867 19 0.5484142 20 0.7084063 21 0.3573127 22 0.4794448 23 0.7119772 24 0.7855878 25 0.8240720 26 0.8964652 27 0.9573405 28 0.9731995 29 0.9212093 30 0.9433731 31 0.9853203 32 0.9893963 33 0.6522803 34 0.7801931 35 0.8783030 36 0.9226636 37 0.7827093 38 0.8344399 39 0.9507268 40 0.9629541 41 0.7152881 42 0.8101781 43 0.9057445 44 0.9345587 45 0.8188185 46 0.8520359 47 0.9595797 48 0.9675094 In order to include all FIVE variables, we must do it the LONG way… resp_geeglm_ex %&gt;% emmeans::emmeans(~ centre + treatment + sex + age + BL_status, at = list(age = seq(from = 11, to = 68, by = 1)), type = &quot;response&quot;, level = .68) %&gt;% data.frame() %&gt;% ggplot(aes(x = age, y = prob, group = interaction(sex, treatment))) + geom_ribbon(aes(ymin = asymp.LCL, ymax = asymp.UCL, fill = fct_rev(sex)), alpha = .3) + geom_line(aes(color = fct_rev(sex), linetype = fct_rev(treatment))) + theme_bw() + facet_grid(centre ~ BL_status, labeller = label_both) + labs(x = &quot;Age, years&quot;, y = &quot;Predicted Probability of GOOD Respiratory Status&quot;, color = &quot;Sex:&quot;, fill = &quot;Sex:&quot;, linetype = &quot;Assignment:&quot;) + theme(legend.position = &quot;bottom&quot;) 16.5.6.3 Females in Center 1 This example uses default settings. interactions::interact_plot(model = resp_geeglm_ex, pred = age, modx = treatment, mod2 = BL_status, at = list(sex = &quot;female&quot;, centre = &quot;1&quot;)) 16.5.6.4 Males in Center 2 This example is more preped for publication. interactions::interact_plot(model = resp_geeglm_ex, pred = age, modx = treatment, mod2 = BL_status, at = list(sex = &quot;male&quot;, centre = &quot;2&quot;), x.label = &quot;Age in Years&quot;, y.label = &quot;Predicted Probability of &#39;Good&#39; Respiratory Status&quot;, legend.main = &quot;Intervention: &quot;, mod2.labels = c(&quot;Poor at Baseline&quot;, &quot;Good at Baseline&quot;), colors = rep(&quot;black&quot;, times = 2)) + theme_bw() + theme(legend.position = c(1, 0), legend.justification = c(1.1, -0.1), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(1.5, &quot;cm&quot;)) + labs(caption = &quot;Note: Probibilities shown are specific to males at center 2&quot;) resp_geeglm_ex %&gt;% emmeans::emmeans(~ centre + treatment + sex + age + BL_status, at = list(age = seq(from = 11, to = 68, by = 1), sex = &quot;male&quot;, centre = &quot;2&quot;), type = &quot;response&quot;, level = .68) %&gt;% data.frame() %&gt;% ggplot(aes(x = age, y = prob, group = interaction(sex, treatment))) + geom_ribbon(aes(ymin = asymp.LCL, ymax = asymp.UCL), alpha = .2) + geom_line(aes(linetype = fct_rev(treatment))) + theme_bw() + facet_grid(~ BL_status) + labs(x = &quot;Age, years&quot;, y = &quot;Predicted Probability of\\nGOOD Respiratory Status&quot;, color = &quot;Sex:&quot;, fill = &quot;Sex:&quot;, linetype = &quot;Assignment:&quot;) + theme(legend.position = &quot;bottom&quot;) 16.6 Conclusion The Research Question The question of interest is to assess whether the treatment is effective and to estimate its effect. The Conclusion After accounting for baseline status, age, sex and center, participants in the active treatment group had nearly four times higher odds of having ‘good’ respiratory status, when compared to the placebo, exp(b) = 3.881, p&lt;.001, 95% CI [1.85, 8.14]. "],["gee-count-outcome-epilepsy.html", "17 GEE, Count Outcome: Epilepsy 17.1 Packages 17.2 Background 17.3 Exploratory Data Analysis 17.4 Poisson Regression (GLM) 17.5 Generalized Estimating Equations (GEE) 17.6 Conculsion", " 17 GEE, Count Outcome: Epilepsy 17.1 Packages 17.1.1 CRAN library(tidyverse) # all things tidy library(pander) # nice looking genderal tabulations library(furniture) # nice table1() descriptives library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(psych) # contains some useful functions, like headTail library(lme4) # Linear, generalized linear, &amp; nonlinear mixed models library(corrplot) # Vizualize correlation matrix library(gee) # Genderalized Estimation Equation Solver library(geepack) # Genderalized Estimation Equation Package library(MuMIn) # Multi-Model Inference (caluclate QIC) library(HSAUR) # package with the dataset 17.1.2 GitHub Helper extract functions for exponentiating parameters form generalized regression models within a texreg table of model parameters. # remotes::install_github(&quot;sarbearschwartz/texreghelpr&quot;) # first time library(texreghelpr) 17.2 Background This dataset was used as an example in Chapter 11 of “A Handbook of Statistical Analysis using R” by Brian S. Everitt and Torsten Hothorn. The authors include this data set in their HSAUR package on CRAN. The Background In this clinical trial, 59 patients suffering from epilepsy were randomized to groups receiving either the anti-epileptic drug “Progabide”” or a placebo in addition to standard chemotherapy. The numbers of seizures suffered in each of four, two-week periods were recorded for each patient along with a baseline seizure count for the 8 weeks prior to being randomized to treatment and age. The Research Question The main question of interest is whether taking progabide reduced the number of epileptic seizures compared with placebo. The Data Indicators subject the patient ID, a factor with levels 1 to 59 period treatment period, an ordered factor with levels 1 to 4 Outcome or dependent variable +seizure.rate the number of seizures (2-weeks) Main predictor or independent variable of interest treatment the treatment group, a factor with levels placebo and Progabide Time-invariant Covariates age the age of the patient base the number of seizures before the trial (8 weeks) 17.2.1 Read in the data data(&quot;epilepsy&quot;, package = &quot;HSAUR&quot;) Problem: The outcome (seizure.rate) were counts over a TWO-week period and we would like to interpret the results in terms of effects on the WEEKLY rate. If we divide the values by TWO to get weekly rates, the outcome might be a decimal number The Poisson distribution may only be used for whole numbers (not deciamls) Solution: We need to include an offset term in the model that indicates the LOG DURATION of each period. Every observation period is exactly 2 weeks in this experiment Create a variable in the original dataset that is equal to the LOG DURATION (per = log(2)) To the formula for the glm() or gee(), add: + offset(per) 17.2.2 Long Format data_long &lt;- epilepsy %&gt;% dplyr::select(subject, age, treatment, base, period, seizure.rate) %&gt;% dplyr::mutate(per = log(2)) %&gt;% # new variable to use with the offset dplyr::mutate(base_wk = base/8) str(data_long) &#39;data.frame&#39;: 236 obs. of 8 variables: $ subject : Factor w/ 59 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 1 1 2 2 2 2 3 3 ... $ age : int 31 31 31 31 30 30 30 30 25 25 ... $ treatment : Factor w/ 2 levels &quot;placebo&quot;,&quot;Progabide&quot;: 1 1 1 1 1 1 1 1 1 1 ... $ base : int 11 11 11 11 11 11 11 11 6 6 ... $ period : Ord.factor w/ 4 levels &quot;1&quot;&lt;&quot;2&quot;&lt;&quot;3&quot;&lt;&quot;4&quot;: 1 2 3 4 1 2 3 4 1 2 ... $ seizure.rate: int 5 3 3 3 3 5 3 3 2 4 ... $ per : num 0.693 0.693 0.693 0.693 0.693 ... $ base_wk : num 1.38 1.38 1.38 1.38 1.38 ... psych::headTail(data_long, top = 10, bottom = 6) subject age treatment base period seizure.rate per base_wk 1 1 31 placebo 11 1 5 0.69 1.38 110 1 31 placebo 11 2 3 0.69 1.38 112 1 31 placebo 11 3 3 0.69 1.38 114 1 31 placebo 11 4 3 0.69 1.38 2 2 30 placebo 11 1 3 0.69 1.38 210 2 30 placebo 11 2 5 0.69 1.38 212 2 30 placebo 11 3 3 0.69 1.38 214 2 30 placebo 11 4 3 0.69 1.38 3 3 25 placebo 6 1 2 0.69 0.75 310 3 25 placebo 6 2 4 0.69 0.75 ... &lt;NA&gt; ... &lt;NA&gt; ... &lt;NA&gt; ... ... ... 582 58 36 Progabide 13 3 0 0.69 1.62 583 58 36 Progabide 13 4 0 0.69 1.62 59 59 37 Progabide 12 1 1 0.69 1.5 591 59 37 Progabide 12 2 4 0.69 1.5 592 59 37 Progabide 12 3 3 0.69 1.5 593 59 37 Progabide 12 4 2 0.69 1.5 17.2.3 Wide Format data_wide &lt;- data_long %&gt;% tidyr::spread(key = period, value = seizure.rate, sep = &quot;_&quot;) %&gt;% dplyr::arrange(subject) str(data_wide) &#39;data.frame&#39;: 59 obs. of 10 variables: $ subject : Factor w/ 59 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... $ age : int 31 30 25 36 22 29 31 42 37 28 ... $ treatment: Factor w/ 2 levels &quot;placebo&quot;,&quot;Progabide&quot;: 1 1 1 1 1 1 1 1 1 1 ... $ base : int 11 11 6 8 66 27 12 52 23 10 ... $ per : num 0.693 0.693 0.693 0.693 0.693 ... $ base_wk : num 1.38 1.38 0.75 1 8.25 ... $ period_1 : int 5 3 2 4 7 5 6 40 5 14 ... $ period_2 : int 3 5 4 4 18 2 4 20 6 13 ... $ period_3 : int 3 3 0 1 9 8 0 23 6 6 ... $ period_4 : int 3 3 5 4 21 7 2 12 5 0 ... psych::headTail(data_wide) subject age treatment base per base_wk period_1 period_2 period_3 period_4 1 1 31 placebo 11 0.69 1.38 5 3 3 3 2 2 30 placebo 11 0.69 1.38 3 5 3 3 3 3 25 placebo 6 0.69 0.75 2 4 0 5 4 4 36 placebo 8 0.69 1 4 4 1 4 ... &lt;NA&gt; ... &lt;NA&gt; ... ... ... ... ... ... ... 56 56 26 Progabide 22 0.69 2.75 1 23 19 8 57 57 21 Progabide 25 0.69 3.12 2 3 0 1 58 58 36 Progabide 13 0.69 1.62 0 0 0 0 59 59 37 Progabide 12 0.69 1.5 1 4 3 2 17.3 Exploratory Data Analysis 17.3.1 Summarize 17.3.1.1 Demographics and Baseline data_wide %&gt;% dplyr::group_by(treatment) %&gt;% furniture::table1(age, base, base_wk, total = TRUE, test = TRUE, digits = 2, type = &quot;full&quot;, output = &quot;markdown&quot;) Total placebo Progabide Test P-Value n = 59 n = 28 n = 31 age T-Test: 0.76 0.449 28.34 (6.30) 29.00 (6.00) 27.74 (6.60) base T-Test: -0.12 0.907 31.22 (26.88) 30.79 (26.10) 31.61 (27.98) base_wk T-Test: -0.12 0.907 3.90 (3.36) 3.85 (3.26) 3.95 (3.50) 17.3.1.2 Outcome Across Time Note: The Poisson distribution specifies that the MEAN = VARIANCE In this dataset, the variance is much larger than the mean, at all time points for both groups. This is evidence of overdispersion and suggest the scale parameter should be greater than one. data_long %&gt;% dplyr::group_by(treatment, period) %&gt;% dplyr::summarise(N = n(), M = mean(seizure.rate), VAR = var(seizure.rate), SD = sd(seizure.rate)) %&gt;% pander::pander() summarise() has grouped output by ‘treatment’. You can override using the .groups argument. treatment period N M VAR SD placebo 1 28 9.4 103 10.1 placebo 2 28 8.3 67 8.2 placebo 3 28 8.8 215 14.7 placebo 4 28 8.0 58 7.6 Progabide 1 31 8.6 333 18.2 Progabide 2 31 8.4 141 11.9 Progabide 3 31 8.1 193 13.9 Progabide 4 31 6.7 127 11.3 17.3.1.3 Correlation Across Time Raw Scale data_long %&gt;% dplyr::select(subject, period, seizure.rate ) %&gt;% tidyr::spread(key = period, value = seizure.rate ) %&gt;% dplyr::select(-subject) %&gt;% cor() %&gt;% corrplot::corrplot.mixed() Log Scale data_long %&gt;% dplyr::mutate(rate_wk = log(seizure.rate + 1)) %&gt;% dplyr::select(subject, period, rate_wk) %&gt;% tidyr::spread(key = period, value = rate_wk) %&gt;% dplyr::select(-subject) %&gt;% cor() %&gt;% corrplot::corrplot.mixed() 17.3.2 Visualize 17.3.2.1 Oucome on the Raw Scale There appear to be quite a few extreme values or outliers, particularly for the Progabide group during period one. Since the outcome is truely a COUNT, we will model it with a Poisson distribution combined with a LOG link. data_long %&gt;% ggplot(aes(x = period, y = seizure.rate)) + geom_boxplot() + theme_bw() + facet_grid(.~ treatment) To investigate possible outliers, we should transform the outcome with the log function first. Note: Since some participants reported no seizures during a two week period and the log(0) is unndefinded, we must add some amount to the values before transforming. Here we have chosen to add the value of \\(1\\). data_long %&gt;% ggplot(aes(x = period, y = log(seizure.rate + 1))) + geom_boxplot() + theme_bw() + facet_grid(.~ treatment) data_long %&gt;% ggplot(aes(x = period, y = log(seizure.rate + 1))) + geom_line(aes(group = subject)) + theme_bw() + facet_grid(.~ treatment) data_long %&gt;% ggplot(aes(x = period, y = log(seizure.rate + 1))) + geom_smooth(aes(group = subject), method = &quot;lm&quot;, se = FALSE) + geom_smooth(aes(group = 1), color = &quot;red&quot;, size = 1.5, method = &quot;lm&quot;, se = FALSE) + theme_bw() + facet_grid(.~ treatment) 17.4 Poisson Regression (GLM) Note: THIS IS NEVER APPROPRIATE TO CONDUCT A GLM ON REPEATED MEASURES. THIS IS DONE FOR ILLUSTRATION PURPOSES ONLY! 17.4.1 Fit the model fit_glm &lt;- glm(seizure.rate ~ base + age + treatment + offset(per), data = data_long, family = poisson(link = &quot;log&quot;)) summary(fit_glm) Call: glm(formula = seizure.rate ~ base + age + treatment + offset(per), family = poisson(link = &quot;log&quot;), data = data_long) Deviance Residuals: Min 1Q Median 3Q Max -4.4360 -1.4034 -0.5029 0.4842 12.3223 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -0.1306156 0.1356191 -0.963 0.3355 base 0.0226517 0.0005093 44.476 &lt; 2e-16 *** age 0.0227401 0.0040240 5.651 1.59e-08 *** treatmentProgabide -0.1527009 0.0478051 -3.194 0.0014 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for poisson family taken to be 1) Null deviance: 2521.75 on 235 degrees of freedom Residual deviance: 958.46 on 232 degrees of freedom AIC: 1732.5 Number of Fisher Scoring iterations: 5 17.4.2 Tabulate the Model Parameters texreg::knitreg(list(fit_glm, texreghelpr::extract_glm_exp(fit_glm, include.any = FALSE)), custom.model.names = c(&quot;b (SE)&quot;, &quot;IRR [95 CI]&quot;), caption = &quot;Poisson Generalized Linear Regression (GLM)&quot;, single.row = TRUE, digits = 3, ci.test = 1) Poisson Generalized Linear Regression (GLM)   b (SE) IRR [95 CI] (Intercept) -0.131 (0.136) 0.878 [0.672; 1.144] base 0.023 (0.001)*** 1.023 [1.022; 1.024]* age 0.023 (0.004)*** 1.023 [1.015; 1.031]* treatmentProgabide -0.153 (0.048)** 0.858 [0.782; 0.943]* AIC 1732.459   BIC 1746.314   Log Likelihood -862.229   Deviance 958.464   Num. obs. 236   ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 (or Null hypothesis value outside the confidence interval). 17.5 Generalized Estimating Equations (GEE) 17.5.1 Match Poisson Regresssion (GLM) correlation structure: independence scale parameter = \\(1\\) fit_gee_ind_s1 &lt;- gee::gee(seizure.rate ~ base + age + treatment + offset(per), data = data_long, family = poisson(link = &quot;log&quot;), id = subject, corstr = &quot;independence&quot;, scale.fix = TRUE, scale.value = 1) (Intercept) base age treatmentProgabide -0.13061561 0.02265174 0.02274013 -0.15270095 summary(fit_gee_ind_s1) GEE: GENERALIZED LINEAR MODELS FOR DEPENDENT DATA gee S-function, version 4.13 modified 98/01/27 (1998) Model: Link: Logarithm Variance to Mean Relation: Poisson Correlation Structure: Independent Call: gee::gee(formula = seizure.rate ~ base + age + treatment + offset(per), id = subject, data = data_long, family = poisson(link = &quot;log&quot;), corstr = &quot;independence&quot;, scale.fix = TRUE, scale.value = 1) Summary of Residuals: Min 1Q Median 3Q Max -4.9195387 0.1808059 1.7073405 4.8850644 69.9658560 Coefficients: Estimate Naive S.E. Naive z Robust S.E. Robust z (Intercept) -0.13061561 0.1356191185 -0.9631062 0.365148155 -0.3577058 base 0.02265174 0.0005093011 44.4761250 0.001235664 18.3316325 age 0.02274013 0.0040239970 5.6511312 0.011580405 1.9636736 treatmentProgabide -0.15270095 0.0478051054 -3.1942393 0.171108915 -0.8924196 Estimated Scale Parameter: 1 Number of Iterations: 1 Working Correlation [,1] [,2] [,3] [,4] [1,] 1 0 0 0 [2,] 0 1 0 0 [3,] 0 0 1 0 [4,] 0 0 0 1 The estimates and the naive standard errors match the GLM exactly. The naive SE’s are much smaller (half) than the robust (sandwich) SE’s, suggesting a poor fit. 17.5.2 Change Correlation Sturucture correlation structure: exchangeable scale parameter = \\(1\\) fit_gee_exc_s1 &lt;- gee::gee(seizure.rate ~ base + age + treatment + offset(per), data = data_long, family = poisson(link = &quot;log&quot;), id = subject, corstr = &quot;exchangeable&quot;, scale.fix = TRUE, scale.value = 1) (Intercept) base age treatmentProgabide -0.13061561 0.02265174 0.02274013 -0.15270095 summary(fit_gee_exc_s1) GEE: GENERALIZED LINEAR MODELS FOR DEPENDENT DATA gee S-function, version 4.13 modified 98/01/27 (1998) Model: Link: Logarithm Variance to Mean Relation: Poisson Correlation Structure: Exchangeable Call: gee::gee(formula = seizure.rate ~ base + age + treatment + offset(per), id = subject, data = data_long, family = poisson(link = &quot;log&quot;), corstr = &quot;exchangeable&quot;, scale.fix = TRUE, scale.value = 1) Summary of Residuals: Min 1Q Median 3Q Max -4.9195387 0.1808059 1.7073405 4.8850644 69.9658560 Coefficients: Estimate Naive S.E. Naive z Robust S.E. Robust z (Intercept) -0.13061561 0.2004416507 -0.6516391 0.365148155 -0.3577058 base 0.02265174 0.0007527342 30.0926122 0.001235664 18.3316325 age 0.02274013 0.0059473665 3.8235638 0.011580405 1.9636736 treatmentProgabide -0.15270095 0.0706547450 -2.1612270 0.171108915 -0.8924196 Estimated Scale Parameter: 1 Number of Iterations: 1 Working Correlation [,1] [,2] [,3] [,4] [1,] 1.0000000 0.3948033 0.3948033 0.3948033 [2,] 0.3948033 1.0000000 0.3948033 0.3948033 [3,] 0.3948033 0.3948033 1.0000000 0.3948033 [4,] 0.3948033 0.3948033 0.3948033 1.0000000 Although the estimated beta parameters are not much different, the naive SE’s are some closer to the robust SE’s. 17.5.2.1 Interpretation For determining significance, no p-values are given, however the p-value will be &lt; .05 when the z-score is &gt; 1.96. “For this example, the estimates of standard errors under the independence are about half of the corresponding robust estimates, and the the situation improves only a little when an exchangeable structure is fitted. Using naive standard errors leads, in particular, to a highly significant treatment effect which disappears when the robust estimates are used.” 17.5.3 Estimate the Additional Scale Parameter “The problem with the GEE approach here, using either the independence or exchangeable correlation structure lies in constraining the scale parameter to be one. For these data there is overdispersion (variance is larger than the mean value) which has to be accommodated by allowing this parameter to be freely estimated.” correlation structure: exchangeable scale parameter = freely estimated fit_gee_exc_sf &lt;- gee::gee(seizure.rate ~ base + age + treatment + offset(per), data = data_long, family = poisson(link = &quot;log&quot;), id = subject, corstr = &quot;exchangeable&quot;, scale.fix = FALSE) (Intercept) base age treatmentProgabide -0.13061561 0.02265174 0.02274013 -0.15270095 summary(fit_gee_exc_sf) GEE: GENERALIZED LINEAR MODELS FOR DEPENDENT DATA gee S-function, version 4.13 modified 98/01/27 (1998) Model: Link: Logarithm Variance to Mean Relation: Poisson Correlation Structure: Exchangeable Call: gee::gee(formula = seizure.rate ~ base + age + treatment + offset(per), id = subject, data = data_long, family = poisson(link = &quot;log&quot;), corstr = &quot;exchangeable&quot;, scale.fix = FALSE) Summary of Residuals: Min 1Q Median 3Q Max -4.9195387 0.1808059 1.7073405 4.8850644 69.9658560 Coefficients: Estimate Naive S.E. Naive z Robust S.E. Robust z (Intercept) -0.13061561 0.45219954 -0.2888451 0.365148155 -0.3577058 base 0.02265174 0.00169818 13.3388301 0.001235664 18.3316325 age 0.02274013 0.01341735 1.6948302 0.011580405 1.9636736 treatmentProgabide -0.15270095 0.15939823 -0.9579840 0.171108915 -0.8924196 Estimated Scale Parameter: 5.089608 Number of Iterations: 1 Working Correlation [,1] [,2] [,3] [,4] [1,] 1.0000000 0.3948033 0.3948033 0.3948033 [2,] 0.3948033 1.0000000 0.3948033 0.3948033 [3,] 0.3948033 0.3948033 1.0000000 0.3948033 [4,] 0.3948033 0.3948033 0.3948033 1.0000000 The naive SE’s are much closer inline with the robust SE’s. The scale parameter is estimated to be much larger than \\(1\\). “When this is done (scale parameter is freely estimated) it gives the last set of results shown above. THe estimate of \\(\\phi\\) is 5.09 and the naive and robust estimates of the standard errors are now very similar.” 17.5.4 Compare Models 17.5.4.1 Raw Estimates (logit scale) texreg::knitreg(list(fit_glm, fit_gee_ind_s1, fit_gee_exc_s1, fit_gee_exc_sf), custom.model.names = c(&quot;GLM&quot;, &quot;GEE-Indep(1)&quot;, &quot;GEE-Exchg(1)&quot;, &quot;GEE-Exchg(free)&quot;), caption = &quot;Estimates on Logit Scale&quot;, digits = 3) Estimates on Logit Scale   GLM GEE-Indep(1) GEE-Exchg(1) GEE-Exchg(free) (Intercept) -0.131 -0.131 -0.131 -0.131   (0.136) (0.365) (0.365) (0.365) base 0.023*** 0.023*** 0.023*** 0.023***   (0.001) (0.001) (0.001) (0.001) age 0.023*** 0.023* 0.023* 0.023*   (0.004) (0.012) (0.012) (0.012) treatmentProgabide -0.153** -0.153 -0.153 -0.153   (0.048) (0.171) (0.171) (0.171) AIC 1732.459       BIC 1746.314       Log Likelihood -862.229       Deviance 958.464       Num. obs. 236 236 236 236 Scale   1.000 1.000 5.090 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 17.5.4.2 Exponentiate the Estimates (odds ratio scale) texreg::knitreg(list(extract_glm_exp(fit_glm), extract_gee_exp(fit_gee_ind_s1), extract_gee_exp(fit_gee_exc_s1), extract_gee_exp(fit_gee_exc_sf)), custom.model.names = c(&quot;GLM&quot;, &quot;GEE-Indep(1)&quot;, &quot;GEE-Exchg(1)&quot;, &quot;GEE-Exchg(free)&quot;), digits = 3, caption = &quot;Estimates on Odds-Ratio Scale&quot;, caption.above = TRUE, ci.test = 1) Estimates on Odds-Ratio Scale   GLM GEE-Indep(1) GEE-Exchg(1) GEE-Exchg(free) (Intercept) 0.878 0.878 0.878 0.878   [0.672; 1.144] [0.429; 1.795] [0.429; 1.795] [0.429; 1.795] base 1.023* 1.023* 1.023* 1.023*   [1.022; 1.024] [1.020; 1.025] [1.020; 1.025] [1.020; 1.025] age 1.023* 1.023* 1.023* 1.023*   [1.015; 1.031] [1.000; 1.046] [1.000; 1.046] [1.000; 1.046] treatmentProgabide 0.858* 0.858 0.858 0.858   [0.782; 0.943] [0.614; 1.200] [0.614; 1.200] [0.614; 1.200] Dispersion   1.000 1.000 5.090 * Null hypothesis value outside the confidence interval. 17.6 Conculsion The Research Question The main question of interest is whether taking progabide reduced the number of epileptic seizures compared with placebo. The Conclusion There is no evidence that Progabide effects the weekly rate of epileptic seizures differently than placebo. "],["gee-count-outcome-antibiotics-for-leprosy.html", "18 GEE, Count Outcome: Antibiotics for Leprosy 18.1 Packages 18.2 Background 18.3 Exploratory Data Analysis 18.4 Generalized Estimating Equations (GEE) 18.5 Follow-up Analysis 18.6 Conclusion", " 18 GEE, Count Outcome: Antibiotics for Leprosy 18.1 Packages 18.1.1 CRAN library(tidyverse) # all things tidy library(pander) # nice looking genderal tabulations library(furniture) # nice table1() descriptives library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(psych) # contains some useful functions, like headTail library(interactions) library(performance) library(lme4) # Linear, generalized linear, &amp; nonlinear mixed models library(corrplot) # Vizualize correlation matrix library(gee) # Genderalized Estimation Equation Solver library(geepack) # Genderalized Estimation Equation Package library(MuMIn) # Multi-Model Inference (caluclate QIC) 18.1.2 GitHub Helper extract functions for exponentiating parameters form generalized regression models within a texreg table of model parameters. # remotes::install_github(&quot;sarbearschwartz/texreghelpr&quot;) # first time library(texreghelpr) 18.2 Background The following example is presented in the textbook: “Applied Longitudinal Analysis” by Garrett Fitzmaurice, Nan Laird &amp; James Ware The dataset maybe downloaded from: https://content.sph.harvard.edu/fitzmaur/ala/ Data on count of leprosy bacilli pre- and post-treatment from a clinical trial of antibiotics for leprosy. Source: Table 14.2.1 (page 422) in Snedecor, G.W. and Cochran, W.G. (1967). Statistical Methods, (6th edn). Ames, Iowa: Iowa State University Press With permission of Iowa State University Press. Reference: Snedecor, G.W. and Cochran, W.G. (1967). Statistical Methods, (6th edn). Ames, Iowa: Iowa State University Press The Background The dataset consists of count data from a placebo-controlled clinical trial of 30 patients with leprosy at the Eversley Childs Sanitorium in the Philippines. Participants in the study were randomized to either of two antibiotics (denoted treatment drug A and B) or to a placebo (denoted treatment drug C). Prior to receiving treatment, baseline data on the number of leprosy bacilli at six sites of the body where the bacilli tend to congregate were recorded for each patient. After several months of treatment, the number of leprosy bacilli at six sites of the body were recorded a second time. The outcome variable is the total count of the number of leprosy bacilli at the six sites. The Research Question In this study, the question of main scientific interest is whether treatment with antibiotics (drugs A and B) reduces the abundance of leprosy bacilli at the six sites of the body when compared to placebo (drug C). The Data Outcome or dependent variable(s) count.pre Pre-Treatment Bacilli Count count.post Post-Treatment Bacilli Count Main predictor or independent variable of interest drug the treatment group: antibiotics (drugs A and B) or placebo (drug C) 18.2.1 Enter the data by hand! data_raw &lt;- tibble::tribble( ~drug, ~count_pre, ~count_post, &quot;A&quot;, 11, 6, &quot;B&quot;, 6, 0, &quot;C&quot;, 16, 13, &quot;A&quot;, 8, 0, &quot;B&quot;, 6, 2, &quot;C&quot;, 13, 10, &quot;A&quot;, 5, 2, &quot;B&quot;, 7, 3, &quot;C&quot;, 11, 18, &quot;A&quot;, 14, 8, &quot;B&quot;, 8, 1, &quot;C&quot;, 9, 5, &quot;A&quot;, 19, 11, &quot;B&quot;, 18, 18, &quot;C&quot;, 21, 23, &quot;A&quot;, 6, 4, &quot;B&quot;, 8, 4, &quot;C&quot;, 16, 12, &quot;A&quot;, 10, 13, &quot;B&quot;, 19, 14, &quot;C&quot;, 12, 5, &quot;A&quot;, 6, 1, &quot;B&quot;, 8, 9, &quot;C&quot;, 12, 16, &quot;A&quot;, 11, 8, &quot;B&quot;, 5, 1, &quot;C&quot;, 7, 1, &quot;A&quot;, 3, 0, &quot;B&quot;, 15, 9, &quot;C&quot;, 12, 20) 18.2.2 Wide Format data_wide &lt;- data_raw %&gt;% dplyr::mutate(drug = factor(drug)) %&gt;% dplyr::mutate(id = row_number()) %&gt;% dplyr::select(id, drug, count_pre, count_post) str(data_wide) tibble [30 × 4] (S3: tbl_df/tbl/data.frame) $ id : int [1:30] 1 2 3 4 5 6 7 8 9 10 ... $ drug : Factor w/ 3 levels &quot;A&quot;,&quot;B&quot;,&quot;C&quot;: 1 2 3 1 2 3 1 2 3 1 ... $ count_pre : num [1:30] 11 6 16 8 6 13 5 7 11 14 ... $ count_post: num [1:30] 6 0 13 0 2 10 2 3 18 8 ... psych::headTail(data_wide) id drug count_pre count_post 1 1 A 11 6 2 2 B 6 0 3 3 C 16 13 4 4 A 8 0 5 ... &lt;NA&gt; ... ... 6 27 C 7 1 7 28 A 3 0 8 29 B 15 9 9 30 C 12 20 18.2.3 Long Format data_long &lt;- data_wide %&gt;% tidyr::gather(key = obs, value = count, starts_with(&quot;count&quot;)) %&gt;% dplyr::mutate(time = case_when(obs == &quot;count_pre&quot; ~ 0, obs == &quot;count_post&quot; ~ 1)) %&gt;% dplyr::select(id, drug, time, count) %&gt;% dplyr::arrange(id, time) str(data_long) tibble [60 × 4] (S3: tbl_df/tbl/data.frame) $ id : int [1:60] 1 1 2 2 3 3 4 4 5 5 ... $ drug : Factor w/ 3 levels &quot;A&quot;,&quot;B&quot;,&quot;C&quot;: 1 1 2 2 3 3 1 1 2 2 ... $ time : num [1:60] 0 1 0 1 0 1 0 1 0 1 ... $ count: num [1:60] 11 6 6 0 16 13 8 0 6 2 ... psych::headTail(data_long) id drug time count 1 1 A 0 11 2 1 A 1 6 3 2 B 0 6 4 2 B 1 0 5 ... &lt;NA&gt; ... ... 6 29 B 0 15 7 29 B 1 9 8 30 C 0 12 9 30 C 1 20 18.3 Exploratory Data Analysis 18.3.1 Summary Statistics data_long %&gt;% dplyr::group_by(drug, time) %&gt;% dplyr::summarise(N = n(), M = mean(count), VAR = var(count), SD = sd(count)) %&gt;% pander::pander() summarise() has grouped output by ‘drug’. You can override using the .groups argument. drug time N M VAR SD A 0 10 9.3 23 4.8 A 1 10 5.3 22 4.6 B 0 10 10.0 28 5.2 B 1 10 6.1 38 6.2 C 0 10 12.9 16 4.0 C 1 10 12.3 51 7.2 18.3.2 Visualize data_long %&gt;% dplyr::mutate(time_name = case_when(time == 0 ~ &quot;Pre&quot;, time == 1 ~ &quot;Post&quot;) %&gt;% factor(levels = c(&quot;Pre&quot;, &quot;Post&quot;))) %&gt;% dplyr::mutate(drug_name = fct_recode(drug, &quot;Antibiotic A&quot; = &quot;A&quot;, &quot;Antibiotic B&quot; = &quot;B&quot;, &quot;Placebo&quot; = &quot;C&quot;)) %&gt;% ggplot(aes(x = time_name, y = count)) + geom_line(aes(group = id)) + facet_grid(.~ drug_name) + theme_bw() + labs(x = NULL, y = &quot;Number of Leprosy Bacilli at Six Sites of the Body&quot;) data_long %&gt;% dplyr::mutate(time_name = case_when(time == 0 ~ &quot;Pre&quot;, time == 1 ~ &quot;Post&quot;) %&gt;% factor(levels = c(&quot;Pre&quot;, &quot;Post&quot;))) %&gt;% dplyr::mutate(drug_name = fct_recode(drug, &quot;Antibiotic A&quot; = &quot;A&quot;, &quot;Antibiotic B&quot; = &quot;B&quot;, &quot;Placebo&quot; = &quot;C&quot;)) %&gt;% ggplot(aes(x = time, y = count)) + geom_line(aes(group = id), color = &quot;gray&quot;) + geom_smooth(aes(group = drug), method = &quot;lm&quot;) + facet_grid(.~ drug_name) + theme_bw() + labs(x = NULL, y = &quot;Number of Leprosy Bacilli at Six Sites of the Body&quot;) data_long %&gt;% dplyr::mutate(time_name = case_when(time == 0 ~ &quot;Pre&quot;, time == 1 ~ &quot;Post&quot;) %&gt;% factor(levels = c(&quot;Pre&quot;, &quot;Post&quot;))) %&gt;% dplyr::mutate(drug_name = fct_recode(drug, &quot;Antibiotic A&quot; = &quot;A&quot;, &quot;Antibiotic B&quot; = &quot;B&quot;, &quot;Placebo&quot; = &quot;C&quot;)) %&gt;% ggplot(aes(x = time, y = count)) + geom_smooth(aes(group = drug, color = drug_name, fill = drug_name), method = &quot;lm&quot;, alpha = .2) + theme_bw() + labs(x = NULL, y = &quot;Number of Leprosy Bacilli at Six Sites of the Body&quot;, color = NULL, fill = NULL) + scale_x_continuous(breaks = 0:1, labels = c(&quot;Pre-Treatment&quot;, &quot;Post-Treatment&quot;)) 18.4 Generalized Estimating Equations (GEE) 18.4.1 Explore Various Correlation Structures 18.4.1.1 Fit the models - to determine correlation structure The gee() function in the gee package mod_gee_ind &lt;- gee::gee(count ~ drug*time, data = data_long, family = poisson(link = &quot;log&quot;), id = id, corstr = &quot;independence&quot;) (Intercept) drugB drugC time drugB:time drugC:time 2.23001440 0.07257069 0.32721291 -0.56230758 0.06801126 0.51467953 mod_gee_exc &lt;- gee::gee(count ~ drug*time, data = data_long, family = poisson(link = &quot;log&quot;), id = id, corstr = &quot;exchangeable&quot;) (Intercept) drugB drugC time drugB:time drugC:time 2.23001440 0.07257069 0.32721291 -0.56230758 0.06801126 0.51467953 mod_gee_uns &lt;- gee::gee(count ~ drug*time, data = data_long, family = poisson(link = &quot;log&quot;), id = id, corstr = &quot;unstructured&quot;) (Intercept) drugB drugC time drugB:time drugC:time 2.23001440 0.07257069 0.32721291 -0.56230758 0.06801126 0.51467953 The GEE models display the robust (sandwich) standard errors. 18.4.1.2 Raw Estimates (Logit Scale) texreg::knitreg(list(mod_gee_ind, mod_gee_exc, mod_gee_uns), custom.model.names = c(&quot;Independence&quot;, &quot;Exchangeable&quot;, &quot;Unstructured&quot;), single.row = TRUE, digits = 3, caption = &quot;GEE - Estimates on Log Scale&quot;) GEE - Estimates on Log Scale   Independence Exchangeable Unstructured (Intercept) 2.230 (0.154)*** 2.230 (0.154)*** 2.230 (0.154)*** drugB 0.073 (0.220) 0.073 (0.220) 0.073 (0.220) drugC 0.327 (0.179) 0.327 (0.179) 0.327 (0.179) time -0.562 (0.176)** -0.562 (0.176)** -0.562 (0.176)** drugB:time 0.068 (0.246) 0.068 (0.246) 0.068 (0.246) drugC:time 0.515 (0.221)* 0.515 (0.221)* 0.515 (0.221)* Scale 3.474 3.474 3.474 Num. obs. 60 60 60 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 18.4.1.3 Exponentiate the Estimates (odds ratio scale) texreg::knitreg(list(extract_gee_exp(mod_gee_ind), extract_gee_exp(mod_gee_exc), extract_gee_exp(mod_gee_uns)), custom.model.names = c(&quot;Independence&quot;, &quot;Exchangeable&quot;, &quot;Unstructured&quot;), single.row = TRUE, digits = 3, ci.test = 1, caption = &quot;GEE - Estimates on Count Scale (RR)&quot;) GEE - Estimates on Count Scale (RR)   Independence Exchangeable Unstructured (Intercept) 9.300 [6.882; 12.567]* 9.300 [6.882; 12.567]* 9.300 [6.882; 12.567]* drugB 1.075 [0.699; 1.655] 1.075 [0.699; 1.655] 1.075 [0.699; 1.655] drugC 1.387 [0.977; 1.970] 1.387 [0.977; 1.970] 1.387 [0.977; 1.970] time 0.570 [0.404; 0.805]* 0.570 [0.404; 0.805]* 0.570 [0.404; 0.805]* drugB:time 1.070 [0.661; 1.734] 1.070 [0.661; 1.734] 1.070 [0.661; 1.734] drugC:time 1.673 [1.086; 2.578]* 1.673 [1.086; 2.578]* 1.673 [1.086; 2.578]* Dispersion 3.474 3.474 3.474 * Null hypothesis value outside the confidence interval. 18.4.1.4 Manual Extraction mod_gee_exc %&gt;% coef() %&gt;% exp() (Intercept) drugB drugC time drugB:time drugC:time 9.3000000 1.0752688 1.3870968 0.5698925 1.0703774 1.6731022 0.5699*1.6731 [1] 0.9534997 18.4.2 Interpretation Antibiotic A Group: Starts with mean of 9.3 and drops by 45% (nearly cut in half) over the course of treatment. Antibiotic B Group: Starts at about the same mean at Antibiotic A group and experiences the same decrease. Control Group (C): Starts at about the same mean at Antibiotic A group BUT experiences a less than a 5% decrease over the student period while on the placebo pills. 18.4.3 Visualize the Final Model 18.4.3.1 Refit with the geeglm() function in the geepack package mod_geeglm_exc &lt;- geepack::geeglm(count ~ drug*time, data = data_long, family = poisson(link = &quot;log&quot;), id = id, corstr = &quot;exchangeable&quot;) summary(mod_geeglm_exc) Call: geepack::geeglm(formula = count ~ drug * time, family = poisson(link = &quot;log&quot;), data = data_long, id = id, corstr = &quot;exchangeable&quot;) Coefficients: Estimate Std.err Wald Pr(&gt;|W|) (Intercept) 2.23001 0.15362 210.736 &lt;2e-16 *** drugB 0.07257 0.22000 0.109 0.7415 drugC 0.32721 0.17907 3.339 0.0677 . time -0.56231 0.17601 10.206 0.0014 ** drugB:time 0.06801 0.24599 0.076 0.7822 drugC:time 0.51468 0.22056 5.445 0.0196 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Correlation structure = exchangeable Estimated Scale Parameters: Estimate Std.err (Intercept) 3.126 0.5128 Link = identity Estimated Correlation Parameters: Estimate Std.err alpha 0.7352 0.08097 Number of clusters: 30 Maximum cluster size: 2 18.4.3.2 Quick interactions::interact_plot(model = mod_geeglm_exc, pred = time, modx = drug) 18.4.3.3 More Polished mod_geeglm_exc %&gt;% emmeans::emmeans(~ drug*time, type = &quot;response&quot;) drug time rate SE df asymp.LCL asymp.UCL A 0 9.3 1.43 Inf 6.88 12.57 B 0 10.0 1.57 Inf 7.34 13.62 C 0 12.9 1.19 Inf 10.77 15.45 A 1 5.3 1.39 Inf 3.17 8.87 B 1 6.1 1.85 Inf 3.37 11.04 C 1 12.3 2.14 Inf 8.74 17.31 Covariance estimate used: vbeta Confidence level used: 0.95 Intervals are back-transformed from the log scale mod_geeglm_exc %&gt;% emmeans::emmeans(~ drug*time, type = &quot;response&quot;) %&gt;% data.frame() drug time rate SE df asymp.LCL asymp.UCL 1 A 0 9.3 1.429 Inf 6.882 12.567 2 B 0 10.0 1.575 Inf 7.344 13.616 3 C 0 12.9 1.187 Inf 10.771 15.449 4 A 1 5.3 1.393 Inf 3.166 8.872 5 B 1 6.1 1.846 Inf 3.370 11.040 6 C 1 12.3 2.145 Inf 8.739 17.312 mod_geeglm_exc %&gt;% emmeans::emmeans(~ drug*time, type = &quot;response&quot;, level = .68) %&gt;% data.frame() %&gt;% dplyr::mutate(time_name = case_when(time == 0 ~ &quot;Pre&quot;, time == 1 ~ &quot;Post&quot;) %&gt;% factor(levels = c(&quot;Pre&quot;, &quot;Post&quot;))) %&gt;% dplyr::mutate(drug_name = fct_recode(drug, &quot;Antibiotic A&quot; = &quot;A&quot;, &quot;Antibiotic B&quot; = &quot;B&quot;, &quot;Placebo&quot; = &quot;C&quot;)) %&gt;% ggplot(aes(x = time_name, y = rate, group = drug_name %&gt;% fct_rev, color = drug_name %&gt;% fct_rev)) + geom_errorbar(aes(ymin = asymp.LCL, ymax = asymp.UCL), width = .3, position = position_dodge(width = .25)) + geom_point(position = position_dodge(width = .25)) + geom_line(position = position_dodge(width = .25)) + theme_bw() + labs(x = NULL, y = &quot;Estimated Marginal Mean\\nNumber of Leprosy Bacilli at Six Sites of the Body&quot;, color = NULL) + theme(legend.position = c(0, 0), legend.justification = c(-0.1, -0.1), legend.background = element_rect(color = &quot;black&quot;)) 18.5 Follow-up Analysis 18.5.1 Collapse the Predictor data_remodel &lt;- data_long %&gt;% dplyr::mutate(antibiotic = drug %&gt;% forcats::fct_collapse(yes = c(&quot;A&quot;, &quot;B&quot;), no = c(&quot;C&quot;))) 18.5.2 Reduce the Model - gee::gee() mod_gee_exc2 &lt;- gee::gee(count ~ antibiotic:time , data = data_remodel, family = poisson(link = &quot;log&quot;), id = id, corstr = &quot;exchangeable&quot;) (Intercept) antibioticyes:time antibioticno:time 2.3734 -0.6329 0.1362 summary(mod_gee_exc2) GEE: GENERALIZED LINEAR MODELS FOR DEPENDENT DATA gee S-function, version 4.13 modified 98/01/27 (1998) Model: Link: Logarithm Variance to Mean Relation: Poisson Correlation Structure: Exchangeable Call: gee::gee(formula = count ~ antibiotic:time, id = id, data = data_remodel, family = poisson(link = &quot;log&quot;), corstr = &quot;exchangeable&quot;) Summary of Residuals: Min 1Q Median 3Q Max -9.6185 -4.7333 -0.4843 3.5167 12.3815 Coefficients: Estimate Naive S.E. Naive z Robust S.E. Robust z (Intercept) 2.37335 0.1028 23.07801 0.08014 29.61589 antibioticyes:time -0.52487 0.1024 -5.12426 0.11124 -4.71827 antibioticno:time -0.01076 0.1142 -0.09421 0.15722 -0.06842 Estimated Scale Parameter: 3.405 Number of Iterations: 5 Working Correlation [,1] [,2] [1,] 1.0000 0.7803 [2,] 0.7803 1.0000 18.5.3 Compare Parameters texreg::knitreg(list(extract_gee_exp(mod_gee_exc), extract_gee_exp(mod_gee_exc2)), custom.model.names = c(&quot;Original&quot;, &quot;Refit&quot;), single.row = TRUE, digits = 3, ci.test = 1, caption = &quot;Estimates on Count Scale (Exchangeable)&quot;) Estimates on Count Scale (Exchangeable)   Original Refit (Intercept) 9.300 [6.882; 12.567]* 10.733 [9.173; 12.559]* drugB 1.075 [0.699; 1.655]   drugC 1.387 [0.977; 1.970]   time 0.570 [0.404; 0.805]*   drugB:time 1.070 [0.661; 1.734]   drugC:time 1.673 [1.086; 2.578]*   antibioticyes:time   0.592 [0.476; 0.736]* antibioticno:time   0.989 [0.727; 1.346] Dispersion 3.474 3.406 * Null hypothesis value outside the confidence interval. 18.5.4 Interpretation The grand mean is a count of 10.73 at pre-treatment. The mean count dropped by about 40% among those on antibiotics, but there was no decrease for those on placebo pills, exp(b) = 0.592, p &lt; .05, 95% CI [0.476, 0.74]. 18.5.5 Visualize 18.5.5.1 Refit with geepack::geeglm() mod_geeglm_exc2 &lt;- geepack::geeglm(count ~ antibiotic:time, data = data_remodel, family = poisson(link = &quot;log&quot;), id = id, corstr = &quot;exchangeable&quot;) 18.5.5.2 Quick interactions::interact_plot(model = mod_geeglm_exc2, pred = time, modx = antibiotic) + theme_bw() 18.5.5.3 More Polished mod_geeglm_exc2 %&gt;% emmeans::emmeans(~ antibiotic*time, type = &quot;response&quot;, level = .68) %&gt;% data.frame() %&gt;% dplyr::mutate(time_name = case_when(time == 0 ~ &quot;Pre&quot;, time == 1 ~ &quot;Post&quot;) %&gt;% factor(levels = c(&quot;Pre&quot;, &quot;Post&quot;))) %&gt;% ggplot(aes(x = time_name, y = rate, group = antibiotic, color = antibiotic)) + geom_errorbar(aes(ymin = asymp.LCL, ymax = asymp.UCL), width = .3, position = position_dodge(width = .25)) + geom_point(position = position_dodge(width = .25)) + geom_line(position = position_dodge(width = .25)) + theme_bw() + labs(x = NULL, y = &quot;Estimated Marginal Mean\\nNumber of Leprosy Bacilli at Six Sites of the Body&quot;, color = &quot;Antibiotic A or B&quot;) + theme(legend.position = c(0, 0), legend.justification = c(-0.1, -0.1), legend.background = element_rect(color = &quot;black&quot;)) mod_geeglm_exc2 %&gt;% emmeans::emmeans(pairwise ~ time | antibiotic, adjust = &quot;none&quot;) $emmeans antibiotic = yes: time emmean SE df asymp.LCL asymp.UCL 0 2.37 0.0801 Inf 2.22 2.53 1 1.84 0.1635 Inf 1.52 2.16 antibiotic = no: time emmean SE df asymp.LCL asymp.UCL 0 2.37 0.0801 Inf 2.22 2.53 1 2.37 0.1774 Inf 2.02 2.72 Covariance estimate used: vbeta Results are given on the log (not the response) scale. Confidence level used: 0.95 $contrasts antibiotic = yes: contrast estimate SE df z.ratio p.value time0 - time1 0.5307 0.113 Inf 4.694 &lt;.0001 antibiotic = no: contrast estimate SE df z.ratio p.value time0 - time1 0.0029 0.157 Inf 0.018 0.9855 Results are given on the log (not the response) scale. mod_geeglm_exc2 %&gt;% emmeans::emmeans(pairwise ~ time | antibiotic, type = &quot;response&quot;, adjust = &quot;none&quot;) $emmeans antibiotic = yes: time rate SE df asymp.LCL asymp.UCL 0 10.73 0.86 Inf 9.17 12.6 1 6.31 1.03 Inf 4.58 8.7 antibiotic = no: time rate SE df asymp.LCL asymp.UCL 0 10.73 0.86 Inf 9.17 12.6 1 10.70 1.90 Inf 7.56 15.2 Covariance estimate used: vbeta Confidence level used: 0.95 Intervals are back-transformed from the log scale $contrasts antibiotic = yes: contrast ratio SE df null z.ratio p.value time0 / time1 1.7 0.192 Inf 1 4.694 &lt;.0001 antibiotic = no: contrast ratio SE df null z.ratio p.value time0 / time1 1.0 0.157 Inf 1 0.018 0.9855 Tests are performed on the log scale 18.6 Conclusion The Research Question In this study, the question of main scientific interest is whether treatment with antibiotics (drugs A and B) reduces the abundance of leprosy bacilli at the six sites of the body when compared to placebo (drug C). The Conclusion Both of these antibiotics significantly reduce leprosy bacilli from the pre-level (M = 10.7, equivalent groups at baseline) to lower (M = 6.3), compared to no change seen when on the placebo. "],["glmm-binary-outcome-contraception-amenorrhea.html", "19 GLMM, Binary Outcome: Contraception &amp; Amenorrhea 19.1 Packages 19.2 Data Prep 19.3 Exploratory Data Analysis 19.4 GLMM - Basic 19.5 GLMM - Optimizers 19.6 Quadratic Time?", " 19 GLMM, Binary Outcome: Contraception &amp; Amenorrhea 19.1 Packages 19.1.1 CRAN library(tidyverse) # all things tidy library(pander) # nice looking genderal tabulations library(furniture) # nice table1() descriptives library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(psych) # contains some useful functions, like headTail library(lme4) # Linear, generalized linear, &amp; nonlinear mixed models library(effects) # Plotting estimated marginal means library(emmeans) library(interactions) library(performance) library(optimx) # Unify and streamline optimization capabilities in R 19.1.2 GitHub Helper extract functions for exponentiating parameters form generalized regression models within a texreg table of model parameters. # install.packages(&quot;devtools&quot;) # library(devtools) # install_github(&quot;SarBearSchwartz/texreghelpr&quot;) library(texreghelpr) 19.2 Data Prep Data on Amenorrhea from Clinical Trial of Contracepting Women. Source: Table 1 (page 168) of Machin et al. (1988). With permission of Elsevier. Reference: Machin D, Farley T, Busca B, Campbell M and d’Arcangues C. (1988). Assessing changes in vaginal bleeding patterns in contracepting women. Contraception, 38, 165-179. Description: The data are from a longitudinal clinical trial of contracepting women. In this trial women received an injection of either 100 mg or 150 mg of depot-medroxyprogesterone acetate (DMPA) on the day of randomization and three additional injections at 90-day intervals. There was a final follow-up visit 90 days after the fourth injection, i.e., one year after the first injection. Throughout the study each woman completed a menstrual diary that recorded any vaginal bleeding pattern disturbances. The diary data were used to determine whether a women experienced amenorrhea, the absence of menstrual bleeding for a specified number of days. A total of 1151 women completed the menstrual diaries and the diary data were used to generate a binary sequence for each woman according to whether or not she had experienced amenorrhea in the four successive three month intervals. In clinical trials of modern hormonal contraceptives, pregnancy is exceedingly rare (and would be regarded as a failure of the contraceptive method), and is not the main outcome of interest in this study. Instead, the outcome of interest is a binary response indicating whether a woman experienced amenorrhea in the four successive three month intervals. A feature of this clinical trial is that there was substantial dropout. More than one third of the women dropped out before the completion of the trial. Variable List: Indicators id participant identification occasion denotes the four 90-day periods Outcome or dependent variable amenorrhea Amenorrhea Status: 1=Amenorrhea, 0=No Amenorrhea Main predictor or independent variable of interest dose 0 = Low (100 mg), 1 = High (150 mg) 19.2.1 Import data_raw &lt;- read.table(&quot;https://raw.githubusercontent.com/CEHS-research/data/master/MLM/RCTcontraception.txt&quot;, header=TRUE) str(data_raw) &#39;data.frame&#39;: 4604 obs. of 4 variables: $ id : int 1 1 1 1 2 2 2 2 3 3 ... $ dose : int 0 0 0 0 0 0 0 0 0 0 ... $ occasion : int 1 2 3 4 1 2 3 4 1 2 ... $ amenorrhea: chr &quot;0&quot; &quot;.&quot; &quot;.&quot; &quot;.&quot; ... psych::headTail(data_raw, top = 10) id dose occasion amenorrhea 1 1 0 1 0 2 1 0 2 . 3 1 0 3 . 4 1 0 4 . 5 2 0 1 0 6 2 0 2 . 7 2 0 3 . 8 2 0 4 . 9 3 0 1 0 10 3 0 2 . ... ... ... ... &lt;NA&gt; 4601 1151 1 1 1 4602 1151 1 2 1 4603 1151 1 3 1 4604 1151 1 4 1 19.2.2 Long Format data_long &lt;- data_raw %&gt;% dplyr::mutate(id = factor(id)) %&gt;% dplyr::mutate(dose = factor(dose, levels = c(&quot;0&quot;, &quot;1&quot;), labels = c(&quot;Low&quot;, &quot;High&quot;))) %&gt;% dplyr::mutate(time = occasion - 1) %&gt;% dplyr::mutate(amenorrhea = amenorrhea %&gt;% # outcome needs to be numeric as.character() %&gt;% as.numeric()) %&gt;% dplyr::filter(complete.cases(amenorrhea)) %&gt;% # dump missing occations dplyr::arrange(id, time) str(data_long) &#39;data.frame&#39;: 3616 obs. of 5 variables: $ id : Factor w/ 1151 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... $ dose : Factor w/ 2 levels &quot;Low&quot;,&quot;High&quot;: 1 1 1 1 1 1 1 1 1 1 ... $ occasion : int 1 1 1 1 1 1 1 1 1 1 ... $ amenorrhea: num 0 0 0 0 0 0 0 0 0 0 ... $ time : num 0 0 0 0 0 0 0 0 0 0 ... psych::headTail(data_long, bottom = 10) id dose occasion amenorrhea time 1 1 Low 1 0 0 2 2 Low 1 0 0 3 3 Low 1 0 0 4 4 Low 1 0 0 ... &lt;NA&gt; &lt;NA&gt; ... ... ... 3607 1149 High 3 1 2 3608 1149 High 4 1 3 3609 1150 High 1 1 0 3610 1150 High 2 1 1 3611 1150 High 3 1 2 3612 1150 High 4 1 3 3613 1151 High 1 1 0 3614 1151 High 2 1 1 3615 1151 High 3 1 2 3616 1151 High 4 1 3 19.2.3 Wide Format data_wide &lt;- data_long %&gt;% dplyr::select(-time) %&gt;% tidyr::pivot_wider(id_cols = c(id, dose), names_from = occasion, names_prefix = &quot;occasion_&quot;, values_from = amenorrhea) str(data_wide) tibble [1,151 × 6] (S3: tbl_df/tbl/data.frame) $ id : Factor w/ 1151 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... $ dose : Factor w/ 2 levels &quot;Low&quot;,&quot;High&quot;: 1 1 1 1 1 1 1 1 1 1 ... $ occasion_1: num [1:1151] 0 0 0 0 0 0 0 0 0 0 ... $ occasion_2: num [1:1151] NA NA NA NA NA NA NA NA NA NA ... $ occasion_3: num [1:1151] NA NA NA NA NA NA NA NA NA NA ... $ occasion_4: num [1:1151] NA NA NA NA NA NA NA NA NA NA ... psych::headTail(data_wide, bottom = 10) id dose occasion_1 occasion_2 occasion_3 occasion_4 1 1 Low 0 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 2 2 Low 0 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 3 3 Low 0 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 4 4 Low 0 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 5 &lt;NA&gt; &lt;NA&gt; ... ... ... ... 6 1142 High 1 1 1 1 7 1143 High 1 1 1 1 8 1144 High 1 1 1 1 9 1145 High 1 1 1 1 10 1146 High 1 1 1 1 11 1147 High 1 1 1 1 12 1148 High 1 1 1 1 13 1149 High 1 1 1 1 14 1150 High 1 1 1 1 15 1151 High 1 1 1 1 19.3 Exploratory Data Analysis 19.3.1 Summary Statistics data_summary &lt;- data_long %&gt;% dplyr::group_by(dose, occasion) %&gt;% dplyr::summarise(N = n(), M = mean(amenorrhea), SD = sd(amenorrhea), SE = SD/sqrt(N)) data_summary # A tibble: 8 × 6 # Groups: dose [2] dose occasion N M SD SE &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Low 1 576 0.186 0.389 0.0162 2 Low 2 477 0.262 0.440 0.0202 3 Low 3 409 0.389 0.488 0.0241 4 Low 4 361 0.501 0.501 0.0264 5 High 1 575 0.205 0.404 0.0169 6 High 2 476 0.336 0.473 0.0217 7 High 3 389 0.494 0.501 0.0254 8 High 4 353 0.535 0.499 0.0266 19.3.2 Visualize data_summary %&gt;% ggplot(aes(x = occasion, y = M, fill = dose)) + geom_col(position = &quot;dodge&quot;) + theme_bw() + theme(legend.position = c(0, 1), legend.justification = c(-0.1, 1.1), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(1.5, &quot;cm&quot;)) + labs(x = &quot;90-day windows&quot;, y = &quot;Observed Proportion of Amenorrhea&quot;, fill = &quot;Dosage&quot;) + scale_x_continuous(breaks = 1:4, labels = c(&quot;First&quot;, &quot;Second&quot;, &quot;Third&quot;, &quot;Fourth&quot;)) data_summary %&gt;% ggplot(aes(x = occasion, y = M, color = dose %&gt;% fct_rev())) + geom_errorbar(aes(ymin = M - SE, ymax = M + SE), width = .3, position = position_dodge(width = .25)) + geom_point(position = position_dodge(width = .25)) + geom_line(position = position_dodge(width = .25)) + theme_bw() + theme(legend.position = c(0, 1), legend.justification = c(-0.1, 1.1), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(1.5, &quot;cm&quot;)) + labs(x = &quot;90-day windows&quot;, y = &quot;Observed Proportion of Amenorrhea&quot;, color = &quot;Dosage&quot;) + scale_x_continuous(breaks = 1:4, labels = c(&quot;First&quot;, &quot;Second&quot;, &quot;Third&quot;, &quot;Fourth&quot;)) 19.4 GLMM - Basic 19.4.1 Fit Models fit_1 &lt;- lme4::glmer(amenorrhea ~ time*dose + (1 | id), data = data_long, family = binomial(link = &quot;logit&quot;)) fit_2 &lt;- lme4::glmer(amenorrhea ~ time + dose + (1 | id), data = data_long, family = binomial(link = &quot;logit&quot;)) 19.4.1.1 Compare via LRT Should the interaction be included? No. anova(fit_1, fit_2) Data: data_long Models: fit_2: amenorrhea ~ time + dose + (1 | id) fit_1: amenorrhea ~ time * dose + (1 | id) npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) fit_2 4 3931 3956 -1961 3923 fit_1 5 3932 3963 -1961 3922 0.69 1 0.41 performance::compare_performance(fit_1, fit_2, rank = TRUE) # Comparison of Model Performance Indices Name | Model | R2 (cond.) | R2 (marg.) | ICC | RMSE | Sigma | Log_loss | Score_log | Score_spherical | AIC weights | AICc weights | BIC weights | Performance-Score -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- fit_1 | glmerMod | 0.619 | 0.126 | 0.564 | 0.303 | 1.000 | 0.311 | -Inf | 0.004 | 0.342 | 0.342 | 0.023 | -Inf% fit_2 | glmerMod | 0.619 | 0.126 | 0.563 | 0.303 | 1.000 | 0.311 | -Inf | 0.003 | 0.658 | 0.658 | 0.977 | -Inf% 19.4.2 Model Parameter Tables 19.4.2.1 Logit Scale texreg::knitreg(list(fit_1, fit_2), custom.model.names = c(&quot;with&quot;, &quot;without&quot;), single.row = TRUE, caption = &quot;MLM Parameter Estimates: Inclusion of Interaction (SE and p-values)&quot;) MLM Parameter Estimates: Inclusion of Interaction (SE and p-values)   with without (Intercept) -2.55 (0.17)*** -2.61 (0.16)*** time 0.87 (0.07)*** 0.91 (0.05)*** doseHigh 0.39 (0.21) 0.50 (0.16)** time:doseHigh 0.08 (0.09)   AIC 3932.14 3930.83 BIC 3963.11 3955.61 Log Likelihood -1961.07 -1961.42 Num. obs. 3616 3616 Num. groups: id 1151 1151 Var: id (Intercept) 4.26 4.24 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 19.4.2.2 Odds ratio scale texreg::knitreg(list(extract_glmer_exp(fit_1), extract_glmer_exp(fit_2)), custom.model.names = c(&quot;with&quot;, &quot;without&quot;), ci.test = 1, single.row = TRUE, caption = &quot;MLM Parameter Estimates: Inclusion of Interaction (95% CI&#39;s)&quot;) MLM Parameter Estimates: Inclusion of Interaction (95% CI’s)   with without (Intercept) 0.08 [0.06; 0.11]* 0.07 [0.05; 0.10]* time 2.40 [2.08; 2.75]* 2.49 [2.24; 2.77]* doseHigh 1.48 [0.98; 2.22] 1.64 [1.19; 2.27]* time:doseHigh 1.08 [0.90; 1.30]   * Null hypothesis value outside the confidence interval. 19.4.3 Visualize the Model 19.4.3.1 Sclae = Likert interactions::interact_plot(model = fit_2, pred = time, modx = dose, interval = TRUE, outcome.scale = &quot;link&quot;, y.label = &quot;Likert Scale for Ammenorea&quot;) 19.4.3.2 Scale = Probability interactions::interact_plot(model = fit_2, pred = time, modx = dose, interval = TRUE, outcome.scale = &quot;response&quot;, y.label = &quot;Estimated Marginal Probability of Ammenorea&quot;) interactions::interact_plot(model = fit_2, pred = time, modx = dose, interval = TRUE, outcome.scale = &quot;response&quot;, x.label = &quot;Months&quot;, y.label = &quot;Predicted Probability of Amenorrhea&quot;, legend.main = &quot;Dosage:&quot;, colors = c(&quot;black&quot;, &quot;black&quot;)) + geom_hline(yintercept = 0.5, # reference lines color = &quot;gray&quot;, size = 1.5, alpha = .5) + theme_bw() + theme(legend.position = c(0, 1), legend.justification = c(-0.1, 1.1), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(1.5, &quot;cm&quot;)) + scale_x_continuous(breaks = 0:3, labels = c(&quot;0&quot;, &quot;3&quot;, &quot;6&quot;, &quot;9&quot;)) effects::Effect(focal.predictors = c(&quot;dose&quot;, &quot;time&quot;), xlevels = list(time = seq(from = 0, to = 3, by = .1)), mod = fit_2) %&gt;% data.frame %&gt;% ggplot(aes(x = time, y = fit)) + geom_hline(yintercept = c(0, 0.5, 1), # reference lines color = &quot;gray&quot;, size = 1.5) + geom_ribbon(aes(ymin = fit - se, ymax = fit + se, fill = dose), alpha = .2) + geom_line(aes(color = dose), size = 1.5) + theme_bw() + labs(y = &quot;Predicted Probability&quot;) Remove the error bands: effects::Effect(focal.predictors = c(&quot;dose&quot;, &quot;time&quot;), xlevels = list(time = seq(from = 0, to = 3, by = .1)), mod = fit_2) %&gt;% data.frame %&gt;% ggplot(aes(x = time, y = fit)) + geom_hline(yintercept = c(0, 0.5), color = &quot;gray&quot;, size = 1.5) + geom_line(aes(linetype = dose), size = 1) + theme_bw() + theme(legend.position = c(0, 1), legend.justification = c(-0.1, 1.1), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(1.5, &quot;cm&quot;)) + labs(x = &quot;90-day Window&quot;, y = &quot;Predicted Probability of Amenorrhea&quot;, linetype = &quot;Dosage:&quot;) + scale_x_continuous(breaks = 0:3, labels = c(&quot;First&quot;, &quot;Second&quot;, &quot;Third&quot;, &quot;Fourth&quot;)) 19.5 GLMM - Optimizers From the documentation: The lme4::glmer() function fits a generalized linear mixed model, which incorporates both fixed-effects parameters and random effects in a linear predictor, via maximum likelihood. The linear predictor is related to the conditional mean of the response through the inverse link function defined in the GLM family. The expression for the likelihood of a mixed-effects model is an integral over the random effects space. For a linear mixed-effects model (LMM), as fit by lmer, this integral can be evaluated exactly. For a GLMM the integral must be approximated. The most reliable approximation for GLMMs is adaptive Gauss-Hermite quadrature, at present implemented only for models with a single scalar random effect. The nAGQ argument controls the number of nodes in the quadrature formula. A model with a single, scalar random-effects term could reasonably use up to 25 quadrature points per scalar integral. The lme4::lmerControl() function includes an argument for the optimizer, which is the name of a optimizing function(s). IT is a character vector or list of functions: length 1 for lmer or glmer, possibly length 2 for glmer). The built-in optimizers are Nelder_Mead and bobyqa (from the minqa package). Other minimizing functions are allows (constraints do apply). Special provisions are made for bobyqa, Nelder_Mead, and optimizers wrapped in the optimx package; to use the optimx optimizers (including L-BFGS-B from base optim and nlminb), pass the method argument to optim in the optCtrl argument (you may also need to load the optimx package manually using library(optimx). 19.5.1 Adaptive Gauss-Hermite Quadrature: Increase the number of quadrature points nAGQ (integer scalar) the number of points per axis for evaluating the adaptive Gauss-Hermite approximation to the log-likelihood. Defaults to 1, corresponding to the Laplace approximation. Values greater than 1 produce greater accuracy in the evaluation of the log-likelihood at the expense of speed. A value of zero uses a faster but less exact form of parameter estimation for GLMMs by optimizing the random effects and the fixed-effects coefficients in the penalized iteratively reweighted least squares step. (See Details.) fit_3a &lt;- lme4::glmer(amenorrhea ~ time + I(time^2) + time:dose + I(time^2):dose + (1 | id), data = data_long, nAGQ = 50, # increase the number of points family = binomial) 19.5.2 Laplace Approximation: switch to the Nelder_Mead optimizer fit_3b &lt;- lme4::glmer(amenorrhea ~ time + I(time^2) + time:dose + I(time^2):dose + (1 | id), data = data_long, control = glmerControl(optimizer =&quot;Nelder_Mead&quot;), family = binomial) 19.5.3 Laplace Approximation: Switch to the L-BFGS-B method fit_3c &lt;- lme4::glmer(amenorrhea ~ time + I(time^2) + time:dose + I(time^2):dose + (1 | id), data = data_long, control = glmerControl(optimizer =&#39;optimx&#39;, optCtrl=list(method=&#39;L-BFGS-B&#39;)), family = binomial) 19.5.4 Laplace Approximation: Switch to the nlminb method fit_3d &lt;- lme4::glmer(amenorrhea ~ time + I(time^2) + time:dose + I(time^2):dose + (1 | id), data = data_long, control = glmerControl(optimizer =&#39;optimx&#39;, optCtrl=list(method=&#39;nlminb&#39;)), family = binomial) 19.6 Quadratic Time? Assess need for quadratic time with the LRT anova(fit_2, fit_3d) Data: data_long Models: fit_2: amenorrhea ~ time + dose + (1 | id) fit_3d: amenorrhea ~ time + I(time^2) + time:dose + I(time^2):dose + (1 | id) npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) fit_2 4 3931 3956 -1961 3923 fit_3d 6 3925 3962 -1957 3913 9.72 2 0.0078 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 texreg::knitreg(list(fit_3a, fit_3b, fit_3c, fit_3d), custom.model.names = c(&quot;nAGQ&quot;, &quot;Nelder_Mead&quot;, &quot;L BFGS B&quot;, &quot;nlminb&quot;), caption = &quot;GLMM: Various methods of ML approximation&quot;, digits = 4) GLMM: Various methods of ML approximation   nAGQ Nelder_Mead L BFGS B nlminb (Intercept) -2.4829*** -2.4604*** -2.4601*** -2.4604***   (0.1416) (0.1397) (0.1397) (0.1397) time 0.7714*** 0.7561*** 0.7558*** 0.7561***   (0.2026) (0.1985) (0.1985) (0.1985) time^2 0.0346 0.0340 0.0341 0.0340   (0.0667) (0.0655) (0.0655) (0.0655) time:doseHigh 0.8920*** 0.8861*** 0.8860*** 0.8861***   (0.2574) (0.2513) (0.2513) (0.2513) time^2:doseHigh -0.2599** -0.2579** -0.2579** -0.2579**   (0.0895) (0.0879) (0.0879) (0.0879) AIC 3879.4906 3925.1127 3925.1128 3925.1127 BIC 3916.6493 3962.2715 3962.2715 3962.2715 Log Likelihood -1933.7453 -1956.5564 -1956.5564 -1956.5564 Num. obs. 3616 3616 3616 3616 Num. groups: id 1151 1151 1151 1151 Var: id (Intercept) 5.0794 4.3478 4.3494 4.3479 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 interactions::interact_plot(model = fit_3d, pred = time, modx = dose, interval = TRUE) effects::Effect(focal.predictors = c(&quot;dose&quot;, &quot;time&quot;), xlevels = list(time = seq(from = 0, to = 3, by = .1)), mod = fit_3d) %&gt;% data.frame %&gt;% ggplot(aes(x = time, y = fit)) + geom_hline(yintercept = c(0, 0.5), color = &quot;gray&quot;, size = 1.5) + geom_line(aes(linetype = dose), size = 1) + theme_bw() + theme(legend.position = c(0, 1), legend.justification = c(-0.1, 1.1), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(1.5, &quot;cm&quot;)) + labs(x = &quot;90-day Window&quot;, y = &quot;Predicted Probability of Amenorrhea&quot;, linetype = &quot;Dosage:&quot;) + scale_x_continuous(breaks = 0:3, labels = c(&quot;First&quot;, &quot;Second&quot;, &quot;Third&quot;, &quot;Fourth&quot;)) 19.6.1 Post hoc compairsons fit_3d %&gt;% emmeans::emmeans(pairwise ~ dose, at = c(time = 0)) $emmeans dose emmean SE df asymp.LCL asymp.UCL Low -2.46 0.14 Inf -2.73 -2.19 High -2.46 0.14 Inf -2.73 -2.19 Results are given on the logit (not the response) scale. Confidence level used: 0.95 $contrasts contrast estimate SE df z.ratio p.value Low - High 0 0 Inf NaN NaN Results are given on the log odds ratio (not the response) scale. fit_3d %&gt;% emmeans::emmeans(pairwise ~ dose, at = c(time = 0), type = &quot;response&quot;) $emmeans dose prob SE df asymp.LCL asymp.UCL Low 0.0787 0.0101 Inf 0.061 0.101 High 0.0787 0.0101 Inf 0.061 0.101 Confidence level used: 0.95 Intervals are back-transformed from the logit scale $contrasts contrast odds.ratio SE df null z.ratio p.value Low / High 1 0 Inf 1 NaN NaN Tests are performed on the log odds ratio scale fit_3d %&gt;% emmeans::emmeans(pairwise ~ dose, at = c(time = 1)) $emmeans dose emmean SE df asymp.LCL asymp.UCL Low -1.67 0.141 Inf -1.95 -1.394 High -1.04 0.132 Inf -1.30 -0.783 Results are given on the logit (not the response) scale. Confidence level used: 0.95 $contrasts contrast estimate SE df z.ratio p.value Low - High -0.628 0.169 Inf -3.711 0.0002 Results are given on the log odds ratio (not the response) scale. fit_3d %&gt;% emmeans::emmeans(pairwise ~ dose, at = c(time = 1), type = &quot;response&quot;) $emmeans dose prob SE df asymp.LCL asymp.UCL Low 0.158 0.0188 Inf 0.125 0.199 High 0.261 0.0255 Inf 0.214 0.314 Confidence level used: 0.95 Intervals are back-transformed from the logit scale $contrasts contrast odds.ratio SE df null z.ratio p.value Low / High 0.533 0.0903 Inf 1 -3.711 0.0002 Tests are performed on the log odds ratio scale fit_3d %&gt;% emmeans::emmeans(pairwise ~ dose, at = c(time = 2), type = &quot;response&quot;) $emmeans dose prob SE df asymp.LCL asymp.UCL Low 0.307 0.0304 Inf 0.251 0.37 High 0.482 0.0348 Inf 0.415 0.55 Confidence level used: 0.95 Intervals are back-transformed from the logit scale $contrasts contrast odds.ratio SE df null z.ratio p.value Low / High 0.477 0.0935 Inf 1 -3.776 0.0002 Tests are performed on the log odds ratio scale fit_3d %&gt;% emmeans::emmeans(pairwise ~ dose, at = c(time = 3), type = &quot;response&quot;) $emmeans dose prob SE df asymp.LCL asymp.UCL Low 0.528 0.0418 Inf 0.446 0.609 High 0.611 0.0403 Inf 0.530 0.686 Confidence level used: 0.95 Intervals are back-transformed from the logit scale $contrasts contrast odds.ratio SE df null z.ratio p.value Low / High 0.714 0.166 Inf 1 -1.446 0.1482 Tests are performed on the log odds ratio scale "],["glmm-binary-outcome-muscatine-obesity.html", "20 GLMM, Binary Outcome: Muscatine Obesity 20.1 Packages 20.2 Data Prep 20.3 Exploratory Data Analysis 20.4 Analysis Goal 20.5 GLM Analysis 20.6 GEE Analysis 20.7 GLMM Analysis 20.8 Compare Methods", " 20 GLMM, Binary Outcome: Muscatine Obesity 20.1 Packages 20.1.1 CRAN library(tidyverse) # all things tidy library(pander) # nice looking genderal tabulations library(furniture) # nice table1() descriptives library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(psych) # contains some useful functions, like headTail library(lme4) # Linear, generalized linear, &amp; nonlinear mixed models library(gee) # Generalized Estimating Equations library(effects) # Plotting estimated marginal means library(performance) library(interactions) library(patchwork) # combining graphics 20.1.2 GitHub Helper extract functions for exponentiating parameters form generalized regression models within a texreg table of model parameters. # install.packages(&quot;devtools&quot;) # library(devtools) # install_github(&quot;SarBearSchwartz/texreghelpr&quot;) library(texreghelpr) 20.2 Data Prep Data on Obesity from the Muscatine Coronary Risk Factor Study. Source: Table 10 (page 96) in Woolson and Clarke (1984). With permission of Blackwell Publishing. Reference: Woolson, R.F. and Clarke, W.R. (1984). Analysis of categorical incompletel longitudinal data. Journal of the Royal Statistical Society, Series A, 147, 87-99. Description: The Muscatine Coronary Risk Factor Study (MCRFS) was a longitudinal study of coronary risk factors in school children in Muscatine, Iowa (Woolson and Clarke 1984; Ekholm and Skinner 1998). Five cohorts of children were measured for height and weight in 1977, 1979, and 1981. Relative weight was calculated as the ratio of a child’s observed weight to the median weight for their age-sex-height group. Children with a relative weight greater than 110% of the median weight for their respective stratum were classified as obese. The analysis of this study involves binary data (1 = obese, 0 = not obese) collected at successive time points. This data was also using in an article title “Missing data methods in longitudinal studies: a review” (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3016756/). Variable List: Indicators id Child’s unique identification number occas Occasion number: 1, 2, 3 Outcome or dependent variable obesity Obesity Status, 0 = no, 1 = yes Main predictor or independent variable of interest gender 0 = Male, 1 = Female baseage Baseline Age, mid-point of age-cohort currage Current Age, mid-point of age-cohort 20.2.1 Import data_raw &lt;- read.table(&quot;https://raw.githubusercontent.com/CEHS-research/data/master/MLM/Muscatine.txt&quot;, header=TRUE) str(data_raw) &#39;data.frame&#39;: 14568 obs. of 6 variables: $ id : int 1 1 1 2 2 2 3 3 3 4 ... $ gender : int 0 0 0 0 0 0 0 0 0 0 ... $ baseage: int 6 6 6 6 6 6 6 6 6 6 ... $ currage: int 6 8 10 6 8 10 6 8 10 6 ... $ occas : int 1 2 3 1 2 3 1 2 3 1 ... $ obesity: chr &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; ... psych::headTail(data_raw, top = 10) id gender baseage currage occas obesity 1 1 0 6 6 1 1 2 1 0 6 8 2 1 3 1 0 6 10 3 1 4 2 0 6 6 1 1 5 2 0 6 8 2 1 6 2 0 6 10 3 1 7 3 0 6 6 1 1 8 3 0 6 8 2 1 9 3 0 6 10 3 1 10 4 0 6 6 1 1 ... ... ... ... ... ... &lt;NA&gt; 14565 4855 1 14 18 3 0 14566 4856 1 14 14 1 . 14567 4856 1 14 16 2 . 14568 4856 1 14 18 3 0 20.2.2 Restrict to 350ID’s of children with complete data for Class Demonstration Dealing with missing-ness and its implications are beyond the score of this class. Instead we are going to restrict our class analysis to a subset of 350 children who have complete data I am using the set.seed() function so that I can replicate the restults later. complete_ids &lt;- data_raw %&gt;% dplyr::filter(obesity %in% c(&quot;0&quot;, &quot;1&quot;)) %&gt;% dplyr::group_by(id) %&gt;% dplyr::summarise(n = n()) %&gt;% dplyr::filter(n == 3) %&gt;% dplyr::pull(id) set.seed(8892) # needed? use_ids &lt;- complete_ids %&gt;% sample(350) head(use_ids) [1] 3574 805 3458 3537 679 655 20.2.3 Long Format data_long &lt;- data_raw %&gt;% dplyr::filter(id %in% use_ids) %&gt;% mutate(id = id %&gt;% factor) %&gt;% mutate(gender = gender %&gt;% factor(levels = 0:1, labels = c(&quot;Male&quot;, &quot;Female&quot;))) %&gt;% mutate(age_base = baseage %&gt;% factor) %&gt;% mutate(age_curr = currage %&gt;% factor) %&gt;% mutate(occation = occas %&gt;% factor) %&gt;% mutate(obesity = obesity %&gt;% factor(levels = 0:1, labels = c(&quot;No&quot;, &quot;Yes&quot;))) %&gt;% select(id, gender, age_base, age_curr, occation, obesity) str(data_long) &#39;data.frame&#39;: 1050 obs. of 6 variables: $ id : Factor w/ 350 levels &quot;1&quot;,&quot;5&quot;,&quot;10&quot;,&quot;16&quot;,..: 1 1 1 2 2 2 3 3 3 4 ... $ gender : Factor w/ 2 levels &quot;Male&quot;,&quot;Female&quot;: 1 1 1 1 1 1 1 1 1 1 ... $ age_base: Factor w/ 5 levels &quot;6&quot;,&quot;8&quot;,&quot;10&quot;,&quot;12&quot;,..: 1 1 1 1 1 1 2 2 2 2 ... $ age_curr: Factor w/ 7 levels &quot;6&quot;,&quot;8&quot;,&quot;10&quot;,&quot;12&quot;,..: 1 2 3 1 2 3 2 3 4 2 ... $ occation: Factor w/ 3 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;: 1 2 3 1 2 3 1 2 3 1 ... $ obesity : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 2 2 2 2 2 2 2 2 ... psych::headTail(data_long, top = 10) id gender age_base age_curr occation obesity 1 1 Male 6 6 1 Yes 2 1 Male 6 8 2 Yes 3 1 Male 6 10 3 Yes 4 5 Male 6 6 1 Yes 5 5 Male 6 8 2 Yes 6 5 Male 6 10 3 Yes 7 10 Male 8 8 1 Yes 8 10 Male 8 10 2 Yes 9 10 Male 8 12 3 Yes 10 16 Male 8 8 1 Yes ... &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 1047 3582 Female 14 18 3 No 1048 3584 Female 14 14 1 No 1049 3584 Female 14 16 2 No 1050 3584 Female 14 18 3 No 20.2.4 Wide Format data_wide &lt;- data_long %&gt;% tidyr::pivot_wider(names_from = occation, names_sep = &quot;_&quot;, values_from = c(obesity, age_curr)) %&gt;% mutate_if(is.character, factor)%&gt;% group_by(id) %&gt;% mutate(num_miss = sum(is.na(c(obesity_1, obesity_2, obesity_3)))) %&gt;% ungroup() %&gt;% mutate(num_miss = as.factor(num_miss)) str(data_wide) tibble [350 × 10] (S3: tbl_df/tbl/data.frame) $ id : Factor w/ 350 levels &quot;1&quot;,&quot;5&quot;,&quot;10&quot;,&quot;16&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... $ gender : Factor w/ 2 levels &quot;Male&quot;,&quot;Female&quot;: 1 1 1 1 1 1 1 1 1 1 ... $ age_base : Factor w/ 5 levels &quot;6&quot;,&quot;8&quot;,&quot;10&quot;,&quot;12&quot;,..: 1 1 2 2 2 3 3 3 4 4 ... $ obesity_1 : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 2 2 2 2 2 2 2 2 ... $ obesity_2 : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 2 2 2 2 2 2 2 2 ... $ obesity_3 : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 2 2 2 2 2 2 2 2 ... $ age_curr_1: Factor w/ 7 levels &quot;6&quot;,&quot;8&quot;,&quot;10&quot;,&quot;12&quot;,..: 1 1 2 2 2 3 3 3 4 4 ... $ age_curr_2: Factor w/ 7 levels &quot;6&quot;,&quot;8&quot;,&quot;10&quot;,&quot;12&quot;,..: 2 2 3 3 3 4 4 4 5 5 ... $ age_curr_3: Factor w/ 7 levels &quot;6&quot;,&quot;8&quot;,&quot;10&quot;,&quot;12&quot;,..: 3 3 4 4 4 5 5 5 6 6 ... $ num_miss : Factor w/ 1 level &quot;0&quot;: 1 1 1 1 1 1 1 1 1 1 ... psych::headTail(data_wide, top = 10) id gender age_base obesity_1 obesity_2 obesity_3 age_curr_1 age_curr_2 1 1 Male 6 Yes Yes Yes 6 8 2 5 Male 6 Yes Yes Yes 6 8 3 10 Male 8 Yes Yes Yes 8 10 4 16 Male 8 Yes Yes Yes 8 10 5 21 Male 8 Yes Yes Yes 8 10 6 30 Male 10 Yes Yes Yes 10 12 7 44 Male 10 Yes Yes Yes 10 12 8 50 Male 10 Yes Yes Yes 10 12 9 60 Male 12 Yes Yes Yes 12 14 10 61 Male 12 Yes Yes Yes 12 14 11 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 12 3580 Female 14 No No No 14 16 13 3581 Female 14 No No No 14 16 14 3582 Female 14 No No No 14 16 15 3584 Female 14 No No No 14 16 age_curr_3 num_miss 1 10 0 2 10 0 3 12 0 4 12 0 5 12 0 6 14 0 7 14 0 8 14 0 9 16 0 10 16 0 11 &lt;NA&gt; &lt;NA&gt; 12 18 0 13 18 0 14 18 0 15 18 0 20.3 Exploratory Data Analysis 20.3.1 Summary Statistics 20.3.1.1 Demographics and Baseline data_wide %&gt;% dplyr::group_by(gender) %&gt;% furniture::table1(&quot;Baseline Age&quot; = age_base, &quot;Baseline Obesity&quot; = obesity_1, total = TRUE, test = TRUE, na.rm = FALSE, output = &quot;markdown&quot;) Total Male Female P-Value n = 350 n = 166 n = 184 Baseline Age 0.759 6 42 (12%) 17 (10.2%) 25 (13.6%) 8 91 (26%) 47 (28.3%) 44 (23.9%) 10 92 (26.3%) 41 (24.7%) 51 (27.7%) 12 67 (19.1%) 33 (19.9%) 34 (18.5%) 14 58 (16.6%) 28 (16.9%) 30 (16.3%) NA 0 (0%) 0 (0%) 0 (0%) Baseline Obesity 0.608 No 286 (81.7%) 138 (83.1%) 148 (80.4%) Yes 64 (18.3%) 28 (16.9%) 36 (19.6%) NA 0 (0%) 0 (0%) 0 (0%) 20.3.1.2 Status over Time data_summary &lt;- data_long %&gt;% dplyr::group_by(gender, age_curr) %&gt;% dplyr::mutate(obesityN = case_when(obesity == &quot;Yes&quot; ~ 1, obesity == &quot;No&quot; ~ 0)) %&gt;% dplyr::filter(complete.cases(gender, age_curr, obesityN)) %&gt;% dplyr::summarise(n = n(), prob_est = mean(obesityN), prob_SD = sd(obesityN), prob_SE = prob_SD/sqrt(n)) data_summary # A tibble: 14 × 6 # Groups: gender [2] gender age_curr n prob_est prob_SD prob_SE &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Male 6 17 0.118 0.332 0.0805 2 Male 8 64 0.172 0.380 0.0475 3 Male 10 105 0.143 0.352 0.0343 4 Male 12 121 0.198 0.400 0.0364 5 Male 14 102 0.225 0.420 0.0416 6 Male 16 61 0.213 0.413 0.0529 7 Male 18 28 0.143 0.356 0.0673 8 Female 6 25 0.16 0.374 0.0748 9 Female 8 69 0.203 0.405 0.0488 10 Female 10 120 0.275 0.448 0.0409 11 Female 12 129 0.256 0.438 0.0386 12 Female 14 115 0.243 0.431 0.0402 13 Female 16 64 0.281 0.453 0.0566 14 Female 18 30 0.267 0.450 0.0821 20.3.2 Visualize 20.3.2.1 By cohort and gender data_long %&gt;% dplyr::group_by(gender, age_base, age_curr) %&gt;% dplyr::mutate(obesityN = case_when(obesity == &quot;Yes&quot; ~ 1, obesity == &quot;No&quot; ~ 0)) %&gt;% dplyr::filter(complete.cases(gender, age_curr, obesityN)) %&gt;% dplyr::summarise(n = n(), prob_est = mean(obesityN), prob_SD = sd(obesityN), prob_SE = prob_SD/sqrt(n)) %&gt;% ggplot(aes(x = age_curr, y = prob_est, group = age_base, color = age_base)) + geom_point() + geom_line() + theme_bw() + labs(x = &quot;Child&#39;s Age, years&quot;, y = &quot;Proportion Obese&quot;) + facet_grid(. ~ gender) 20.3.2.2 BY only gender data_summary %&gt;% ggplot(aes(x = age_curr, y = prob_est, group = gender)) + geom_ribbon(aes(ymin = prob_est - prob_SE, ymax = prob_est + prob_SE, fill = gender), alpha = .3) + geom_point(aes(color = gender, shape = gender)) + geom_line(aes(linetype = gender, color = gender)) + theme_bw() + scale_color_manual(values = c(&quot;dodger blue&quot;, &quot;hot pink&quot;)) + scale_fill_manual(values = c(&quot;dodger blue&quot;, &quot;hot pink&quot;)) + labs(x = &quot;Child&#39;s Age, years&quot;, y = &quot;Proportion Obese&quot;) Smooth out the trends data_summary %&gt;% ggplot(aes(x = age_curr, y = prob_est, group = gender, color = gender)) + geom_smooth(method = &quot;lm&quot;, formula = y ~ poly(x, 2), se = FALSE) + theme_bw() + scale_color_manual(values = c(&quot;dodger blue&quot;, &quot;hot pink&quot;)) + scale_fill_manual(values = c(&quot;dodger blue&quot;, &quot;hot pink&quot;)) + labs(x = &quot;Child&#39;s Age, years&quot;, y = &quot;Proportion Obese&quot;) 20.4 Analysis Goal Does risk of obesity increase with age and are patterns of change similar for both sexes? There are 5 age cohorts that were measured each for 3 years, baseage and currage are age midpoints of those cohort groups. Which to include, current age or occasion? Assume no cohort effects. If you do think this is an issue, include baseline age (age_base) and current age minus baseline age (time) in model. data_long %&gt;% group_by(gender, age_base, occation) %&gt;% summarise(n = n(), count = sum(obesity == &quot;Yes&quot;), prop = mean(obesity == &quot;Yes&quot;), se = sd(obesity == &quot;Yes&quot;)/sqrt(n)) %&gt;% mutate(time = (occation %&gt;% as.numeric) * 2 - 2) %&gt;% ggplot(aes(x = time, y = prop, fill = gender)) + geom_ribbon(aes(ymin = prop - se, ymax = prop + se), alpha = 0.2) + geom_point(aes(color = gender)) + geom_line(aes(color = gender)) + theme_bw() + facet_wrap(~ age_base, labeller = label_both) + labs(title = &quot;Observed Obesity Rates, by Gender within Cohort&quot;, subtitle = &quot;Subset of 350 children with complete data&quot;, x = &quot;Time, years from 1977&quot;, y = &quot;Proportion of Children Characterized as Obese&quot;) + scale_fill_manual(values = c(&quot;dodgerblue3&quot;, &quot;red&quot;)) + scale_color_manual(values = c(&quot;dodgerblue3&quot;, &quot;red&quot;)) + scale_x_continuous(breaks = seq(from = 0, to = 4, by = 2)) + theme(legend.position = c(1, 0), legend.justification = c(1, 0), legend.background = element_rect(color = &quot;black&quot;)) data_long %&gt;% group_by(gender, age_curr) %&gt;% summarise(n = n(), count = sum(obesity == &quot;Yes&quot;), prop = mean(obesity == &quot;Yes&quot;), se = sd(obesity == &quot;Yes&quot;)/sqrt(n)) %&gt;% ggplot(aes(x = age_curr %&gt;% as.character %&gt;% as.numeric, y = prop, group = gender, fill = gender)) + geom_ribbon(aes(ymin = prop - se, ymax = prop + se), alpha = 0.2) + geom_point(aes(color = gender)) + geom_line(aes(color = gender)) + theme_bw() + geom_vline(xintercept = 12, linetype = &quot;dashed&quot;, size = 1, color = &quot;navyblue&quot;) + labs(title = &quot;Observed Obesity Rates, by Gender (collapsing cohorts)&quot;, subtitle = &quot;Subset of 350 children with complete data&quot;, x = &quot;Age of Child, years&quot;, y = &quot;Proportion of Children Characterized as Obese&quot;) + scale_fill_manual(values = c(&quot;dodgerblue3&quot;, &quot;red&quot;)) + scale_color_manual(values = c(&quot;dodgerblue3&quot;, &quot;red&quot;)) + scale_x_continuous(breaks = seq(from = 6, to = 18, by = 2)) + theme(legend.position = c(0, 1), legend.justification = c(-0.05, 1.05), legend.background = element_rect(color = &quot;black&quot;)) 20.4.1 Center time at twelve years old data_long &lt;- data_long %&gt;% dplyr::mutate(age_center = age_curr %&gt;% as.character %&gt;% as.numeric -12) %&gt;% dplyr::mutate(obesity_num = obesity %&gt;% as.numeric - 1) psych::headTail(data_long) id gender age_base age_curr occation obesity age_center obesity_num 1 1 Male 6 6 1 Yes -6 1 2 1 Male 6 8 2 Yes -4 1 3 1 Male 6 10 3 Yes -2 1 4 5 Male 6 6 1 Yes -6 1 ... &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ... ... 1047 3582 Female 14 18 3 No 6 0 1048 3584 Female 14 14 1 No 2 0 1049 3584 Female 14 16 2 No 4 0 1050 3584 Female 14 18 3 No 6 0 20.5 GLM Analysis 20.5.1 Standard logistic regression fit_glm_1 &lt;- glm(obesity_num ~ gender*age_center + gender*I(age_center^2), data = data_long, family = binomial(link = &quot;logit&quot;)) fit_glm_2 &lt;- glm(obesity_num ~ gender + age_center + I(age_center^2), data = data_long, family = binomial(link = &quot;logit&quot;)) texreg::knitreg(list(extract_glm_exp(fit_glm_1), extract_glm_exp(fit_glm_2)), custom.model.names = c(&quot;Interaction&quot;, &quot;Main Effects&quot;), caption = &quot;GLM: Parameter EStimates&quot;, single.row = TRUE, ci.test = 1) GLM: Parameter EStimates   Interaction Main Effects (Intercept) 0.25 [0.18; 0.33]* 0.24 [0.19; 0.31]* genderFemale 1.43 [0.97; 2.12] 1.48 [1.10; 2.00]* age_center 1.05 [0.97; 1.14] 1.04 [0.99; 1.10] age_center^2 0.99 [0.96; 1.01] 0.99 [0.98; 1.01] genderFemale:age_center 0.99 [0.89; 1.09]   genderFemale:age_center^2 1.00 [0.97; 1.04]   * Null hypothesis value outside the confidence interval. plot_pred_glm &lt;- Effect(c(&quot;gender&quot;, &quot;age_center&quot;), fit_glm_2, xlevels = list(age_center = seq(from = -6, to = 6, by = 0.25))) %&gt;% data.frame %&gt;% mutate(age = age_center + 12) %&gt;% dplyr::mutate(gender = forcats::fct_rev(gender)) %&gt;% ggplot(aes(x = age, y = fit, group = gender, linetype = gender, fill = gender)) + geom_ribbon(aes(ymin = fit - se, ymax = fit + se), alpha = .3) + geom_line(aes(color = gender), size = 1.5) + theme_bw() + labs(title = &quot;Generalized Linear Model: Model #2&quot;, x = &quot;Child&#39;s Age, years&quot;, y = &quot;Predicted Probability of Obesity&quot;, linetype = &quot;Gender&quot;, fill = &quot;Gender&quot;, color = &quot;Gender&quot;) + theme(legend.position = c(0, 1), legend.justification = c(-0.05, 1.05), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(2, &quot;cm&quot;)) + scale_x_continuous(breaks = seq(from = 6, to = 18, by = 3)) plot_pred_glm 20.6 GEE Analysis ALWAYS: fix the scale parameter to 1 with binomial data!!! fit_gee_1in &lt;- gee::gee(obesity_num ~ gender*age_center + gender*I(age_center^2), id = id, data = data_long, family = binomial(link = &quot;logit&quot;), corstr = &#39;independence&#39;, scale.fix = TRUE, scale.value = 1) (Intercept) genderFemale -1.398469 0.361111 age_center I(age_center^2) 0.048638 -0.011231 genderFemale:age_center genderFemale:I(age_center^2) -0.014399 0.004094 fit_gee_1ex &lt;- gee::gee(obesity_num ~ gender*age_center + gender*I(age_center^2), id = id, data = data_long, family = binomial(link = &quot;logit&quot;), corstr = &#39;exchangeable&#39;, scale.fix = TRUE, scale.value = 1) (Intercept) genderFemale -1.398469 0.361111 age_center I(age_center^2) 0.048638 -0.011231 genderFemale:age_center genderFemale:I(age_center^2) -0.014399 0.004094 fit_gee_1un &lt;- gee::gee(obesity_num ~ gender*age_center + gender*I(age_center^2), id = id, data = data_long, family = binomial(link = &quot;logit&quot;), corstr = &#39;unstructured&#39;, scale.fix = TRUE, scale.value = 1) (Intercept) genderFemale -1.398469 0.361111 age_center I(age_center^2) 0.048638 -0.011231 genderFemale:age_center genderFemale:I(age_center^2) -0.014399 0.004094 texreg::knitreg(list(extract_gee_exp(fit_gee_1in), extract_gee_exp(fit_gee_1ex), extract_gee_exp(fit_gee_1un)), custom.model.names = c(&quot;Independent&quot;, &quot;Exchangable&quot;, &quot;Unstructured&quot;), caption = &quot;Gee Model Parameters: With Interactions&quot;, single.row = TRUE, ci.test = 1) Gee Model Parameters: With Interactions   Independent Exchangable Unstructured (Intercept) 0.25 [0.17; 0.35]* 0.24 [0.17; 0.35]* 0.25 [0.17; 0.35]* genderFemale 1.43 [0.87; 2.35] 1.40 [0.87; 2.24] 1.39 [0.86; 2.23] age_center 1.05 [0.94; 1.17] 1.07 [0.97; 1.17] 1.06 [0.97; 1.16] age_center^2 0.99 [0.96; 1.01] 0.99 [0.97; 1.01] 0.99 [0.97; 1.01] genderFemale:age_center 0.99 [0.86; 1.13] 1.02 [0.91; 1.14] 1.02 [0.91; 1.14] genderFemale:age_center^2 1.00 [0.97; 1.04] 1.01 [0.98; 1.03] 1.01 [0.98; 1.03] Dispersion 1.00 1.00 1.00 * Null hypothesis value outside the confidence interval. 20.6.1 Drop the interaction with gender. fit_gee_2in &lt;- gee::gee(obesity_num ~ gender + age_center + I(age_center^2), id = id, data = data_long, family = binomial(link = &quot;logit&quot;), corstr = &#39;independence&#39;, scale.fix = TRUE, scale.value = 1) (Intercept) genderFemale age_center I(age_center^2) -1.416998 0.392184 0.039733 -0.008682 fit_gee_2ex &lt;- gee::gee(obesity_num ~ gender + age_center + I(age_center^2), id = id, data = data_long, family = binomial(link = &quot;logit&quot;), corstr = &#39;exchangeable&#39;, scale.fix = TRUE, scale.value = 1) (Intercept) genderFemale age_center I(age_center^2) -1.416998 0.392184 0.039733 -0.008682 fit_gee_2un &lt;- gee::gee(obesity_num ~ gender + age_center + I(age_center^2), id = id, data = data_long, family = binomial(link = &quot;logit&quot;), corstr = &#39;unstructured&#39;, scale.fix = TRUE, scale.value = 1) (Intercept) genderFemale age_center I(age_center^2) -1.416998 0.392184 0.039733 -0.008682 texreg::knitreg(list(extract_gee_exp(fit_gee_2in), extract_gee_exp(fit_gee_2ex), extract_gee_exp(fit_gee_2un)), custom.model.names = c(&quot;Independent&quot;, &quot;Exchangable&quot;, &quot;Unstructured&quot;), caption = &quot;Gee Model Parameters: Main Effects Only&quot;, single.row = TRUE, ci.test = 1) Gee Model Parameters: Main Effects Only   Independent Exchangable Unstructured (Intercept) 0.24 [0.17; 0.34]* 0.23 [0.17; 0.33]* 0.24 [0.17; 0.33]* genderFemale 1.48 [0.97; 2.27] 1.49 [0.97; 2.29] 1.48 [0.96; 2.27] age_center 1.04 [0.98; 1.11] 1.07 [1.02; 1.13]* 1.07 [1.02; 1.13]* age_center^2 0.99 [0.98; 1.01] 0.99 [0.98; 1.01] 0.99 [0.98; 1.01] Dispersion 1.00 1.00 1.00 * Null hypothesis value outside the confidence interval. 20.6.2 Select the “final” model. fit_geeglm_2un &lt;- geepack::geeglm(obesity_num ~ gender + age_center + I(age_center^2), id = id, data = data_long, family = binomial(link = &quot;logit&quot;), corstr = &#39;unstructured&#39;) interactions::interact_plot(model = fit_geeglm_2un, pred = age_center, modx = gender) plot_pred_gee &lt;- fit_geeglm_2un %&gt;% emmeans::emmeans(~ gender*age_center, at = list(age_center = seq(from = -6, to = 6, by = .1)), type = &quot;response&quot;, level = .685) %&gt;% data.frame() %&gt;% mutate(gender = fct_rev(gender)) %&gt;% mutate(age = age_center + 12) %&gt;% ggplot(aes(x = age, y = prob, group = gender)) + geom_ribbon(aes(ymin = asymp.LCL, ymax = asymp.UCL, fill = gender), alpha = .2) + geom_line(aes(linetype = gender, color = gender), size = 1.5) + theme_bw() + labs(title = &quot;Generalized Estimating Equation: Model #2, unstructured&quot;, x = &quot;Child&#39;s Age, years&quot;, y = &quot;Predicted Proportion with Obesity&quot;, linetype = &quot;Gender&quot;, fill = &quot;Gender&quot;, color = &quot;Gender&quot;) + theme(legend.position = c(0, 1), legend.justification = c(-0.05, 1.05), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(2, &quot;cm&quot;)) + scale_x_continuous(breaks = seq(from = 6, to = 18, by = 3)) plot_pred_gee 20.7 GLMM Analysis IT IS GENERALLY NOT RECOMMENDED THAT RANDOM-SLOPES BE ESTIMATED FOR BINOMIAL GLMMS fit_glmm_1 &lt;- lme4::glmer(obesity_num ~ age_center*gender + I(age_center^2)*gender + (1|id), data = data_long, family = binomial(link = &quot;logit&quot;)) fit_glmm_2 &lt;- lme4::glmer(obesity_num ~ gender + age_center + I(age_center^2) + (1|id), data = data_long, family = binomial(link = &quot;logit&quot;)) Indicates smaller model is better, no interaction at higher level necessary anova(fit_glmm_1, fit_glmm_2) Data: data_long Models: fit_glmm_2: obesity_num ~ gender + age_center + I(age_center^2) + (1 | id) fit_glmm_1: obesity_num ~ age_center * gender + I(age_center^2) * gender + (1 | id) npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) fit_glmm_2 5 883 908 -436 873 fit_glmm_1 7 885 920 -436 871 1.54 2 0.46 texreg::knitreg(list(extract_glmer_exp(fit_glmm_1), extract_glmer_exp(fit_glmm_2)), custom.model.names = c(&quot;Interaction&quot;, &quot;Main Effects&quot;), caption = &quot;GLMM: Parameter EStimates&quot;, single.row = TRUE, ci.test = 1) GLMM: Parameter EStimates   Interaction Main Effects (Intercept) 0.00 [0.00; 0.01]* 0.00 [0.00; 0.01]* age_center 1.21 [0.97; 1.50] 1.25 [1.09; 1.44]* genderFemale 1.54 [0.41; 5.74] 2.05 [0.61; 6.91] age_center^2 0.97 [0.92; 1.02] 0.98 [0.95; 1.01] age_center:genderFemale 1.08 [0.81; 1.43]   genderFemale:age_center^2 1.03 [0.97; 1.10]   * Null hypothesis value outside the confidence interval. interactions::interact_plot(model = fit_glmm_2, pred = age_center, modx = gender, int.width = .685, interval = TRUE) Effect(c(&quot;gender&quot;, &quot;age_center&quot;),fit_glmm_2) %&gt;% data.frame %&gt;% mutate(fit_exp = exp(fit)) gender age_center fit se lower upper fit_exp 1 Male -6 0.0002128 0.0002429 2.271e-05 0.001991 1.000 2 Female -6 0.0004365 0.0005150 4.321e-05 0.004394 1.000 3 Male -3 0.0006428 0.0005657 1.145e-04 0.003601 1.001 4 Female -3 0.0013179 0.0012250 2.129e-04 0.008112 1.001 5 Male 0 0.0014512 0.0011546 3.048e-04 0.006880 1.001 6 Female 0 0.0029726 0.0025307 5.590e-04 0.015646 1.003 7 Male 3 0.0024489 0.0018764 5.445e-04 0.010941 1.002 8 Female 3 0.0050111 0.0041414 9.878e-04 0.025010 1.005 9 Male 6 0.0030909 0.0027490 5.393e-04 0.017504 1.003 10 Female 6 0.0063206 0.0059705 9.861e-04 0.039377 1.006 plot_pred_glmm &lt;- fit_glmm_2 %&gt;% emmeans::emmeans(~ gender*age_center, at = list(age_center = seq(from = -6, to = 6, by = .1)), type = &quot;response&quot;, level = .685) %&gt;% data.frame() %&gt;% mutate(gender = fct_rev(gender)) %&gt;% mutate(age = age_center + 12) %&gt;% ggplot(aes(x = age, y = prob, group = gender)) + geom_ribbon(aes(ymin = asymp.LCL, ymax = asymp.UCL, fill = gender), alpha = .2) + geom_line(aes(linetype = gender, color = gender), size = 1.5) + theme_bw() + labs(title = &quot;Generalized Linear Mixed Effects: Model #2, Random Interepts&quot;, x = &quot;Child&#39;s Age, years&quot;, y = &quot;Predicted Probability of Obesity&quot;, linetype = &quot;Gender&quot;, fill = &quot;Gender&quot;, color = &quot;Gender&quot;) + theme(legend.position = c(0, 1), legend.justification = c(-0.05, 1.05), legend.background = element_rect(color = &quot;black&quot;), legend.key.width = unit(2, &quot;cm&quot;)) + scale_x_continuous(breaks = seq(from = 6, to = 18, by = 3)) plot_pred_glmm 20.8 Compare Methods texreg::knitreg(list(extract_glm_exp(fit_glm_2), extract_gee_exp(fit_gee_2un), extract_glmer_exp(fit_glmm_2)), custom.model.names = c(&quot;GLM&quot;, &quot;GEE&quot;, &quot;GLMM&quot;), caption = &quot;Compare Methods: Parameter EStimates&quot;, single.row = TRUE, ci.test = 1) Compare Methods: Parameter EStimates   GLM GEE GLMM (Intercept) 0.24 [0.19; 0.31]* 0.24 [0.17; 0.33]* 0.00 [0.00; 0.01]* genderFemale 1.48 [1.10; 2.00]* 1.48 [0.96; 2.27] 2.05 [0.61; 6.91] age_center 1.04 [0.99; 1.10] 1.07 [1.02; 1.13]* 1.25 [1.09; 1.44]* age_center^2 0.99 [0.98; 1.01] 0.99 [0.98; 1.01] 0.98 [0.95; 1.01] Dispersion   1.00   * Null hypothesis value outside the confidence interval. The goal of patchwork is to make it ridiculously simple to combine separate ggplots into the same graphic. As such it tries to solve the same problem as gridExtra::grid.arrange() and cowplot::plot_grid() but using an API that incites exploration and iteration, and scales to arbitrarily complex layouts. Website: https://patchwork.data-imaginist.com/index.html plot_pred_glm / plot_pred_gee / plot_pred_glmm (plot_pred_glm + theme(legend.position = &quot;none&quot;)) / (plot_pred_gee + theme(legend.position = &quot;none&quot;)) / plot_pred_glmm + plot_annotation(tag_levels = &quot;A&quot;) + plot_layout(guides = &quot;auto&quot;) data_long %&gt;% dplyr::mutate(pred = predict(fit_glmm_2, re.form = ~ (1|id))) %&gt;% dplyr::mutate(odds = exp(pred)) %&gt;% dplyr::mutate(prob = odds/(1 + odds)) %&gt;% ggplot(aes(x = age_curr, y = prob, group = id)) + geom_line(aes(color = gender)) + theme_bw() "],["glmm-count-outcome-bolous.html", "21 GLMM, Count Outcome: Bolous 21.1 Packages 21.2 Data Prep 21.3 Exploratory Data Analysis 21.4 GEE Analysis 21.5 GLMM Analysis", " 21 GLMM, Count Outcome: Bolous GLMM FAQ by: Ben Bolker and others https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html 21.1 Packages 21.1.1 CRAN library(tidyverse) # all things tidy library(pander) # nice looking genderal tabulations library(furniture) # nice table1() descriptives library(texreg) # Convert Regression Output to LaTeX or HTML Tables library(psych) # contains some useful functions, like headTail library(lme4) # Linear, generalized linear, &amp; nonlinear mixed models library(gee) # Generalized Estimating Equations library(optimx) # Optimizers for use in lme4::glmer library(MuMIn) # Multi-Model Inference (caluclate QIC) library(effects) # Plotting estimated marginal means library(interactions) library(performance) library(patchwork) # combining graphics 21.1.2 GitHub Helper extract functions for exponentiating parameters form generalized regression models within a texreg table of model parameters. # install.packages(&quot;devtools&quot;) # library(devtools) # install_github(&quot;SarBearSchwartz/texreghelpr&quot;) library(texreghelpr) 21.2 Data Prep Bolus data from Weiss 2005 Patient controlled analgesia (PCA) comparing 2 dosing regimes to self-control pain The dataset has the number of requests per interval in 12 successive four-hourly intervals following abdominal surgery for 65 patients in a clinical trial to compare two groups (bolus/lock-out combinations). Bolus= Large dose of medication given (usually intravenously by direct infusion injection or gravity drip) to raise blood-level concentrations to a therapeutic level A ‘lock-out’ period followed each dose, where subject may not administer more medication. Lock-out time is twice as long in 2mg/dose group Allows for max of 30 dosages in 2mg/dose and 60 in 1mg/dose group in any 4-hour period No responses neared these upper limits Variable List: Indicators id unique subject indicator time 11 consecutive 4-hour periods: 0, 1, 2, …, 10 Outcome or dependent variable count # doses recorded for in that 4-hour period Main predictor or independent variable of interest group1mg/dose group, 2mg/dose group 21.2.1 Import data_raw &lt;- read.table(&quot;https://raw.githubusercontent.com/CEHS-research/data/master/MLM/bolus.txt&quot;, header = TRUE) str(data_raw) # see the structure &#39;data.frame&#39;: 715 obs. of 4 variables: $ id : int 1 1 1 1 1 1 1 1 1 1 ... $ group: chr &quot;1mg&quot; &quot;1mg&quot; &quot;1mg&quot; &quot;1mg&quot; ... $ time : int 0 1 2 3 4 5 6 7 8 9 ... $ count: int 2 2 5 2 4 0 2 4 4 4 ... psych::headTail(data_raw) id group time count 1 1 1mg 0 2 2 1 1mg 1 2 3 1 1mg 2 5 4 1 1mg 3 2 ... ... &lt;NA&gt; ... ... 712 65 2mg 7 6 713 65 2mg 8 1 714 65 2mg 9 2 715 65 2mg 10 0 21.2.2 Long Format data_long &lt;- data_raw %&gt;% dplyr::mutate(id = factor(id)) %&gt;% dplyr::mutate(timeF = factor(time)) %&gt;% dplyr::mutate(group = factor(group)) str(data_long) # see the structure &#39;data.frame&#39;: 715 obs. of 5 variables: $ id : Factor w/ 65 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... $ group: Factor w/ 2 levels &quot;1mg&quot;,&quot;2mg&quot;: 1 1 1 1 1 1 1 1 1 1 ... $ time : int 0 1 2 3 4 5 6 7 8 9 ... $ count: int 2 2 5 2 4 0 2 4 4 4 ... $ timeF: Factor w/ 11 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... psych::headTail(data_long) id group time count timeF 1 1 1mg 0 2 0 2 1 1mg 1 2 1 3 1 1mg 2 5 2 4 1 1mg 3 2 3 ... &lt;NA&gt; &lt;NA&gt; ... ... &lt;NA&gt; 712 65 2mg 7 6 7 713 65 2mg 8 1 8 714 65 2mg 9 2 9 715 65 2mg 10 0 10 21.2.3 Wide Format data_wide &lt;- data_long %&gt;% dplyr::select(-timeF) %&gt;% tidyr::pivot_wider(names_from = time, names_prefix = &quot;time_&quot;, values_from = count) str(data_wide) # see the structure tibble [65 × 13] (S3: tbl_df/tbl/data.frame) $ id : Factor w/ 65 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... $ group : Factor w/ 2 levels &quot;1mg&quot;,&quot;2mg&quot;: 1 1 1 1 1 1 1 1 1 1 ... $ time_0 : int [1:65] 2 3 4 1 7 6 4 2 10 0 ... $ time_1 : int [1:65] 2 5 8 4 7 5 7 10 9 7 ... $ time_2 : int [1:65] 5 4 4 3 6 4 4 8 8 1 ... $ time_3 : int [1:65] 2 4 3 3 8 8 8 17 9 9 ... $ time_4 : int [1:65] 4 5 12 3 6 7 4 4 4 8 ... $ time_5 : int [1:65] 0 0 1 1 5 4 4 2 2 0 ... $ time_6 : int [1:65] 2 6 0 7 4 6 3 6 6 1 ... $ time_7 : int [1:65] 4 3 6 5 7 5 1 8 1 5 ... $ time_8 : int [1:65] 4 2 5 0 2 2 3 9 8 4 ... $ time_9 : int [1:65] 4 7 3 1 5 4 4 1 5 1 ... $ time_10: int [1:65] 2 4 5 1 0 4 6 4 5 0 ... psych::headTail(data_wide) id group time_0 time_1 time_2 time_3 time_4 time_5 time_6 time_7 time_8 1 1 1mg 2 2 5 2 4 0 2 4 4 2 2 1mg 3 5 4 4 5 0 6 3 2 3 3 1mg 4 8 4 3 12 1 0 6 5 4 4 1mg 1 4 3 3 3 1 7 5 0 5 &lt;NA&gt; &lt;NA&gt; ... ... ... ... ... ... ... ... ... 6 62 2mg 0 13 7 9 19 4 5 11 10 7 63 2mg 11 7 6 9 3 4 6 6 6 8 64 2mg 8 4 22 11 16 4 5 9 12 9 65 2mg 2 2 2 4 4 3 7 6 1 time_9 time_10 1 4 2 2 7 4 3 3 5 4 1 1 5 ... ... 6 6 9 7 6 0 8 12 7 9 2 0 21.3 Exploratory Data Analysis 21.3.1 Summary Statistics 21.3.1.1 Demographics and Baseline data_wide %&gt;% dplyr::group_by(group) %&gt;% furniture::table1(&quot;Count at Baseline&quot; = time_0, total = TRUE, test = TRUE, na.rm = FALSE, digits = 2, output = &quot;markdown&quot;) Total 1mg 2mg P-Value n = 65 n = 30 n = 35 Count at Baseline 0.448 6.03 (5.17) 5.50 (4.70) 6.49 (5.58) 21.3.1.2 Status over Time data_wide %&gt;% dplyr::group_by(group) %&gt;% furniture::table1(time_0, time_1, time_2, time_3, time_4, time_5, time_6, time_7, time_8, time_9, time_10, total = TRUE, test = TRUE, na.rm = FALSE, digits = 2, output = &quot;markdown&quot;) Total 1mg 2mg P-Value n = 65 n = 30 n = 35 time_0 0.448 6.03 (5.17) 5.50 (4.70) 6.49 (5.58) time_1 0.127 6.74 (6.37) 5.43 (4.00) 7.86 (7.75) time_2 0.005 7.37 (6.25) 5.13 (4.13) 9.29 (7.12) time_3 0.165 8.68 (5.94) 7.57 (5.01) 9.63 (6.56) time_4 0.084 6.43 (4.74) 5.33 (3.36) 7.37 (5.54) time_5 0.024 5.37 (4.80) 3.93 (4.19) 6.60 (5.00) time_6 0.052 4.82 (4.16) 3.73 (3.04) 5.74 (4.78) time_7 0.73 4.77 (3.62) 4.60 (3.16) 4.91 (4.01) time_8 0.236 5.69 (4.75) 4.93 (3.63) 6.34 (5.51) time_9 0.008 4.98 (4.43) 3.50 (2.06) 6.26 (5.45) time_10 0.034 4.77 (4.80) 3.47 (2.83) 5.89 (5.81) 21.3.1.3 Correlation over Time data_wide %&gt;% dplyr::select(starts_with(&quot;time_&quot;)) %&gt;% cor() %&gt;% corrplot::corrplot.mixed() 21.3.2 Visualize 21.3.2.1 Person Profile Plots, by Group data_long %&gt;% ggplot(aes(x = time, y = count)) + geom_line(aes(group = id), color = &quot;black&quot;, alpha = .4) + facet_grid(. ~ group) + theme_bw() data_long %&gt;% ggplot(aes(x = time, y = count)) + geom_smooth(aes(group = id), method = &quot;lm&quot;, color = &quot;gray&quot;, se = FALSE) + facet_grid(. ~ group) + theme_bw() data_long %&gt;% ggplot(aes(x = time, y = count)) + geom_smooth(aes(group = id), method = &quot;lm&quot;, formula = y ~ poly(x, 2), color = &quot;gray&quot;, se = FALSE) + facet_grid(. ~ group) + theme_bw() data_long %&gt;% ggplot(aes(x = time, y = count)) + geom_line(aes(group = id)) + geom_smooth(method = &quot;loess&quot;, color = &quot;blue&quot;, se = FALSE) + geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;, se = FALSE) + facet_grid(. ~ group) + theme_bw() data_long %&gt;% ggplot(aes(x = time, y = log(count + .1))) + geom_line(aes(group = id)) + geom_smooth(method = &quot;loess&quot;, color = &quot;blue&quot;, se = FALSE) + geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;, se = FALSE) + facet_grid(. ~ group) + theme_bw() data_long %&gt;% ggplot(aes(x = time, y = count)) + geom_smooth(aes(color = &quot;1. Loess&quot;), method = &quot;loess&quot;, se = FALSE) + geom_smooth(aes(color = &quot;2. Linear&quot;), method = &quot;lm&quot;, se = FALSE) + geom_smooth(aes(color = &quot;3. Quadratic&quot;), method = &quot;lm&quot;, formula = y ~ poly(x, 2), se = FALSE) + geom_smooth(aes(color = &quot;4. Cubic&quot;), method = &quot;lm&quot;, formula = y ~ poly(x, 3), se = FALSE) + facet_grid(. ~ group) + theme_bw() + labs(color = &quot;Type of Smoother&quot;) + theme(legend.position = &quot;bottom&quot;, legend.key.width = unit(1.5, &quot;cm&quot;)) 21.3.2.2 Means over Time, BY Group data_long %&gt;% dplyr::group_by(group, timeF) %&gt;% dplyr::summarise(M = mean(count)) %&gt;% ggplot(aes(x = timeF, y = M, group = group, color = group)) + geom_point() + geom_line() + theme_bw() + labs(x = &quot;Four-week Intervals&quot;, y = &quot;Sample Mean Count&quot;, color = &quot;Dose&quot;) data_long %&gt;% dplyr::group_by(group, timeF) %&gt;% dplyr::summarise(M = mean(log(count + .1))) %&gt;% ggplot(aes(x = timeF, y = M, group = group, color = group)) + geom_point() + geom_line() + theme_bw() + labs(x = &quot;Four-week Intervals&quot;, y = &quot;Sample Mean Count&quot;, color = &quot;Dose&quot;) 21.4 GEE Analysis 21.4.1 Fit Various Correlation Structures 21.4.1.1 Exchangable mod_gee_ex &lt;- gee::gee(count ~ group + time + I(time^2), id = id, data = data_long, family = poisson(link = &quot;log&quot;), corstr = &#39;exchangeable&#39;) (Intercept) group2mg time I(time^2) 1.721889 0.362804 -0.002225 -0.004167 summary(mod_gee_ex) GEE: GENERALIZED LINEAR MODELS FOR DEPENDENT DATA gee S-function, version 4.13 modified 98/01/27 (1998) Model: Link: Logarithm Variance to Mean Relation: Poisson Correlation Structure: Exchangeable Call: gee::gee(formula = count ~ group + time + I(time^2), id = id, data = data_long, family = poisson(link = &quot;log&quot;), corstr = &quot;exchangeable&quot;) Summary of Residuals: Min 1Q Median 3Q Max -7.9757 -3.5612 -0.9249 2.1911 27.0751 Coefficients: Estimate Naive S.E. Naive z Robust S.E. Robust z (Intercept) 1.733829 0.120291 14.41360 0.117644 14.73794 group2mg 0.342576 0.140827 2.43261 0.140090 2.44539 time -0.002224 0.026992 -0.08239 0.030992 -0.07175 I(time^2) -0.004168 0.002689 -1.55019 0.002734 -1.52491 Estimated Scale Parameter: 3.989 Number of Iterations: 2 Working Correlation [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [1,] 1.0000 0.4179 0.4179 0.4179 0.4179 0.4179 0.4179 0.4179 0.4179 0.4179 [2,] 0.4179 1.0000 0.4179 0.4179 0.4179 0.4179 0.4179 0.4179 0.4179 0.4179 [3,] 0.4179 0.4179 1.0000 0.4179 0.4179 0.4179 0.4179 0.4179 0.4179 0.4179 [4,] 0.4179 0.4179 0.4179 1.0000 0.4179 0.4179 0.4179 0.4179 0.4179 0.4179 [5,] 0.4179 0.4179 0.4179 0.4179 1.0000 0.4179 0.4179 0.4179 0.4179 0.4179 [6,] 0.4179 0.4179 0.4179 0.4179 0.4179 1.0000 0.4179 0.4179 0.4179 0.4179 [7,] 0.4179 0.4179 0.4179 0.4179 0.4179 0.4179 1.0000 0.4179 0.4179 0.4179 [8,] 0.4179 0.4179 0.4179 0.4179 0.4179 0.4179 0.4179 1.0000 0.4179 0.4179 [9,] 0.4179 0.4179 0.4179 0.4179 0.4179 0.4179 0.4179 0.4179 1.0000 0.4179 [10,] 0.4179 0.4179 0.4179 0.4179 0.4179 0.4179 0.4179 0.4179 0.4179 1.0000 [11,] 0.4179 0.4179 0.4179 0.4179 0.4179 0.4179 0.4179 0.4179 0.4179 0.4179 [,11] [1,] 0.4179 [2,] 0.4179 [3,] 0.4179 [4,] 0.4179 [5,] 0.4179 [6,] 0.4179 [7,] 0.4179 [8,] 0.4179 [9,] 0.4179 [10,] 0.4179 [11,] 1.0000 21.4.1.2 Auto-regressive (correlation decay over time) mod_gee_ar &lt;- gee::gee(count ~ group + time + I(time^2), id = id, data = data_long, family = poisson(link = &quot;log&quot;), corstr = &quot;AR-M&quot;, Mv = 1) (Intercept) group2mg time I(time^2) 1.721889 0.362804 -0.002225 -0.004167 summary(mod_gee_ar) GEE: GENERALIZED LINEAR MODELS FOR DEPENDENT DATA gee S-function, version 4.13 modified 98/01/27 (1998) Model: Link: Logarithm Variance to Mean Relation: Poisson Correlation Structure: AR-M , M = 1 Call: gee::gee(formula = count ~ group + time + I(time^2), id = id, data = data_long, family = poisson(link = &quot;log&quot;), corstr = &quot;AR-M&quot;, Mv = 1) Summary of Residuals: Min 1Q Median 3Q Max -7.6777 -3.5564 -0.9632 2.3223 27.3599 Coefficients: Estimate Naive S.E. Naive z Robust S.E. Robust z (Intercept) 1.661182 0.114381 14.5232 0.122138 13.6009 group2mg 0.355589 0.105265 3.3780 0.136941 2.5967 time 0.022501 0.042041 0.5352 0.032654 0.6891 I(time^2) -0.005865 0.004118 -1.4242 0.002939 -1.9959 Estimated Scale Parameter: 4.009 Number of Iterations: 2 Working Correlation [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [1,] 1.000000 0.533655 0.284788 0.15198 0.08110 0.04328 0.02310 0.01233 [2,] 0.533655 1.000000 0.533655 0.28479 0.15198 0.08110 0.04328 0.02310 [3,] 0.284788 0.533655 1.000000 0.53366 0.28479 0.15198 0.08110 0.04328 [4,] 0.151978 0.284788 0.533655 1.00000 0.53366 0.28479 0.15198 0.08110 [5,] 0.081104 0.151978 0.284788 0.53366 1.00000 0.53366 0.28479 0.15198 [6,] 0.043282 0.081104 0.151978 0.28479 0.53366 1.00000 0.53366 0.28479 [7,] 0.023097 0.043282 0.081104 0.15198 0.28479 0.53366 1.00000 0.53366 [8,] 0.012326 0.023097 0.043282 0.08110 0.15198 0.28479 0.53366 1.00000 [9,] 0.006578 0.012326 0.023097 0.04328 0.08110 0.15198 0.28479 0.53366 [10,] 0.003510 0.006578 0.012326 0.02310 0.04328 0.08110 0.15198 0.28479 [11,] 0.001873 0.003510 0.006578 0.01233 0.02310 0.04328 0.08110 0.15198 [,9] [,10] [,11] [1,] 0.006578 0.003510 0.001873 [2,] 0.012326 0.006578 0.003510 [3,] 0.023097 0.012326 0.006578 [4,] 0.043282 0.023097 0.012326 [5,] 0.081104 0.043282 0.023097 [6,] 0.151978 0.081104 0.043282 [7,] 0.284788 0.151978 0.081104 [8,] 0.533655 0.284788 0.151978 [9,] 1.000000 0.533655 0.284788 [10,] 0.533655 1.000000 0.533655 [11,] 0.284788 0.533655 1.000000 21.4.1.3 Unstructured mod_gee_un &lt;- gee::gee(count ~ group + time + I(time^2), id = id, data = data_long, family = poisson(link = &quot;log&quot;), corstr = &quot;unstructured&quot;) (Intercept) group2mg time I(time^2) 1.721889 0.362804 -0.002225 -0.004167 summary(mod_gee_un) GEE: GENERALIZED LINEAR MODELS FOR DEPENDENT DATA gee S-function, version 4.13 modified 98/01/27 (1998) Model: Link: Logarithm Variance to Mean Relation: Poisson Correlation Structure: Unstructured Call: gee::gee(formula = count ~ group + time + I(time^2), id = id, data = data_long, family = poisson(link = &quot;log&quot;), corstr = &quot;unstructured&quot;) Summary of Residuals: Min 1Q Median 3Q Max -6.6661 -2.6978 -0.2413 2.9980 28.4379 Coefficients: Estimate Naive S.E. Naive z Robust S.E. Robust z (Intercept) 1.562649 0.136816 11.422 0.135840 11.504 group2mg 0.284092 0.151482 1.875 0.141552 2.007 time 0.043984 0.031621 1.391 0.040017 1.099 I(time^2) -0.009419 0.002544 -3.703 0.003708 -2.540 Estimated Scale Parameter: 4.884 Number of Iterations: 11 Working Correlation [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [1,] 1.00000 0.5364 0.5054 0.4452 0.1958 0.2503 0.2412 0.01557 0.2516 0.2408 [2,] 0.53641 1.0000 0.8486 0.6189 0.4691 0.3295 0.3091 0.22892 0.6117 0.5272 [3,] 0.50543 0.8486 1.0000 0.6950 0.5126 0.4635 0.3188 0.29785 0.7877 0.5554 [4,] 0.44520 0.6189 0.6950 1.0000 0.6056 0.4314 0.4065 0.31539 0.6111 0.6445 [5,] 0.19583 0.4691 0.5126 0.6056 1.0000 0.3786 0.2775 0.35148 0.5374 0.4410 [6,] 0.25027 0.3295 0.4635 0.4314 0.3786 1.0000 0.4126 0.37182 0.4792 0.4416 [7,] 0.24115 0.3091 0.3188 0.4065 0.2775 0.4126 1.0000 0.28205 0.2613 0.2911 [8,] 0.01557 0.2289 0.2979 0.3154 0.3515 0.3718 0.2820 1.00000 0.4936 0.2334 [9,] 0.25156 0.6117 0.7877 0.6111 0.5374 0.4792 0.2613 0.49357 1.0000 0.4560 [10,] 0.24084 0.5272 0.5554 0.6445 0.4410 0.4416 0.2911 0.23340 0.4560 1.0000 [11,] 0.21857 0.4445 0.5932 0.7372 0.4356 0.6329 0.4043 0.32292 0.4933 0.7378 [,11] [1,] 0.2186 [2,] 0.4445 [3,] 0.5932 [4,] 0.7372 [5,] 0.4356 [6,] 0.6329 [7,] 0.4043 [8,] 0.3229 [9,] 0.4933 [10,] 0.7378 [11,] 1.0000 21.4.2 Compare Corelation Structures 21.4.2.1 QIC: Model Fit MuMIn::model.sel(mod_gee_ex, mod_gee_ar, mod_gee_un, rank = &quot;QIC&quot;) #sorts the best to the TOP, uses QIC(I) to choose corelation structure Model selection table (Intrc) group time time^2 corstr Mv qLik QIC delta weight mod_gee_ex 1.734 + -0.002224 -0.004168 ex 3464 -6904 0.00 0.432 mod_gee_ar 1.661 + 0.022500 -0.005865 AR-M 1 3462 -6904 0.18 0.395 mod_gee_un 1.563 + 0.043980 -0.009419 un 3407 -6902 1.83 0.173 Abbreviations: corstr: ex = &#39;exchangeable&#39;, un = &#39;unstructured&#39; Models ranked by QIC(x) performance::compare_performance(mod_gee_ex, mod_gee_ar, mod_gee_un, rank = TRUE) # Comparison of Model Performance Indices Name | Model | RMSE | Sigma | Score_log | Score_spherical | Performance-Score ------------------------------------------------------------------------------------ mod_gee_ex | gee | 4.996 | 5.010 | -3.535 | 0.030 | 76.57% mod_gee_ar | gee | 4.998 | 5.012 | -3.537 | 0.030 | 72.74% mod_gee_un | gee | 5.081 | 5.096 | -3.614 | 0.030 | 25.00% 21.4.2.2 Model Parameter Table texreg::knitreg(list(extract_gee_exp(mod_gee_ex), extract_gee_exp(mod_gee_ar), extract_gee_exp(mod_gee_un)), custom.model.names = c(&quot;Exchangable&quot;, &quot;AutoRegressive&quot;, &quot;Unstructured&quot;), caption = &quot;GEE: Main Effects Only, with Quadratic Time&quot;, single.row = TRUE, ci.test = 1) GEE: Main Effects Only, with Quadratic Time   Exchangable AutoRegressive Unstructured (Intercept) 5.66 [4.50; 7.13]* 5.27 [4.14; 6.69]* 4.77 [3.66; 6.23]* group2mg 1.41 [1.07; 1.85]* 1.43 [1.09; 1.87]* 1.33 [1.01; 1.75]* time 1.00 [0.94; 1.06] 1.02 [0.96; 1.09] 1.04 [0.97; 1.13] time^2 1.00 [0.99; 1.00] 0.99 [0.99; 1.00]* 0.99 [0.98; 1.00]* Dispersion 3.99 4.01 4.88 * Null hypothesis value outside the confidence interval. 21.4.3 Add Interaction Terms mod_gee_ar2 &lt;- gee::gee(count ~ group*time + group*I(time^2), id = id, data = data_long, family = poisson(link = &quot;log&quot;), corstr = &quot;AR-M&quot;, Mv = 1) (Intercept) group2mg time I(time^2) 1.7400506 0.3334615 0.0004752 -0.0051756 group2mg:time group2mg:I(time^2) -0.0040973 0.0015817 summary(mod_gee_ar2) GEE: GENERALIZED LINEAR MODELS FOR DEPENDENT DATA gee S-function, version 4.13 modified 98/01/27 (1998) Model: Link: Logarithm Variance to Mean Relation: Poisson Correlation Structure: AR-M , M = 1 Call: gee::gee(formula = count ~ group * time + group * I(time^2), id = id, data = data_long, family = poisson(link = &quot;log&quot;), corstr = &quot;AR-M&quot;, Mv = 1) Summary of Residuals: Min 1Q Median 3Q Max -7.5267 -3.4662 -0.7786 2.2661 27.5763 Coefficients: Estimate Naive S.E. Naive z Robust S.E. Robust z (Intercept) 1.7220483 0.148755 11.5764 0.14094 12.21793 group2mg 0.2568280 0.191465 1.3414 0.19142 1.34172 time 0.0077450 0.068090 0.1137 0.05263 0.14717 I(time^2) -0.0057086 0.006751 -0.8456 0.00489 -1.16742 group2mg:time 0.0240638 0.086606 0.2779 0.06720 0.35808 group2mg:I(time^2) -0.0002992 0.008524 -0.0351 0.00612 -0.04888 Estimated Scale Parameter: 4.014 Number of Iterations: 3 Working Correlation [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [1,] 1.000000 0.534560 0.285754 0.15275 0.08166 0.04365 0.02333 0.01247 [2,] 0.534560 1.000000 0.534560 0.28575 0.15275 0.08166 0.04365 0.02333 [3,] 0.285754 0.534560 1.000000 0.53456 0.28575 0.15275 0.08166 0.04365 [4,] 0.152753 0.285754 0.534560 1.00000 0.53456 0.28575 0.15275 0.08166 [5,] 0.081655 0.152753 0.285754 0.53456 1.00000 0.53456 0.28575 0.15275 [6,] 0.043650 0.081655 0.152753 0.28575 0.53456 1.00000 0.53456 0.28575 [7,] 0.023333 0.043650 0.081655 0.15275 0.28575 0.53456 1.00000 0.53456 [8,] 0.012473 0.023333 0.043650 0.08166 0.15275 0.28575 0.53456 1.00000 [9,] 0.006668 0.012473 0.023333 0.04365 0.08166 0.15275 0.28575 0.53456 [10,] 0.003564 0.006668 0.012473 0.02333 0.04365 0.08166 0.15275 0.28575 [11,] 0.001905 0.003564 0.006668 0.01247 0.02333 0.04365 0.08166 0.15275 [,9] [,10] [,11] [1,] 0.006668 0.003564 0.001905 [2,] 0.012473 0.006668 0.003564 [3,] 0.023333 0.012473 0.006668 [4,] 0.043650 0.023333 0.012473 [5,] 0.081655 0.043650 0.023333 [6,] 0.152753 0.081655 0.043650 [7,] 0.285754 0.152753 0.081655 [8,] 0.534560 0.285754 0.152753 [9,] 1.000000 0.534560 0.285754 [10,] 0.534560 1.000000 0.534560 [11,] 0.285754 0.534560 1.000000 MuMIn::QIC(mod_gee_ar, mod_gee_ar2, typeR = TRUE) # NOT the default QIC mod_gee_ar -6900 mod_gee_ar2 -6897 texreg::knitreg(list(extract_gee_exp(mod_gee_ar), extract_gee_exp(mod_gee_ar2)), single.row = TRUE, ci.test = 1) &lt;table class=&quot;texreg&quot; style=&quot;margin: 10px auto;border-collapse: collapse;border-spacing: 0px;caption-side: bottom;color: #000000;border-top: 2px solid #000000;&quot;&gt; &lt;caption&gt;Statistical models&lt;/caption&gt; &lt;thead&gt; &lt;tr&gt; &lt;th style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;&amp;nbsp;&lt;/th&gt; &lt;th style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;Model 1&lt;/th&gt; &lt;th style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;Model 2&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr style=&quot;border-top: 1px solid #000000;&quot;&gt; &lt;td style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;(Intercept)&lt;/td&gt; &lt;td style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;5.27 [4.14; 6.69]&lt;sup&gt;&amp;#42;&lt;/sup&gt;&lt;/td&gt; &lt;td style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;5.60 [4.25; 7.38]&lt;sup&gt;&amp;#42;&lt;/sup&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;group2mg&lt;/td&gt; &lt;td style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;1.43 [1.09; 1.87]&lt;sup&gt;&amp;#42;&lt;/sup&gt;&lt;/td&gt; &lt;td style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;1.29 [0.89; 1.88]&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;time&lt;/td&gt; &lt;td style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;1.02 [0.96; 1.09]&lt;/td&gt; &lt;td style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;1.01 [0.91; 1.12]&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;time^2&lt;/td&gt; &lt;td style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;0.99 [0.99; 1.00]&lt;sup&gt;&amp;#42;&lt;/sup&gt;&lt;/td&gt; &lt;td style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;0.99 [0.98; 1.00]&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;group2mg:time&lt;/td&gt; &lt;td style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;&amp;nbsp;&lt;/td&gt; &lt;td style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;1.02 [0.90; 1.17]&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;group2mg:time^2&lt;/td&gt; &lt;td style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;&amp;nbsp;&lt;/td&gt; &lt;td style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;1.00 [0.99; 1.01]&lt;/td&gt; &lt;/tr&gt; &lt;tr style=&quot;border-top: 1px solid #000000;&quot;&gt; &lt;td style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;Dispersion&lt;/td&gt; &lt;td style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;4.01&lt;/td&gt; &lt;td style=&quot;padding-left: 5px;padding-right: 5px;&quot;&gt;4.01&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;tfoot&gt; &lt;tr&gt; &lt;td style=&quot;font-size: 0.8em;&quot; colspan=&quot;3&quot;&gt;&lt;sup&gt;&amp;#42;&lt;/sup&gt; Null hypothesis value outside the confidence interval.&lt;/td&gt; &lt;/tr&gt; &lt;/tfoot&gt; &lt;/table&gt; 21.4.4 Visualize Best Model 21.4.4.1 Model Parameter Table texreg::knitreg(extract_gee_exp(mod_gee_ar), caption = &quot;GEE: Final Model (auto regressive)&quot;, single.row = TRUE, ci.test = 1) GEE: Final Model (auto regressive)   Model 1 (Intercept) 5.27 [4.14; 6.69]* group2mg 1.43 [1.09; 1.87]* time 1.02 [0.96; 1.09] time^2 0.99 [0.99; 1.00]* Dispersion 4.01 * 1 outside the confidence interval. 21.4.4.2 Refit via geepack::geeglm() mod_geeglm_ar &lt;- geepack::geeglm(count ~ group + time + I(time^2), id = id, data = data_long, family = poisson(link = &quot;log&quot;), corstr = &quot;ar1&quot;) summary(mod_geeglm_ar) Call: geepack::geeglm(formula = count ~ group + time + I(time^2), family = poisson(link = &quot;log&quot;), data = data_long, id = id, corstr = &quot;ar1&quot;) Coefficients: Estimate Std.err Wald Pr(&gt;|W|) (Intercept) 1.63046 0.12861 160.71 &lt;2e-16 *** group2mg 0.34575 0.13880 6.21 0.013 * time 0.03588 0.03558 1.02 0.313 I(time^2) -0.00665 0.00321 4.31 0.038 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Correlation structure = ar1 Estimated Scale Parameters: Estimate Std.err (Intercept) 4 0.435 Link = identity Estimated Correlation Parameters: Estimate Std.err alpha 0.775 0.0333 Number of clusters: 65 Maximum cluster size: 11 interactions::interact_plot(model = mod_geeglm_ar, pred = time, modx = group) 21.4.4.3 Predict over a Grid Estimated Marginal Mean Counts expand.grid(time = 0:10, group = levels(data_long$group)) %&gt;% dplyr::mutate(fit = predict(mod_geeglm_ar, newdata = ., type = &quot;response&quot;)) %&gt;% tidyr::spread(key = group, value = fit) time 1mg 2mg 1 0 5.11 7.22 2 1 5.26 7.43 3 2 5.34 7.55 4 3 5.36 7.57 5 4 5.30 7.49 6 5 5.17 7.31 7 6 4.98 7.04 8 7 4.74 6.69 9 8 4.44 6.28 10 9 4.11 5.81 11 10 3.76 5.31 21.4.4.4 Plot Estimated Marginal Means expand.grid(time = 0:10, group = levels(data_long$group)) %&gt;% dplyr::mutate(fit = predict(mod_geeglm_ar, newdata = ., type = &quot;response&quot;)) %&gt;% ggplot(aes(x = time, y = fit, color = fct_rev(group))) + geom_point() + geom_line() + theme_bw() + labs(x = &quot;Four-week Intervals&quot;, y = &quot;Estimated Populaton Mean Count&quot;, color = &quot;Dose&quot;) + scale_x_continuous(breaks = 0:10) 21.5 GLMM Analysis 21.5.1 RI: Random Intercepts Only mod_glmer_ri &lt;- lme4::glmer(count ~ group + I(time/4) + I((time/4)^2) + (1|id), data = data_long, family = poisson(link = &quot;log&quot;)) summary(mod_glmer_ri) Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [glmerMod] Family: poisson ( log ) Formula: count ~ group + I(time/4) + I((time/4)^2) + (1 | id) Data: data_long AIC BIC logLik deviance df.resid 4061 4083 -2025 4051 710 Scaled residuals: Min 1Q Median 3Q Max -3.109 -1.092 -0.150 0.814 5.628 Random effects: Groups Name Variance Std.Dev. id (Intercept) 0.283 0.532 Number of obs: 715, groups: id, 65 Fixed effects: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 1.6099 0.1062 15.15 &lt;2e-16 *** group2mg 0.3110 0.1371 2.27 0.023 * I(time/4) -0.0089 0.0706 -0.13 0.900 I((time/4)^2) -0.0667 0.0281 -2.37 0.018 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Correlation of Fixed Effects: (Intr) grp2mg I(t/4) group2mg -0.700 I(time/4) -0.276 0.000 I((tm/4)^2) 0.226 0.000 -0.960 21.5.2 RIAS: Random Intercepts and Slopes mod_glmer_rias &lt;- lme4::glmer(count ~ group + I(time/4) + I((time/4)^2) + (I(time/4)|id), data = data_long, family = poisson(link = &quot;log&quot;)) summary(mod_glmer_rias) Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [glmerMod] Family: poisson ( log ) Formula: count ~ group + I(time/4) + I((time/4)^2) + (I(time/4) | id) Data: data_long AIC BIC logLik deviance df.resid 3964 3996 -1975 3950 708 Scaled residuals: Min 1Q Median 3Q Max -3.045 -0.927 -0.142 0.682 4.886 Random effects: Groups Name Variance Std.Dev. Corr id (Intercept) 0.3147 0.561 I(time/4) 0.0825 0.287 -0.36 Number of obs: 715, groups: id, 65 Fixed effects: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 1.5993 0.1093 14.63 &lt;2e-16 *** group2mg 0.3002 0.1360 2.21 0.0274 * I(time/4) 0.0452 0.0807 0.56 0.5751 I((time/4)^2) -0.1035 0.0286 -3.62 0.0003 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Correlation of Fixed Effects: (Intr) grp2mg I(t/4) group2mg -0.679 I(time/4) -0.363 0.005 I((tm/4)^2) 0.224 0.001 -0.848 anova(mod_glmer_ri, mod_glmer_rias) Data: data_long Models: mod_glmer_ri: count ~ group + I(time/4) + I((time/4)^2) + (1 | id) mod_glmer_rias: count ~ group + I(time/4) + I((time/4)^2) + (I(time/4) | id) npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) mod_glmer_ri 5 4061 4083 -2025 4051 mod_glmer_rias 7 3964 3996 -1975 3950 100 2 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 texreg::knitreg(list(extract_glmer_exp(mod_glmer_ri), extract_glmer_exp(mod_glmer_rias)), custom.model.names = c(&quot;Intecepts&quot;, &quot;Intercepts and Slopes&quot;), caption = &quot;GLMM: Compare Random Effects&quot;, single.row = TRUE, ci.test = 1) GLMM: Compare Random Effects   Intecepts Intercepts and Slopes (Intercept) 5.00 [4.05; 6.17]* 4.95 [3.99; 6.15]* group2mg 1.36 [1.04; 1.79]* 1.35 [1.03; 1.77]* time/4 0.99 [0.86; 1.14] 1.05 [0.89; 1.23] (time/4)^2 0.94 [0.88; 0.99]* 0.90 [0.85; 0.95]* * Null hypothesis value outside the confidence interval. 21.5.3 RAIS: Add Interaction See the GLMM - Optimizers page for more information on convergence problems. mod_glmer_rias2 &lt;- lme4::glmer(count ~ group*I(time/4) + group*I((time/4)^2) + ( I(time/4)|id ), data = data_long, control = glmerControl(optimizer =&quot;Nelder_Mead&quot;), # convergence issues resolved family = poisson(link = &quot;log&quot;)) anova(mod_glmer_rias, mod_glmer_rias2) Data: data_long Models: mod_glmer_rias: count ~ group + I(time/4) + I((time/4)^2) + (I(time/4) | id) mod_glmer_rias2: count ~ group * I(time/4) + group * I((time/4)^2) + (I(time/4) | id) npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) mod_glmer_rias 7 3964 3996 -1975 3950 mod_glmer_rias2 9 3968 4009 -1975 3950 0.14 2 0.93 21.5.4 Visualize Best Model 21.5.4.1 Model Parameter Table texreg::knitreg(list(extract_glmer_exp(mod_glmer_rias)), caption = &quot;GLMM: Final Model&quot;, single.row = TRUE, ci.test = 1) GLMM: Final Model   Model 1 (Intercept) 4.95 [3.99; 6.15]* group2mg 1.35 [1.03; 1.77]* time/4 1.05 [0.89; 1.23] (time/4)^2 0.90 [0.85; 0.95]* * 1 outside the confidence interval. interactions::interact_plot(model = mod_glmer_rias, pred = time, modx = group) 21.5.4.2 Estimated Marginal Means effects::Effect(focal.predictors = c(&quot;group&quot;, &quot;time&quot;), xlevels = list(time = 0:10), mod = mod_glmer_rias) %&gt;% data.frame %&gt;% dplyr::select(group, time, fit) %&gt;% tidyr::spread(key = group, value = fit, sep = &quot;_&quot;) time group_1mg group_2mg 1 0 4.95 6.68 2 1 4.97 6.71 3 2 4.93 6.66 4 3 4.83 6.52 5 4 4.67 6.30 6 5 4.46 6.02 7 6 4.20 5.67 8 7 3.90 5.27 9 8 3.58 4.84 10 9 3.25 4.38 11 10 2.90 3.92 21.5.4.3 Plot Estimated Marginal Means effects::Effect(focal.predictors = c(&quot;group&quot;, &quot;time&quot;), xlevels = list(time = seq(from = 0, to = 10, by = .25)), mod = mod_glmer_rias) %&gt;% data.frame %&gt;% ggplot(aes(x = time, y = fit)) + geom_ribbon(aes(ymin = fit - se, ymax = fit + se, fill = group), alpha = .3) + geom_line(aes(color = group)) + theme_bw() + labs(x = &quot;Four-week Intervals&quot;, y = &quot;Estimated Populaton Mean Count&quot;, color = &quot;Dose&quot;, fill = &quot;Dose&quot;) + scale_x_continuous(breaks = 0:10) data_long %&gt;% dplyr::mutate(fit = predict(mod_glmer_rias, newdata = ., type = &quot;response&quot;)) %&gt;% ggplot(aes(x = time, y = fit)) + geom_line(aes(group = id)) + facet_grid(.~ group) + theme_bw() "],["sample-size-and-power.html", "22 Sample Size and Power 22.1 Key Publications 22.2 R packages 22.3 Online Interactive Interfaces 22.4 Stand-alone Computer Programs", " 22 Sample Size and Power 22.1 Key Publications Start here, its short: Snijders TAB (2005). Power and Sample Size in Multilevel Linear Models. In: Everitt BS, Howell DC (Hrsg.). Encyclopedia of Statistics in Behavioral Science. Chichester, UK: John Wiley and Sons, Ltd. doi: 10.1002/0470013192.bsa492 This paper includes some nice power curves for reference: Scherbaum, C. A., &amp; Ferreter, J. M. (2009). Estimating statistical power and required sample sizes for organizational research using multilevel modeling. Organizational Research Methods, 12(2), 347-367. This paper tabulated the effect of number of clusters, size of cluser, and ICC: Maas, C. J., &amp; Hox, J. J. (2005). Sufficient Sample Sizes for Multilevel Modeling. Methodology, 1(3), 86-92. This paper focuses on binary outcomes in hierarchical or clustered strkucture: Moineddin R, Matheson FI, Glazier RH. (2007). A simulation study of sample size for multilevel logistic regression models. BMC medical research methodology, 7, 34. doi:10.1186/1471-2288-7-34 This paper presents a data simulation method for estimating power for commonly used relationships research designs (via MPlus) and includes two worked examples from relationships research. Lane, S. P., &amp; Hennes, E. P. (2018). Power struggles: Estimating sample size for multilevel relationships research. Journal of Social and Personal Relationships, 35(1), 7-31. This paper is very clean and organized with clear notation, tables, and figures. It investigates the performance of random effect binary outcome multilevel models under varying methods of estimation, level-1 and level-2 sample size, outcome prevalence, variance component sizes, and number of predictors using SAS software Schoeneberger, J. A. (2016). The impact of sample size and other factors when estimating multilevel logistic models. The Journal of Experimental Education, 84(2), 373-397. This paper’s focus is three level models: Kerkhoff, D., &amp; Nussbeck, F. W. (2019). The influence of sample size on parameter estimates in three-level random-effects models. Frontiers in psychology, 10. 22.2 R packages 22.2.1 powerlmm powerlmm package described in: Raudenbush, S. W., and L. Xiao-Feng (2001). “Effects of Study Duration, Frequency of Observation, and Sample Size on Power in Studies of Group Differences in Polynomial Change.” Psychological Methods 6 (4): 387–401. Kristoffer Magnusson has posted an examle walk-through called Power Analysis for Two-level Longitudinal Models with Missing Data You can also access an interactive shiny interfaces with the following code (once you install the package in R): library(powerlmm) shiny_powerlmm() 22.2.2 simr simr package computed power analysis for generalised linear mixed models (GLMMs) by Monte Carlo simulation and is designed to work with models fit using the ‘lme4’ package. It includes tools for: running a power analysis for a given model and design; and calculating power curves to assess trade‐offs between power and sample size The paper below presents a tutorial using a simple example of count data with mixed effects (with structure representative of environmental monitoring data) to guide the user along a gentle learning curve, adding only a few commands or options at a time. Green, P., &amp; MacLeod, C. J. (2016). SIMR: an R package for power analysis of generalized linear mixed models by simulation. Methods in Ecology and Evolution, 7(4), 493-498. 22.2.3 sjstats::smpsize_lmm() Note: this is for ‘standard designs’ and is very simple sjstats::smpsize_lmm() compute an approximated sample size for linear mixed models (two-level-designs), based on power-calculation for standard design and adjusted for design effect for 2-level-designs. Cohen J. (1988). Statistical power analysis for the behavioral sciences (2nd ed.). Hillsdale,NJ: Lawrence Erlbaum. Hsieh FY, Lavori PW, Cohen HJ, Feussner JR (2003). An Overview of Variance Inflation Factors for Sample-Size Calculation. Evaluation and the Health Professions 26: 239-257. Snijders TAB (2005). Power and Sample Size in Multilevel Linear Models. In: Everitt BS, Howell DC (Hrsg.). Encyclopedia of Statistics in Behavioral Science. Chichester, UK: John Wiley and Sons, Ltd. doi: 10.1002/0470013192.bsa492 22.2.4 MLPowSim MLPowSim is a free-download that guides you through questions and then writes R Syntax for you based on your responses. 22.3 Online Interactive Interfaces No, G*Power won’t help you with this. 22.3.1 GLIMMPSE GLIMMPSE 2.0 from the University of Colorado Denver, School of Public Health (NIH) Sample Size Shop Website Slides two examples start on slide 62 22.3.2 WebPower WebPower Statistical Power Analysis Online or http://psychstat.org/crt2arm Miao (“Michelle”) Yang, Mar 2015 22.4 Stand-alone Computer Programs 22.4.1 Optimal Design Note: Works on Windows but not Mac OS. Optimal Design was created by Steve Raudenbush and colleagues. THis program estimates power using the intraclass correlation, effect size, a level, and sample sizes for cluster-randomized, multisite, and repeated measures designs. The user can manipulate one factor at a time to examine the impact on power. All results are presented graphically as power curves, which is helpful for understanding how power could be affected by particular changes in sample sizes, effect sizes, and intraclass correlations. The program is user friendly and comes with extensive documentation. Raudenbush, S.W. (1997). Statistical analysis and optimal design for cluster randomized trials. Psychological Methods, 2: 173–185. Raudenbush, S.W., &amp; Liu, X.-F. (2000). Statistical power and optimal design for multisite randomized trials. Psychological Methods, 5: 199–213. Raudenbush, S.W., &amp; Liu, Xiao-Feng. (2001). Effects of study duration, frequency of observation, and sample size on power in studies of group differences in polynomial change. Psychological Methods, 6: 387–401. 22.4.2 PinT PinT: Power in Two-levels was created by Tom Snijders, Roel Bosker, and Henk Guldemond. This is the oldest program, but it can be used to estimate the standard errors of simple fixed effects and cross-level interactions. It can provide standard error estimates for a variety of complex models. The major difficulty in using this program is that it requires the user to input the means, variances, and covariances for all explanatory variables and the variance and covariance for the random effects. The major advantage is that an extensive user manual is available and the formulas used by the program are presented in Snijders and Bosker (1993). This program is recommended for models that include several Level 1 or Level 2 variables. Snijders, T.A.B. &amp; Bosker, R.J. (1993). Standard errors and sample sizes for two-level research. Journal of Educational Statistics, 18: 237–259. 22.4.3 RMASS2 RMASS2 calculates the sample size for a two-group repeated measures design, allowing for attrition, according to: Hedeker, D., Gibbons, R.D., &amp; Waternaux, C. (1999). Sample size estimation for longitudinal designs with attrition: comparing time-related contrasts between two groups. Journal of Educational and Behavioral Statistics, 24:70–93. 22.4.4 ACluster ACluster calculates required sample sizes for various types of cluster randomized designs, not only for continuous but also for binary and time-to-event outcomes, as described in: Donner, A., &amp; Klar, N. (2000). Design and Analysis of Cluster Randomization Trials in Health Research. London: Arnold. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
